# **Convergent Coherence: The Architecture of Failure**

**Abstract**

Coherentist theories face the isolation objection: internally consistent systems may detach from reality. This paper develops Emergent Pragmatic Coherentism, a form of **naturalized proceduralism**—systemic externalism grounded in empirical pragmatic selection—resolving this by grounding coherence in long-term viability. We introduce *systemic brittleness*—observable costs from applying principles—as a health diagnostic. Mind-independent constraints create selective pressure filtering brittle systems. This discovers an objective **Apex Network** of viable principles, emerging necessarily from reality's topology. Justification demands internal coherence and resilience against consequences. This offers naturalistic objectivity, truth aligning with this emergent structure.

### Glossary
- **Apex Network**: Emergent structure of maximal viability.
- **Brittleness**: Accumulated systemic costs.
- **Consensus Network**: Fallible, collective knowledge system.
- **Constrained Interpretation**: Methodology assessing brittleness via physical-biological anchors, comparative-diachronic analysis, and convergent evidence for pragmatic objectivity.
- **Emergent Pragmatic Coherentism**: Framework grounding coherence in demonstrated viability.
- **Modal Necessity (of Apex Network)**: Necessary structure from pragmatic constraints, counterfactually stable across histories.
- **Negative Canon**: Catalogue of empirically invalidated principles generating catastrophic costs.
- **Pragmatic Objectivity**: Objectivity for comparative assessment through convergent evidence, determined by mind-independent constraints.
- **Standing Predicate**: Reusable predicate for cost-reduction.
- **Systemic Externalism**: Externalist epistemology locating justification reliability in demonstrated viability of public systems.

### **1. Introduction: From a Static Web to a Dynamic Process**

#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**

This paper offers an externalist, pragmatic response. Knowledge system justification is disciplined by long-term viability, assessed via real-world costs from applying principles. This preserves coherentism's holism while adding external checks. Justification demands two elements: internal coherence within a shared network, and that network's demonstrated reliability through low systemic brittleness.

The framework's key claim is not just historical. Pragmatic filtering discovers, not creates, an objective structure emerging necessarily from reality's constraint topology. Constraints—physical laws, biological limits, logic, social necessities—form a fitness landscape of possible configurations. The peaks on this landscape—the low-cost, sustainable solutions—are objective features of reality's terrain. These optimal structures, the **Apex Network**, exist independently, like a physical system's lowest energy state.

This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.

#### **1.3 Roadmap of the Argument**

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To prevent misunderstanding, this is not a foundationalist epistemology nor a general theory of justification, but a specialized framework for cumulative knowledge systems where pragmatic consequences provide feedback. The argument proceeds as a systematic construction. Section 2 introduces the diagnostic framework for assessing systemic health through the concept of brittleness and defines our core analytical units. Section 3 develops the logic of viability that drives this evolutionary process, grounding epistemic norms in pragmatic constraints. Section 4 presents the Apex Network as an emergent necessary structure, articulating the modal claims that ground this account of naturalistic objectivity. Section 5 situates the framework within contemporary epistemology, showing how it refines or corrects related research programs. Finally, Section 6 defends the framework against key objections while acknowledging its principled limitations.

### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**

Following naturalized epistemology (Goldman 1979; Kitcher 1993), we focus on public structures rather than private states. This shift enables tractable analysis of observable phenomena in systems beyond individual cognition. The process deflates from private beliefs to public tools.

The process starts with *belief*, a private state inaccessible to public knowledge theories. First, isolate testable content as a *proposition*: a falsifiable, linguistic claim for collective assessment. It must pass coherence testing—not mere logical consistency, but thick pragmatic risk analysis. A resource-constrained network asks: will this proposition raise or lower long-term brittleness?

Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**

A shared network is active, pressured by *pragmatic pushback*—the systemic version of Quine's recalcitrant experience. It sums concrete consequences from applying principles: collapsing bridges, failed treatments, fragmented societies. This material feedback, not argument, creates two cost types.

**First-Order Costs** are direct consequences: failed predictions, wasted resources, instability (e.g., excess mortality)—signals of environmental misalignment. **Systemic Costs** are internal expenses managing first-order costs, revealing fragility. Key forms:

*   **Conceptual Debt:** Fragility from complex patches protecting flawed principles.
*   **Coercive Overheads:** Resources enforcing compliance and handling dissent, a data stream of unacceptable costs.

This diagnostic lens applies even in abstract domains. In theoretical mathematics, for example, pragmatic pushback manifests not as a collapsing bridge but as the accumulation of internal systemic costs, such as escalating proof complexity for marginal explanatory gain or the proliferation of ad-hoc axioms to manage paradoxes.

To operationalize brittleness assessment, we employ a toolkit of indicators that can be measured empirically:

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |
| **P(t)** | Conceptual Debt | Ratio of anomaly-resolution publications to novel-prediction publications |
| **C(t)** | Coercive Overhead | Ratio of security/suppression budget to productive/R&D budget |
| **M(t)** | Model Complexity | Rate of parameter/complexity growth vs. marginal performance gains |
| **R(t)** | Resilience Reserve | Breadth of independent, cross-domain confirmations of core principles |

To illustrate this toolkit in action, consider two brief examples.

**Case 1: Ptolemaic Astronomy (c. 1500 CE).** High rising brittleness. **M(t)** acute: ~80 epicycles, adding 2–3 per decade for diminishing returns. **P(t)** high: most work resolved anomalies, not novel predictions.

**Case 2: Contemporary AI Development.** Early rising brittleness signs. **M(t)**: exponential parameter escalation for marginal gains (Sevilla et al. 2022). **P(t)**: proliferation of alignment/safety research as patches. R(t) declines: isolated from science, limited cross-domain use. Suggests brittleness like late Ptolemaic astronomy.

While our framework focuses on epistemic brittleness in descriptive knowledge systems, the concept could be extended to normative domains as "normative brittleness"—a measure of misalignment with the constraints on stable human cooperation. This extension, while promising, introduces controversial metaethical commitments and is reserved for future research.



#### **2.3 Preempting the Circularity Objection: A Methodology of Constrained Interpretation**

The operationalization of this framework faces a significant objection: measuring systemic costs objectively—distinguishing "waste" from "investment," or "excess" mortality from a baseline—appears to require the very normative standards our theory aims to provide, creating a vicious circularity.

This circularity cannot be eliminated entirely, as all empirical assessment is theory-laden to some degree. However, it can be managed through a disciplined methodology we term **constrained interpretation**. This approach does not aim for an impossible, view-from-nowhere neutrality but for *pragmatic objectivity*: a level of objectivity sufficient for comparative assessment and institutional evaluation, achieved through a protocol that disciplines interpretive judgment. This protocol relies on three principles:

1.  **Physical-Biological Anchors:** Assessments are anchored in outcomes that register as failures across widely divergent theoretical frameworks. Demographic collapse, catastrophic infrastructure failure, or sustained mass mortality serve as relatively theory-neutral indicators of systemic breakdown. While the *interpretation* of these failures is contestable, their *status as failures* generally is not.

2.  **Comparative-Diachronic Judgments:** The methodology avoids absolute claims about a system's health. Instead, its power lies in relative and temporal comparisons. The key diagnostic questions are comparative: Is System A *more* brittle than its contemporary, System B, when facing similar pressures? Or they are diachronic: Is System A's brittleness *rising* over time? These judgments can be made robustly even when absolute standards are unavailable.

3.  **Convergent Evidence:** A robust diagnosis of brittleness requires agreement across multiple, independent indicators. For example, a research program is plausibly degenerating only if it exhibits rising conceptual debt (e.g., an accelerating rate of ad-hoc modifications), increasing model complexity for diminishing returns, *and* a growing need for institutional resources to suppress rival paradigms. Systematic convergence across these measures becomes increasingly difficult to dismiss as mere interpretive bias.

This methodology does not eliminate judgment, but it makes that judgment systematic, transparent, and accountable to multiple streams of evidence. It provides structured tools for a fallibilistic research program, not a mechanical algorithm for truth.

##### **2.3.1 How the Causal Hierarchy Addresses the Circularity Objection**

Critics object that classifying spending as "productive" vs. "coercive" requires prior normative commitments, making the framework circular. The causal hierarchy provides an operational solution through trajectory analysis rather than categorical definition.

**The Operational Protocol:**

**Step 1: Measurement Without Classification**
Track resource allocation over time without labeling it:
- Proportion to internal security/surveillance/enforcement (S)
- Proportion to infrastructure/health/education/R&D (P)
- Total resource base (R)

**Step 2: Correlate With First-Order Indicators**
Measure demographic and economic trajectories:
- Mortality rates (rising/stable/falling)
- Morbidity indicators
- Economic output per capita
- Innovation metrics (patents, new technologies, productivity gains)
- Population stability

**Step 3: Apply Diagnostic Rules**

A spending category functions as coercive overhead when increasing allocation correlates with rising First-Order Costs, the system requires accelerating investment to maintain baseline stability (diminishing returns), or reduction correlates with improved outcomes.

A spending category functions as productive investment when increasing allocation correlates with falling First-Order Costs, returns are constant or increasing, and it generates positive spillovers to other domains.

**Concrete Example: Criminal Justice Spending**

Society A doubles police budget (year 1: 2% GDP → year 10: 4% GDP):
- Crime rates: -40%
- Incarceration rate: -20%
- Homicide rate: -60%
- Community survey trust: +35%
- Recidivism: -25%
**Diagnosis:** Productive investment. Rising S correlates with falling First-Order Costs.

Society B doubles police budget (year 1: 2% GDP → year 10: 4% GDP):
- Crime rates: +5%
- Incarceration rate: +300%
- Homicide rate: -10%
- Community survey trust: -50%
- Social instability indicators: +60%
- By year 10, requires 6% GDP to maintain control
**Diagnosis:** Coercive overhead. Rising S correlates with rising total systemic costs despite some metrics improving.

**Why This Isn't Circular:**

The classification emerges from empirical trajectories, not a priori theory. We don't ask "what is policing's essential nature?" We ask "what measurable effects does this spending pattern have on systemic costs over time?"

The robustness comes from convergent evidence. A single metric (e.g., crime rate) can be ambiguous, but when demographic indicators, economic output, innovation rates, stability metrics, and coercive spending ratios all move in the same direction, the diagnosis becomes robust to interpretive variation.

##### **2.3.2 Triangulation Across Independent Baselines**

To further address the circularity, we employ triangulation across independent baselines for assessing systemic costs, particularly in domains where direct measurement is challenging.

**1. Comparative-Historical Baseline:** Compare outcomes across contemporaneous societies with similar technology, resources, and environmental constraints. A society with 50% child mortality when peers achieve 30% exhibits measurable excess relative to its era's pragmatic constraints.

**2. Trajectory Analysis:** Track whether key indicators are rising (degrading), stable (maintaining), or falling (improving). When System A shows rising mortality while System B shows falling mortality under comparable conditions, we have empirical grounds for diagnosis without absolute baselines.

**3. Demographic Viability Thresholds:** Some thresholds are biologically determinate: Total Fertility Rate < 2.1 indicates population decline; infant mortality > 30% signals demographic stress; life expectancy < 30 indicates crisis. These are structural constraints, not normative judgments.

**Triangulation Methodology:** Diagnose "excess" through convergent evidence across baselines. Systematic convergence across independent indicators provides robust diagnosis, as in empirical science where multiple experimental paradigms confirm a theory.

### **3. The Logic of Viability: A Naturalistic Engine for Inquiry**

The diagnostic framework detailed in Section 2 is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems. This section explains the logic of that selective process, showing how it grounds epistemic norms in pragmatic necessity and transforms the abstract notion of coherence into a concrete tool for risk assessment.

#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.

Second, an **instrumental argument**: the framework makes a falsifiable, empirical claim that networks with a high and rising degree of measured brittleness are statistically more likely to collapse or require radical revision. From this descriptive claim follows a conditional recommendation: *if* an agent or institution has the goal of ensuring its long-term stability and problem-solving capacity, *then* it has a powerful, evidence-based reason to adopt principles that demonstrably lower its systemic brittleness.

This grounding goes deeper than mere instrumentalism. The end—viable inquiry—is not an arbitrary preference but a structural precondition for any system to participate in cumulative knowledge production. The means—low-brittleness principles—are themselves recursively constrained, as they must also demonstrate their own viability. This prevents purely expedient solutions and ensures that the resulting epistemic norms are responsive to objective pragmatic constraints. When the model describes one network as "better" or identifies "epistemic progress," these are not subjective value judgments but technical descriptions of systemic performance. A "better" network is one with lower measured brittleness and thus a higher predicted resilience against failure.

#### **3.2 Coherence as Forward-Looking Risk Assessment**

Under viability logic, coherence is thick, forward-looking cost-benefit analysis. Resource-constrained systems use heuristics to assess if a proposition raises or lowers brittleness. Traditional virtues are practical calculus principles:

*   **Logical Consistency:** A hedge against the infinite future costs of inferential paralysis.
*   **Explanatory Power:** A measure of a proposition’s potential return on investment, reducing future inquiry costs by paying down conceptual debt.
*   **Simplicity:** A direct measure of systemic overhead; overly complex propositions increase long-term maintenance costs.
*   **Evidential Support:** An assessment of integrative risk; a well-supported claim is a low-risk investment, unlikely to trigger a cascade of costly future revisions.

One might object that this account reduces scientific judgment to purely pragmatic considerations, ignoring theoretical virtues like explanatory depth or mathematical elegance. However, these virtues are themselves pragmatically valuable within our framework. Explanatory depth reduces future conceptual debt by unifying disparate phenomena under a single principle, while mathematical elegance often signals a structural efficiency that minimizes long-term maintenance costs. Our framework does not eliminate traditional theoretical virtues but rather explains their pragmatic function within the evolutionary process of knowledge development.

This forward-looking model of coherence also explains how revolutionary science is possible. When a dominant network begins to exhibit high and rising systemic brittleness—a state that corresponds to what Thomas Kuhn (1962) described as a "crisis"—the cost-benefit analysis for new propositions shifts dramatically. A radical new hypothesis, while having low coherence with the *specific principles* of the existing network, may promise a massive long-term reduction in the *systemic costs* that are crippling the incumbent paradigm. The new proposition is not accepted because it fits neatly with the old, failing parts, but because it offers a viable path to restoring low-brittleness for the system as a whole. Coherence, in this dynamic sense, is not a conservative check for conformity, but a pragmatic assessment of a proposition's potential to resolve systemic crisis.

### **4. The Emergent Architecture of Objectivity**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of our knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We argue that the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

#### **4.1 A Negative Methodology: Charting the Landscape of Failure**

Our objectivity account starts with empirical evidence of failure, not speculative truth. Popperian insight: secure knowledge is of the unworkable. Systemic collapse—crippling inefficiency, stagnation, decay—yields clear data.

The systematic analysis of these failures allows us to build what we term the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been empirically invalidated by the catastrophic systemic costs they reliably generate. This canon includes systems exhibiting *epistemic brittleness*, such as phlogiston chemistry or Lysenkoist biology, whose core principles generated catastrophic causal failures. It also includes systems exhibiting *normative brittleness*, such as chattel slavery, whose unsustainability was demonstrated by the immense and ever-rising coercive overheads required to manage the normative pushback they generated.

By charting what demonstrably fails, we are not merely learning what to avoid; we are effectively reverse-engineering the constraints of a real territory. The Negative Canon functions like a reef chart for inquiry. It is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and prevents a collapse into relativism. Crucially, the Canon reveals necessary features of the constraint landscape, not just historical accidents. When a structural principle, such as organizing labor through chattel ownership, generates catastrophic brittleness across maximally different societies, this reveals a deep fact about the constraints on viable human cooperation. The Canon thus provides prospective guidance by mapping the structural features of failure.

#### **4.2 The Apex Network: An Emergent Structure of Modal Necessity**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the **Apex Network**. The isolation objection to coherentism assumes that a belief system could float free of reality. Our framework denies this possibility through a claim about emergent necessity: pragmatic constraints form a topology that necessarily generates optimal structures, which any viable system must approximate.

##### 4.2.1 The Modal Status of the Apex Network

The modal necessity of the Apex Network can be defended through a structured argument:

**Premise 1: Reality Imposes Non-Negotiable Constraints.** These are mind-independent, including physical laws (thermodynamics), biological facts (nutritional needs), logical requirements (non-contradiction), and social coordination necessities (collective action solutions).

**Premise 2: Constraints Generate a Solution Space (Fitness Landscape).** This landscape is not flat; its topology of peaks (viable, low-cost configurations) and valleys (brittleness traps) is an objective feature of reality's structure, emerging necessarily from how constraints interact.

**Premise 3: Optimal Solutions Emerge Necessarily.** Given a constrained system, optimal solutions emerge from the topology itself, existing independently of our knowledge. Like a molecule's lowest-energy state emerging necessarily from quantum constraints, these are not invented by inquiry but are factual consequences of how the system is organized.

**Conclusion: The Apex Network IS that Optimal Structure.** It is the complete configuration space of maximally viable solutions determined by the constraint topology. Historical filtering is the **discovery process** that reveals this emergent structure; it is not the creation mechanism. Its structure is therefore counterfactually stable: any sufficiently comprehensive exploration of the constraint landscape, across any possible history, would be forced to converge upon it.

#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
*   **Level 1: Objective Truth.** The ultimate, regulative ideal of the process. A proposition is objectively true if its principles are part of the real, emergent **Apex Network**. While this structure is never fully mapped, it functions as the formal standard that makes our comparative judgments of "more" or "less" brittle meaningful. It is the structure toward which the reduction of systemic costs forces our knowledge systems to converge.

This layered framework avoids a simplistic "Whig history." Newtonian mechanics earned its Level 2 status by being a maximally low-brittleness system for its problem-space for over two centuries. Its replacement by relativity does not retroactively invalidate that status; it shows the evolutionary process at work, where an expanding problem-space revealed pragmatic constraints that required a new, more viable system. A claim can thus be justifiably true in its time (Level 2) yet still be objectively false (not Level 1) when judged against the Apex Network from the perspective of a more resilient successor. To add epistemic humility, a system might achieve Level 2 status simply because it has not yet been tested against a sufficiently broad problem-space; even the highest practical justification remains provisional.

This framework also predicts convergence in proportion to pragmatic constraint tightness. In domains with tight constraints (the "Convergent Core," e.g., thermodynamics, germ theory), pragmatic pressures lead to a single, stable consensus—the rare, sharp peaks on the fitness landscape. In looser domains (the "Pluralist Frontier," e.g., quantum interpretations, consciousness models), multiple viable theories can coexist, like a high-altitude plateau where slightly different configurations achieve similar viability, reflecting legitimate pluralism rather than error.

#### **4.4 How Propositions Become Truth Itself**

Propositions earn Level 2 or Level 1 status through a journey from hypothesis to hard core. A proposition starts as a tentative claim, tested for coherence and cost-reduction. Success promotes it to Standing Predicate, then to core principle if it unifies domains and minimizes brittleness. Pragmatic filtering acts most directly on practices and Standing Predicates guiding them. Individual propositions derive justification from their role within viable practices. A proposition like 'electron mass is 9.109×10⁻³¹ kg' achieves Level 2 Justified Truth as certified output of low-brittleness physics enterprise, with its interlocking methods. Its warrant is inherited from systemic health of generative practices. This process grounds deflationary truth theories: truth is not correspondence but alignment with emergent viable structures, providing substantive grounding for linguistic deflationism.

### **5. Animating the Web of Belief**

Quine’s "Web of Belief" provided a powerful, static model of confirmational holism, but it has been criticized for lacking a corresponding dynamic account of its formation and change. This section provides that dynamic physiology. It details the naturalistic process by which a successful discovery migrates from the tentative "periphery" of the web to its load-bearing "core," a process driven by the pragmatic pressures outlined in our framework.

A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, unifying disparate domains and dramatically reducing the conceptual debt of nineteenth-century physics. As its revision became increasingly costly, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is now a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This process is driven by a powerful, naturalistic pressure. As Herbert Simon (1972) argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources. The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem.

This pragmatic physiology provides the two mechanisms needed to animate Quine’s static web. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness.

### **6. Situating the Framework: A Systemic, Pragmatic Externalism**

The framework developed in this paper can be termed **Systemic Externalism**—a form of externalist epistemology that locates the reliability condition for justification not in an individual's cognitive processes, but in the demonstrated, historical viability of public knowledge systems. This section clarifies this position by examining its relationship to several major philosophical research programs.

#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**

While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**

Naturalistic epistemologies often treat mathematics as a difficult boundary case. Our framework, however, treats it as a paradigm demonstration of its core mechanisms, where pragmatic selection operates on a purely internal landscape. Mathematical frameworks face pragmatic pushback not through external falsification but through rising *internal inefficiency*.

The discovery of Russell's paradox in naive set theory, for instance, revealed a state of infinite brittleness, as it paralyzed all inference within the system. The subsequent development of Zermelo-Fraenkel set theory and Type Theory were competing attempts to create a new, low-brittleness foundation, each making different trade-offs between conceptual complexity and restrictive power. The mathematical community's eventual convergence on ZF set theory for most purposes reflects a collective, pragmatic assessment of which system offered greater long-term viability. Similarly, a mathematical research program that requires proofs of escalating complexity for diminishing explanatory returns is exhibiting the classic signs of a degenerating, high-brittleness system.

This shows the universality of the framework. All domains of inquiry—physical, social, and mathematical—face pragmatic selection. The feedback mechanism varies, from external prediction failures to internal incoherence, but the underlying filter is the same: systems that accumulate unsustainable brittleness are eventually replaced by more viable alternatives.

### **7. Objections, Limitations, and Principled Scope**

A philosophical model must be tested against its most difficult cases and its own conceptual boundaries. This section addresses key challenges to the framework, not as external objections to be deflected, but as core test cases that clarify its explanatory power and define its appropriate scope.

#### **7.1 The Problem of History: Endurance, Power, and Hindsight**

A powerful challenge concerns the interpretation of history. If viability is the standard, how do we account for flawed systems that endure for centuries, and how can we apply this standard to live controversies without the benefit of hindsight?

First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state's productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, exemplifies this state. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability therefore requires a balance between low-cost stability and adaptive capacity.

Finally, the concern about hindsight is best addressed by clarifying the framework's goal: it is not deterministic prediction but epistemic risk management. Retrospective analysis of historical cases is the necessary process of calibrating our diagnostic tools. We study known failures to learn the empirical signatures of rising brittleness before applying these tools to live, unresolved debates. A rising trend in a system's brittleness indicators does not prove its core claims are false, but it provides a strong, evidence-based signal that it is becoming a higher-risk, degenerating research program.

##### **7.1.1 From Retrospective Calibration to Prospective Diagnosis**

This methodological principle can be framed as a two-stage process:

**Stage 1 (Calibration):** We use historical data from the Negative Canon to calibrate diagnostic instruments. Studying collapsed systems reveals empirical signatures (rising P(t), C(t), etc.) that reliably precede failure.

**Stage 2 (Diagnosis):** We apply calibrated instruments to contemporary systems not for deterministic prediction, but to assess epistemic risk and identify progressive or degenerating research programs.

#### **7.2 The Problem of Scope: From Systemic Health to Individual Belief**

It is crucial to be precise about this model's scope. It is a macro-epistemology designed to explain the long-term viability of public knowledge systems. It does not primarily aim to solve traditional problems in micro-epistemology, such as Gettier cases or the justification of an individual's perceptual beliefs. Instead, it provides a robust bridge between these levels through the concept of higher-order evidence. The diagnosed health of a public system provides a powerful defeater (or corroborator) for an individual’s beliefs derived from that system.

The diagnosed brittleness of a knowledge system provides a powerful form of higher-order evidence that should determine an agent's **rational prior probability** when assessing a claim from that source. In a Bayesian framework, a claim from a certified low-brittleness network (e.g., an IPCC report) warrants a high prior. Conversely, a claim from a demonstrably high-brittleness network (a conspiracy theory) warrants an extremely low one. The macro-level diagnosis of systemic health thus provides a rational, non-circular, and empirically grounded basis for an individual's allocation of trust, formalizing the intuition that we should defer to resilient epistemic systems.

Formally, this bridge can be expressed as a Bayesian update: P(claim | source health) = [P(source health | claim) * P(claim)] / P(source health). Where source health is diagnosed via brittleness metrics, providing empirical priors for individual beliefs. This formalization grounds micro-justification in macro-viability, ensuring that individual epistemic practices are responsive to systemic constraints.

#### **7.3 Principled Limitations**

Philosophical honesty requires acknowledging not just what a framework can do, but what it cannot. The following are not flaws to be rebutted but principled limitations that define the theory's appropriate scope.

*   **Species-Specific Objectivity:** The truths discovered through this process are objective for creatures with our biological and social architecture. Hypothetical beings with radically different constraints would discover a different Apex Network. This is relativism at the species level, not the cultural level, and represents a form of appropriate epistemic modesty.
*   **The "Tragic" Nature of Epistemic Progress:** The framework suggests that much of our most secure objective knowledge, particularly in the normative domain, is discovered through the analysis of large-scale systemic failures. The Negative Canon is, in effect, a record of historical suffering. This raises the difficult conclusion that our knowledge of what constitutes a viable social system seems to depend on the data generated by unviable ones.
*   **A Floor, Not a Ceiling:** The framework is most powerful at identifying catastrophic failures and mapping necessary constraints (the "floor" of viability). It is not designed to provide sufficient conditions for human flourishing (the "ceiling"). It leaves substantial space for legitimate value pluralism above the floor of what is demonstrably unworkable.
*   **Expert Dependence:** Accurate brittleness assessment requires technical expertise in domains like historical analysis, statistics, and systems theory. This creates an epistemic inequality similar to that in other specialized scientific fields, which in turn creates challenges for democratic legitimacy that require institutional solutions (such as data transparency and adversarial review).

These limitations do not undermine the framework's contribution. Instead, they define it as a powerful but specialized diagnostic tool for assessing the health of our collective knowledge systems.

#### **7.4 A Falsifiable Research Program**

The framework's claims ground a concrete empirical research program. Its core causal hypothesis is a falsifiable prediction: *networks with high or rising measured brittleness carry a statistically higher probability of collapse or major revision when facing comparable external shocks.*

**Methodology:** This can be tested via comparative historical analysis. Using large-scale databases like the **Seshat Databank**, one could operationalize brittleness proxies (e.g., security vs. R&D budget ratios for C(t); auxiliary hypothesis rates in scientific literature for P(t)). One would then compare outcomes across cohorts of systems with different pre-existing brittleness scores when faced with similar shocks (e.g., climate change, epidemics, technological disruption), statistically controlling for the shock's magnitude.

**Testable Hypothesis:** A regression model should show that pre-shock brittleness indicators are a significant predictor of system collapse or major restructuring within a defined timeframe.

**Falsifiability:** If broad, methodologically sound historical analysis revealed no significant correlation between the indicators of systemic cost and subsequent fragility, the theory's causal engine would be severely undermined. The framework's ultimate test lies in its prospective application: diagnosing rising brittleness in current paradigms yields testable predictions about their statistical likelihood of being superseded when facing novel challenges.

### **8. Conclusion**

This paper has developed Emergent Pragmatic Coherentism as a response to the isolation objection that has long troubled coherentist theories of justification. By grounding coherence in the demonstrated, long-term viability of knowledge systems, our framework provides the external constraint that coherentism requires while preserving its holistic insights. The concept of systemic brittleness offers a naturalistic diagnostic tool for evaluating epistemic systems, while the notion of a constraint-determined Apex Network explains how objective knowledge can arise from our fallible, historical practices.

The result is a form of Systemic Externalism that offers a novel approach for resolving long-standing problems in post-Quinean epistemology. It explains how Quine's web of belief is dynamically revised, grounds a robust but fallibilist realism, and provides the conceptual foundation for a falsifiable research program into the health of our most critical knowledge-generating institutions.

This model is a framework for future inquiry, not a complete system. We started with distinguishing viable knowledge from brittle dogma. The theory posits that the ultimate arbiter is not system elegance or consensus, but its consequences. Operating at high abstraction, its data comes from ground-level experiences: suffering, instability, frustrated goals. Dissent and protest are epistemological data on rising brittleness. Ultimately, the brittleness toolkit isn't just for philosophers; it could and should be used by institutional designers, science funders, and policy analysts to assess the structural health of our most critical knowledge-producing institutions in real time. This positions the paper not just as a solution to an old philosophical puzzle, but as a practical and urgent research program for the 21st century.

By demonstrating that the same logic of viability applies to domains as disparate as public health and pure mathematics, where pragmatic pushback manifests as either external consequence or internal inefficiency, the architecture of failure reveals itself as a truly universal lens for understanding the evolution of human knowledge.

In sum, the architecture of failure reveals the convergent coherence of viable knowledge systems, where objective truth emerges from the pragmatic filtering of brittleness. This framework not only resolves epistemological puzzles but offers a diagnostic lens for navigating the challenges of the 21st century, ensuring that our pursuit of knowledge remains grounded in the realities of human flourishing.