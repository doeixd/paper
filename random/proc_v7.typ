#set text(
  font: "New Computer Modern"
)
#set text(
  weight: "light"
)

#set page(margin: (top: 1.25cm, bottom: 1cm, left: 3.5cm, right: 3.5cm))

#show heading.where(level: 3): set block(above: 2.2em, below: 1.125em)
#show heading.where(level: 3): set text(size: 1.2em)

#show heading.where(level: 2): set block(above: 3em, below: 1.2em)
#show heading.where(level: 2): set text(size: 1.3em)

#show heading.where(level: 1): set block(above: 0.1em, below: 2em)
#show heading.where(level: 1): set text(size: 1.4em)
#show heading.where(level: 1): set par(leading: 0.7em)
= A Procedural and Naturalistic Model of Moral Objectivity
<a-procedural-and-naturalistic-model-of-moral-objectivity>
== Abstract
<abstract>
This paper develops Pragmatic Procedural Realism, a naturalistic theory
of moral objectivity grounded in systems theory and historical analysis.
Extending Emergent Pragmatic Coherentism to metaethics, we treat moral
progress as systemic debugging: identifying and removing normative
principles that generate catastrophic costs. Unlike Kantian
proceduralism's idealized rational procedures, we identify objectivity
with historical filtering through pragmatic constraints. We
operationalize this through measurable brittleness metrics and construct
a Negative Canon of empirically falsified principles, showing that moral
objectivity emerges as a procedural fact about which normative
architectures prove resilient under real-world testing. This objectivity
rests on practical necessity given human constraints, not historical
accident. The Apex Network (the structure of viable norms) exists as a
constraint-determined fact discovered through pragmatic filtering. The
result is a fallibilist realism that naturalizes moral reference while
responding to error theory and quasi-realism, reframing moral inquiry as
an empirical discipline.

== 1. Introduction: From Static Gaps to a Dynamic Filter
<introduction-from-static-gaps-to-a-dynamic-filter>
=== 1.1. A Unified Theory of Justification: Emergent Pragmatic Coherentism (EPC)
<a-unified-theory-of-justification-emergent-pragmatic-coherentism-epc>
Our previous work introduced Emergent Pragmatic Coherentism (EPC) as a
general theory of justification. EPC treats inquiry as epistemic
engineering: building resilient public knowledge structures whose
viability is assessed through their Systemic Brittleness Index (SBI), a
measure of real-world costs from misalignment with pragmatic
constraints. High costs appear as failed predictions, ad-hoc patches,
and accumulating epistemic debt.

This paper extends EPC to metaethics. We argue the is/ought gap results
from static thinking that overlooks a unified cost-based justification
mechanism. In a dynamic view, both factual and normative claims face the
same pragmatic filter. The diagnostic tools for scientific theories can
assess social and ethical systems. This dissolves the is/ought problem
and grounds Pragmatic Procedural Realism, a naturalistic moral
objectivity.

One might object that scientific and normative systems are fundamentally
different kinds of entities, making this extension inappropriate. We
argue the opposite. At the level of systems dynamics, both are
informational architectures designed to solve problems of coordination
(science coordinates our beliefs with the causal world, while ethics
coordinates our actions with each other). Both generate measurable,
real-world costs when their core principles are misaligned with their
respective constraints. This shared functional challenge justifies a
unified diagnostic approach.

==== 1.1.1. EPC Foundations: A Brief Overview
<epc-foundations-a-brief-overview>
For readers unfamiliar with the foundational framework, we provide a
condensed summary of EPC's core machinery. Traditional epistemology
seeks foundations (axioms, basic beliefs, sense data) or coherence
(mutual support within a web of beliefs). EPC offers a third path:
justification through demonstrated resilience under pragmatic pressure.

The central insight is that knowledge claims function as engineering
specifications for cognitive and social systems. Just as bridge designs
are tested by whether bridges stand, knowledge structures are tested by
whether they enable successful coordination with reality. This generates
a unified standard: a system's justification correlates with its
Systemic Brittleness Index, the accumulated costs from misalignment with
operative constraints.

The Systemic Brittleness Index (SBI). In science, a theory with high SBI
exhibits: failed predictions requiring ad-hoc modifications,
accumulating anomalies that resist integration, increasing complexity
without corresponding explanatory gain, and vulnerability to replacement
by simpler alternatives. Consider Ptolemaic astronomy, which required
ever more epicycles to accommodate observational data, signaling rising
brittleness. The theory functioned locally but incurred mounting costs.
Copernican heliocentrism succeeded by dramatically lowering these costs.

The SBI is a composite diagnostic, not a single metric. It integrates
multiple indicators: prediction failures, explanatory gaps, ad-hoc
patches (what we call "patch velocity"), and resistance to integration
with other domains. High brittleness indicates structural problems
requiring debugging, not merely surface errors requiring correction.

Three-Level Framework for Truth. EPC distinguishes three levels of
epistemic status:

#emph[Level 1: Contextual Truth (Coherence within a System).] A claim is
contextually true if it coheres with the network's internal rules. In
Ptolemaic astronomy, "Mars moves on epicycles" was contextually true-it
followed from the system's principles and observational methods. This
level provides procedural correctness but no external justification.

#emph[Level 2: Justified Truth (External Validation via Track Record).]
A claim is justifiedly true if the system containing it demonstrates low
brittleness over time-minimal ad-hoc patching, successful predictions,
integration with other domains, and resilience under challenge.
Copernican heliocentrism earned justified truth through its superior
track record: fewer epicycles, correct predictions of planetary phases,
integration with Newtonian mechanics. This is the highest epistemic
status we can actually achieve.

#emph[Level 3: Objective Truth (Regulative Ideal).] A claim is
objectively true if it belongs to the ideally optimal system-the
complete set of maximally coherent and pragmatically successful
principles. This regulative ideal represents the formal standard against
which Level 2 justification is assessed. We know Newtonian mechanics is
not objectively true (relativity superseded it), yet it was justifiedly
true given Newton's evidence. This distinction preserves fallibilism
while maintaining realist commitments.

The Entrenchment Mechanism. How do principles migrate from peripheral
hypotheses to core commitments? Through a process of pragmatic
entrenchment driven by bounded rationality. A principle begins at the
periphery as a testable hypothesis. As it proves indispensable for
reducing systemic brittleness-enabling predictions, solving anomalies,
integrating domains-revising it becomes increasingly costly. The
principle migrates inward, becoming infrastructure that other claims
depend on. Eventually, it achieves core status through "systemic
caching": the system embeds it so deeply that revision would require
abandoning the conceptual tools needed for coordination itself. Core
principles are not self-evident axioms but highly optimized discoveries
that have survived extensive testing. Their justification is their
proven functional indispensability.

Unified Diagnostic Across Domains. EPC's key innovation is applying this
framework across all domains of inquiry. In science, we measure
brittleness through prediction failures and ad-hoc modifications. In
mathematics, we measure it through proof complexity and the need for
exception-handling. In ethics-the subject of this paper-we measure it
through social costs: coercion required to maintain compliance,
ideological justifications needed to explain failures, and bio-social
costs from violating human needs. The diagnostic toolkit adapts to
domain-specific texture while maintaining a unified underlying logic:
high systemic costs indicate structural misalignment requiring revision.

The is/ought gap appears unbridgeable only if we assume different
justificatory standards for facts and values. EPC shows both are
justified by the same mechanism: demonstrated viability under pragmatic
constraints. Scientific theories must accommodate physical constraints
(experimental results, mathematical consistency). Normative systems must
accommodate pragmatic constraints (human biology, coordination
requirements, cognitive limitations). Different constraints, same
filtering process.

Implications for This Paper. We extend this machinery to metaethics.
Normative principles are Standing Predicates-reusable action-guiding
concepts that function as social infrastructure. Like scientific
theories, they can be elegant in principle but brittle in practice. Our
diagnostic tools measure their brittleness: the Coercion Ratio C(t)
tracks maintenance costs, Patch Velocity P(t) tracks ideological debt,
and bio-social costs track friction with human needs. Principles that
generate high costs across these dimensions are debugged through
historical filtering, just as failed scientific theories are replaced.
The result is Pragmatic Procedural Realism: moral objectivity grounded
in the empirical discovery of constraint-determined optimal solutions.
The Apex Network-the complete set of maximally viable normative
principles-exists as the regulative ideal (Level 3), discovered through
historical testing (Level 2), even as particular societies implement
imperfect approximations (Level 1).

With this foundation established, we can now develop the specific
application to moral progress and normative objectivity.

=== 1.2. Thesis: Moral Progress as Systemic Debugging
<thesis-moral-progress-as-systemic-debugging>
Moral principles function like engineering designs for social worlds.
Like any design, they can be elegant in theory but flawed in practice.
Flawed bridge designs generate stress, cracks, and collapse. Flawed
normative designs (such as those built on slavery) generate social
stress (dissent, rebellion), structural cracks (coercive costs, economic
stagnation), and collapse. We develop diagnostic tools to detect these
structural flaws before catastrophic failure.

Our central thesis: moral progress is a real, observable process of
systemic debugging, not teleological advance. Applying the SBI framework
to history identifies brittle normative predicates (those generating
catastrophic costs) and catalogs them in a Negative Canon of falsified
moral principles. This reveals moral objectivity as an emergent
procedural fact. Moral truths are reverse-engineered from systemic
failures, like mapping reefs from shipwrecks.

The argument proceeds in four stages: (1) operationalizing SBI for
normative analysis with measurable proxies; (2) applying this to model
moral progress as predicate replacement; (3) situating Pragmatic
Procedural Realism as a naturalistic alternative in metaethics; (4)
defending against objections. The result unifies inquiry: pragmatic
system-building discovers objective truth in science and ethics.

=== 1.3. Scope and Limitations
<scope-and-limitations>
This paper does not solve normativity's ultimate grounding or provide a
non-circular defense of valuing survival. Instead, it takes a
conditional, descriptive approach. The Constitutive Condition of
Persistence serves as a procedural filter: normative systems must endure
to be historically analyzable. Persistence is not a smuggled value but
the entry requirement for justification. We do not claim to derive
categorical imperatives from facts, but rather identify the empirical
patterns through which normative claims are tested and refined. Our aim
is not proving we ought to persist, but describing the rules of the game
we play, building a testable model of the evolutionary process through
which values are filtered and earn their authority.

== 2. The Diagnostic Engine: Operationalizing Normative Brittleness
<the-diagnostic-engine-operationalizing-normative-brittleness>
=== 2.1. Units of Selection: Standing Predicates
<units-of-selection-standing-predicates>
EPC provides a unified test for public knowledge systems: claims are
justified by the system's demonstrated viability. Drawing from
evolutionary theory, we distinguish the informational structure (core
normative predicates and their relations) as the replicator-the abstract
code transmitted over time. Social groups and institutions serve as the
interactor-the physical vehicle for testing this code. A system
"survives" by propagating its principles, even if the original group
dissolves (as when Roman law was rediscovered in the Renaissance). This
avoids naive group selectionism by focusing on long-term viability of
the normative code.

This structure consists of Standing Predicates: reusable, action-guiding
concepts that function as cultural "genes." A principle like "slavery is
acceptable" is not just a statement but a predicate enabling actions,
justifications, and social relations. We track these predicates'
viability through historical testing.

Consider the normative predicate `...is a binding promise.` When a
community treats this predicate as 'standing,' it doesn't just classify
an utterance; it automatically licenses a cascade of normative judgments
and social actions: the promiser incurs an obligation, the promisee
gains a legitimate expectation, and third parties are licensed to apply
social sanction (e.g., reputational damage) in case of non-fulfillment.
The viability of this predicate is tested by its long-term success in
reducing the costs of social friction and enabling complex cooperation.

=== 2.2. Tiered Diagnostic Framework
<tiered-diagnostic-framework>
To avoid circularity, we arrange costs hierarchically, from basic
biological facts to complex systemic effects. The SBI is a composite
index; our analysis focuses on three core tiers:

Tier 1: Bio-Social Costs. Direct material consequences of friction with
human persistence conditions, measured by objective proxies like excess
mortality/morbidity rates, chronic malnutrition, and demographic
decline. Systems generating these costs fail fundamentally.

Tier 2: Systemic Friction Costs. Resources expended managing dissent
from Tier 1 costs, measured by the Coercion Ratio (C(t)), which tracks
resources spent on suppression versus production. Rising C(t) indicates
high maintenance costs for flawed designs.

Tier 3: Ideological Costs. Informational expenses justifying Tier 1 and
2 costs, measured by Patch Velocity (P(t)), the rate of ad-hoc
ideological justifications (such as divine mandates for suffering). High
P(t) signals accumulating ideological debt in failing systems.

These tiers form a causal cascade. Unaddressed Tier 1 costs (famine)
generate dissent, forcing Tier 2 costs (higher C(t) through
suppression). To justify these failures, the system generates Tier 3
costs (accelerating P(t)). High Tier 3 readings are lagging indicators
of deep, unresolved Tier 1 problems.

=== 2.3. Falsifiability and Triangulation
<falsifiability-and-triangulation>
This framework is empirically testable. Robust brittleness diagnosis
requires convergent evidence from three baselines: (1)
Comparative-Historical analysis against contemporaneous peers, (2)
Diachronic comparison against the system's own trajectory, and (3)
Biological Thresholds representing non-negotiable viability limits.

The core claim is that systemic costs predict long-term fragility. This
would be falsified if historical analysis showed:

+ No Correlation: No significant link between high costs (e.g.,
  violence) and systemic fragility
+ High-Cost Superiority: Coercive systems prove more
  innovative/resilient than cooperative ones
+ Negative Canon Failure: High-cost predicates (e.g., "slavery
  acceptable") enhance long-term viability

We acknowledge that measuring these costs is most straightforward in
state-level societies with formal institutions. For informal normative
systems, proxies must be more creative, relying on data from
ethnographic studies, legal records of disputes, or bioarchaeological
markers of stress within marginalized subgroups. The core principle
remains: the costs are real and have empirical signatures, even when
their measurement is indirect.

==== Calibrating Timescales for Brittleness Predictions
<calibrating-timescales-for-brittleness-predictions>
The brittleness framework makes predictions about system fragility, but
the relevant timescale varies systematically with system
characteristics. Specifying these timescales is essential for
falsifiability.

Scale Effects. Small-scale systems (city-states, local communities,
organizations) exhibit faster feedback loops. Brittleness manifests in
collapse timescales of decades to a century. Large-scale systems
(empires, major civilizations, international orders) possess greater
inertia and buffering capacity, with complete collapse timescales of
centuries to millennia. However, decline indicators (rising C(t),
accelerating P(t)) typically precede collapse by 50-150 years even in
large systems. The Roman Empire's fall took centuries, but brittleness
symptoms appeared generations earlier.

Interconnectedness Effects. Isolated systems can persist in
high-brittleness states longer because external competitive pressure is
delayed. Pre-modern empires with geographic buffers (mountain ranges,
deserts, oceans) could sustain inefficient configurations for extended
periods. Interconnected systems face accelerated filtering-competitive
pressure forces collapse or adaptation more quickly. Modern
nation-states in a global economy cannot sustain high-brittleness
configurations as long as geographically isolated historical empires.

Equilibrium Brittleness versus Collapse Timing. A critical distinction.
We predict brittleness-collapse correlation, not deterministic timing.
High-brittleness systems: (1) always incur higher maintenance costs (by
definition-that's what brittleness measures), (2) show characteristic
warning signs (rising C(t), accelerating P(t), accumulating systemic
debt), (3) are more vulnerable to shocks (external pressures, internal
crises, succession problems), and (4) eventually collapse or
fundamentally transform. Timing varies with scale, context, and shock
magnitude, but the pattern holds.

Probabilistic Predictions: Our predictions are conditional and
probabilistic: "System X with brittleness profile Y has Z% probability
of major crisis within W years, conditional on shocks of magnitude M."
These are calibrated using historical base rates for comparable systems,
with confidence intervals widening for longer predictions. We're not
offering deterministic prophecy but epistemic risk assessment grounded
in historical patterns.

The "Successful Coercion" Illusion: Long persistence of high-coercion
systems doesn't falsify our model if: (1) C(t) was rising over time
(indicating increasing fragility), (2) the system underwent periodic
collapses and reconstitutions (showing brittleness), (3) decline
indicators preceded ultimate collapse, or (4) comparison to peers shows
higher costs and greater vulnerability. What would falsify our claim: a
system with sustainably low and stable C(t) and P(t) that nonetheless
collapsed, while peers with high C(t) and P(t) proved more durable. The
historical record provides no clear examples of this pattern.

Prospective Application: For contemporary systems, we diagnose rising
brittleness before collapse by identifying rising C(t) trends, tracking
P(t) acceleration, monitoring bio-social cost indicators, and comparing
to historical patterns. This provides actionable epistemic risk
assessment. We cannot predict exact timing of collapse (too many
variables), but we can assess relative fragility and identify systems
requiring urgent debugging. This is analogous to earthquake science: we
cannot predict when specific earthquakes will occur, but we can map
fault lines and assess seismic risk.

=== 2.4. Operationalizing Brittleness: A Worked Example
<operationalizing-brittleness-a-worked-example>
To demonstrate empirical testability, we provide a detailed worked
example showing how to operationalize C(t), P(t), and bio-social costs
for a well-documented historical case.

==== Case Study: The Antebellum South (1830-1860)
<case-study-the-antebellum-south-1830-1860>
The slave system of the American South provides an ideal test case:
abundant documentation, clear normative architecture centered on
"slavery is acceptable," and known historical outcome (catastrophic
collapse 1861-1865). We can retrospectively calculate brittleness
metrics and assess whether they predicted the system's fragility.

A. Measuring C(t): The Coercion Ratio

Operational Definition: C(t) = (Total resources dedicated to internal
coercion and suppression) / (Total economic output)

Data Sources: - State and local government budgets (militia, slave
patrols, judicial systems) - Private expenditures on surveillance and
enforcement (overseers, slave catchers, weaponry) - Opportunity costs of
labor diverted to coercion (white men in patrol service, guard duty) -
Insurance and compensation systems for captured fugitives -
Infrastructure costs (jails, patrol stations, communication networks for
suppression)

Sample Calculation for Virginia, 1850:

Total economic output (1850): Approximately \$220 million (agricultural
and industrial production)

Coercion costs: - State militia and patrol appropriations:
\~\$800,000/year - County-level slave patrols and constabulary: \~\$1.2
million/year - Private overseers and supervision (est. 5,000 overseers ×
\$400/year): \~\$2 million/year - Legal system costs (slave courts,
fugitive enforcement): \~\$500,000/year - Opportunity cost of patrol
labor (est. 15,000 men × 20 days/year × \$2/day): \~\$600,000/year -
Insurance, bounties, and enforcement infrastructure: \~\$400,000/year

Total coercion costs: \~\$5.5 million/year C(t) for Virginia (1850) ≈
5.5/220 = #strong[2.5%]

Comparative Baseline: - Northern free states (1850): C(t) ≈ 0.8-1.2%
(standard law enforcement and judicial costs) - Britain (1850): C(t) ≈
1.0% (peacetime metropolitan police and courts) - Virginia's coercion
ratio is 2-3× higher than peer societies, indicating a high-brittleness
configuration requiring extraordinary maintenance costs.

Trend Analysis (1820-1860): Historical budget data shows C(t) rising
over time: - 1820: C(t) ≈ 1.8% - 1840: C(t) ≈ 2.2% - 1860: C(t) ≈ 3.1%

Rising C(t) indicates increasing brittleness as the system struggles to
maintain stability despite growing internal resistance.

B. Measuring P(t): Patch Velocity

Operational Definition: P(t) = Rate of new ideological justifications
produced per decade, measured by publication counts, legislative
preambles, and doctrinal innovations

Data Sources: - Pro-slavery treatises and pamphlets (bibliographic
records) - Theological defenses published by Southern clergy -
Scientific racism texts and articles - Legislative declarations and
constitutional provisions - Shift in dominant justificatory frameworks

Quantitative Analysis:

Publication counts (pro-slavery justifications per decade): - 1790-1800:
\~12 major treatises ("necessary evil" framework) - 1800-1810: \~18
treatises (defensive responses to abolition) - 1810-1820: \~25 treatises
(introduction of "positive good" theology) - 1820-1830: \~40 treatises
(biblical literalism, Curse of Ham arguments) - 1830-1840: \~75
treatises (responding to immediate abolitionism) - 1840-1850: \~110
treatises (scientific racism, ethnology, polygenesis) - 1850-1860: \~160
treatises (desperate theological innovations, secessionist ideology)

P(t) acceleration: - 1790-1820: Modest increase (roughly linear growth)
\- 1820-1850: Sharp acceleration (exponential growth phase) - 1850-1860:
Peak velocity (system in crisis, generating maximum ideological output)

Qualitative Analysis:

Track the rapidity of doctrinal shifts: - Early period: Slavery as
"necessary evil," temporary institution - 1820s shift: Slavery as
"positive good," divinely ordained - 1830s innovation: Biblical
literalism, Ham's curse theological arguments - 1840s innovation:
Scientific racism, biological hierarchy claims - 1850s desperation:
Constitutional arguments for slavery's expansion, secessionist ideology
as ultimate "patch"

Each shift represents accumulated ideological debt. The system cannot
maintain legitimacy with existing justifications and must generate new
ones at accelerating rates. High P(t) is a lagging indicator of
unresolved Tier 1 and Tier 2 costs.

C. Measuring Tier 1 Bio-Social Costs

Operational Definition: Excess mortality, morbidity, and demographic
stress beyond baseline human needs

Data Sources: - Mortality records (parish registers, plantation records,
census data) - Bioarchaeological evidence (skeletal remains showing
nutritional stress, trauma) - Demographic analysis (birth rates, death
rates, natural increase) - Contemporary medical and observer accounts

Quantitative Indicators:

Mortality differentials (deaths per 1,000 per year): - Enslaved
population: \~30-35 per 1,000 - Free white population: \~18-22 per 1,000
\- Excess mortality among enslaved: #strong[\+50-70%] above baseline

Infant mortality (deaths before age 5): - Enslaved children: \~35-40% -
Free white children: \~18-25% - Catastrophic excess mortality indicating
severe systemic costs

Nutritional stress markers (bioarchaeological data): - High prevalence
of enamel hypoplasia (childhood malnutrition) - Skeletal indicators of
protein deficiency - Stunted growth patterns

Violence-related deaths: - Estimated 5-10 per 1,000 enslaved persons per
year died from direct violence (whipping injuries, executions,
suppression of resistance) - This excludes the trauma from non-lethal
violence

Demographic sustainability: - Unlike Caribbean slave systems (which
required continuous importation due to negative natural increase), the
U.S. South achieved positive natural increase only through forcing
reproduction - This masks underlying bio-social costs-the system
"worked" only by violating reproductive autonomy

D. Triangulated Diagnosis

The three baselines converge on a high-brittleness diagnosis:

+ Comparative-Historical:

- Antebellum South vs.~Northern free states: Higher C(t) (2.5-3%
  vs.~0.8-1.2%), vastly higher bio-social costs
- Antebellum South vs.~Britain: Similar differentials
- Antebellum South vs.~Caribbean slavery: Lower bio-social costs
  (positive natural increase) but still catastrophically high; higher
  P(t) (Caribbean systems didn't generate the same ideological
  apparatus)

#block[
#set enum(numbering: "1.", start: 2)
+ Diachronic:
]

- 1820 vs.~1860 trajectory: All metrics worsening (rising C(t),
  accelerating P(t), sustained high bio-social costs)
- System becoming more brittle over time despite apparent economic
  prosperity
- The "Cotton Kingdom" prosperity was achieved through intensifying
  exploitation, raising maintenance costs

#block[
#set enum(numbering: "1.", start: 3)
+ Biological Thresholds:
]

- Excess mortality and malnutrition far exceed minimum viability
  thresholds
- Chronic violence and trauma impose severe bio-social costs
- System survives only through massive coercive expenditure

Diagnosis: The slave system exhibited pathologically high brittleness
across all three tiers. The system was a fitness trap: locally stable
(economically profitable for stakeholders) but globally inefficient,
maintained only through escalating coercive expenditure and ideological
justification.

E. Historical Validation

The brittleness diagnosis predicts fragility under stress. Historical
outcome:

- System collapsed within 5 years of 1860 data endpoint
- Collapse required external shock (Civil War), but internal brittleness
  explains:
  - Why the South fought rather than compromising (ideological rigidity
    from high P(t))
  - Why the system couldn't be reformed (entrenched interests tied to
    high C(t) infrastructure)
  - Why collapse was catastrophic rather than managed (no resilience,
    high brittleness)
- Post-war attempts to reconstruct similar systems (sharecropping,
  convict leasing) also exhibited high brittleness and eventually failed

The framework successfully diagnoses brittleness retrospectively. High
C(t), accelerating P(t), and catastrophic bio-social costs indicated a
system under severe stress maintained through unsustainable coercion.

F. Methodological Notes

Inter-Rater Reliability: Ideally, multiple historians would
independently: - Code the same primary sources - Calculate C(t) using
the same data categories - Assess P(t) using the same publication
databases - Report confidence intervals rather than point estimates -
Flag contested measurements for discussion

Uncertainty Quantification: Our figures are estimates with significant
uncertainty: - C(t) = 2.5% ± 0.5% (uncertainty from incomplete budget
records, valuation disagreements) - P(t) acceleration is qualitatively
robust but exact counts depend on inclusion criteria - Bio-social costs
are most robust (demographic data is well-documented)

Replication: Other scholars using this framework should be able to: -
Access the same primary sources (government records, plantation
documents, publications) - Apply the same operationalizations - Reach
similar conclusions (within uncertainty bounds) - Challenge our coding
decisions with alternative interpretations

This worked example demonstrates that the brittleness framework is
empirically testable, not merely conceptual. The metrics can be
operationalized, measured with inter-rater reliability, and validated
against known historical outcomes. While measurement is challenging, it
is feasible and far more rigorous than typical moral philosophy's
reliance on intuition alone.

With this diagnostic toolkit established and operationalized, we can now
apply it to additional historical cases to model the process of moral
progress.

== 3. Moral Progress in Action: Diagnostic Case Studies
<moral-progress-in-action-diagnostic-case-studies>
=== 3.1. Non-Teleological Progress Model
<non-teleological-progress-model>
EPC models moral progress as systemic debugging: identifying and
removing high-cost predicates. This is not teleological advance toward
utopia but backward-looking correction of failures. Progress is
empirically observable SBI reduction over time. A change qualifies as
progress if the successor network has measurably lower SBI than its
predecessor.

=== 3.2. Paradigm Case: Slavery's Systemic Failure
<paradigm-case-slaverys-systemic-failure>
Abolition of chattel slavery exemplifies systemic debugging. Its status
as objective progress rests not on modern sentiment but pragmatic
diagnosis of "slavery is acceptable" as a catastrophic design flaw.
Slave societies were high-brittleness fitness traps: locally stable but
globally inefficient, sustained by immense coercive expenditure.

The costs were severe: pathologically high C(t) for surveillance and
suppression; catastrophic bio-social costs from endemic violence and
revolt risk; profound economic losses from suppressed human capital;
accelerating ideological patches (from "Curse of Ham" to race science),
indicating high P(t). Abolitionist arguments diagnosed this
inefficiency. The replacement predicate "slavery is wrong" succeeded by
promising dramatically lower SBI. The successor system, while imperfect,
proved significantly less brittle.

=== 3.3. Complex Case: Patriarchy's Systemic Costs
<complex-case-patriarchys-systemic-costs>
EPC analyzes ongoing debates like patriarchy's decline. The predicate
"women's roles are private and subordinate" proves profoundly
inefficient: massive economic losses from excluding half the population;
informational costs from silencing female perspectives; high coercive
costs enforcing rigid roles.

Transition to egalitarianism involves short-term friction costs from
social conflict. However, this is an investment that pays down
patriarchal debt. Feminist critique wagers that fully utilizing all
human resources yields greater long-term innovation and resilience
(lower SBI). This transforms value clashes into empirical questions
about social design efficiency. This wager is increasingly supported by
development economics, which finds strong correlations between gender
equality in education and economic participation and metrics of national
prosperity and stability (World Bank 2012; Duflo 2012).

=== 3.4. Challenging Cases: Addressing Apparent Counterexamples
<challenging-cases-addressing-apparent-counterexamples>
The slavery and patriarchy cases provide clear examples where high
brittleness correlates with eventual collapse or transformation.
However, a robust framework must address apparent counterexamples.
History provides cases that might challenge our brittleness model:
long-lived empires with high coercive costs, and failed egalitarian
experiments. How does the framework handle these without ad hoc
modification?

Case 1: Imperial China and Long-Lived Hierarchical Systems

The Challenge: Imperial China persisted for roughly two millennia (221
BCE - 1911 CE) with hierarchical, often highly coercive governance.
Significant resources were dedicated to bureaucratic control, military
suppression, and maintaining rigid social hierarchies. If high C(t)
indicates brittleness, why did these systems prove so durable?

Framework Response: Several clarifications resolve this apparent
counterexample.

First, distinguish longevity of the #emph[replicator] (the informational
template) from stability of specific #emph[interactors] (particular
dynasties). What persisted was a cultural and institutional
framework-Confucian bureaucratic governance-that was repeatedly
reimplemented after catastrophic collapses. The "Chinese Empire"
underwent multiple complete dynastic collapses (Han, Tang, Song, Yuan,
Ming, Qing), foreign conquests, massive peasant rebellions, and
mortality events killing millions. What endured was not a continuous
stable system but a recurring pattern of rise, brittleness-driven
decline, collapse, and reconstitution. The longevity of the template
doesn't imply low brittleness of implementations.

Second, examine C(t) trajectories within dynastic cycles. Successful
dynasties typically exhibited relatively low C(t) during founding
periods, having earned legitimacy through reform or military success.
C(t) rose systematically during decline phases as the "Mandate of
Heaven" eroded, requiring increased coercion to maintain control.
Collapse followed predictably when C(t) became unsustainable. The
pattern confirms rather than contradicts the brittleness model-it's
cyclical rather than linear, but the brittleness-collapse correlation
holds within each cycle.

Third, identify which elements persisted and which proved brittle. The
durable core principles-meritocratic examination systems, rule of law
ideals, reciprocity norms between ruler and ruled-are precisely the
low-brittleness elements. The high-brittleness elements-emperor worship,
eunuch bureaucracies, extreme hierarchical rigidity-systematically
correlated with decline phases. Reformers who succeeded in establishing
new dynasties typically debugged the most brittle features while
preserving viable core principles.

Fourth, calibrate for system scale and isolation. Large, geographically
isolated systems have longer collapse timescales (centuries rather than
decades) due to greater buffering capacity and reduced competitive
pressure. But brittleness still predicts fragility and eventual
transformation. The framework's timescale predictions must account for
system characteristics.

Conclusion: Imperial China confirms rather than contradicts the
framework once we analyze at the appropriate granularity. The persistent
template encoded low-brittleness coordination solutions. Specific
implementations cycled through phases of lower and higher brittleness,
with collapse following predictably from rising C(t).

Case 2: Failed Egalitarian Experiments

The Challenge: Some egalitarian, low-coercion societies collapsed
rapidly despite apparently exhibiting low internal C(t). Examples
include the Paris Commune (months), Spanish anarchist collectives during
the Civil War (years), and various intentional communities (decades at
most). If low coercion correlates with viability, why did these systems
fail?

Framework Response: This conflates internal brittleness with external
vulnerability.

First, distinguish internal from external coercion costs. These systems
often had genuinely low #emph[internal] C(t)-participants cooperated
voluntarily with minimal internal suppression. However, they faced
overwhelming #emph[external] coercive pressure: military attack,
economic blockade, and deliberate destruction by threatened powers. Our
framework measures internal systemic costs, not military vulnerability
to external attack. A low-brittleness society can still be conquered by
a high-brittleness military empire. This doesn't falsify the viability
claim; it shows that viability is not identical to invincibility.

Second, separate startup costs from maintenance costs. Revolutionary
transitions always incur high short-term costs: chaos from dismantling
existing institutions, economic disruption, coordination failures as new
systems are established. Our brittleness metrics apply to
#emph[equilibrium] functioning, not revolutionary transition periods.
Many egalitarian experiments failed during the startup phase before
reaching stable equilibrium. This provides evidence about transition
difficulty, not about the long-term viability of the target
configuration.

Third, recognize scale and context dependency. Small-scale communities
face coordination challenges that may require specific institutional
solutions. The failure of a particular small-scale implementation
doesn't falsify general principles about coercion and viability.
Moreover, experiments conducted under siege conditions (economic
isolation, military threat) cannot fairly test long-term viability. We
need data from egalitarian systems operating under normal conditions,
not extraordinary stress.

Fourth, the framework is not axiomatically committed to egalitarianism.
If specific egalitarian configurations consistently fail (for example,
"abolish all formal coordination mechanisms without replacement"), they
enter the Negative Canon alongside authoritarianism. The brittleness
framework empirically tests which configurations work, including which
forms of egalitarianism prove viable. Some egalitarian principles (equal
basic rights, democratic accountability) show low brittleness; others
(absolute equality of outcome regardless of contribution) may prove
brittle. This is an empirical question.

Conclusion: Failed egalitarian experiments don't contradict the
brittleness framework. They primarily demonstrate external vulnerability
and transition difficulties, not high internal brittleness in
equilibrium. Where internal brittleness exists, the framework should
identify it.

Case 3: The "Viable Evil" Scenario Revisited

The Challenge: Could a deeply morally repugnant system achieve genuinely
low brittleness-minimal coercive costs, stable demographics, sustained
innovation? If so, our framework would have to accept it as viable.

Framework Response: We maintain intellectual honesty by accepting this
implication while making an empirical bet.

First, intellectual honesty requires acknowledging that the framework
maps pragmatic viability, not all dimensions of moral value. If a
repugnant system achieved genuinely low C(t), low P(t), and minimal
bio-social costs while sustaining innovation and adaptation, it would
qualify as viable within our framework. Such a system would belong to
the Pluralist Frontier, not the Negative Canon. The framework doesn't
claim to capture every moral consideration-only the structural
requirements of viability.

Second, our empirical wager is that such systems are sociological
impossibilities. Apparent historical examples of "stable oppression"
consistently reveal hidden costs under closer analysis. Consider:

- #emph[Ottoman devşirme system];: Appeared stable (Christian boys
  converted into loyal Muslim soldiers/administrators), but required
  constant coercive intake, generated resentment in source populations,
  and proved fragile under external stress.

- #emph[Indian caste system];: Thousands of years of apparent stability,
  yet anthropological and economic analysis reveals high coercive
  overheads (enforcement of purity rules, suppression of mobility),
  innovation lags (rigid occupational categories hindered technological
  adoption), and demographic stress (untouchability imposed severe
  bio-social costs).

- #emph[Brave New World scenarios];: Oppression through pleasure and
  conditioning rather than overt coercion. Yet suppressing cognitive
  capacities (critical thinking, autonomy) incurs massive Tier 2
  information suppression costs that cripple long-term adaptation. A
  society that cannot question its assumptions cannot debug errors.

True, cost-free internalization of oppression would require eliminating
the capacity to recognize one's condition as oppressive, which
eliminates the capacity for critical assessment generally. This creates
catastrophic information costs and brittleness under novel challenges.

Third, measurement challenges exist but don't undermine the framework.
For historical oppressive systems, data limitations may obscure costs.
But absence of evidence isn't evidence of absence. The burden is on
critics to demonstrate a genuinely low-C(t), low-P(t),
low-bio-social-cost system that is morally repugnant. Historical record
provides no clear examples.

Conclusion: We accept the logical possibility while maintaining strong
empirical skepticism. The framework's limitation is also its strength-it
makes falsifiable empirical predictions rather than building in
normative conclusions a priori.

General Lessons from Hard Cases

These challenging cases refine rather than refute the brittleness
framework:

+ Distinguish replicators from interactors: Template persistence doesn't
  imply implementation stability
+ Calibrate timescales by system characteristics: Larger, isolated
  systems exhibit longer cycles
+ Separate internal from external pressures: Viability is not
  invincibility
+ Distinguish transition from equilibrium: Startup costs don't measure
  maintenance costs
+ Maintain empirical openness: Framework tests which configurations
  work, not which we prefer

The core claim survives: high systemic costs (C(t), P(t), bio-social)
correlate with long-term fragility. Apparent counterexamples, upon
analysis, typically confirm the framework at finer granularity or reveal
crucial distinctions (internal vs.~external costs, replicator
vs.~interactor persistence). Where genuine anomalies exist, they sharpen
our understanding of boundary conditions and measurement challenges.

Having operationalized the brittleness framework and demonstrated its
application to historical cases, we can now articulate its metaethical
implications. The diagnostic work establishes that moral progress is
empirically observable as SBI reduction. But what does this tell us
about the nature of moral truth itself? Section 4 develops the
philosophical foundations of Pragmatic Procedural Realism.

== 4. Pragmatic Procedural Realism: The Metaethical Framework
<pragmatic-procedural-realism-the-metaethical-framework>
=== 4.1. Metaethical Position
<metaethical-position>
Pragmatic Procedural Realism is the metaethical instantiation of
Emergent Pragmatic Coherentism. While EPC provides the general theory of
justification applicable across all domains, Pragmatic Procedural
Realism specifies how that framework operates in the normative domain.
The relationship is one of general theory to domain-specific
application: EPC is the diagnostic methodology, Pragmatic Procedural
Realism is its normative realization.

Pragmatic Procedural Realism is a naturalistic moral realism (Boyd 1988;
Railton 1986). Its objectivity claims are:

- #strong[Realist];: Objective, mind-independent truths exist about
  normative viability. "Slavery is wrong" refers to structural facts
  about predicates' incoherence with the Apex Network, the emergent
  structure of viable norms.
- #strong[Procedural];: Moral truths are emergent relational facts
  discovered historically. Truth-makers are objective facts about
  networks' pragmatic resilience (low SBI).
- #strong[Externalist];: Justification rests on demonstrated historical
  track records, not internal coherence or cultural consensus.

While moral truths are objective in being determined by pragmatic
constraints, our knowledge of them remains fallible and requires
empirical triangulation, avoiding overconfidence in any particular
historical assessment.

==== 4.1.1. The Pragmatic Procedure of Moral Inquiry
<the-pragmatic-procedure-of-moral-inquiry>
What is the 'procedure' in Pragmatic Procedural Realism? It is a
multi-stage, iterative process of collective inquiry grounded in
historical empirics:

+ #strong[Hypothesis Generation];: Communities propose normative
  principles as potential solutions to social coordination problems.
+ #strong[Empirical Testing];: These principles are implemented in
  social systems, where they are subjected to the filter of pragmatic
  consequences over historical time.
+ #strong[Data Collection and Diagnosis];: We analyze the historical
  track record of these systems using the tiered diagnostic toolkit to
  measure their brittleness (Tier 1 costs, C(t), P(t)).
+ #strong[Mapping the Landscape];: Through comparative analysis, we
  identify principles that reliably generate high costs and enter them
  into the Negative Canon (mapping the 'floor'). We also identify
  principles that repeatedly emerge in low-brittleness systems and add
  them to the Convergent Core.
+ #strong[Revision and Refinement];: Armed with this evolving map, we
  revise our current normative systems, debugging high-cost principles
  and engineering more viable alternatives.

This procedure is empirical, fallible, and ongoing-the collective,
scientific-historical method for discovering the objective contours of
the viable normative landscape. This five-stage procedure grounds our
realism: moral truths are objective because they are determined by this
mind-independent filtering process, not by our subjective preferences or
cultural conventions.

==== 4.1.2. The Independence of Pragmatic Constraints
<the-independence-of-pragmatic-constraints>
One might object that our procedure appears circular: we claim moral
truths are discovered by filtering through pragmatic constraints, but
how do we know which constraints are "pragmatic" rather than merely
contingent preferences? Doesn't this depend on prior normative
commitments? If we identify non-negotiable constraints by which
societies happen to survive, aren't we simply reading norms off
historical winners?

This objection misunderstands the relationship between the filtering
process and the constraints that do the filtering. We must distinguish:
(1) the filtering process itself (historical testing of normative
principles), and (2) the constraints that do the filtering (biological,
physical, cognitive, and logical necessities). The constraints are not
products of the procedure; they are preconditions for any social
organization whatsoever.

Biological Constraints. These are empirical facts about human organisms,
discoverable through physiology, nutrition science, and epidemiology
without any prior normative commitments. Humans require minimum caloric
intake (approximately 1,500-2,000 calories per day for adults to
maintain basic functions). Chronic malnutrition produces immune
dysfunction, elevated mortality, and demographic decline. Humans
reproduce sexually with roughly nine-month gestation and extended
childhood dependency requiring caregiver investment. Social isolation
causes measurable psychological and physical harm. A normative system
that systematically violates these requirements incurs costs-mortality,
morbidity, demographic collapse-that are objective, measurable, and
independent of anyone's values.

Cognitive Constraints. From psychology, cognitive science, and
behavioral economics, we discover that humans exhibit bounded
rationality (Simon 1972): we cannot compute optimal solutions to complex
problems in real-time. Working memory is limited (roughly seven items).
We are vulnerable to coordination failures without institutional
support. We possess specific social learning capacities that enable
cultural transmission but also specific limitations that constrain what
information can be effectively transmitted. These constraints shape what
normative architectures are even implementable. A system requiring
perfect rationality or unlimited information processing simply cannot
function with human agents, regardless of its moral appeal. These are
empirical facts about human cognition, not value judgments.

Coordination Constraints. From game theory, mechanism design, and
institutional economics, we learn that cooperation under conditions of
potential defection requires enforcement mechanisms (Axelrod 1984).
Common-pool resource management requires boundary rules and monitoring
(Ostrom 1990). Large-scale coordination requires division of labor and
information aggregation mechanisms. These are mathematical and logical
facts about strategic interaction under specified conditions. They apply
to any system where individuals have private information and potentially
divergent incentives. They are derivable from formal models, not read
off normative intuitions.

Physical Constraints. From physics, ecology, and thermodynamics, we know
that energy must be extracted from the environment to sustain
organization. Entropy requires continuous work to maintain structured
systems. Finite resources constrain population size and consumption
patterns. These physical facts impose hard limits on what social
organizations can achieve.

The Critical Move: Constraints Are Not Values. None of these constraints
represent normative commitments we endorse-they are descriptive facts
about what human bodies need to function, how human minds process
information, what strategic interaction requires for cooperation, and
what physical reality demands for maintaining organization. They are
discoverable through standard empirical inquiry (physiology, psychology,
economics, physics) without assuming any particular normative framework.
A society committed to asceticism still faces biological caloric
requirements. A society valuing hierarchy still faces coordination
constraints. A society denying thermodynamics still must extract energy
from its environment.

How This Dissolves Circularity. The historical filtering procedure
discovers which normative principles successfully navigate these
independently-specified constraints. This is no more circular than:

- Engineering, which tests which bridge designs withstand gravity (where
  gravity is an independent constraint discovered through physics)
- Medicine, which tests which treatments reduce mortality (where
  biological health requirements are independent constraints discovered
  through physiology)
- Economics, which tests which institutions enable cooperation (where
  coordination requirements are independent constraints discovered
  through game theory)

In each case, there's a discovery procedure (testing) and independent
constraints (physical laws, biological needs, strategic requirements)
that determine success or failure. The procedure is legitimate precisely
because it tracks these mind-independent constraints.

Anticipated Response: "But you're still choosing to value
persistence/survival by focusing on these constraints!"

We address this concern in §5.4's Constitutive Defense. Here the point
is different: #emph[given] that we're studying persistent systems (the
only ones available in the historical record for analysis), the
constraints that filter them are objective, empirical facts, not
normative commitments. The choice of domain (persistent social systems)
is methodological; the constraints operating within that domain are
empirical. We don't assume persistence is good; we observe that
persistent systems are the ones we can study, and we discover
empirically what constraints they must satisfy.

The Analogy to Natural Selection. Consider why natural selection isn't
circular even though fitness is defined by reproductive success and
which traits are fit is determined by which organisms reproduce. The
answer: environmental constraints (resource availability, predation,
climate, physical laws) that determine fitness are independent of the
selection process. Similarly, viability in normative systems is defined
by persistence, and which principles are viable is determined by which
systems persist. But the pragmatic constraints (biology, cognition,
coordination, physics) that determine viability are independent of the
historical filtering process. The process discovers which architectures
successfully navigate the constraints; it doesn't create the constraints
themselves.

This independence is what grounds our realism. The pragmatic constraints
that filter normative systems are objective, empirically discoverable
features of the human condition. They are not products of our values or
the historical process but preconditions that any viable social
organization must accommodate. The historical filtering process reveals
which normative architectures successfully navigate these constraints-it
doesn't invent the constraints themselves. Moral inquiry discovers
constraint-determined structures, just as science discovers physical
laws and mathematics discovers logical necessities.

=== 4.2. The Apex Network
<the-apex-network>
Our objectivity rests on the Apex Network: the complete set of maximally
coherent, pragmatically viable normative predicates. The Apex Network's
objectivity stems not from historical contingency but from practical
necessity given the deep, enduring constraints of human cooperation.
These constraints-biological facts about human needs, cognitive
limitations on information processing, physical requirements for
maintaining organization, and logical necessities of strategic
interaction-are not metaphysically necessary in the strongest sense (we
can imagine possible worlds where they differ), but they are effectively
invariant across human history.

Reality imposes these non-negotiable constraints, determining a
landscape of possible normative configurations where some solutions are
viable and others catastrophic. There exists an optimal configuration
for navigating these constraints-or more precisely, a region of optimal
solutions-just as engineering problems have optimal solutions determined
by physical constraints whether anyone has calculated them. The Apex
Network is that constraint-determined structure, existing independently
of which societies have discovered it and independently of our beliefs
about it.

We need not claim a single unique optimum to ground objectivity. The
Apex Network may comprise a bounded region of normative space rather
than a single point. What matters for realism is that pragmatic
constraints dramatically restrict the viable region. Most of normative
space is simply unworkable-catastrophic failures that violate
biological, cognitive, or coordination requirements. The landscape has
definite structure: catastrophic failures (the floor), viable solutions
(bounded peaks), and non-viable configurations (deep valleys). This
structure exists independently of our discovery of it.

Historical filtering is how we discover this structure, not how we
create it. Failed systems function as experiments revealing where the
landscape drops off. Over time, with sufficient experiments across
diverse conditions and contexts, we triangulate toward the viable
regions. This mirrors engineering convergence: independent societies
discovered the arch, the lever, and the wheel not through cultural
transmission but because physical constraints (gravity, materials
science, mechanics) determine optimal solutions to recurring problems.
Discovery processes vary wildly; the constraint-determined solutions do
not. Similarly, independent cultures converged on reciprocity norms and
harm prohibitions because pragmatic constraints on sustainable
coordination determine optimal solutions, not because these cultures
shared values or communicated.

This practical necessity is relative to the actual constraints that have
defined human cooperation: biological needs (nutrition, safety,
reproduction), cognitive architecture (bounded rationality, social
learning capacities), and coordination requirements (communication,
trust, reciprocity enforcement). These constraints are empirical facts,
not metaphysical necessities. Should radical technological change (for
example, cognitive enhancement eliminating bounded rationality, or
post-scarcity economics removing resource constraints) or evolutionary
change fundamentally alter these constraints, the viable normative
landscape would shift accordingly. Our realism is thus robust within the
space of actual human social organization but not dogmatically committed
to eternal, unchanging moral truths across all possible worlds. The Apex
Network is discovered, not invented-but it is discovered relative to
actual human constraints, not derived from pure reason or metaphysical
necessity alone.

=== 4.3. The Structure of the Viable Normative Landscape: The `Floor` and the `Ceiling`
<the-structure-of-the-viable-normative-landscape-the-floor-and-the-ceiling>
This framework maps normativity's "floor" (non-negotiable viability
conditions), not its "ceiling" of flourishing or aesthetics. Societies
must secure the floor before pursuing higher goals.

- Negative Canon (Floor): Most secure objective knowledge, what is
  demonstrably unworkable. Provides boundaries preventing relativism,
  mapped from historical failures like a "reef chart."
- Convergent Core: Principles (such as reciprocity) independently
  discovered across cultures, suggesting stable, low-cost coordination
  solutions.
- Pluralist Frontier: Domain of multiple viable solutions (such as
  different organizational models). Accommodates cultural diversity and
  disagreement as empirical questions about boundaries.

=== 4.4. Three-Level Normative Justification
<three-level-normative-justification>
This multi-level account applies the general three-level truth framework
developed in EPC (see Glenn, Forthcoming, Section 4.3) to resolve the
tension between relativism and objectivity. Normativity ascends through
justificatory levels, from local coherence to objective viability.

#strong[Level 1: Contextual Rightness (the 'Ought' of Coherence).] This
is the realm of cultural relativity, where normativity follows a
network's internal rules. In a 17th-century dueling society, the
predicate `insults must be met with a challenge to a duel` was
contextually right. Failing to issue a challenge was 'wrong' by the
system's internal logic. This level provides procedural correctness
without objective justification. The 'Ought of Coherence' commands: "If
in this network, follow its rules." It binds locally but lacks external
authority, explaining how abhorrent actions were once "right" while
creating coherence traps that externalist checks must overcome.

#strong[Level 2: Justified Rightness (the 'Ought' of Viability).] This
level provides external, empirical justification based on demonstrated
track records. While the dueling code was contextually right, historical
diagnosis reveals catastrophic Tier 1 Bio-Social Costs (premature deaths
of valuable community members) and high Tier 2 Costs (resources managing
feuds and vendettas). The predicate is therefore justifiedly wrong,
warranting its entry into the Negative Canon. The 'Ought of Viability'
commands: "If we aim for resilient cooperation, adopt low-brittleness
principles and avoid Negative Canon predicates."

#strong[Level 3: Objective Rightness (the 'Ought' of Optimal Design).]
This represents the regulative ideal and formal standard for Level 2
comparisons. The dueling predicate is objectively wrong because its
high-cost nature conflicts with efficient, low-cost cooperation
principles that form the Apex Network's modally necessary structure. The
'Ought of Optimal Design' represents the commands of a system that has
solved for maximal viability. Principles like reciprocity that pass
independent convergence tests are our strongest candidates. The dueling
code demonstrably fails to achieve this solution.

=== 4.5. The Entrenchment of Moral Principles: From Hypothesis to Core Norm
<the-entrenchment-of-moral-principles-from-hypothesis-to-core-norm>
How does a normative principle like
`innocent people should not be punished` achieve its foundational
status? The entrenchment mechanism detailed in EPC (Glenn, Forthcoming)
explains this journey of earning pragmatic indispensability:

+ Peripheral Hypothesis: The principle begins as a contested proposal, a
  potential solution to the high costs of rival principles like
  collective punishment.
+ Migration Inward: As it demonstrates immense value in lowering
  systemic brittleness (reducing C(t) by increasing legitimacy and
  stability), its revision becomes prohibitively costly. It becomes a
  Standing Predicate used to vet new laws and policies.
+ Core Principle (Systemic Caching): Its indispensability becomes so
  profound that it is embedded in the infrastructure of viable legal
  systems (constitutions, legal training, judicial review). This
  systemic caching is a rational response to bounded rationality; the
  system entrenches its most successful discoveries to avoid re-deriving
  them for every new case.

A core moral principle is not a self-evident axiom but a piece of highly
optimized social technology that has survived rigorous pragmatic
stress-testing. Its justification is its proven, indispensable
functional role in viable social architectures. This entrenchment
reflects pragmatic indispensability driven by bounded rationality (Simon
1972). The costs of revision become effectively infinite. Revising basic
justice principles requires abandoning the conceptual tools needed to
coordinate social expectations, resolve disputes, or maintain legitimate
authority. After centuries of implementation, legal systems worldwide
presuppose core fairness principles. Revision would generate
catastrophic first-order costs, undermining the stability and legitimacy
on which functional governance depends.

=== 4.6. Relationship to Kitcher's Ethical Project
<relationship-to-kitchers-ethical-project>
Philip Kitcher's #emph[The Ethical Project] (2011) represents the
closest existing approach to Pragmatic Procedural Realism. Both views
treat ethics as social technology that evolved to solve coordination
problems, employ historical methods, embrace naturalism, and reject
foundationalist approaches. Given these substantial similarities, what
distinguishes PPR from Kitcher's pragmatic naturalism?

Core Similarities: Kitcher and PPR share crucial commitments. Both
reject the search for self-evident moral axioms, instead grounding
ethics in its functional role solving practical problems of cooperation.
Both employ historical analysis rather than a priori reasoning. Both are
thoroughly naturalistic, explaining normativity within a scientific
worldview. Both affirm that moral progress is real and explicable
through functional improvement.

Critical Difference 1: Metaethical Status. The most significant
divergence concerns the robustness of moral objectivity. Kitcher
endorses "practical/emotional realism"-a sophisticated quasi-realism
where moral statements express commitments rather than beliefs about
mind-independent facts. Progress is functional enhancement relative to a
historical baseline. PPR, in contrast, endorses robust naturalistic
realism: moral statements refer to objective facts about normative
architectures' systemic brittleness. "Slavery is wrong" is true because
slavery generates catastrophic costs incompatible with the
constraint-determined structure of the Apex Network. For Kitcher, moral
inquiry discovers what works for us given our starting point; for PPR,
it discovers constraint-determined optimal structures that exist
independently. PPR thus offers stronger objectivity claims and is less
vulnerable to relativism.

Critical Difference 2: Diagnostic Framework. Kitcher evaluates normative
changes by whether they reduce altruism failures relative to previous
states, but provides no unified quantitative framework. PPR deploys the
Systemic Brittleness Index with operationalizable metrics-C(t), P(t),
bio-social costs-creating a unified diagnostic toolkit applicable across
domains. Kitcher's functional assessment risks historicism (what counts
as progress depends on starting point), while PPR's absolute brittleness
metrics allow cross-cultural and cross-temporal comparison without
privileging any baseline. This makes PPR more readily operationalized
and empirically testable.

Critical Difference 3: Scope. Kitcher focuses primarily on altruism
problems-psychological failures of cooperation between individuals. PPR
addresses all coordination problems, including institutional design,
economic systems, and political structures. Where Kitcher emphasizes
moral psychology and interpersonal morality, PPR treats moral psychology
as one component of broader systemic analysis. This gives PPR wider
applicability to questions of structural justice, institutional
evaluation, and policy design.

Critical Difference 4: Grounding. Kitcher grounds ethics in its function
of solving altruism problems-a pragmatic standard without deeper
foundation. PPR grounds ethics in independently-specified pragmatic
constraints (biological, cognitive, coordination requirements) that are
empirical facts discoverable through standard science. These constraints
aren't products of the historical process but preconditions any viable
society must satisfy. This provides stronger anti-relativist grounding.
Where Kitcher acknowledges a degree of historicism and contingency, PPR
argues for practical necessity given actual human constraints.

Critical Difference 5: The Apex Network. Kitcher's framework lacks an
equivalent to the Apex Network. Progress is trajectory-dependent
movement away from dysfunction, with no ultimate target or optimal
structure. PPR posits the Apex Network as both regulative ideal and
discovered structure-the constraint-determined region of maximal
viability. This provides a goal (approximate the Apex) not merely a
direction (away from failure), enabling stronger claims about which
systems are absolutely better, not just better than their historical
baseline.

Complementary Projects: Rather than competitors, these approaches are
better understood as complementary with different emphases. Kitcher
provides rich historical narrative explaining how ethics emerged from
psychological and social needs. PPR provides a diagnostic framework for
evaluating ethical systems' viability. If Kitcher explains the
#emph[origins] of ethics, PPR supplies #emph[evaluation criteria] for
ethical systems. A complete account might integrate both: Kitcher's
historical psychology explains why certain problems arose; PPR's
brittleness framework explains which solutions prove viable.

We gratefully acknowledge our debt to Kitcher's pioneering work opening
paths for naturalistic, historically-grounded metaethics. PPR builds on
this foundation by adding a unified diagnostic toolkit (SBI, C(t),
P(t)), providing stronger realist foundations through the Apex Network,
extending scope beyond altruism to all normative coordination, and
grounding in EPC's general theory of justification. Where Kitcher
demonstrates that ethics can be naturalized without loss of normative
force, we demonstrate that naturalized ethics can be robustly realist
about constraint-determined structures of viability.

== 5. Objections, Defenses, and Principled Limitations
<objections-defenses-and-principled-limitations>
=== 5.1. Objection: Might Makes Right
<objection-might-makes-right>
Pragmatic theories allegedly justify any enduring oppressive system.
This confuses endurance with viability. Viability requires low SBI
maintenance. Oppressive systems persisting through coercion are
high-cost, high-brittleness traps. Longevity measures energy (high C(t))
needed for instability management, not strength.

=== 5.2. Objection: Ideological Co-optation
<objection-ideological-co-optation>
Ideology might convince agents to endure failures, preventing revision.
This mistakes brittleness symptoms for solutions. Ideological patches
function as normative patching, analogous to ad-hoc scientific
hypotheses that create epistemic debt. High P(t) (accelerating patch
production) diagnoses rising SBI, not system health.

=== 5.3. Objection: Testing Asymmetry
<objection-testing-asymmetry>
Empirical claims test quickly, moral claims slowly over generations.
This asymmetry is predicted, not a flaw. EPC's unified filter
acknowledges system complexity determines feedback timescale and
texture.

=== 5.4. Objection: Circularity and Grounding
<objection-circularity-and-grounding>
Making viability the standard appears circular, seemingly smuggling in a
normative commitment to persistence. We offer a two-part defense.

First, the Constitutive Defense. The Persistence Condition is not a
value we endorse but a structural precondition for the existence of
normative systems that can be analyzed. Any normative system that fails
to persist drops out of the historical record, making it unavailable for
comparative study. Persistence is the entry condition for having a track
record to evaluate. It functions as a methodological filter on available
data, not as a substantive value commitment.

Second, the Instrumental Defense. Our framework offers a conditional
ought: "If a community aims to persist while solving coordination
problems, then it should adopt principles aligned with the Apex Network
and avoid Negative Canon predicates." This hypothetical imperative does
not claim persistence is categorically good, only that it is the goal
relative to which our diagnostic tools provide guidance. For communities
indifferent to persistence, our framework offers no normative force. But
for communities that do aim to persist, our framework identifies which
architectural features reliably support or undermine that goal.

=== 5.5. Additional Objections and Replies
<additional-objections-and-replies>
Objection: Cultural Relativism - Different cultures have viable but
incompatible norms. Reply: Compatible with pluralism in periphery while
maintaining floor constraints. Cultural diversity exists within
viability boundaries.

Objection: Moral Progress Skepticism - Progress claims are Western bias.
Reply: Framework predicts pluralist periphery but universal floor.
Progress diagnosed empirically via SBI reduction, not cultural
superiority.

Objection: Scientific Imperialism - Reducing ethics to science
(cf.~Putnam 2002). Reply: Not scientism but unified pragmatic filter.
Moral claims remain normative but justified externally like scientific
ones.

Objection: Evolutionary Debunking - Evolutionary pressures shaped moral
intuitions for survival, not truth (cf.~Street 2006). Reply: EPC
resolves Street's dilemma by collapsing one of its horns. The dilemma
assumes that truth and adaptiveness are independent aims, making their
alignment a coincidence. Our framework denies this premise. For us,
moral truth #emph[is] a specific, demanding form of long-term systemic
adaptiveness (i.e., viability). Evolution is not a distorting influence
that the realist must explain away; it is the broader category of
filtering processes within which the specific, cost-based discovery of
moral truth takes place. Pragmatic viability is what moral truth
supervenes on.

Objection: The Naturalistic Fallacy. The framework seems to define 'the
good' as 'the viable,' improperly deriving a value from a fact. Reply:
This misinterprets the project. We offer a naturalistic reconstruction
of the function of our normative practice. The claim is that what our
successful moral discourse has actually been tracking are facts about
systemic viability. 'Wrongness' is not being defined as
high-brittleness; rather, high-brittleness is the underlying natural
property that the term 'wrongness' has been imperfectly latching onto.
This is a semantic externalist move: just as 'water' successfully
referred to H₂O long before we understood molecular chemistry, 'wrong'
has been successfully tracking high-brittleness principles long before
we developed the diagnostic tools to measure it explicitly. This
naturalizes the reference of our moral terms, explaining their
functional authority without committing a fallacy.

Objection: How does this differ from Kitcher's 'Ethical Project'? Reply:
Our project shares much with Kitcher's (2011) view of ethics as a social
technology for solving problems of altruism. However, EPC offers two
crucial advancements. First, it provides a more general diagnostic
toolkit (the SBI) that applies equally to scientific and ethical
'technologies,' grounding the project in a unified theory of
justification. Second, EPC's concept of the modally necessary Apex
Network provides a more robustly realist foundation. Where Kitcher's
progress is defined by functional enhancement relative to a historical
starting point, our framework grounds progress in convergence toward an
objective, mind-independent structure of viability. This offers a
stronger defense against charges of historicism or relativism.

Objection: Hindsight Rationalization. The framework can only diagnose
brittleness after failure, making it merely retrospective rather than
providing prospective guidance. Reply: This misunderstands the
calibration process. We use clear historical data (the Negative Canon)
to calibrate our diagnostic instruments, identifying the empirical
signatures that reliably precede collapse. These calibrated instruments
then enable prospective diagnosis, not deterministic prediction, but
epistemic risk assessment for contemporary systems. This parallels
medical science: we learn disease patterns from past cases to diagnose
present patients before symptoms become catastrophic. The framework thus
operates in two stages: retrospective calibration using historical
failures to identify brittleness indicators, then prospective
application of these calibrated metrics to assess current systems and
identify degenerating research programs before collapse.

Objection: Why Historical over Idealized Procedures? Contemporary
constructivists (Rawls 1971; Korsgaard 1996) also ground normativity in
procedures, but use idealized rational procedures (original position,
categorical imperative procedure) rather than historical filtering. What
makes PPR's historical procedure superior? Doesn't idealization avoid
the contamination of actual history by power, ignorance, and bias?

Reply: There are three problems with idealized procedures and
corresponding advantages to historical ones.

#emph[First, the Epistemic Problem];: Idealized procedures generate
conclusions only as reliable as the idealizations themselves. But how do
we know which idealizations are appropriate? Why these constraints on
the original position and not others? Why this formulation of the
categorical imperative and not alternatives? The choice of idealizations
typically encodes substantive normative commitments-but those
commitments themselves need justification. We face a regress:
idealizations need grounding, but that grounding requires prior
normative principles. Historical procedures avoid this regress by
grounding in independently-specifiable empirical constraints
(biological, cognitive, coordination requirements). These aren't
idealizations we stipulate but facts we discover through science.

#emph[Second, the Application Problem];: Idealized procedures generate
principles for ideal agents under ideal conditions. But we are non-ideal
agents in non-ideal conditions. The application gap is severe-we need
"non-ideal theory" to bridge from idealized conclusions to actual
practice, but this requires additional normative principles that aren't
derived from the idealized procedure. Historical procedures operate in
the actual world with actual agents from the start, so their conclusions
directly apply to our situation. The filtering process already accounts
for human cognitive limitations, informational constraints, and
coordination problems because it tested principles under precisely those
conditions.

#emph[Third, the Contamination Worry Is Overstated];: The objection
assumes actual history is too contaminated by power and bias to provide
objective normative knowledge. But this overlooks the power of
convergent historical evidence. We're not simply reading off conclusions
from one biased historical trajectory-we're synthesizing patterns across
many historical experiments, looking for robust convergences and
systematic failures. Multiple independent societies discovering the same
principles (reciprocity, harm prohibition) provides triangulation that
idealized procedures lack. The historical record, while imperfect, gives
us convergent empirical data that armchair idealization cannot match.
Moreover, negative results (the Negative Canon) are especially robust to
bias-when a principle generates catastrophic costs across diverse
contexts, that's strong evidence against it regardless of whose
interests were served.

#emph[Fourth, Proceduralism Can Be Historical];: PPR is
proceduralist-moral truths emerge from a procedure (historical filtering
through pragmatic constraints)-but uses actual history rather than
idealized reasoning. The procedure's legitimacy stems from: (a)
independence of constraints (they're empirical facts, not value
commitments), (b) convergent validation (multiple independent historical
experiments), and (c) falsifiability (makes predictions about which
systems prove viable). Idealized procedures lack these epistemic
virtues. They trade testability for purity.

We're not opposed to idealized procedures in principle. Rawls's original
position and Korsgaard's categorical imperative procedure may well
converge with our historical findings (indeed, we'd expect them to if
properly constructed, since they aim at sustainable cooperation). But
where they diverge, we trust historical evidence over armchair
idealization. The historical record is messy, but it's data. Idealized
procedures are elegant, but they're speculation. For naturalistic
metaethics, data trumps speculation.

=== 5.6. Principled Limitations
<principled-limitations>
The Viable Evil Possibility. If a deeply repugnant system achieved
genuinely low brittleness (minimal coercive costs, stable demographics,
sustained innovation, and adaptation), our framework would acknowledge
it as viable, though not necessarily just by other moral standards.
Consider a hypothetical perfectly internalized caste system where lower
castes genuinely accept their position with minimal coercion, no
demographic stress, stable innovation, and low enforcement costs, yet
remains intuitively morally repugnant.

We accept this implication for intellectual honesty. The framework maps
pragmatic viability, not all moral dimensions. If such a system existed,
it would fall in the Pluralist Frontier, not the Negative Canon.

However, our empirical wager is that such systems are inherently
brittle. Apparent stability in historical examples like Ottoman devşirme
or Indian caste systems masked high coercive overheads, innovation lags,
and fragility under shocks (cf.~Acemoglu & Robinson 2012; Turchin 2003).
True, cost-free internalization is likely a sociological impossibility.
Oppression generates hidden costs that manifest under stress. Even
systems like #emph[Brave New World] that suppress cognitive capacities
incur massive information suppression costs (Tier 2) that cripple
long-term adaptation.

Species-Specific: Apex Network for cooperative primates like humans.
Empirical discipline, not relativism.

Floor Not Ceiling: Maps viability necessities, not flourishing
sufficiency.

Tragic Knowledge: Most reliable moral insights from catastrophic
failures. Progress real but costly.

Fallibilism: Our assessments remain provisional; historical analysis can
be contested, and new evidence may revise the Negative Canon.

=== 5.7. The Division of Moral Labor
<the-division-of-moral-labor>
A final objection concerns epistemic accessibility. Applying this
framework requires historical expertise, data analysis, brittleness
calculations, and interdisciplinary collaboration. How can ordinary
moral agents use it? Does PPR collapse into expert technocracy where
philosophers and social scientists dictate moral truth to the masses?

The Division of Labor Is Standard: In medicine, patients rely on
researchers and doctors rather than analyzing trials themselves. In law,
citizens rely on scholars and courts. In science, non-scientists accept
relativity without deriving field equations. Each domain divides
epistemic labor: specialists conduct technical analysis, practitioners
apply findings, citizens consult experts and internalize established
results. Moral inquiry operates similarly. Generating the Negative Canon
and mapping the Convergent Core requires specialist work, but
conclusions (slavery is wrong, reciprocity enables cooperation) are
publicly accessible.

Folk Morality Tracks Specialist Conclusions: Historical filtering isn't
exclusively academic. Ordinary people participate through practical
testing, cultural transmission, social critique, and collective wisdom.
Specialists systematize and validate what practical experience
discovers, not replace folk moral knowledge. This parallels how
physicists formalize principles engineers and builders have implicitly
used for millennia.

Practical Reasoning Doesn't Require Calculations: Ordinary moral agents
rarely calculate C(t) or P(t). They consult cached results: the Negative
Canon avoids catastrophic policies, Convergent Core principles guide
cooperation. For peripheral questions, experts evaluate proposals while
citizens participate democratically. For novel situations (AI ethics,
climate institutions), specialists project systemic costs to inform
public deliberation.

Against Technocracy: This isn't technocratic dictatorship because: (1)
the framework is publicly explicable; (2) expert claims are falsifiable;
(3) historical experience comes from lived practice, not elite
imposition; (4) many questions admit multiple viable answers; (5)
misdiagnoses are corrected by actual costs. Moral inquiry parallels
scientific inquiry: collaborative truth-seeking with public access and
openness to revision.

Ordinary moral agents needn't become historians to reason morally, any
more than they need physics degrees to navigate the world. Just as we
benefit from physics when building bridges, we benefit from systematic
moral inquiry when building institutions. The division of labor enables
democratic discourse.

== 6. Conclusion: The Pragmatic Craft of Building a More Viable World
<conclusion-the-pragmatic-craft-of-building-a-more-viable-world>
Pragmatic Procedural Realism reframes moral philosophy from searching
for ultimate metaphysical foundations to the ongoing, fallible craft of
pragmatic navigation. It maps normativity's 'floor' (necessary,
evidence-based foundations for successful cooperation) not its 'ceiling'
(diverse forms of flourishing). By providing a naturalistic, falsifiable
method for identifying structural principles of systemic viability, it
offers empirical grounding for more aspirational projects, steering by
humanity's enduring successes and hard-won lessons from failures. It
learns from the architecture of failure to engineer more viable
cooperation. This reframes moral inquiry as collaborative engineering
among philosophers, social scientists, and policymakers to diagnose and
debug critical social systems. Its application to contemporary
challenges (from AI ethics to institutions for global cooperation)
represents a promising direction with practical implications.

== Glossary
<glossary>
Apex Network: The complete set of maximally coherent, pragmatically
viable normative predicates, existing as a necessary structure
determined by pragmatic constraints rather than historical contingency

Brittleness: Accumulated systemic costs indicating structural fragility

C(t) (Coercion Ratio): Proportion of a system's resources dedicated to
internal coercion versus productive output

Emergent Pragmatic Coherentism (EPC): General theory of justification
grounding coherence in demonstrated viability across all inquiry domains

Fitness Trap: A locally stable but globally inefficient high-brittleness
configuration maintained by high coercive costs

Floor vs.~Ceiling: The 'floor' comprises non-negotiable viability
principles; the 'ceiling' comprises underdetermined dimensions of human
flourishing

Negative Canon: Catalogue of empirically falsified normative principles
demonstrating high brittleness

Normative Patching: Creation of ad-hoc ideological justifications
masking Tier 1 and Tier 2 costs

P(t) (Patch Velocity): Rate at which a system generates ideological
justifications to explain accumulating costs

Standing Predicate: Reusable, action-guiding normative concept
functioning as a unit of cultural transmission

Systemic Debt: Accumulated, unaddressed costs of a brittle normative
system, often paid suddenly during crisis

== References
<references>
Acemoglu, Daron, and James A. Robinson. 2012. #emph[Why Nations Fail:
The Origins of Power, Prosperity, and Poverty];. New York: Crown
Business.

Axelrod, Robert. 1984. #emph[The Evolution of Cooperation];. New York:
Basic Books.

Bagnoli, Carla, ed.~2013. Constructivism in Ethics. Cambridge: Cambridge
University Press.

Bennett, Andrew, and Jeffrey T. Checkel, eds.~2014. Process Tracing:
From Metaphor to Analytic Tool. Cambridge University Press.

Blackburn, Simon. 1993. #emph[Essays in Quasi-Realism];. New York:
Oxford University Press.

Boyd, Richard N. 1988. “How to Be a Moral Realist.” In Essays on Moral
Realism, edited by Geoffrey Sayre-McCord, 181--228. Ithaca, NY: Cornell
University Press.

Buchanan, Allen, and Russell Powell. 2018. #emph[The Evolution of Moral
Progress: A Biocultural Theory];. New York: Oxford University Press.

Conquest, Robert. 1990. The Great Terror: A Reassessment. Oxford
University Press.

Dewey, John. (1929) 1960. #emph[The Quest for Certainty: A Study of the
Relation of Knowledge and Action];. New York: Capricorn Books.

Duflo, Esther. 2012. "Women Empowerment and Economic Development."
#emph[Journal of Economic Literature] 50(4): 1051--79.

Foot, Philippa. 2001. #emph[Natural Goodness];. Oxford: Clarendon Press.

Gil Martín, Francisco, and Jesús Encabo. 2008. “Truth and Moral
Objectivity: Procedural Realism in Putnam's Pragmatism.” Poznan Studies
in the Philosophy of the Sciences and the Humanities 95: 265--285.

Glenn, Patrick. Forthcoming. "The Architecture of Failure: How Systemic
Brittleness Drives Convergent Coherence to Forge Objective Truth."

Heidler, Raphaela, et al.~2019. "Bayesian Analysis for Historians."
Historical Methods 52(3): 143--162.

Habermas, Jürgen. 1990. Moral Consciousness and Communicative Action.
Translated by Christian Lenhardt and Shierry Weber Nicholsen. Cambridge,
MA: MIT Press.

Harding, Sandra G. 2004. #emph[The Feminist Standpoint Theory Reader];.
New York: Routledge.

Henrich, Joseph. 2015. #emph[The Secret of Our Success: How Culture Is
Driving Human Evolution, Domesticating Our Species, and Making Us
Smarter];. Princeton, NJ: Princeton University Press.

Holling, C. S. 1973. “Resilience and Stability of Ecological Systems.”
#emph[Annual Review of Ecology and Systematics] 4: 1--23.

Joyce, Richard. 2001. The Myth of Morality. Cambridge: Cambridge
University Press.

Kitcher, Philip. 2011. The Ethical Project. Cambridge, MA: Harvard
University Press.

Korsgaard, Christine M. 1996. The Sources of Normativity. Cambridge:
Cambridge University Press.

Mackie, J. L. 1977. #emph[Ethics: Inventing Right and Wrong];. London:
Penguin Books.

Ostrom, Elinor. 1990. #emph[Governing the Commons: The Evolution of
Institutions for Collective Action];. Cambridge: Cambridge University
Press.

Patterson, Orlando. 1982. #emph[Slavery and Social Death: A Comparative
Study];. Cambridge, MA: Harvard University Press.

Popper, Karl. (1934) 1959. #emph[The Logic of Scientific Discovery];.
London: Hutchinson.

Putnam, Hilary. 2002. The Collapse of the Fact/Value Dichotomy and Other
Essays. Cambridge, MA: Harvard University Press.

Quine, W. V. O. 1951. “Two Dogmas of Empiricism.” #emph[The
Philosophical Review] 60 (1): 20--43.

Railton, Peter. 1986. “Moral Realism.” #emph[The Philosophical Review]
95 (2): 163--207.

Rawls, John. 1971. A Theory of Justice. Cambridge, MA: Harvard
University Press.

Rorty, Richard. 1979. #emph[Philosophy and the Mirror of Nature];.
Princeton, NJ: Princeton University Press.

Scott, James C. 1998. Seeing Like a State: How Certain Schemes to
Improve the Human Condition Have Failed. New Haven, CT: Yale University
Press.

Street, Sharon. 2006. “A Darwinian Dilemma for Realist Theories of
Value.” #emph[Philosophical Studies] 127 (1): 109--66.

Streumer, Bart. 2025. "Quasi-Realism for Realists." Philosophers'
Imprint 25 (10): 1--17.

Tainter, Joseph A. 1988. #emph[The Collapse of Complex Societies];.
Cambridge: Cambridge University Press.

Turchin, Peter. 2003. #emph[Historical Dynamics: Why States Rise and
Fall];. Princeton, NJ: Princeton University Press.

World Bank. 2012. #emph[World Development Report 2012: Gender Equality
and Development];. Washington, DC: World Bank.
