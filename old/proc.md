### **A Procedural and Naturalistic Model of Moral Objectivity**

### **Abstract**

This paper addresses the long-standing problem of moral objectivity by proposing a procedural, naturalistic model designed to dissolve the is/ought gap. We argue that this gap is an artifact of a static epistemology and introduce **Emergent Pragmatic Coherentism (EPC)**, a framework that deploys the dynamic architecture of the Network of Predicates. Citing a previously developed model of inquiry, we show how both descriptive and normative claims can be adjudicated by a unified, external standard.

Our argument proceeds in two stages. First, we articulate a rigorous, falsifiable methodology for this external validation, which we term the **Pragmatic Test**. This test is grounded in the objective measurement of a network's **Pragmatic Viability**—its long-term homeostatic efficiency, identified through empirical proxies for systemic costs. The primary tool for this analysis is the construction of a **Negative Canon**, an evidence-based catalogue of normative principles that have been historically falsified by their own unsustainable costs.

Second, we apply this framework to metaethics. We contend that moral progress is a real, observable phenomenon of systemic revision, whereby societies identify and replace predicates from the Negative Canon. This model grounds moral objectivity not in metaphysical fiat or non-cognitive attitudes, but in the observable, empirical facts about which normative systems prove most resilient and efficient. We conclude by situating EPC as a novel form of **procedural realism**, one capable of answering challenges from both error theory and quasi-realism by naturalizing the reference of our moral terms.

### **1. Introduction: Reframing the Is/ought Problem**

### **1.1 The Metaethical Impasse**

For centuries, a foundational chasm has divided our landscape of knowledge: the gap between the world of facts ("is") and the world of values ("ought"). This apparent gap, famously articulated by Hume, is more than a philosopher's puzzle; it is an intellectual crisis. It has pushed metaethics into a long and seemingly intractable stalemate, leaving us intellectually disarmed. On one side, moral realists defend the objectivity of our values, but often at the cost of invoking mysterious, non-natural properties that are incompatible with a scientific worldview. On the other, a host of anti-realist positions—from non-cognitivism to error theory—reduce our deepest normative commitments to mere expressions of emotion, subjective preferences, or a collective, systematic delusion. The cost of this impasse is profound: it cripples our ability to ground normative critique and leaves the public square vulnerable to the charge that morality is, at bottom, nothing more than arbitrary taste or tribal power.

### **1.2 A New Diagnosis: The Static Epistemology**

This paper argues that the is/ought problem is not a fundamental feature of reality, but a conceptual artifact of a static, foundationalist epistemology. The problem is not with the world, but with our tools. The gap appears insurmountable only when we treat "is" claims and "ought" claims as fundamentally different *kinds* of propositions, requiring different and incompatible modes of justification.

The solution is not to build another speculative bridge across this artificial chasm, but to adopt a different epistemology where no such gap arises in the first place. This requires a shift in our core analytical stance: from a static focus on the *content* of our claims to a dynamic focus on the *procedure* by which they are tested, validated, and revised. To do this, we will deploy the model of knowledge developed in "The Architecture of Inquiry: The Dynamics of a Pragmatic Network." That paper argued that all our collective knowledge—scientific, cultural, and practical—can be modeled as a public, evolving **Network of Predicates**, a learning architecture whose claims are validated by a two-part test of internal coherence and external, real-world success.

### **1.3 Thesis: A Procedural Solution**

This paper's thesis is that by applying this dynamic architecture to the domain of normative inquiry, we can dissolve the is/ought gap and ground a robust, naturalistic form of moral objectivity. We will show that both descriptive predicates (e.g., `...is an atom`) and normative predicates (e.g., `...is just`) are ultimately subject to the same unforgiving, amoral filter: the test of long-term **pragmatic viability**.

The argument will proceed in three stages. First, we will articulate the methodology of this **Pragmatic Test**. We will show how the abstract concept of viability can be operationalized through a multi-proxy analysis of a network’s real-world systemic costs, and we will introduce our primary epistemological tool for this analysis: the construction of a **Negative Canon** of demonstrably failed normative principles. Second, we will apply this full framework to metaethics, introducing **Emergent Pragmatic Coherentism (EPC)** as a model of moral progress—a real, observable process of "debugging" our societal code. Finally, we will situate EPC within the metaethical landscape, arguing that it represents a novel form of procedural realism, one that can preserve the objectivity of our moral claims while decisively answering the challenges of both non-naturalism and anti-realism.

### **2. The Epistemological Engine: A Summary**

Before we can construct a naturalistic model of moral objectivity, we must first have in hand a naturalistic model of inquiry itself. The framework deployed in this paper is not an ad-hoc ethical theory but the direct application of a general, dynamic epistemology. As established in "The Architecture of Inquiry: The Dynamics of a Pragmatic Network," any successful model of public knowledge must move beyond the static, psychological "Web of Belief" to a functional, structural, and evolutionary architecture (Patrick Glenn, 2025).

### **2.1 The Network as a Learning System**

The core claim of that architectural model is that our collective knowledge is best understood as a **Network of Predicates**. This public, structural network emerges from the forced convergence of countless individual webs under the relentless pressure of shared pragmatic tasks. Its core analytical unit is the **Predicate**—the reusable, functional "gene" of knowledge that is tested, selected, and propagated over time.

Crucially, this network is not a static library; it is a dynamic learning system. It develops specialized **Shared Networks** (like science or law) to solve specific problems with high efficiency. Most importantly, it learns by systematically upgrading its own processing methods through a mechanism we term the **Functional Transformation**, wherein its most successful and well-established propositions are repurposed from mere data into new, internal coherence-testing rules. This provides a plausible, non-teleological mechanism for how our collective maps of the world achieve cumulative and progressive success.

### **2.2 The Unfinished Task: Grounding the Network**

A central conclusion of the architectural model is that any claim to "truth" is justified by a two-part, procedural test. A proposition must satisfy both an internal and an external condition:

1. **The Internal Condition (Coherence):** The proposition must demonstrate robust logical and explanatory fit within its relevant Shared Network.
2. **The External Condition (Pragmatic Viability):** The network itself must be externally validated by its demonstrated long-term success in navigating the real world.

The previous paper was dedicated to a full explication of the *internal* condition. It detailed the rich, complex architecture of coherence, consistency, and systemic learning. However, it left the *external* condition—what we called the **Pragmatic Anchor**—as a necessary but largely unanalyzed requirement.

This leaves us with a powerful but incomplete picture: an engine without a chassis, a map without a territory. An ungrounded network, no matter how internally coherent, risks becoming a self-validating fantasy, vulnerable to the classic charge of relativism.

The primary task of this paper, therefore, is to complete the project. We will now articulate a rigorous, falsifiable methodology for the external test of pragmatic viability. By moving from the internal logic of coherence to the external test of real-world consequences, we will forge the final and most crucial tool necessary to ground a naturalistic and objective ethics.

### **3. The Pragmatic Test: A Methodology for External Validation**

The two-part test for truth requires that a coherent network also be externally validated by the real world. This section articulates the methodology of that external test—the **Pragmatic Test**. This is not a single experiment, but a programmatic approach to a posteriori inquiry, drawing on the methods of complex systems science and comparative history. It is designed to provide a rigorous, empirical, and non-circular standard for objectively ranking the success of different knowledge networks, be they scientific or normative, by analyzing their historical performance.

### **3.1 Defining Pragmatic Viability**

The ultimate metric of the Pragmatic Test is not "truth" in a metaphysical sense, nor is it "survival" in a crude, Social Darwinist sense. It is **Pragmatic Viability**. We define this as a measure of a system's **homeostatic efficiency**: its ability to maintain and propagate its core informational structure over time with low internal friction and minimal coercive and energetic cost.

The distinction between pragmatic viability and **mere endurance** is critical for immunizing the theory against the "might makes right" objection. An oppressive empire that lasts for a millennium through brutal force is *enduring*, but it is not *viable*. It is a high-cost, inefficient, and **brittle** system. Its apparent stability masks a profound internal fragility, like a badly designed engine that runs only by consuming vast quantities of fuel and being constantly repaired. A viable network, by contrast, maintains its stability through efficient, voluntary coordination, like a well-designed engine that produces maximum output with minimal waste. Pragmatic Viability is therefore a measure of a network's systemic health and adaptive efficiency, not just its longevity.

### **3.2 Operationalizing Viability: A Causal Hierarchy of Systemic Costs**

To move "viability" from an abstract concept to an operational one, we must identify its empirical signatures. The core claim of this methodology is that a network's lack of viability manifests as measurable **Systemic Costs**. These costs are the energetic price a network must pay to manage the friction generated by its misalignment with the constraints of reality. We propose a multi-proxy, programmatic methodology for identifying these costs, which exist in a causal hierarchy.

1. **First-Order Costs:** These are the most direct, material consequences of a network's dysfunction—the "friction burns" generated by a poorly designed system where its predicates conflict with the constitutive drive to endure. They are the primary, objective data for our analysis and can be identified through metrics such as:
    - Excess mortality and morbidity rates (measurable via demographic and bioarchaeological data).
    - Frequencies of internal violent conflict (riots, rebellions, purges, state-led terror).
    - Systemic resource depletion and widespread malnutrition.
2. **Systemic Costs (Energetic & Informational):** These are the secondary, non-productive costs a network must pay to *manage* its First-Order Costs. These costs are causally downstream but diagnostically crucial. Proxies include:
    - **Energetic Costs:** The proportion of a society's total resources (labor, GDP) allocated to non-productive internal control, such as state security, surveillance, and propaganda, rather than to productive capacity or resilience-building.
    - **Informational Costs:** The costs incurred by actively suppressing corrective feedback, measured by things like censorship laws, persecution of dissenters, and innovation lags in critical sectors.

A consistent pattern of high costs across these proxies provides evidence-based, falsifiable grounds for diagnosing a network's low pragmatic viability. The causal link is key: networks that generate high First-Order Costs (e.g., widespread suffering) must typically pay high Systemic Costs (e.g., a large secret police force) to manage the resulting dissent.

### **3.3 The Primary Tool: Constructing the Negative Canon**

Our epistemic access to what constitutes a "maximally viable" network is indirect and fallible. Following Karl Popper (1959), our most reliable objective knowledge is not of what is definitively true, but of what is demonstrably false. Therefore, the primary empirical project of this methodology is a negative one: the construction of a **Negative Canon**.

The Negative Canon is a robust, cross-cultural, and evidence-based catalogue of predicates that have been empirically demonstrated to reliably generate catastrophic First-Order Costs and normative brittleness across multiple historical contexts. The predicate `slavery is an acceptable organizing principle` belongs in this canon. The historical record, as documented by scholars from Patterson (1982) to modern cliodynamicists, provides overwhelming evidence that networks predicated on this principle consistently generate immense costs and are thus inherently brittle. By meticulously mapping these failures, we empirically chart the boundaries of pragmatic possibility. The Negative Canon serves as our most reliable, objectively-grounded guide to which "design principles" for a society have been decisively falsified by history.

Crucially, the data signaling these failures is generated most intensely at the points of greatest systemic friction. This provides a purely pragmatic, non-moralistic justification for the central insight of **standpoint epistemology** (Harding 2004). The testimony of the marginalized is treated as **epistemically privileged**, not as a concession to moral sentiment, but as a **diagnostic necessity**. Those experiencing the highest First-Order Costs have the most direct and unmediated access to the data signaling a network's impending failure. Their dissent is not noise to be managed; it is the primary signal that a network’s core predicates are misaligned with reality, making their perspective essential for accurately charting the wreckage.

### **3.4 Methodological Challenges and Scope Conditions**

The Pragmatic Test is a powerful philosophical tool, but its application is not a simple algorithm. A candid acknowledgment of its methodological challenges is essential for defining the scope and nature of its claims.

1. **Confounding Variables:** A network's failure can result from countless factors beyond its normative structure, such as climate shocks, novel diseases, or foreign invasion. A simplistic attempt to prove "predicate X *caused* empire Y to fall" would be a caricature of this methodology. The framework's power lies not in single-case causal proofs, but in **probabilistic, comparative inference**. By analyzing multiple, independent historical cases across different environments and technological contexts (as in the work of Tainter 1988 or Turchin 2003), we can identify robust patterns. If networks predicated on chattel slavery consistently exhibit high coercive costs and internal fragility across different continents and centuries, we gain high confidence that the predicate itself is a primary source of dysfunction.
2. **Selection Bias:** The historical and archaeological records are profoundly biased toward large, complex, state-level societies that left durable traces. The Negative Canon is therefore necessarily built from the record of these systems. This defines a crucial **scope condition**: the EPC framework is most powerful for diagnosing the failure modes of *large-scale, complex societies*. Its claims about the full spectrum of successful small-scale, non-state societies must be more circumspect. This is a feature, not a bug, that keeps the theory from over-reaching and respects the limits of its evidentiary base.

### **3.5 Falsification Conditions**

This entire methodological framework is an empirical hypothesis and must itself be falsifiable. The central causal claim is that a predictable, statistically significant relationship exists between a network's internal costs and its long-term viability. This framework would be falsified if broad, cross-cultural historical analysis, accounting for the challenges above, revealed any of the following:

1. **No Correlation Between Costs:** If there were no predictable, statistically significant correlation between high First-Order Costs and high Systemic Costs.
2. **The Superiority of High-Cost Systems:** If high-cost, coercive networks were shown to be *more* innovative, adaptable, and resilient to external shocks over the long term than low-cost, cooperative networks when controlling for confounding variables.
3. **The Failure of the Negative Canon:** If predicates in the proposed Negative Canon (e.g., `slavery is acceptable`) were found in multiple, controlled historical comparisons to *enhance* a network's long-term resilience and adaptive efficiency.

If the historical record showed high-friction systems to be just as resilient and adaptable as low-friction ones, the model's central causal claim would be broken, and the theory itself would be falsified.

### **4. Emergent Pragmatic Coherentism (EPC): A Model of Moral Objectivity**

With the architecture of inquiry and the methodology of the Pragmatic Test in place, we can now deploy this full framework to the domain of metaethics. The result is a model we term **Emergent Pragmatic Coherentism (EPC)**. EPC reframes moral inquiry not as a search for mysterious, non-natural properties, but as a fallible, empirical project of engineering more viable systems of social cooperation. It argues that moral truth is not found, but forged in the crucible of history.

### **4.1 Moral Claims as Testable, Functional Predicates**

The foundational move of EPC is to apply the deflationary analysis of knowledge to normative claims. Moral terms like "...is wrong," "...is just," or "...is a virtue" are treated as **predicates**—functional, reusable tools that ascribe a normative property to a subject. From this procedural perspective, the is/ought gap dissolves at the level of analysis. Both descriptive predicates (`...is an atom`) and normative predicates (`...is wrong`) are formally identical types of operators. Their "truth" is therefore adjudicated by the exact same, unified, two-part test: a normative proposition is "true" if it is coherent within a Shared Network of norms that has itself demonstrated long-term pragmatic viability.

This does not imply that the *process* of testing a moral claim is identical to testing a simple physical claim. The feedback loop for "water boils at 100°C" is rapid and direct. The feedback loop for "autocracy is an unstable form of government" is slow, diffuse, and plays out over generations. This observed asymmetry is not a flaw in the model but a direct prediction of it. The complexity of the system being tested determines the timescale and texture of the pragmatic feedback. The authority of an "ought" is therefore harder-won, emerging not from a simple experiment, but from the immense, procedural weight of historical evidence.

### **4.2 Moral Progress as Non-Teleological "Debugging"**

EPC provides a robust, non-teleological account of moral progress as a real and observable phenomenon. Moral progress is the process of **"debugging" our societal code**: the identification and removal of predicates from the **Negative Canon** from our society's operative shared normative network. It is not a march toward a pre-ordained utopian endpoint ("teleology"), but a backward-looking process of learning from and correcting catastrophic failures.

The abolition of slavery serves as the paradigm case study. A critic might argue that slavery-based empires were highly successful and endured for millennia, thus passing the pragmatic test. This objection rests on the crucial error of conflating mere endurance with pragmatic viability.

- **The Bug:** The predicate `slavery is an acceptable organizing principle` was a core component of the normative networks of many societies.
- **The Pragmatic Costs:** As predicted by our methodology, this predicate generated immense and unsustainable **First-Order Costs** (systemic violence, constant threat of revolt, suppressed innovation) and massive **Systemic Costs** (the energetic expenditure on the coercive apparatus of control). These are not modern judgments projected onto the past; they are descriptions of the systemic friction that these societies themselves had to constantly and expensively manage.
- **The Debugging Process:** The moral arguments of abolitionists were not just appeals to a novel sentiment. They were, in effect, arguments that the incumbent normative network was catastrophically inefficient and brittle. They proposed a "patch": the replacement of the predicate `slavery is acceptable` with `slavery is wrong`. This new proposition succeeded not because it was more emotionally appealing, but because it proposed a solution to the immense and growing pragmatic costs the incumbent network was generating.
- **The Result:** The eventual triumph of abolition was not a mere change of opinion. It was a profound act of systemic debugging. A demonstrably failed organizing principle, a predicate firmly in the Negative Canon, was identified and removed. We can state with objective confidence that this was not just a change, but *progress*, because the resulting network was significantly more pragmatically viable—more stable, efficient, and resilient.

### **4.3 The Structure of Moral Knowledge: Core, Periphery, and Ambiguity**

This model does not predict a single, uniquely correct answer for every moral question, thereby avoiding the charge of dogmatic absolutism. Instead, it predicts a structured moral landscape with distinct zones. Crucially, the boundaries of these zones are an **ongoing and fallible empirical question**, not an a priori declaration.

1. **The Convergent Core:** This consists of a set of universally viable normative predicates that represent stable solutions to universal coordination problems. Norms of basic reciprocity, for example, are a likely candidate. Their recurrent, independent emergence across vastly different cultures suggests not mere coincidence, but the repeated discovery of a game-theoretically stable and highly viable "design principle" for human societies (Axelrod 1984). The core may be smaller than our modern intuitions suggest, but its existence is a strong empirical prediction of the model.
2. **The Pluralist Periphery:** This accounts for legitimate and persistent cultural disagreements without collapsing into relativism. It identifies domains where multiple, culturally-specific yet *equally viable* solutions may exist. For instance, a well-regulated capitalist welfare state and a robust social democracy may represent distinct but demonstrably viable strategies for organizing a complex modern economy. The ongoing debate between them is not a sign that one must be objectively wrong, but a pragmatic negotiation between different sets of second-order costs and benefits.

This structure allows EPC to ground a robust cultural critique. The judgment that a practice like "honor killings" is objectively wrong is not rooted in the superiority of one culture's values, but in the empirical evidence of its catastrophic **First-Order Costs** (elevated violence, social fragmentation). It is a failed predicate that belongs in the **Negative Canon**, not a viable option within the Pluralist Periphery.

### **4.4 Adjudicating Ambiguous Cases**

The framework's true test lies in ambiguous cases where the pragmatic costs and benefits are not as clear-cut as with slavery. For these cases, EPC's value is as a **diagnostic, not a dogmatic, tool**. It does not provide easy answers but clarifies the empirical questions we should be asking.

- **Religious Systems:** A religious system that provides meaning and social cohesion (a viability-enhancing benefit) while also imposing costs (e.g., through violent inquisitions or the suppression of scientific inquiry) presents a complex case. The EPC framework provides the tools to analyze the **net effect**: does the social cohesion it provides outweigh the informational and other costs it generates, leading to a more resilient system overall compared to its secular alternatives in a similar context? The answer is an empirical one, to be investigated on a case-by-case basis.
- **Democratic Transitions:** A transition to democracy often increases short-term instability (a high cost). The EPC analysis would frame this not as a failure, but as a high-cost "investment" made to "debug" a less viable autocratic system. The ultimate test is empirical: does the new democratic network, once stabilized, demonstrate greater long-term viability (lower costs, higher resilience) than the system it replaced?

By framing these complex issues in terms of measurable, if difficult, pragmatic consequences, EPC moves them from the realm of intractable value-clashes into the realm of empirical, historical inquiry.

### **5. Defending the Model: Answering Core Objections**

With the EPC framework fully articulated, we must now test its resilience against the most pressing critical objections. The model's ability to provide robust, non-circular answers to these challenges strengthens the case for its adoption as a coherent naturalistic framework for moral objectivity. We will address four: the stability of evil, the power of ideology, the asymmetry of testing, and the fundamental grounding problem.

### **5.1 Objection: The Stability of Evil ("Might Makes Right")**

The most common objection to any evolutionary or pragmatic ethical theory is that it collapses into a form of "might makes right," thereby justifying the oppressive practices of any society that manages to endure. An opponent might argue: "An oppressive society endures for millennia. Hasn't its network 'won' the pragmatic test, thereby making its oppressive predicates objectively true by EPC's standards?"

This objection, however, rests on two crucial errors that the EPC framework is specifically designed to correct: it mistakes **mere endurance** for the richer, more demanding concept of **pragmatic viability**, and it misidentifies the unit of selection.

First, the ‘stable evil’ society is not a coherent network but a system defined by the costly conflict between the oppressive network of the dominant and the resistant networks of the subjugated. The predicates of the oppressed are an ineliminable part of the total emergent structure, and their persistent dissent is the primary source of the system's friction.

Second, the objection conflates temporary stability with long-term viability. As defined in Section 3.1, viability is a measure of a system's homeostatic efficiency, not its sheer longevity. An oppressive state that persists through immense coercion is not viable; it is a high-cost, inefficient system. Its longevity is not a sign of strength but a measure of the immense energy it must burn to manage its own self-inflicted instability. Such systems are often functionally parasitic, their endurance subsidized by external extraction (e.g., conquest or resource windfalls) that masks their internal normative decay. Historical analysis reveals that seemingly "stable" oppressive systems follow predictable decay patterns: (1) escalating coercive costs as resistance adapts; (2) information degradation as suppression blocks crucial feedback; and (3) institutional brittleness as adaptive capacity atrophies. Their endurance does not make their predicates true; it merely makes them long-running failed experiments whose wreckage we chart to build our **Negative Canon**.

### **5.2 Objection: The Power of Ideology**

A more sophisticated objection concedes that oppressive systems generate pragmatic costs but argues that **ideology** can co-opt the revision process. A network, it is argued, can create powerful "patch" predicates (e.g., “your suffering is a noble trial,” “the state is infallible”) that convince agents to endure, and even embrace, pragmatic failure rather than revise the network's core. Can’t a sufficiently powerful ideology effectively override the pragmatic test?

This objection correctly identifies a key tactic in network evolution, but it underestimates the non-negotiable price of such tactics. Ideological patches are never pragmatically free. They introduce cascading **Systemic Costs** of their own, making them a powerful diagnostic indicator rather than a genuine counter-example. The EPC framework analyzes ideological patches in two ways:

1. **Direct Systemic Costs:** While ideology can mask the *symptoms* of pragmatic pushback, it cannot eliminate the underlying dysfunction. To maintain a predicate that is generating high First-Order Costs, a network must invest heavily in managing those costs. This requires a vast and expensive apparatus of ideological maintenance: state-controlled media for propaganda, a surveillance state to monitor dissent, and a coercive police force to enforce conformity. These are not just theoretical problems; they are measurable **Energetic and Informational Costs** that divert immense resources from productive capacity and resilience-building.
2. **The Epistemic Red Flag:** The very need for costly ideological insulation is itself a primary, present-tense diagnostic indicator of low pragmatic viability. It provides an urgent epistemic red flag: **we have a powerful pragmatic reason to be deeply skeptical of any network that requires immense and continuous energy expenditure to suppress dissent and explain away suffering.** The presence of these defense mechanisms is a powerful epistemological signal that the network’s core predicates are misaligned with reality. It gives us a reason to heed dissenters *now*, as they possess the epistemically privileged data signaling long-term systemic failure. A healthy, viable network does not need to spend a third of its budget convincing its citizens that they are not suffering.

A paradigmatic example is the Soviet Union's enforcement of Lysenkoism; the ideological rejection of genetics incurred catastrophic First-Order costs in the form of failed agricultural policies and famine. To maintain this flawed predicate, the state had to pay immense Systemic Costs in the form of propaganda and the brutal suppression of dissenting scientists, demonstrating the tangible, non-negotiable price of a flawed ideological patch.

### **5.3 Objection: The Asymmetry of Testing**

A third objection targets the claim of a unified test for all predicates. An opponent might argue that the test for an empirical claim (“water boils at 100°C”) is immediate, decisive, and universal, while the test for a moral claim (“slavery is unacceptable”) is slow, diffuse, contested, and unfolds over generations. This asymmetry, it is argued, is so great that it renders the claim of a unified pragmatic test meaningless.

This observed asymmetry, however, is not a flaw in the model but a **direct and crucial prediction of it**. EPC posits a unified *filter*—pragmatic selection—while fully acknowledging that the *complexity of the system being tested* determines the timescale and texture of the feedback.

- An empirical predicate like `...boils at 100°C` is a hypothesis about a **simple physical system**. Such systems have very few variables and produce rapid, clear, and easily replicable feedback loops.
- A moral predicate like `...is an unjust form of social organization` is a hypothesis about structuring a **complex adaptive social system**. Such systems have countless interacting variables, feedback loops that are distributed, holistic, and often delayed, and outcomes that are subject to historical contingency.

The difference in velocity and clarity of the feedback does not imply a difference in the kind of the ultimate arbiter (reality's feedback). It is an expected consequence of the complexity of the subject matter. This asymmetry explains precisely why moral knowledge is so much harder-won than simple empirical knowledge. It requires the aggregation of vast amounts of historical data—the very project of constructing the Negative Canon. This does not undermine the claim that both are ultimately adjudicated in the same objective, pragmatic court; it simply underscores the immense methodological challenge of reading the verdict for our most complex and important claims.

### **5.4 The Grounding Problem Revisited**

The final and most fundamental challenge is that the theory appears circular. It seems to rest on a smuggled normative premise—the ultimate value of endurance—and cannot justify why we *ought* to care about what survives its pragmatic filter. This objection, while powerful, misunderstands the model's explicit **descriptive and conditional turn**. The EPC framework answers this challenge with a two-part defense that separates the grounding of the *arena* of justification from the source of *normative force* within it.

First, the **transcendental defense** establishes endurance not as a value to be chosen, but as the **constitutive precondition** for inquiry itself. Its primacy is established by a form of **naturalized transcendental argument**.

- It is *transcendental* in that it identifies an inescapable precondition for the possibility of evaluation: a system that does not persist cannot evaluate, revise, or remember anything. Any form of inquiry, from science to ethics, is a practice that unfolds over time and thus presupposes the continuity of its own informational substrate.
- It is *naturalized* in that this precondition is not a feature of a disembodied "pure reason" (as in Kant), but a physical and structural requirement for any real-world, information-bearing system. This gives the "drive to endure" a **dual-aspect status**: descriptively, it is an observable evolutionary fact that non-persistent systems vanish; transcendentally, it is the inescapable condition that makes having such facts possible at all.

This argument's purpose is not to prove that "endurance is good." Its purpose is to answer the question, "Why is endurance the ultimate filter, rather than justice, or flourishing, or any other value?" The answer is that endurance is the **inescapable arena** in which the viability of all other values is tested. Justice and flourishing are predicates *within* the network whose long-term viability is subject to the pragmatic test; endurance is the non-negotiable structural property of the testing ground itself. Its authority is physical, not moral, analogous to the authority of gravity over architecture. An architect does not need to *value* gravity, but they must work within its constraints for any other value (beauty, utility) to be realized.

Second, with endurance established as the non-normative arena, the **instrumental defense** provides the conditional normative force. The "ought" that emerges from EPC is a **wide-scope, instrumental, and system-level ought**:

> If a system is engaged in the project of inquiry (and thus, per the transcendental argument, is already engaged in the project of enduring), then it has a powerful, evidence-based, strategic reason to adopt predicates that have been demonstrated to lead to high pragmatic viability and avoid those in the Negative Canon.
> 

This "ought" is purely instrumental and non-categorical. It does not command commitment from the outside. It is a piece of strategic advice, mapping the consequences for any system with a de facto commitment to persistence. This two-part defense provides a non-circular and robust grounding: the transcendental argument identifies the non-normative, inescapable arena of justification, and the instrumental argument provides the pragmatic, non-question-begging normative force for any agent or system operating within that arena.

### **6. Situating EPC in the Metaethical Landscape**

Emergent Pragmatic Coherentism is not an isolated proposal but a synthesis that occupies a unique and powerful position in the philosophical landscape. This section will situate the EPC model by contrasting it with major rival accounts, demonstrating its distinct advantages in grounding a robust, anti-metaphysical moral realism.

### **6.1 A Form of Procedural, Pragmatic Realism**

EPC is a form of **procedural, pragmatic realism**. It is *realist* in positing objective, mind-independent truths about the viability of normative systems. It is *procedural* in grounding these truths not in mysterious, *sui generis* moral properties, but in the observable, empirical outcomes of an evolutionary process. The "truth-makers" for moral claims are the objective facts about which networks prove most viable against the real-world constraints of pragmatic selection. The validity of a moral predicate is thus an emergent, relational property, much as the ‘fitness’ of an organism is a real, objective property of its relationship with an environment.

This external, systems-level focus distinguishes EPC from other influential naturalist realisms. **Cornell Realists**, for instance, ground moral facts in agent-centric properties, such as an individual's "objective interests" (Railton 1986). EPC’s framework is more robustly externalist: the viability of a network is independent of the interests of any agent within it. A tyrant’s network may perfectly serve his objective interests while being a catastrophic evolutionary failure due to the immense systemic costs it generates, making EPC capable of objectively condemning a "successful" tyrant.

### **6.2 The Quinean and Pragmatist Lineage**

The EPC framework is best understood as the systematic completion of the **Quinean project**, itself a direct heir to **American Pragmatism**. Quine’s (1951) demolition of the analytic/synthetic distinction unified all descriptive claims within a single pragmatic web. EPC extends this holism one decisive step further, showing that normative claims are subject to the same filtering process. Where Quine mapped the static architecture of an individual’s web, EPC supplies the missing piece: the **dynamic, evolutionary engine** of pragmatic selection. In doing so, it fulfills John Dewey’s (1929) project of a naturalized, experimental ethics and provides a naturalistic answer to C.S. Peirce’s “end of inquiry”: the Maximally Viable Network is not a predetermined ideal limit, but the messy, emergent result of what has *actually survived* our collective experiment. This realism distinguishes EPC from the anti-representationalism of neopragmatists like Richard Rorty (1989). Where Rorty replaces objectivity with solidarity, EPC argues that lasting solidarity is an **emergent property** of a network that has achieved objective pragmatic success.

### **6.3 Answering the Anti-Realist Challenge**

EPC's procedural realism provides powerful, naturalistic responses to the most significant anti-realist challenges.

- **vs. Error Theory (Mackie 1977):** We are in full agreement with Mackie's "argument from queerness": there are no strange, non-natural properties in the fabric of the world. However, we deny his conclusion that all positive moral claims are therefore false. EPC offers a diagnosis of **successful but mis-glossed reference**. We argue that the error theorist mistakes the folk-metaphysical *gloss* for the underlying functional *reference*. Our moral discourse has been successfully, if imperfectly, tracking real, procedural facts about the viability of social arrangements all along. The "wrongness" of slavery is not a queer property but a relational, empirical property of a predicate that reliably generates systemic costs. EPC thus aims to **naturalize the reference** of moral terms, not eliminate them.
- **vs. Non-Cognitivism and Quasi-Realism (Blackburn 1993):** The ingenious project of quasi-realism is a brilliant solution to a problem that our framework dissolves. We contend that the realist-sounding grammar of our moral claims is not something our attitudes *earned*, but something our language *evolved to do*. This grammar is sound because it has always been, however imperfectly, **tracking an objective, procedural reality of pragmatic viability.** The EPC explanation is superior because it better explains the high stakes of moral disagreement (it's an empirical dispute about systemic viability, not just a clash of wills) and provides a more robust account of moral error (an entire community can be objectively wrong if its network is unviable).
- **vs. Evolutionary Debunking Arguments (Street 2006):** Sharon Street's "Darwinian Dilemma" poses a formidable challenge to moral realism. She argues that evolutionary pressures have shaped our moral beliefs to track adaptiveness, not mind-independent moral truths. For a realist to hold that these two—adaptiveness and truth—just so happen to coincide is an untenable coincidence. EPC resolves this dilemma by collapsing one of its horns. For EPC, **adaptive viability is not a coincidental correlate of the moral truth; it is the truth-maker itself.** Predicates that enhance the pragmatic viability of a network are objectively "better" in a procedural sense precisely because they are better adapted to the constraints of reality. Evolution is not a distorting influence that a realist must explain away; it is the very filtering process that grounds objectivity, transforming the debunking argument into a causal explanation for a procedural realism.

### **6.4 An Externalist Alternative to Other Naturalisms**

Finally, EPC's systems-level externalism distinguishes it from other influential naturalist rivals.

- **vs. Neo-Aristotelianism (Foot 2001):** The Neo-Aristotelian project grounds normativity in a thick, teleological concept of species-specific "flourishing." The unit of evaluation is the organism. EPC shifts the unit of selection to the **informational network itself**. Its metric is the thin, procedural standard of viability, not a thick concept of "human goodness." This provides a crucial advantage: EPC's thinner standard is less culturally parochial and can robustly account for the **Pluralist Periphery**, where multiple, distinct forms of life may prove equally viable, so long as they do not generate catastrophic First-Order Costs.
- **vs. Moral Progress Literature (Buchanan & Powell 2018):** Many accounts of moral progress struggle to provide a non-teleological mechanism. EPC provides a specific one: progress is a "debugging" process driven by the eliminative pressure of pragmatic failure. It is a purely **backward-looking process of charting failures**, not a forward-looking process aimed at a final, pre-defined goal.

To conclude, EPC occupies a unique "third way." It is a form of moral realism, but it is staunchly naturalistic and anti-metaphysical. It is a form of pragmatism, but one that grounds a robust objectivity. It is an evolutionary theory, but one that avoids relativism and provides a powerful response to debunking arguments.

| View | Truth-makers | Unit of Evaluation | Method | Objectivity |
| --- | --- | --- | --- | --- |
| **EPC** | External viability facts (low cost, high resilience) | Informational network | Comparative, failure-driven empirics | Procedural, fallibilist, externalist |
| **Quasi-realism** | Attitude-dependent projection | Attitudes/practices | Semantic explanation | Deflated, grammar-vindicating |
| **Cornell Realism** | Natural properties of human good | Agent interests | Reflective equilibrium + science | Robust, agent-centered |
| **Neo-Aristotelianism** | Flourishing of organism | Life-form | Teleological evaluation | Robust, teleological |
| **Constructivism** | Idealized procedures | Rational agents | Hypothetical agreement | Procedural, internalist |

### **7. Conclusion: Inquiry as a Pragmatic Project**

This paper began with one of philosophy's oldest chasms: the gap between "is" and "ought." We have argued that this chasm is not a feature of reality, but an artifact of a static epistemology that fails to see inquiry as what it has always been: a single, multi-generational, and fundamentally pragmatic project. The goal of this project, whether in science, politics, or ethics, is the construction of ever more viable maps for navigating a shared reality.

The core argument of Emergent Pragmatic Coherentism is that the is/ought problem is dissolved by a unified theory of inquiry. It reframes all claims as the application of **predicates** within a public **Network**. Both descriptive predicates (`...is an atom`) and normative predicates (`...is just`) are ultimately adjudicated in the same ultimate court of long-term pragmatic selection, where the final measure is not mere persistence, but efficient, resilient **viability**.

This framework offers a naturalistic model for a robust, procedural objectivity. We can be humble in knowing that our current maps are imperfect and that our access to the lessons of history is fallible. Yet we can be hopeful, because moral progress is a real, observable phenomenon. We can state with an objectivity grounded in empirical evidence that abolishing slavery was not a mere change of opinion; it was a profound act of **debugging our societal code**—the identification and removal of a demonstrably failed organizing principle from our **Negative Canon**.

Progress occurs when we treat suffering, dissent, and instability not as mere political problems to be managed, but as primary epistemological data. They are the "check engine" light for society, signaling a misalignment between our normative network and the constraints of reality. This gives us a powerful, pragmatic reason to listen to the marginalized, as they possess the most crucial information about where our collective map is wrong.

The authority of an "ought," therefore, is not found in a mysterious metaphysical foundation, a non-natural realm of values, or the structure of rational agency alone. It lies instead in the immense, procedural weight of historical evidence. An "ought," on this view, is a time-tested predicate coherent with our most pragmatically resilient networks—a hard-won empirical signal that it represents a viable strategy for the shared project of human endurance. EPC reframes moral philosophy from a search for ultimate foundations to the ongoing, fallible craft of pragmatic navigation: the art of steering by the light of humanity's most enduring successes and the hard-won lessons from the wreckage of its failures.

### Glossary

**Convergent Core:** A set of universally viable normative predicates that represent stable, independently discovered solutions to universal coordination problems (e.g., norms of reciprocity). It is identified empirically through the **Test of Independent Convergence** and represents the set of non-negotiable "engineering principles" for any enduring human society.

**Emergent Pragmatic Coherentism (EPC):**
The name for the full metaethical model developed in this paper. It synthesizes a dynamic, coherentist epistemology with a pragmatic, evolutionary test for viability to ground a procedural and naturalistic form of moral objectivity.

**First-Order Costs:**
The direct, material, and measurable consequences of a network's dysfunction, representing the friction between its predicates and the constraints of reality. These costs (e.g., excess mortality, systemic violence) are the primary, objective data for the **Pragmatic Test** and are the direct **cause** of secondary **Systemic Costs**.

**Negative Canon:**
The primary epistemological tool of the Pragmatic Test. It is a robust, cross-cultural, and evidence-based catalogue of normative predicates (e.g., "slavery is acceptable") that have been empirically demonstrated to reliably generate catastrophic **First-Order Costs**. It serves as our most reliable, fallible map of the boundaries of the **Maximally Viable Network**.

**Normative Brittleness:**
A network's inherent vulnerability to external shocks. It is a direct and predictable consequence of high **Systemic Costs**, as the immense energy spent on internal control diverts resources from adaptation and innovation. It is the formal inverse of resilience and a key indicator of low **Pragmatic Viability**.

**Pluralist Periphery:**
The set of culturally-specific but demonstrably viable normative systems. It accounts for legitimate moral disagreements by identifying domains where multiple, different, yet equally viable solutions exist. Its boundaries are defined by the **Negative Canon**, thus preventing a collapse into relativism.

**Pragmatic Anchor:**
The fundamental principle that grounds the entire system and prevents a slide into relativism. It is the requirement that a network's internal **Coherence** must be constantly disciplined by its external efficacy, as measured by the **Pragmatic Test**.

**Pragmatic Test:**
The overall methodology for the external validation of a knowledge network. It is a programmatic, empirical approach that **operationalizes the Pragmatic Anchor** by assessing a network's **Pragmatic Viability** through a multi-proxy analysis of its **Systemic Costs** over time.

**Pragmatic Viability:**
The objective, external metric of a network's success, distinct from **mere endurance**. It is a measure of a system's **homeostatic efficiency**: its ability to maintain and propagate its core informational structure over time with low internal friction, as measured by its total **Systemic Costs**.

**Procedural Realism:**
The metaethical stance of EPC. It is *realist* in that it posits objective, mind-independent truths about which normative systems are superior. It is *procedural* in that these truths are grounded not in metaphysical properties but in the objective, empirical outcomes of the evolutionary **Pragmatic Test**.

**Systemic Costs:**
The secondary, non-productive energetic and informational costs a network must pay to manage its **First-Order Costs**. This includes resources spent on coercion, propaganda, and the suppression of dissent. The total systemic cost of a network is the primary inverse indicator of its **Pragmatic Viability**.