# **Systemic Viability and the Dynamics of Coherence: A Naturalistic Approach to Objectivity**

## **Abstract**

W.V.O. Quine’s “Web of Belief” offers a powerful holistic epistemology but is vulnerable to the classic isolation objection: a perfectly coherent network of beliefs could be a shared delusion, detached from reality. This paper argues that Quine’s static model lacks the dynamic mechanisms to explain how knowledge is disciplined by the world. We resolve this by introducing a naturalistic externalist check: long-term pragmatic viability.

Our model proposes that knowledge systems are tested by the real-world costs generated when their core ideas are applied—a process of pragmatic selection. To analyze this, we develop a framework for assessing a system’s *brittleness*, its vulnerability to stress, by tracking observable costs—from failed predictions to institutional decay. This evolutionary process is driven by two key dynamics: systems learn from failure by empirically mapping what is unviable, and they learn from success by entrenching their most effective, cost-reducing discoveries as core principles for future inquiry.

This leads to a form of *systemic externalism*, where a claim’s justification depends not only on its internal coherence but on the demonstrated historical resilience of the entire public system that certifies it. This framework explains how our fallible knowledge systems are forced to converge on what we term the Apex Network: not a pre-existing blueprint of truth, but the real, emergent structure of viable solutions discovered retrospectively through the historical filtering of what fails. Its objectivity is grounded in mind-independent pragmatic constraints.

The result is a three-level framework for truth that distinguishes mere contextual coherence from justified belief within a resilient system, and objective truth as alignment with this emergent structure of viability. By providing the missing dynamism for Quine’s web, our model explains how the practical project of cultivating more resilient and effective problem-solving systems becomes a self-correcting process for generating objective knowledge. It yields a falsifiable, interdisciplinary research program for assessing the health of our most critical epistemic systems, from scientific paradigms to the networks that structure public discourse.

## **1. Introduction: From a Static Web to a Dynamic Process**

In the 19th century, two rival theories competed to explain the spread of diseases like cholera. The dominant miasma theory held that disease was caused by "bad air" arising from decay, while the challenger, germ theory, posited that invisible microorganisms were the culprits. We now consider the triumph of germ theory a textbook case of scientific progress. But on what grounds do we justify this judgment? A sophisticated miasma theorist could have constructed a system as internally coherent as germ theory. How, then, can we defend our choice without simply appealing to our own network's standards—a move that would leave us vulnerable to the charge of circularity?

This paper argues that the answer lies not in static coherence, but in analyzing the long-term pragmatic viability of these competing systems. The miasma network was demonstrably *brittle*; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad-hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This perspective reveals a deeper truth about how knowledge evolves. Inquiry is not a search for ultimate foundations but an ongoing, evolutionary process of cultivating more viable, less fragile public knowledge systems. This paper develops a model to explain this process. It is a macro-epistemology, a theory about the long-term viability of public, cumulative systems like science and law. The model's reliance on evolutionary analogies is specific: it proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than a purely Darwinian one of blind selection, to account for the intentional nature of inquiry. A crucial distinction must be made to pre-empt a common critique: a system's viability is not mere longevity or "survival of the fittest" in a social Darwinist sense. A brutal empire that persists through coercion is not a viable system in our terms, but a textbook example of a high-brittleness one, whose endurance is merely a measure of the immense energy it wastes to suppress its own self-inflicted instability. Viability is defined here as the capacity to adapt and solve problems with low systemic costs.

This failure-driven, adaptive process grounds a robust but fallible realism. It explains how our evolving knowledge systems are forced to converge on an emergent structure of workable solutions determined by mind-independent pragmatic constraints. The claim of this framework is therefore modest: it proposes that beneath the surface-level 'noise' of contingency and power, there are underlying structural pressures at work. A system accumulating brittleness is not *fated* to collapse on a specific day, but it becomes progressively more *vulnerable* to the very contingent shocks that historians study. This model, therefore, does not offer a deterministic theory of history, but rather proposes the foundation for a falsifiable, probabilistic research program for understanding the structural dynamics that shape the landscape upon which historical events unfold.

## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 The Deflationary Path: From Private Belief to Public Standard**

To analyze the dynamics of public, cumulative knowledge, the rich but opaque concepts of individual psychology are insufficient. We must begin with a systematic, deflationary process that moves from the private and inaccessible to the public and functional. This procedure is not a mere terminological choice; it is a necessary step for any naturalistic theory that seeks to explain how objective knowledge can emerge from the aggregation of subjective experiences.

**Belief → Proposition → Integrated Data → Standing Assertion**

*   **Step 1: From Private Belief to Public Proposition.** The journey begins with *belief*, a private psychological state tied to an individual's consciousness. For a theory of public knowledge, this is analytically inaccessible. The first step is therefore to isolate its testable, public content as a *proposition*: a falsifiable claim that can be articulated, communicated, and collectively assessed.
*   **Step 2: The Pragmatic Coherence Test.** A candidate proposition is subjected to a thick, forward-looking *coherence test*. A shared network, as a resource-constrained system, implicitly asks: will integrating this proposition increase or decrease our long-term systemic brittleness? This is a form of risk analysis, weighing the proposition against heuristics like logical consistency, explanatory power, and parsimony.
*   **Step 3: From Integrated Data to Standing Assertion.** Propositions that pass this test with exceptional success—dramatically reducing a network's systemic costs—undergo a profound status change. They are promoted from being just *data within* the network to becoming part of its core *evaluative architecture*, what we call a **Standing Assertion**. Having proven its immense pragmatic value, a standing assertion is no longer treated as a hypothesis to be constantly re-tested, but as a reliable standard against which new candidate propositions are judged. It has transitioned from *being-tested* to *doing-the-testing*.

### **2.2 The Units of Analysis and Pragmatic Pushback**

Our analysis focuses on public structures: *Shared Networks* (e.g., scientific disciplines) built from *Functional Templates* (reusable concepts like `...is an infectious disease`). These networks are under constant pressure from **pragmatic pushback**: the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback generates two types of costs:
*   **First-Order Costs** are the direct, material consequences: failed predictions, wasted resources, systemic instability (e.g., excess mortality).
*   **Systemic Costs** are the secondary, internal costs a network incurs to *manage, suppress, or explain away* its first-order costs. These non-productive expenditures reveal a network's true fragility. Key forms include:
    *   **Conceptual Debt:** The compounding fragility incurred by adopting flawed, complex "patches" to protect a core principle.
    *   **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing dissent. Dissent, in this model, is a critical data stream signaling that a system is generating costs for its members.

### **2.3 Gauging Brittleness: An Empirical Toolkit**

A system’s *brittleness* is a measure of its accumulated, hidden systemic costs. This can be operationalized by tracking concrete, measurable indicators. A key methodological challenge is distinguishing a degenerative "patch" from a progressive hypothesis in a non-circular way. We can do so by assessing its *explanatory return on investment*. A *progressive hypothesis* offers a high return: for a small investment in added complexity, it yields novel predictions or unifies disparate phenomena. A *degenerative patch* offers a low return: it is a high-cost fix that resolves only the targeted anomaly and often increases the network's overall complexity.

| Indicator                      | Potential Proxy Metric                                       | Data Sources (Illustrative)                        |
| :----------------------------- | :----------------------------------------------------------- | :------------------------------------------------- |
| **Rate of Ad-Hoc Modification**  | Ratio of auxiliary hypotheses vs. novel predictions in published literature. | Academic databases (e.g., arXiv, Scopus)           |
| **Ratio of Coercion to Production** | Ratio of state budget for internal security vs. R&D and public health. | World Bank, National Budget Data, Seshat Databank  |
| **Increasing Model Complexity**  | Escalation in FLOPs for marginal performance gains; growth in prompt-tuning papers relative to foundational advances. | arXiv trends, AI conference proceedings            |

## **3. The Drivers of Adaptation: The Logic of Systemic Viability**

The framework for assessing systemic health is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems. The model’s authority is not grounded in a chosen value, but in a *constitutive condition* for the practice of cumulative, inter-generational inquiry itself. Endurance is not a value *within* the game of inquiry; it is the inescapable precondition that makes the game possible over time. A network that systematically undermines its own ability to persist cannot, by definition, succeed at the project of accumulating and transmitting knowledge.

The pressure to maintain a low-brittleness design is thus the non-negotiable "gravity" of public inquiry. The choice is not between a viable system and an equally valid but non-viable one; it is between a system that can persist long enough to learn and one that collapses under the weight of its own costs. Framed by this logic, the test for coherence is a thick, forward-looking *cost-benefit analysis*: a set of heuristics that a resource-constrained system uses to bet on whether adopting a new proposition will increase or decrease its long-term brittleness.

## **4. Convergence and the Emergence of Objective Structures**

The logic of viability provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible, showing how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

Our account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is of what is demonstrably unworkable. The systematic analysis of these failures allows us to build the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been **empirically invalidated** by the catastrophic Systemic Costs they reliably generate. For example:
- The network design based on the principle `appeals to authority are a final justification` has been historically invalidated by a consistent pattern of institutional stagnation and paradigm collapse (e.g., scholastic physics versus Galilean empiricism).
- The socio-political network design based on the principle `slavery is a viable principle of economic organization` has been invalidated by the immense and unsustainable Systemic Costs required to maintain it. The principle is not wrong because of a modern moral judgment; it is a failed engineering principle.

By charting what demonstrably fails, we are reverse-engineering the hard constraints of a real territory. The Negative Canon functions like a reef chart for inquiry, marking the impassable terrain on the landscape of viability.

### **4.2 The Apex Network: An Emergent Structure of Viability**

The relentless filtering documented in the Negative Canon is also profoundly constructive, as it progressively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we call the **Apex Network**. It is best understood not as a goal aimed at from the start, but as a real, structural property discovered through inquiry. Its ontology is **retrospective and procedural, not teleological**. Its reality is akin to that of the principles of natural selection: an objective fact about how complex systems behave under selective pressure, which we learn about empirically.

It can be visualized as a "fitness landscape" for knowledge systems, where the pressure of pragmatic pushback creates peaks of viability and valleys of failure. We only learn the shape of the landscape by exploring it. The Apex Network is the complete map of principles that occupy the peaks of this landscape, its structure wholly determined by the mind-independent constraints that shape the landscape itself.

### **4.3 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power**

This evolutionary model does not imply a simple, linear march of progress. A system can become locked into a high-brittleness **fitness trap** due to coercive institutions or other contingent historical factors. A slave economy, for instance, is a classic fitness trap: it is objectively brittle in the long run, but it creates path-dependent institutions that make escaping the trap even costlier in the short term. The raw power of the ruling class does not negate the system's brittleness; instead, the resources spent on maintaining that power (the **coercive overheads**) become a primary *indicator* of that brittleness. The persistence of such a system is not a sign of its viability, but a measure of the energy it must expend to resist the structural pressures pushing it toward collapse.

Drawing on cliodynamic analysis, we can begin to metricize this. As Peter Turchin (2003) has shown, polities where coercive overheads consumed an unsustainably high portion of state resources exhibit a significantly higher probability of fragmentation when faced with an external shock. Power and path dependence are not external exceptions to the model; they are core variables within it, measurable as indicators of systemic risk.

### **4.4 A Three-Level Framework for Truth**

This emergent structure grounds our fallibilist but realist account of truth, resolving the isolation objection by reframing truth as a status that propositions acquire through increasingly rigorous stages of validation.
*   **Level 3: Contextual Coherence:** The baseline status. A proposition is coherent *within any specific Shared Network*.
*   **Level 2: Justified Truth:** The highest practically achievable epistemic status. A proposition is justified as true if it is certified by a *Consensus Network* that has itself demonstrated a low and stable degree of systemic brittleness.
*   **Level 1: Objective Truth:** The ultimate, regulative ideal. A proposition is objectively true if its principles are part of the real, emergent **Apex Network**.

This allows for sharp historical judgments. The claim "The sun revolves around the Earth" was contextually coherent (Level 3) within the Ptolemaic network. It never, however, achieved the status of justified truth (Level 2), because the Ptolemaic network itself was demonstrably failing its pragmatic stress test, as evidenced by its rising brittleness.

## **5. Case Studies in Convergence and Brittleness**

### **5.1 Canonical Case: Newtonian to Relativistic Physics**

The transition from Newtonian to relativistic physics offers a canonical example. The Newtonian system, after centuries of viability, began to accumulate catastrophic costs in the late 19th century, manifesting as failed predictions (e.g., Mercury's perihelion) and rising conceptual debt (e.g., the ad-hoc Lorentz-FitzGerald contraction hypothesis). The Einsteinian system proved to be a vastly more effective and resilient solution, paying down this debt and dramatically lowering the systemic costs of physics.

### **5.2 Contemporary Case: The AI Paradigm Shift**

The recent history of Artificial Intelligence provides a clear example of a "brittleness audit" in action.
*   **The High-Brittleness System (Symbolic AI):** The "AI winter" of the late 20th century can be seen as the collapse of the symbolic AI paradigm, which suffered from a catastrophic rate of ad-hoc modification. It required endless hand-coded rules to handle exceptions, a sign of mounting conceptual debt that made it unable to scale.
*   **The Low-Brittleness Successor (Deep Learning):** The deep learning paradigm that followed proved to be a low-brittleness solution for specific tasks, built on more parsimonious and generative principles.
*   **The Current Pluralist Frontier:** The current Large Language Model paradigm is now showing clear signs of rising systemic costs, indicating we are on a new frontier of exploration:
    1.  **Massive Energetic Inefficiency:** The computational cost (measured in FLOPs) for state-of-the-art models is growing at an exponential and unsustainable rate for marginal performance gains.
    2.  **Accelerating Ad-Hoc Modification:** The explosion of post-hoc "patches"—from prompt engineering to alignment tweaks and guardrails—is a clear indicator of mounting conceptual debt. The ratio of papers on such auxiliary modifications to those on foundational architectural advances has increased dramatically.

This illustrates the framework's utility as a real-time diagnostic tool, flagging a research program's rising risk profile long before a full-blown crisis.

## **6. The Dynamism of the Web: How Knowledge Becomes Entrenched**

Quine’s "Web of Belief" provided a brilliant static anatomy of a knowledge system, but it lacked a corresponding physiology. This section provides that dynamic physiology, detailing the naturalistic process by which a successful discovery migrates from the tentative "periphery" of the web to its load-bearing "core."

A proposition is promoted by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the **Conservation of Energy** began as a contested hypothesis. It migrated inward as it proved its indispensable power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This process is driven by the pressures of **bounded rationality** (Simon 1972). By entrenching its most successful discoveries as default assumptions—a form of **systemic caching**—a resource-constrained system avoids the crippling cost of re-deriving everything from first principles. This process animates Quine’s web, providing both the **externalist filter** of pragmatic pushback and a **directed learning mechanism**—a process of entrenchment akin to what Imre Lakatos (1970) described in the development of a research programme's "hard core."

## **7. Systemic Externalism: A Naturalistic Synthesis**

Our model constitutes a form of **Systemic Externalism**. Unlike traditional externalism (e.g., Goldman 1979), which locates reliability in an individual’s cognitive processes, our model scales this insight to the public, historical level. Justification is a property of **beliefs-within-a-proven-system.** This stance allows our framework to synthesize and advance several major research programs:
*   It provides the **dynamic physiology for Quinean holism**, adding the externalist filter and directed learning mechanism needed to animate his static web.
*   It provides an **evolutionary grounding for social epistemology** (e.g., Longino 2002). Norms like peer review are not a priori ideals but adaptive strategies that have survived because they demonstrably cultivate low-brittleness networks. Our model supplies the externalist check of pragmatic viability that purely procedural accounts can lack.
*   It offers a **realist corrective for neopragmatism** (e.g., Rorty 1979). The analysis of systemic failure is the non-discursive filter that more discourse-focused pragmatisms lack. The collapse of Lysenkoist biology was not due to a breakdown in conversation but to the catastrophic first-order costs of agricultural collapse. Lasting solidarity is not an *alternative* to objectivity; it is an **emergent property** of a low-brittleness network.
*   It provides a **naturalistic engine for structural realism** (e.g., Worrall 1989), explaining *how* and *why* inquiry is forced to converge on objective, relational structures (the Apex Network) through the eliminative process of pragmatic selection.

## **8. Addressing Key Challenges**

### **8.1 The Challenge from Coherent Fictions**

Our model resolves the "coherence trap" (e.g., a sophisticated conspiracy theory) by insisting on the second, externalist condition: a demonstrated historical track record of low systemic brittleness. Such networks fail this test catastrophically. They are often **epistemically parasitic**: they generate no novel, productive research but exist only to create after-the-fact explanations for the successes of a host network.

### **8.2 The Macro/Micro Bridge: A Formalization**

While a macro-epistemology, our model connects to individual justification via the concept of **higher-order evidence**. The diagnosed health of a public system provides a powerful defeater (or corroborator) for an individual’s beliefs derived from that system. To formalize this, we can use a Bayesian framework.
An agent’s rational confidence in a claim from a given source, P(Claim|Source), should be treated as a prior probability conditioned by the diagnosed health of the source network. A low-brittleness network (e.g., the IPCC) warrants a high prior; a high-brittleness network (a denialist source) warrants a low one. As Thomas Kelly (2005) argued regarding disagreement, the properties of the source matter. When the agent receives new first-order evidence, *E*, the posterior confidence, P(Claim|Source, E), is updated. This formalizes why the agent should rationally favor the IPCC's claim: even if the denialist source presents a seemingly powerful piece of evidence, the extremely low prior assigned to that network means the agent's posterior confidence in its claims will remain low.

### **8.3 The Challenge of Defining "Costs" without Subjectivity**

The model is anchored in an objective analysis of costs, understood through a tiered diagnostic framework:
*   **Tier 1: Foundational Bio-Social Costs:** The least-contestable costs that directly threaten a system's material substrate (e.g., excess mortality, resource depletion).
*   **Tier 2: Systemic Costs of Internal Friction:** Measurable proxies like the Coercion Ratio, which quantify the energy a system wastes on self-suppression.
*   **Tier 3: Domain-Specific Epistemic Costs:** Inefficiencies relative to a domain's constitutive goals (e.g., rising model complexity or rate of ad-hoc modification in science).
The model can also diagnose **cost-shifting**—where a system appears efficient by deferring its costs onto a social or ecological substrate—as a primary indicator of hidden, long-term brittleness.

## **9. Conclusion: A Falsifiable Research Program**

Quine’s great contribution was to transform the static, foundationalist pyramid of knowledge into a holistic, flexible web. This paper has taken the next logical step: to provide the dynamic physiology for that web. We have framed inquiry not as a project of rational design, but as an evolutionary process of cultivating more resilient public knowledge systems, driven by the selective pressures of real-world costs.

This model is not presented as a final, complete system, but as the foundation for a progressive and falsifiable research program. It generates concrete, interdisciplinary hypotheses that can be empirically tested:
*   **Hypothesis 1 (Cliodynamics):** Polities where the ratio of coercive overheads to productive capacity exceeds a specific threshold for a sustained period will exhibit a statistically higher probability of fragmentation when faced with an external shock.
*   **Hypothesis 2 (History of Science):** A quantitative analysis of scientific literature will show that abandoned research programs were preceded by a measurable acceleration in the ratio of ad-hoc/auxiliary hypotheses to novel predictions compared to their successful rivals.
*   **Hypothesis 3 (Sociology/Economics):** Societies with institutions that systematically shift costs onto marginalized groups or future generations (e.g., environmental externalities) will show lagging indicators of innovation and resilience compared to societies that internalize those costs.

We began with the challenge of distinguishing viable knowledge from brittle dogma. The model we have developed suggests the ultimate arbiter is not the elegance of a theory or the consensus of its adherents, but the trail of consequences it leaves in the world. Dissent, instability, and suffering are not merely political problems; they are epistemic signals that a system is failing its most fundamental pragmatic test. The most pragmatic question—"Is this way of organizing ourselves still working?"—is therefore not a departure from the search for objectivity, but the very heart of it.

## **References**

Goldman, Alvin I. 1979. “What Is Justified Belief?” In *Justification and Knowledge*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.

Kelly, Thomas. 2005. “The Epistemic Significance of Disagreement.” In *Oxford Studies in Epistemology, Vol. 1*, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.

Kuhn, Thomas S. 1962. *The Structure of Scientific Revolutions*. Chicago: University of Chicago Press.

Lakatos, Imre. 1970. “Falsification and the Methodology of Scientific Research Programmes.” In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.

Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.

Popper, Karl. (1934) 1959. *The Logic of Scientific Discovery*. London: Hutchinson.

Quine, W. V. O. 1951. “Two Dogmas of Empiricism.” *The Philosophical Review* 60 (1): 20–43.

Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press.

Simon, Herbert A. 1972. “Theories of Bounded Rationality.” In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.

Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.

Worrall, John. 1989. “Structural Realism: The Best of Both Worlds?” *Dialectica* 43 (1–2): 99–124.