# **Convergent Coherence: How Failure Forges the Emergent Structure of Objective Truth**

## **Abstract**

Coherentist theories of justification remain vulnerable to the isolation objection, which holds that a perfectly coherent belief system could be detached from reality. This paper proposes an externalist framework to resolve this challenge by grounding coherence in long-term pragmatic viability. The framework introduces systemic brittleness, a diagnostic for assessing the structural health of a knowledge system by tracking the observable costs generated when its propositions are applied in the world. It argues that the selective pressure of these costs forces disparate systems of inquiry to converge on a single, maximally coherent system disciplined by mind-independent pragmatic constraints. This failure-driven process reveals an objective structure, which this paper terms the Apex Network: not a pre-existing truth to be discovered, but a bottom-up emergent pattern of viable propositions that has survived historical filtering. This approach yields a form of systemic externalism, where a claim’s justification depends on the proven resilience of the public system that certifies it. The result is a naturalistic theory that redefines objective truth as alignment with this emergent structure, explaining how Quine’s web of belief is pragmatically revised and grounding a falsifiable research program for assessing the health of our most critical epistemic systems.

## **1. Introduction: From a Static Web to a Dynamic Process**

The replacement of miasma theory by germ theory is a canonical example of scientific progress. While often attributed to superior evidence, an analysis of the two systems reveals a deeper dynamic of systemic viability. The miasma theory network, despite some positive public health consequences, generated escalating systemic costs: its principles led to misdirected interventions, and it required an accelerating number of ad hoc hypotheses to account for anomalies, such as the localization of cholera near a specific water pump. Germ theory, by contrast, proved to be a more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and by explaining a wide range of phenomena with a parsimonious set of principles.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what will be termed *systemic costs*. Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective suggests that inquiry is not a search for ultimate foundations but an ongoing, evolutionary process of cultivating more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating such viable knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, this framework distinguishes viability from mere endurance. A dogmatic research program that persists through institutional power is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing anomalies and dissent. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.

However, any credible theory of knowledge evolution must account for the realities of power, path dependence, and historical contingency. This framework incorporates these factors not as exceptions, but as key variables within the model. The exercise of institutional power to maintain a brittle research program, for example, is not a refutation of the model but a primary indicator of that program's non-viability, measured through its high coercive costs. This failure-driven, adaptive process grounds a robust but fallible realism. It explains how our evolving knowledge systems are forced to converge on an emergent structure of workable solutions determined by mind-independent pragmatic constraints. The claim of this framework is therefore modest: it proposes that beneath the surface-level noise of contingency and power, there are underlying structural pressures at work. A system accumulating brittleness is not fated to collapse on a specific day, but it becomes progressively more vulnerable to the very contingent shocks that historians study. In this sense, the concept of brittleness—a system’s vulnerability to collapse due to the accumulation of hidden, internal costs—is a close cousin to the notion of fragility developed by Taleb (2012). This model, therefore, does not offer a deterministic theory of history, but rather proposes the foundation for a falsifiable, probabilistic research program for understanding the structural dynamics that shape the landscape upon which historical events unfold.

## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve while others stagnate, it is necessary to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares an affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 From Private Belief to Public Tool**

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

`[FIGURE 1: Conceptual Flowchart for the Deflationary Path]`
`[Belief (Private State)] --> [Articulation into a Proposition (Public Claim)] --> [Coherence Test] --> [Integration as Validated Data OR Promotion to Standing Predicate (Public Tool)]`

#### **2.1.1 From Private Belief to Public Proposition**

The process begins with belief, the raw material of epistemology. As a private psychological state tied to an individual's consciousness, it is analytically inaccessible for a theory of public knowledge. The first step is therefore to isolate its testable, public content as a *proposition*. A proposition is a falsifiable claim that can be articulated, communicated, and collectively assessed.

#### **2.1.2 The Coherence Test**

Next, a candidate proposition must pass a rigorous test for coherence. This is not the thin, formal consistency of logic, but a thick, forward-looking pragmatic assessment that functions as a form of risk analysis. A shared network, as a resource-constrained system, implicitly asks: will integrating this proposition increase or decrease our long-term systemic brittleness? A proposition that successfully passes this test is accepted into the network as validated data—a reliable claim that can be used and cited within the system.

#### **2.1.3 From Validated Data to Standing Predicate**

Propositions that prove exceptionally effective at reducing a network's systemic costs acquire a new functional role. They are not merely retained as validated data; their core conceptual structure is integrated into the network's evaluative standards.

This process creates what we will call a *standing predicate*. A standing predicate is the reusable, action-guiding conceptual tool within a proposition that has earned a durable and trusted status through pragmatic success. For example, once the proposition "Cholera is an infectious disease" proved its value, its functional component—the predicate `...is an infectious disease`—was promoted. It now functions as a durable piece of conceptual technology. Applying it to a new phenomenon activates a rich sub-network of proven diagnostic heuristics, interventional policies, and licensed inferences. The original proposition has transitioned from a claim *being-tested* to a tool that *tests other claims*. This promotion from data to a trusted, standing tool is a crucial step in a network's ability to learn and upgrade its own conceptual architecture.

### **2.2 The Units of Analysis: Predicates, Networks, and Replicators**

Having established the process by which private beliefs become public tools, we can define the model's core analytical units. The analysis moves from the psychology of individual agents to the public, functional structures of knowledge that are subject to evolutionary pressure.

A *standing predicate* is the primary unit of cultural-epistemic selection. It is the validated, reusable, and action-guiding conceptual tool within a proposition (e.g., `...is an infectious disease`). Functioning as a highly compressed piece of conceptual technology, a standing predicate, when applied, unpacks a suite of previously validated knowledge, including causal models, diagnostic heuristics, and licensed interventions.

These standing predicates are organized and tested within *shared networks*. A shared network is a coherent, public system of core predicates and validated propositions, such as a scientific discipline. These networks are not designed top-down; they are emergent solutions, formed by the convergence of agents tackling shared problems under pragmatic pressure. They function as the structures in which predicates are tested, retained, or discarded, and serve as the primary vehicles for cumulative, inter-generational knowledge.

To be precise about this evolutionary dynamic, one can adopt a distinction from generalized evolutionary theory, as synthesized for the social sciences by Mesoudi (2011). The network’s abstract informational structure—its core standing predicates and their relations—functions as the *replicator*: the "code" that is copied and transmitted. The social group and its institutions, such as the scientific community, function as the *interactor*: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This distinction helps to explain how knowledge structures can persist even when the specific communities that created them do not.

### **2.3 Pragmatic Pushback and Systemic Costs**

A shared network is not a passive library of claims; it is an active system under constant pressure from what this paper terms *pragmatic pushback*: the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback is not an argument but a material outcome, such as a failed prediction or a dysfunctional technology. This process generates two types of costs.

*First-order costs* are the direct, material consequences of a misalignment between a system and its environment, such as failed predictions, wasted resources, or institutional stagnation. These are the objective signals of dysfunction. *Systemic costs* are the secondary, internal costs a network incurs to manage, suppress, or explain away its first-order costs. These non-productive expenditures reveal a network's true fragility. Key forms of systemic cost include:

*   *Conceptual Debt*: The compounding fragility incurred by adopting flawed or complex ad hoc hypotheses—"patches"—to protect a core principle from countervailing evidence.
*   *Coercive Overheads*: The measurable resources allocated to enforcing compliance and managing dissent within a research program. These overheads are a primary mechanism by which power dynamics manifest within the model; the resources spent to maintain a brittle system against internal and external pressures become a direct indicator of its non-viability. Dissent, in this model, is treated as a critical data stream signaling that a system is generating costs for its members.

### **2.4 Gauging Brittleness: An Empirical Toolkit**

A system’s *brittleness* is a measure of its accumulated systemic costs. To make this concept analytically tractable, it can be operationalized by tracking concrete, measurable indicators. The following table provides a conceptual illustration of how one might construct such a diagnostic toolkit for knowledge systems. This is intended not as a final algorithm but as a heuristic guide for a research program aimed at identifying trajectories of rising systemic risk.

| Indicator                  | Domain of Application                                     | Potential Proxy Metric                                                                                                  | Data Sources (Illustrative)                        |
| :------------------------- | :-------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------- |
| Rate of Ad Hoc Modification | Scientific Paradigms                                      | Ratio of auxiliary hypotheses versus novel predictions in published literature.                                         | Academic databases (e.g., arXiv, Scopus)           |
| Coercive Overheads         | Research Programs, Institutions                           | Documented instances of data suppression; ratio of resources for enforcement of dogma versus open inquiry.              | Institutional records, historical analysis         |
| Increasing Model Complexity | Computational Systems (e.g., Deep Learning); Paradigms | Escalation in computational resources required for marginal performance gains; increase in free parameters without new predictions. | arXiv trends, AI conference proceedings, publications |

We present this toolkit as a guide for a challenging research program. Its application requires significant methodological caution. The operationalization of these proxies must be carefully designed to avoid being question-begging; historical data can be sparse or biased; and isolating causal variables from confounding factors is a perennial challenge. The goal of this framework is not to offer deterministic predictions, but to identify trajectories of rising systemic risk and to provide a more rigorous, evidence-based language for a science of institutional health.

A key methodological challenge is distinguishing a degenerative "patch" from a progressive hypothesis in a non-circular way. This can be addressed by assessing a hypothesis's *explanatory return on investment*. A progressive hypothesis offers a high return: for a small investment in added complexity, it yields novel predictions or unifies disparate phenomena. A degenerative patch offers a low return: it is a high-cost fix that resolves only the targeted anomaly and often increases the network's overall complexity without expanding its explanatory power.

## **3. The Drivers of Adaptation: The Logic of Systemic Viability**

The framework for assessing systemic health detailed in Section 2 is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems.

### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). This framework answers this "normativity objection" by grounding epistemic norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, normative epistemology can be treated as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. This framework makes the goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a constitutive argument holds that any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.

Second, an instrumental argument follows from the framework's core empirical claim: that networks with a high and rising degree of measured brittleness are statistically more likely to collapse or require radical revision. From this descriptive claim follows a conditional recommendation: if an agent or institution has the goal of ensuring its long-term stability and problem-solving capacity, then it has a powerful, evidence-based reason to adopt principles that demonstrably lower its systemic brittleness.

This reframes the paper’s normative language. When this model describes one network as "better" or identifies "epistemic progress," these are not subjective value judgments but technical descriptions of systemic performance. A "better" network is one with lower measured brittleness and thus a higher predicted resilience against failure. Viability, in this sense, is not an optional norm to be adopted; it is a structural precondition for any system that manages to become part of the historical record at all.

### **3.2 Coherence as a Forward-Looking Risk Assessment**

Framed by the logic of viability, the test for coherence functions as a thick, forward-looking cost-benefit analysis. It is a set of heuristics that a resource-constrained system uses to assess whether adopting a new proposition will increase or decrease its long-term brittleness. The traditional epistemic virtues can be understood as core principles of this practical calculus. For instance, *logical consistency* serves as a hedge against the future costs of inferential paralysis. *Explanatory power* measures a proposition’s potential return on investment, as it can reduce future inquiry costs by paying down conceptual debt. *Simplicity*, or parsimony, is a direct measure of systemic overhead, since overly complex propositions can increase long-term maintenance costs. Finally, *evidential support* functions as an assessment of integrative risk; a well-supported claim is a low-risk investment because it is less likely to trigger a cascade of costly future revisions.

This forward-looking model of coherence also explains how revolutionary science is possible. When a dominant consensus network begins to exhibit high and rising systemic brittleness—a state that corresponds to a Kuhnian "crisis"—the cost-benefit analysis for new propositions shifts dramatically. A radical new hypothesis, while having low coherence with the existing network's specific principles, may promise a significant long-term reduction in the systemic costs that are crippling the incumbent paradigm. A community's acceptance of such a hypothesis is therefore not based on its fit with failing principles, but on its potential to restore a low-brittleness state for the system as a whole. Coherence, in this dynamic sense, is not a conservative check for conformity, but a pragmatic assessment of a proposition's potential to resolve systemic crisis.

## **4. Convergence and the Emergence of Objective Structures**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. It will show how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

This account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is often of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system—its descent into crippling inefficiency, intellectual stagnation, and institutional decay—provides a clear data point regarding the non-viability of its core principles.

The systematic analysis of these failures allows for the construction of what can be called the *Negative Canon*: a robust, evidence-based catalogue of principles and network designs that have been empirically invalidated by the systemic costs they reliably generate. Systems like scholastic physics, phlogiston chemistry, and Lysenkoist biology entered this canon because their core principles generated catastrophic predictive failures, institutional stagnation, and an accelerating reliance on ad hoc modifications. Their inclusion in the Negative Canon is not a simple matter of historical judgment; it is a diagnosis of their demonstrated structural non-viability. They represent failed blueprints for constructing reliable knowledge of the world.

By charting what demonstrably fails, we are not merely learning what to avoid; we are effectively reverse-engineering the constraints of a real territory. The Negative Canon functions as a kind of reef chart for inquiry. It is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and helps to prevent a collapse into relativism.

### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what this paper terms the *Apex Network*. To be precise about its status, the Apex Network is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. It is best understood as the theoretical limit-point of the process of convergence, a concept that shares an affinity with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our current *consensus network* is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network has a distinct, naturalized ontological status: it is an emergent structural fact about the landscape of pragmatic constraints. It is an objective pattern formed by the historical filtering of unviable systems, not a metaphysical blueprint awaiting discovery. To clarify this concept, consider an analogy. While an individual's color preference may seem entirely subjective, a non-random pattern emerges from cross-cultural data: a striking convergence on the color blue. This is an objective, emergent fact demanding a naturalistic explanation, which lies in the evolutionary selection pressures of our shared terrestrial environment. The objective fact is not a metaphysical rule, but the emergent, structural relationship that, for a species like ours under these constraints, a preference for blue is a highly viable, low-cost perceptual default. The Apex Network has the same ontological status. It is the singular, maximally coherent system of predicates and their relations, forged from the bottom up by the convergence of countless problem-solving efforts under pragmatic pressure. This network represents the set of maximally viable, low-cost solutions to the problems posed by our world, and as such, it constitutes the real, mind-independent structure of viability that disciplines all successful inquiry.

The mechanism that forges this structure is one of bottom-up emergence, not top-down design. Local shared networks, developed to solve specific problems, are forced to cohere with one another because they operate in an interconnected world. This pressure for cross-domain consistency generates a tendency toward a single, maximally integrated network, as maintaining incoherence between successful systems incurs unsustainable systemic costs. The Apex Network is the name for the theoretical limit-point of this process: the singular, maximally coherent system whose structure is determined by the full set of mind-independent pragmatic constraints.

This status as a singular, maximally coherent system allows the Apex Network to function as a standard for objective truth. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge. The "pursuit of truth," then, is the practical, fallible project of refining our current consensus network to better align with this real, objective structure.

`[FIGURE 2: Conceptual Sketch of a Fitness Landscape]`
`[A 3D landscape with "Viability" on the Y-axis. Peaks are labeled with successful systems like "Germ Theory" and "Relativity." Deep valleys and low plateaus are labeled with failed systems from the Negative Canon, such as "Ptolemaic System" and "Lysenkoism."]`

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state; they simply try to solve local problems and reduce costs. The Apex Network is the name given to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

The status of the Apex Network is therefore dual, a distinction critical to this framework's fallibilist realism. Ontologically, it is real; it is the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, however, it remains a regulative ideal. We can never achieve a final, God's-eye view to confirm that our consensus network is a perfect map of it; our knowledge of it is necessarily incomplete and fallible. Its existence is what grounds realism and helps prevent a collapse into relativism, while our epistemic limitations are what make the project of inquiry a permanent and progressive one. The concept is what makes our comparative judgments of "more" or "less" brittle meaningful.

Finally, this framework treats the question of pluralism as an empirical one. While some domains with tight constraints may converge toward a single peak of viability, other complex domains may feature multiple, locally stable peaks. The research program this model proposes is to investigate for any given domain whether pragmatic filtering tends to eliminate all but one viable design, or if it permits a stable pluralism.

### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). This framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status that propositions earn through increasingly rigorous stages of validation.

The framework distinguishes three levels of epistemic status. The first is *contextual coherence*. This is the baseline status for any claim, where a proposition is coherent within a specific shared network, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.

The second level is *justified truth*. This is the highest epistemic status practically achievable for fallible inquirers. A proposition is justified as true if it is certified by a consensus network that has a demonstrated track record of low systemic brittleness. For rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of inquiry itself.

The third and ultimate level is *objective truth*. This is the regulative ideal of the entire process. A proposition is objectively true if its principles are part of the real, emergent Apex Network—the objective structure of viable solutions. While this structure is never fully mapped, it functions as the formal standard that makes our comparative judgments of "more" or "less" brittle meaningful. It is the structure toward which the reduction of systemic costs forces our knowledge systems to converge.

This layered framework avoids a simplistic "Whig history" by recognizing that justified truth is a historically-situated achievement. Newtonian mechanics earned its status as justified truth by being a maximally low-brittleness system for its problem-space for over two centuries. Its replacement by relativity does not retroactively invalidate that status; it shows the evolutionary process at work, where an expanding problem-space revealed pragmatic constraints that required a new, more viable system. This allows for sharp, non-anachronistic historical judgments: a claim can be justifiably true in its time yet still be objectively false when judged against the Apex Network from the perspective of a more resilient successor.

### **4.4 The Evolving Structure of Knowledge: Convergent Core and Pluralist Frontier**

The historical process of pragmatic filtering gives our evolving consensus networks a discernible structure, which can be understood as having two distinct epistemic zones. This distinction is not about the nature of reality itself, but describes the justificatory status of our claims at a given time.

The first zone is the *convergent core*. This represents the load-bearing foundations of our current knowledge. It comprises domains where the relentless pressure of pragmatic selection has eliminated all known rival formulations, leaving a single, or functionally identical, set of low-brittleness principles. Principles reside in this core, such as the laws of thermodynamics or the germ theory of disease, not because they are dogmatically held or self-evident, but because all tested alternatives have been relegated to the Negative Canon after generating catastrophically high systemic costs. While no claim is immune to revision in principle, the principles in the convergent core are functionally unrevisable in practice, as doing so would require dismantling the most successful and resilient knowledge structures we have ever built. A claim from this core achieves the highest degree of justification we can assign, approaching our standard for objective truth.

The second zone is the *pluralist frontier*. This describes the domains of active research where current evidence is insufficient to decide between multiple, competing, and viable reconstructions of the landscape of viability. Here, rival systems, such as different interpretations of quantum mechanics or competing models of consciousness, may co-exist, each with a demonstrably low and stable degree of brittleness. It is crucial to distinguish this constrained, evidence-based pluralism from relativism. The frontier is not an "anything goes" zone; it is a highly restricted space strictly bounded by the Negative Canon. A system based on phlogiston, for example, is not a viable contender on the frontier of chemistry; it is a demonstrably failed research program. This pluralism is therefore a sign of epistemic underdetermination—a feature of our map's current limitations, not a supposed feature of reality itself. This position resonates with pragmatist accounts of functional pluralism (e.g., Price 1992), which treat different conceptual frameworks as tools whose legitimacy is determined by their utility within a specific practice. Within this frontier, the core claims of each viable competing system can be granted the status of justified truth. This is also the zone where non-epistemic factors, such as institutional power or contingent path dependencies, can play their most significant role, sometimes artificially constraining the range of options explored or creating temporary monopolies on what is considered justified.

### **4.5 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power**

This evolutionary model does not imply a simple, linear march of progress. The landscape of viability is complex, and knowledge systems can become stuck in sub-optimal but locally stable states. Power and path dependence are not external exceptions to the model; they are core variables within it.

A knowledge system can become locked into a high-brittleness *fitness trap* due to institutional dogma or path-dependent factors. Soviet Lysenkoism is a canonical example. The system was objectively brittle, leading to agricultural failures and scientific stagnation. However, coercive state power created path-dependent institutions that made escaping the trap immensely costly for individual scientists. The power used to enforce this orthodoxy did not negate the system's brittleness; instead, the resources spent on suppressing dissent and fabricating data—the *coercive overheads*—became a primary indicator of that brittleness. The persistence of such a system is not a sign of its viability, but a measure of the energy it must expend to resist the pragmatic pressures pushing it toward collapse.

While a full quantitative analysis is beyond the scope of this paper, one can see how this concept of a trap might be operationalized. Drawing on analogues from cliodynamic analysis (Turchin 2003), one could hypothesize that a research program where the ratio of resources dedicated to coercive overheads (e.g., enforcing dogma, suppressing dissent) versus productive capacity (e.g., novel research, open inquiry) exceeds a certain threshold for a sustained period becomes highly vulnerable to paradigm shift or collapse when faced with a significant external shock, such as a wave of incontrovertible new data.

## **5. The Dynamism of the Web: How Knowledge Becomes Entrenched**

Quine’s "Web of Belief" provided a powerful static model of a knowledge system, but it has been criticized for lacking a corresponding dynamic account of its formation and change. This section provides that dynamic physiology, detailing the naturalistic process by which a successful discovery migrates from the tentative periphery of the web to its load-bearing core.

### **5.1 From Tentative Hypothesis to Core Principle**

A proposition is promoted to the core by demonstrating its value in lowering the entire network’s systemic brittleness. The principle of the conservation of energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science: its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under *bounded rationality*; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of what can be termed *systemic caching*. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a consensus network with low demonstrated brittleness, it achieves the status of justified truth.

### **5.2 The Payoff: An Animated Web**

This process provides two mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, thereby addressing the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **6. Situating the Framework: Systemic Externalism and Its Relations**

This paper has developed what can be termed systemic externalism—a form of externalist epistemology that locates justification not in individual cognitive processes but in the demonstrated reliability of entire knowledge systems. This section clarifies the framework's position within contemporary epistemology by examining its relationship to four major research programs: coherentist epistemology, social epistemology, evolutionary epistemology, and neopragmatism.

### **6.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This paper's framework offers an externalist solution. It provides the evolutionary backstory for the web's functional structure, arguing that a principle becomes functionally indispensable, as described by Carlson (2015), precisely because it has survived a historical filtering process based on pragmatic consequences. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes this model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, this framework grounds the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

### **6.2 An Evolutionary Grounding for Social Epistemic Practices**

The framework provides a naturalistic foundation for core insights in social epistemology while addressing a persistent problem in that field. Longino (2002) and others have shown that objectivity often emerges from well-structured epistemic communities rather than isolated individuals. Critical discourse norms, peer review, and viewpoint diversity are presented as procedural guarantees of objectivity.

However, purely procedural accounts face what might be called the "parochialism problem": how do we know that a perfectly managed consensus is not simply a stable, shared delusion? If objectivity is secured by following local rules of discourse, what grounds do we have for evaluating those rules themselves?

This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **6.3 Cultural Evolution and the Problem of Fitness**

The framework contributes to evolutionary epistemology (Campbell 1974; Bradie 1986) while avoiding standard problems facing such approaches. Traditional biological models treat beliefs as competing for psychological "survival," but this creates difficulties in defining fitness without circularity—distinguishing genuinely beneficial knowledge from well-adapted "informational viruses."

This framework addresses the circularity problem by providing a hard, non-circular standard for fitness: long-term pragmatic viability as measured by systemic brittleness. The fitness of a principle is not its transmissibility or psychological appeal, but its contribution to the resilience of the knowledge system that hosts it.

This distinction proves diagnostic. Conspiracy theories may achieve high short-term transmissibility through psychological appeal, but they do so by incurring massive conceptual debt, exhibiting accelerating rates of ad-hoc modification, and often requiring high coercive overheads to maintain ideological purity. Their measured brittleness reveals their profound non-viability despite their psychological "fitness."

The framework also addresses evolutionary epistemology's difficulty with the directed nature of human inquiry. Unlike blind Darwinian selection, knowledge systems exhibit Lamarckian-style inheritance of acquired insights through the functional entrenchment of successful solutions in network cores.

It is useful to distinguish this concept of systemic brittleness from related ideas in the philosophy of science, such as Lakatos's (1970) 'degenerative research programmes' and Laudan's (1977) 'problem-solving effectiveness'. While Lakatos provides a brilliant historical description of a degenerating programme—one that relies on ad hoc hypotheses and fails to make novel predictions—this framework aims to provide the underlying causal engine for this degeneration. Brittleness is a measure of the accumulated systemic costs that cause a programme to become degenerative. It is a diagnostic of a system's structural health, not just a historical description of its output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, this approach is subtly different. Systemic brittleness is a forward-looking measure of risk and resilience. A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. This framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

### **6.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, this model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

This framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows this position to be distinguished from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve an emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While this framework agrees that justification is tied to practice, it grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that this framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

### **6.5 A Naturalistic Engine for Structural Realism**

This framework's concept of an emergent Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities, but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? This framework offers a compelling, pragmatic answer to both.

First, on ontology, this model naturalizes the ontology of these structures. The Apex Network *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an emergent structural fact about our world—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.

Second, on epistemology, this framework provides the specific causal mechanism for convergence that structural realism often lacks. We discover these robust structures not through a mysterious act of intellectual insight, but through the eliminative process of pragmatic selection. High-brittleness networks—those whose posited structures misalign with the real landscape of viability—generate unsustainable costs, collapse, and enter the Negative Canon. Low-brittleness networks survive and are built upon. Over historical time, this failure-driven selective pressure is precisely the engine that forces our consensus networks to conform to the objective, relational structure of the Apex Network.

This framework thus provides a dynamic, evolutionary, and fully naturalized engine for the structural realist's core thesis. It explains *how* and *why* our problem-solving practices are forced to converge on objective structures without appealing to metaphysical mysteries, thereby grounding structural realism in a testable, historical process.

### **6.6 Implications, Limitations, and a Naturalized Procedure**

This framework has implications for several contemporary discussions in epistemology. The diagnosed brittleness of knowledge systems, for example, provides powerful higher-order evidence that should influence how agents respond to disagreement (Kelly 2005). Claims from low-brittleness networks warrant higher confidence than those from demonstrably brittle sources. Similarly, testimonial justification depends not only on speaker reliability but on the systemic reliability of the knowledge traditions speakers represent. The brittleness framework also offers tools for applied epistemology, suggesting criteria for identifying degenerating research programs before they reach crisis points.

Several important limitations deserve acknowledgment. The framework applies most naturally to domains where pragmatic pushback is direct and measurable; its application to purely formal domains like mathematics or logic requires further development. Furthermore, while the paper provides conceptual tools for assessing brittleness, operationalizing these measures in non-question-begging ways remains a significant challenge for the research program it proposes.

Finally, the framework's contribution is best understood as a form of naturalized proceduralism. It shares an affinity with procedural realists, such as the later Putnam, who ground objectivity in the properties of a procedure rather than in direct correspondence with a mind-independent reality. The crucial divergence, however, lies in the nature of that procedure. Where rationalist accounts locate objectivity in the idealized norms of discourse, this model grounds it in the empirical, historical process of pragmatic selection. The ultimate arbiter is therefore not the internal coherence of our reasons, but the measurable, non-discursive brittleness of the systems our reasons produce. Our arguments are thus continuously disciplined not merely by other arguments, but by the non-negotiable data of systemic success and failure.

## **7. Defending the Model: Addressing Key Challenges**

A philosophical model is best judged by its ability to resolve the challenges that plague its predecessors. This section demonstrates the resilience of this framework by engaging with a series of classic epistemological problems, treating them as test cases that reveal the explanatory power of analyzing knowledge through the lens of systemic viability.

### **7.1 The Problem of Coherent Fictions**

The most potent challenge to any coherentist model is that of a perfectly self-consistent but detached system, such as a sophisticated conspiracy theory. This model resolves this "coherence trap" by introducing a meta-level, externalist standard. A system's justification depends not just on its internal elegance but on its demonstrated pragmatic performance, as measured by its systemic brittleness.

A "coherent fiction" fails this test catastrophically. While it may achieve a high degree of internal consistency, it does so only by incurring massive and ever-growing systemic costs. Diagnostically, it exhibits an accelerating rate of ad hoc modification to protect its core tenets from inconvenient data, accumulating immense conceptual debt. It often requires high coercive overheads—from social pressure in echo chambers to the active suppression of dissent—to maintain ideological purity. Furthermore, such networks are typically epistemically parasitic: they generate no novel, productive research but exist only to create after-the-fact rationalizations for the successes of a host network, such as mainstream science.

### **7.2 The Problem of Incommensurability and Hindsight**

A second set of challenges concerns the interpretation of history. If paradigms are incommensurable (Kuhn 1996), how can rational comparison be possible? And if viability is the standard, how do we account for flawed systems that endure for centuries, and how can we apply this standard without the benefit of hindsight?

This framework addresses these issues by distinguishing mere endurance from pragmatic viability. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it, as it provides a long-running experiment in which to observe the high price of insulating a flawed core from pragmatic pushback. Its apparent stability was a direct measure of the intellectual and institutional energy it had to expend to function, making it profoundly vulnerable to a more efficient competitor.

While Kuhnian incommensurability may prevent a direct, term-for-term comparison between rival paradigms, the systemic costs generated by each are perfectly comparable. The accelerating need for epicycles in the Ptolemaic system is an objective, cross-paradigm indicator of rising brittleness. A Kuhnian crisis, on this view, is not just a sociological phenomenon; it is the observable state of a network suffering from catastrophically high systemic costs.

This leads to the question of real-time application. The goal of this framework is not deterministic prediction but epistemic risk management. Its retrospective analysis of historical cases serves to calibrate our diagnostic tools. We study known failures to learn the empirical signatures of rising brittleness. Only then can we apply these calibrated tools to live debates, such as the current state of artificial intelligence research. Is the exponential rise in computational costs for large language models a sign of a degenerating research program, even as its short-term performance improves? A rising trend in a system's brittleness indicators does not prove its core claims are false, but it provides a strong, evidence-based signal that it is becoming a higher-risk, less viable research program.

### **7.3 The Problem of Scope: From Systemic Health to Individual Belief**

It is crucial to be precise about this model's scope. It is a macro-epistemology, designed to explain the long-term viability of public knowledge systems. It does not primarily aim to solve traditional problems in micro-epistemology, such as Gettier cases. Instead, it provides a robust bridge between these levels through the concept of higher-order evidence. The diagnosed health of a public system provides a powerful defeater or corroborator for an individual’s beliefs derived from that system.

To formalize this intuition, one can use a Bayesian framework. The diagnosed brittleness of a knowledge system should determine the rational prior probability an agent assigns to any claim from that source. A low-brittleness network warrants a high prior; a high-brittleness network warrants a low one. As Thomas Kelly (2005) argued regarding disagreement, the properties of the source matter. When an agent receives new first-order evidence, their posterior confidence is updated via Bayes' rule. This formalizes why an agent should rationally favor a claim from a low-brittleness network: even if a source from a high-brittleness network presents a seemingly powerful piece of evidence, the extremely low prior assigned to that source means the agent's posterior confidence in its claims will remain low. The macro-level diagnosis thus provides a rational basis for allocating trust.

### **7.4 Defending the Model's Own Status**

Any naturalistic model of objectivity must face ultimate tests of its grounding: can its core metrics be defined objectively, and can the model account for its own epistemic status without circularity?

First, the concept of "cost" is defined through objective indicators, as detailed in Section 2. The most fundamental costs for an epistemic system are those that undermine its own constitutive goals: crippling inefficiency, loss of predictive power, and institutional stagnation, measured by indicators like the rate of ad hoc modification.

Second, the model accounts for its own epistemic status by presenting itself not as an a priori truth but as a falsifiable research program. Its core causal claim is a testable prediction: a network with a high or rising degree of measured brittleness carries a statistically higher probability of collapse or major revision when faced with a comparable external shock. The theory is therefore rigorously falsifiable. If broad, methodologically sound historical analysis revealed no statistically significant correlation between indicators of high systemic cost and long-term network fragility, the framework's core causal engine would be severely undermined. This falsifiability criterion ensures the model meets the empirical standards it advocates for knowledge systems generally.

## **8. Conclusion: An Evolutionary Approach to Inquiry**

This paper has proposed a pragmatic and externalist framework for coherentism as a response to the isolation objection. By grounding justification in the long-term viability of knowledge systems rather than in internal consistency alone, the framework provides the external constraint that coherentism requires while preserving its holistic insights. The concept of systemic brittleness offers a naturalistic diagnostic tool for evaluating systems of inquiry, while the notion of an emergent Apex Network explains how objective knowledge can arise from fallible human practices.

The framework for assessing systemic brittleness makes this evolutionary process analyzable. By systematically studying the record of failed systems, we can begin to discern the contours of the Apex Network: the emergent set of maximally convergent, pragmatically indispensable principles that successful inquiry is forced to discover. The result is a form of systemic externalism that offers a novel approach for resolving long-standing problems in post-Quinean epistemology by grounding justification in the non-discursive filter of systemic consequences.

This model is not presented as a final, complete system, but as the foundation for a progressive and falsifiable research program. Critical future challenges remain, such as fully modeling the role of institutional power in creating path-dependent fitness traps and applying the framework to purely formal domains like mathematics. These are not flaws in the model, but precisely the kinds of tractable, empirical questions it is designed to generate.

This paper began with the challenge of distinguishing viable knowledge from brittle dogma. The model developed here suggests the ultimate arbiter is not the elegance of a theory or the consensus of its adherents, but the trail of consequences it leaves in the world. While the framework operates at a high level of abstraction, its primary data streams originate from the practice of inquiry itself. Systemic costs are ultimately experienced by researchers and communities as wasted effort, persistent anomalies, and the frustration of explanatory goals. Scientific dissent and methodological friction are therefore not merely sociological phenomena; they are primary sources of epistemological data about a system's rising brittleness. This framework, in this light, can be understood as a tool for holding our knowledge-generating systems accountable to their core purpose. By making the indicators of systemic health part of a shared, evidence-based language, it helps us ask the most pragmatic question of all: "Is this way of thinking still working?"



## **Glossary**

### **Part 1: The Core Framework & Philosophical Stance**

**1. Emergent Pragmatic Coherentism (EPC)**
The name for the theoretical framework developed in this paper. It provides a naturalistic account of objectivity that avoids both foundationalism and relativism.
*   **Core Logic:** All knowledge begins within a coherentist web but is then subjected to a pragmatic, evolutionary filter. Objectivity is the *emergent result* of this filtering process, not a foundational starting point.
    *   It is **Pragmatic** because its ultimate court of appeal is the observable, real-world *costs* generated by a knowledge system when its ideas are put into practice.
    *   It is **Coherentist** in that it accepts the Quinean insight that claims are initially justified by their fit within a holistic network, rejecting the idea of isolated, foundational beliefs.
    *   It is **Emergent** because it argues that objectivity is not a pre-given metaphysical structure but an *achieved structural property* that arises from a historical process. As brittle, high-cost systems are filtered out, surviving systems are forced to converge on designs that conform to mind-independent constraints.
*   **Role in the Paper:** This is the overarching philosophical framework that provides the dynamism for Quine's static web, explaining how a coherent system is disciplined by an external, pragmatic check, allowing it to adapt, evolve, and converge on objective knowledge.

**2. Systemic Externalism**
The specific epistemological stance of the model, which synthesizes internalist and externalist conditions for justification.
*   **Core Claim:** Justification is a **two-condition property**. For a proposition to achieve the status of **Justified Truth**, it must meet two conditions: (1) it must cohere with the principles of its certifying **Shared Network** (the internalist condition), and (2) the **Shared Network** itself must be reliable, a status earned through a demonstrated historical capacity to maintain low **Systemic Brittleness** (the externalist condition).
*   **Function:** This solves the "isolation problem" for coherentism by adding an external check based on pragmatic performance. It grounds justification in the observable, historical track record of an entire public system.
*   **Distinction:** Unlike traditional process reliabilism which focuses on the cognitive processes of an individual, Systemic Externalism locates reliability in the *public, verifiable performance of the knowledge-certifying system*. Justification is a property of *propositions-within-a-proven-system*.

**3. Realist Pragmatism**
The model's philosophical identity, uniting two often-opposed traditions by arguing that being a realist is the most pragmatically effective strategy.
*   **Core Synthesis:**
    *   It is **Pragmatist** in its anti-foundationalism, its fallibilism, and its focus on inquiry as a problem-solving process whose success is measured by real-world consequences.
    *   It is staunchly **Realist** in its commitment to the **Apex Network** as a real, emergent structure of viable solutions whose contours are determined by mind-independent pragmatic constraints.
*   **Function:** This synthesis explains *how* a pragmatist inquiry can generate realist outcomes. The relentless, cost-based filtering of our ideas is precisely the mechanism that forces our fallible systems to conform to objective, structural facts about our world.

### **Part 2: The Units of Analysis: How Knowledge is Structured**

**1. Shared Network**
The primary unit of public knowledge and the central object of analysis in our model. This is the entity that evolves and is subject to pragmatic selection.
*   **Definition:** A *Shared Network* is a public, structural system of principles, practices, and validated information (e.g., a scientific discipline, a legal system, a stable craft tradition).
*   **Nature and Origin:** It is not merely an aggregate of individual beliefs. Rather, it is an *emergent solution* to a shared set of problems, formed when agents facing persistent pragmatic pressures are forced to converge on a common set of public concepts and rules.
*   **Function:** This is the entity whose systemic health and viability can be objectively assessed over time by gauging its *brittleness*. It is the vehicle for cumulative, inter-generational knowledge.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
A clarification of the model's naturalistic method, shifting the focus from private mental states to public, functional roles. This progression describes how a claim becomes an entrenched part of a knowledge system.
*   **Belief:** A private, psychological state of an individual agent. It is the raw material from which public claims are articulated but is not directly subject to public evaluation.
*   **Proposition:** The public, linguistic *expression* of a belief; a declarative sentence that makes a testable claim. It is a candidate for integration into a Shared Network.
*   **Standing Predicate:** The validated, reusable, and action-guiding conceptual tool extracted from a highly successful proposition (e.g., the property `...is an infectious disease`). It is a conceptual technology that has earned a durable, trusted status and functions as a core component of a network's processing architecture. It is the primary unit of cultural-epistemic selection.

### **Part 3: The Dynamics of Change: How Knowledge Evolves**

**1. Pragmatic Pushback**
The primary selective force driving the evolution of knowledge systems in our model.
*   **Definition:** The sum of the non-negotiable, non-discursive consequences that arise when a Shared Network's principles are applied to the world.
*   **Nature:** This feedback is not an "argument" but a material outcome: a bridge collapses, a treatment fails, a society fragments. It is the real-world filter for our ideas.
*   **Function:** This constant pressure generates objective, measurable *costs* that act as an evolutionary selection filter, forcing networks to adapt or risk systemic failure.

**2. Systemic Costs: A Two-Level Diagnostic Framework**
The set of concepts used to diagnose a network's health.
*   **First-Order Costs (The Symptoms):** The direct, material consequences of a network’s misalignment with its pragmatic environment. These are the objective, observable signals of dysfunction (e.g., failed predictions, excess mortality, resource waste).
*   **Systemic Costs (The Underlying Condition):** The secondary, internal costs a network incurs to *manage, suppress, or explain away* its first-order costs. They represent non-productive expenditures on internal maintenance rather than on adaptation. Key forms include:
    *   **Conceptual Debt:** The compounding fragility incurred by adopting flawed, complex "patches" to protect a core principle.
    *   **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing the dissent that arises from first-order costs.

**3. Systemic Brittleness & Its Modalities**
The central diagnostic concept for assessing a network's health and its vulnerability to future shocks.
*   **Definition:** A system’s *brittleness* is a measure of its accumulated, hidden systemic costs. A high degree of brittleness signals that a system is inefficient, fragile, and a degenerating research program.
*   **Distinction:** Brittleness is not the opposite of longevity. A brittle system can endure for a long time by expending massive energy on coercion and conceptual patches. *Viability*, in contrast, is the ability to adapt and solve problems with *low* systemic costs.
*   **Modalities:** The framework identifies two primary modalities of failure:
    *   **Epistemic Brittleness:** Failure of alignment with the causal structure of the world (e.g., Ptolemaic astronomy).
    *   **Normative Brittleness:** Failure of alignment with the constraints on stable human cooperation (e.g., a slave economy).

### **Part 4: The Structure of Objectivity: Truth, Reality, and Progress**

**1. Negative Canon**
The model's empirical and historical anchor for objectivity.
*   **Definition:** The robust, evidence-based catalogue of Shared Networks and core principles that have been historically invalidated by their own catastrophic **Systemic Costs**, leading to their collapse or abandonment (e.g., Ptolemaic astronomy, phlogiston chemistry).
*   **Function:** This represents our most secure form of objective knowledge: reliable, empirically grounded knowledge of what is structurally unviable. It functions as a "reef chart" for inquiry, marking the known hazards and providing the external boundary that prevents a collapse into relativism.

**2. The Apex Network vs. The Consensus Network**
The crucial distinction between the objective structure of viability our inquiry aims at (the Apex Network) and our current, fallible map of it (the Consensus Network).
*   **The Apex Network (The Objective Standard):** This is the paper's central realist concept. Its status is dual:
    *   **Ontologically:** The Apex Network is the complete set of all maximally coherent and pragmatically viable principles, whose structure is determined by mind-independent pragmatic constraints. It is not a metaphysical blueprint but an *emergent structural fact about our world*, discovered retrospectively through historical filtering. **Ontologically, it is real.**
    *   **Epistemically:** We can never have a final, complete view of this structure. It functions for us as a **regulative ideal** that makes our comparative judgments of brittleness meaningful. **Epistemically, it is an ideal we approximate.**
    *   **Function:** It is the ultimate, non-negotiable standard for **Objective Truth** (Level 1).

*   **The Consensus Network (Our Best Approximation):** Our current, best, and necessarily fallible reconstruction of the Apex Network's structure (e.g., mainstream contemporary science).
    *   **Authority:** Its authority derives not from social agreement alone, but from its demonstrated historical track record of maintaining low **Systemic Brittleness**.
    *   **Function:** It is the system that certifies **Justified Truth** (Level 2).

**3. The Three Levels of Truth**
The model's synthesized, procedural account of truth, designed to resolve the tension between fallibilism and realism. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.
*   **Level 3: Contextual Coherence:** The baseline status. A proposition is coherent *within any specific Shared Network*, regardless of its long-term viability. This explains the internal rationality of failed paradigms but is not sufficient for justification.
*   **Level 2: Justified Truth:** The highest practically achievable epistemic status. A proposition is justified as true if it is certified by a **Consensus Network** that has itself demonstrated a low and stable degree of systemic brittleness.
*   **Level 1: Objective Truth:** The ultimate, regulative ideal. A proposition is objectively true if its principles are part of the **Apex Network**. This is the standard our inquiry aims to meet, even if we can never be certain we have reached it.


## References

Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business.

Baggio, Guido, and Andrea Parravicini. 2019. "Introduction to Pragmatism and Theories of Emergence." *European Journal of Pragmatism and American Philosophy* XI-2. https://doi.org/10.4000/ejpap.1251.

Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.

Bennett-Hunter, Guy. 2015. "Emergence, Emergentism and Pragmatism." *Theology and Science* 13, no. 3: 337–57. https://doi.org/10.1080/14746700.2015.1053760.

Berlin, Brent, and Paul Kay. 1969. *Basic Color Terms: Their Universality and Evolution*. Berkeley: University of California Press.

BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.

Bradie, Michael. 1986. "Assessing Evolutionary Epistemology." *Biology & Philosophy* 1, no. 4: 401–59. https://doi.org/10.1007/BF00140962.

Brandom, Robert B. 1994. *Making It Explicit: Reasoning, Representing, and Discursive Commitment*. Cambridge, MA: Harvard University Press.

Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.

Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.

El-Hani, Charbel N., and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3, no. 2, article 3. http://commons.pacificu.edu/eip/vol3/iss2/3.

Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.

Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.

Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.

Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press.

Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.

Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.

Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.

Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press.

Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press. Originally published 1962.

Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.

Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.

Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific Research Programmes." In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.

Laudan, Larry. 1977. *Progress and Its Problems: Towards a Theory of Scientific Growth*. Berkeley: University of California Press.

Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.

Lynch, Michael P. 2009. *Truth as One and Many*. Oxford: Oxford University Press.

Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.

Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.

Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.

Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press.

Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press. Originally published 1878.

Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37, no. 7: 1948–70. https://doi.org/10.1080/09515089.2023.2236120.

Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.

Price, Huw. 1992. "Metaphysical Pluralism." *Journal of Philosophy* 89, no. 8: 387–409. https://doi.org/10.2307/2940975.

Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.

Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60, no. 1: 20–43. https://doi.org/10.2307/2181906.

Quine, W. V. O. 1960. *Word and Object*. Cambridge, MA: MIT Press.

Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press.

Rosenstock, Sarita, Cailin O'Connor, and Justin Bruner. 2017. "In Epistemic Networks, Is Less Really More?" *Philosophy of Science* 84, no. 2: 234–52. https://doi.org/10.1086/690641.

Rottschaefer, William A. 2012. "The Moral Realism of Pragmatic Naturalism." *Analyse & Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.

Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.

Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.

Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.

Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.

Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.

Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.

Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.
