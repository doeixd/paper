# **Systemic Viability and the Dynamics of Coherence: A Naturalistic Approach to Objectivity**

## **Abstract**

In an era where coherent but baseless narratives can destabilize societies and established scientific consensus faces ideologically driven attacks, the question of how to distinguish viable knowledge from brittle dogma has become more than an academic exercise. This paper confronts this challenge by addressing a classic vulnerability in W.V.O. Quine’s “Web of Belief”: a perfectly coherent network of beliefs could be a shared delusion, detached from reality. We argue that Quine’s static model lacks the dynamic mechanisms to explain how knowledge is disciplined by the world, and we resolve this by introducing a naturalistic externalist check: long-term pragmatic viability.

Our model proposes that knowledge systems are tested by the real-world costs generated when their core ideas are applied—a process of pragmatic selection. To analyze this, we develop a framework for assessing a system’s *brittleness*, its vulnerability to stress, by tracking observable costs—from failed predictions to institutional decay. This evolutionary process is driven by two key dynamics: systems learn from failure by empirically mapping what is unviable, and they learn from success by entrenching their most effective, cost-reducing discoveries as core principles for future inquiry.

This leads to a form of *systemic externalism*, where a claim’s justification depends not only on its internal coherence but on the demonstrated historical resilience of the entire public system that certifies it. This framework explains how our fallible knowledge systems are forced to converge on what we term the Apex Network: not a pre-existing blueprint of truth, but the real, emergent structure of viable solutions discovered retrospectively through the historical filtering of what fails. Its objectivity is grounded in mind-independent pragmatic constraints.

The result is a three-level framework for truth that distinguishes mere contextual coherence from justified belief within a resilient system, and objective truth as alignment with this emergent structure of viability. By providing the missing dynamism for Quine’s web, our model explains how the practical project of cultivating more resilient and effective problem-solving systems becomes a self-correcting process for generating objective knowledge. It yields an interdisciplinary research program for assessing the health of our most critical epistemic systems, from scientific paradigms to the networks that structure public discourse.

## **1. Introduction: From a Static Web to a Dynamic Process**

In the 19th century, two rival theories competed to explain the spread of diseases like cholera. The dominant miasma theory held that disease was caused by "bad air" arising from decay, while the challenger, germ theory, posited that invisible microorganisms were the culprits. We now consider the triumph of germ theory a textbook case of scientific progress. But on what grounds do we justify this judgment? A sophisticated miasma theorist could have constructed a system as internally coherent as germ theory. How, then, can we defend our choice without simply appealing to our own network's standards—a move that would leave us vulnerable to the charge of circularity?

This paper argues that the answer lies not in static coherence, but in analyzing the long-term pragmatic viability of these competing systems. The miasma network was demonstrably *brittle*; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad-hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions like sanitation and explained a wide range of phenomena with a single, powerful conceptual tool.

This perspective reveals a deeper truth about how knowledge evolves. Inquiry is not a search for ultimate foundations but an ongoing, evolutionary process of cultivating more viable, less fragile public knowledge systems. This paper develops a model to explain this process. It is a macro-epistemology, a theory about the long-term viability of public, cumulative systems like science and law. A crucial distinction must be made to pre-empt a common critique: a system's viability is not mere longevity or "survival of the fittest" in a social Darwinist sense. A brutal empire that persists through coercion is not a viable system in our terms, but a textbook example of a high-brittleness one, whose endurance is merely a measure of the immense energy it wastes to suppress its own self-inflicted instability. Viability is defined here as the capacity to adapt and solve problems with low systemic costs.

This failure-driven, adaptive process grounds a robust but fallible realism. It explains how our evolving knowledge systems are forced to converge on an emergent structure of workable solutions determined by mind-independent pragmatic constraints. The claim of this framework is therefore modest: it proposes that beneath the surface-level 'noise' of contingency and power, there are underlying structural pressures at work. A system accumulating brittleness is not *fated* to collapse on a specific day, but it becomes progressively more *vulnerable* to the very contingent shocks that historians study. This model, therefore, does not offer a deterministic theory of history, but rather a probabilistic framework for understanding the structural dynamics that shape the landscape upon which historical events unfold.

## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 The Deflationary Path: From Private Belief to Public Standard**

To analyze the dynamics of public, cumulative knowledge, the rich but opaque concepts of individual psychology are insufficient. We must begin with a systematic, deflationary process that moves from the private and inaccessible to the public and functional. This procedure is not a mere terminological choice; it is a necessary step for any naturalistic theory that seeks to explain how objective knowledge can emerge from the aggregation of subjective experiences.

`[FIGURE 1: Conceptual Flowchart for the Deflationary Path]`
`[Belief (Private State)] --> [Articulation into a Proposition (Public Claim)] --> [Coherence Test Filter (Pragmatic Assessment)] --> [Integration as Data OR Promotion to Standing Assertion (Public Function)]`

**Belief → Proposition → Integrated Data → Standing Assertion**

1.  **From Private Belief to Public Proposition:** The journey begins with *belief*, the raw material of epistemology. As a private psychological state tied to an individual's consciousness, it is analytically inaccessible for a theory of public knowledge. The first step is therefore to isolate its testable, public content as a *proposition*. A proposition is a falsifiable claim that can be articulated, communicated, and collectively assessed.

2.  **The Coherence Test:** Next, a candidate proposition must be tested for *coherence* with the existing network. This is not the thin, formal consistency of logic, but a thick, forward-looking pragmatic assessment—a form of risk analysis. A shared network, as a resource-constrained system, implicitly asks: will integrating this proposition increase or decrease our long-term systemic brittleness?

3.  **From Integrated Data to Standing Assertion:** Finally, propositions that pass this test with exceptional success by dramatically reducing a network's systemic costs undergo a profound status change. They are promoted from being just *data within* the network to becoming part of its core *evaluative architecture*. This is the process by which a validated proposition becomes what we will call a **Standing Assertion**. Having proven its immense pragmatic value, it is no longer treated as a hypothesis to be constantly re-tested, but as a reliable standard against which new candidate propositions are judged. It has transitioned from *being-tested* to *doing-the-testing*.

### **2.2 The Units of Analysis: Functional Templates and Shared Networks**

With these core concepts established, we can specify the components of our model. Our analysis shifts from the individual to the public, functional structures of knowledge.

*   **Functional Template:** This refers to the reusable, abstract conceptual pattern within a proposition (e.g., `...is an infectious disease`). These templates are the generative "genes" of cultural evolution.
*   **Shared Network:** These templates are embedded in propositions within *Shared Networks*. These are coherent, public systems of standing assertions and validated information that emerge from the forced, bottom-up convergence of individual agents tackling shared problems. Science and the common law are prime examples.

To be precise about the unit of selection, we can adopt a distinction from generalized evolutionary theory. The network’s informational structure functions as the *replicator*: the abstract code that is copied and transmitted. The social group and its institutions function as the *interactor*: the physical vessel through which the code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**

A shared network is not a passive library; it is an active system under constant pressure from *pragmatic pushback*: the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback is not an argument but a material outcome—a bridge collapses, a treatment fails, a society fragments. This process generates two types of costs.

**First-Order Costs** are the direct, material consequences: failed predictions, wasted resources, environmental degradation, or systemic instability (e.g., excess mortality). These are the objective signals of dysfunction. **Systemic Costs** are the secondary, internal costs a network incurs to *manage, suppress, or explain away* its first-order costs. These non-productive expenditures reveal a network's true fragility. Key forms include:
*   **Conceptual Debt:** The compounding fragility incurred by adopting flawed, complex "patches" to protect a core principle.
*   **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing dissent. Dissent, in this model, is a critical data stream signaling that a system is generating costs for its members.

### **2.4 Gauging Brittleness: An Empirical Toolkit**

A system’s *brittleness* is a measure of its accumulated, hidden systemic costs. This can be operationalized by tracking concrete, measurable indicators. To move from philosophical model to empirical practice, the following table provides a proof-of-concept for how such a "brittleness dashboard" could be constructed. While all such proxies are themselves subject to methodological bias audits, they ground the framework in a testable methodology.

| Indicator | Domain of Application | Potential Proxy Metric | Data Sources (Illustrative) |
| :--- | :--- | :--- | :--- |
| **Rate of Ad-Hoc Modification** | Scientific Paradigms | Ratio of auxiliary hypotheses vs. novel predictions in published literature. | Academic databases (e.g., arXiv, Scopus) |
| **Ratio of Coercion to Production** | Socio-Political Networks | Ratio of state budget for internal security vs. R&D and public health. | World Bank, National Budget Data, Seshat Databank |
| **Increasing Model Complexity** | Computational Systems (e.g., Deep Learning) | Escalation in FLOPs for marginal performance gains; growth in prompt-tuning papers relative to foundational advances (e.g., 5x from 2020-2025). | arXiv trends, AI conference proceedings |

These indicators provide an objective check on subjective interpretations of coherence. A key methodological challenge is distinguishing a degenerative "patch" from a progressive hypothesis in a non-circular way. We can do so by assessing its *explanatory return on investment*. A *progressive hypothesis* offers a high return: for a small investment in added complexity, it yields novel predictions or unifies disparate phenomena. A *degenerative patch* offers a low return: it is a high-cost fix that resolves only the targeted anomaly and often increases the network's overall complexity.

## **3. The Drivers of Adaptation: The Logic of Systemic Viability**

The framework for assessing systemic health detailed in Section 2 is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems.

### **3.1 The Constitutive Demands of Inquiry**

A powerful objection must be addressed: that a focus on "viability" simply smuggles in an arbitrary preference for values like efficiency or persistence. The model’s authority is not grounded in a chosen value, but in a *constitutive condition* for the practice of cumulative, inter-generational inquiry itself. The framework does not argue that systems *ought* to value their own persistence. Instead, it makes a structural, descriptive claim: endurance is not a value *within* the game of inquiry; it is the inescapable precondition that makes the game possible over time. A network that systematically undermines its own ability to persist cannot, by definition, succeed at the project of accumulating and transmitting knowledge.

The pressure to maintain a low-brittleness design is thus the non-negotiable "gravity" of public inquiry. The choice is not between a viable system and an equally valid but non-viable one; it is between a system that can persist long enough to learn and one that collapses under the weight of its own costs. Viability isn't an optional norm to be adopted; it is a structural constraint on any informational system that manages to become part of the historical record at all. It is essential to distinguish this *causal mechanism* of pragmatic selection from a theory of rational choice. While agents may consciously seek to reduce costs, the model's primary engine is evolutionary and often impersonal. A high-brittleness system can fail not because its members rationally decide to abandon it, but because it collapses under its own weight.

### **3.2 Coherence as Forward-Looking Risk Assessment**

Framed by the logic of viability, the test for coherence is a thick, forward-looking *cost-benefit analysis*: a set of heuristics that a resource-constrained system uses to bet on whether adopting a new proposition will increase or decrease its long-term brittleness. The traditional epistemic virtues are the core principles of this practical calculus:
*   **Logical Consistency:** A hedge against the infinite future costs of inferential paralysis.
*   **Explanatory Power:** A measure of a proposition’s potential return on investment, reducing future inquiry costs by paying down conceptual debt.
*   **Simplicity / Parsimony:** A direct measure of systemic overhead; complex propositions increase long-term maintenance costs.
*   **Evidential Support:** An assessment of integrative risk; a well-supported claim is a low-risk investment unlikely to trigger a cascade of costly future revisions.

## **4. Convergence and the Emergence of Objective Structures**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We will show how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

Our account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system—its descent into crippling inefficiency, intellectual stagnation, and institutional decay—provides a clear, non-negotiable data point. The systematic analysis of these failures allows us to build the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been **empirically invalidated** by the catastrophic Systemic Costs they reliably generate.

We focus on failure because it provides the clearest, least ambiguous signal. For example:
- The network design based on the principle `appeals to authority are a final justification` has been historically invalidated by a consistent pattern of institutional stagnation, accumulating conceptual debt, and eventual paradigm collapse (e.g., scholastic physics versus Galilean empiricism). Its high brittleness across contexts reveals it to be a demonstrably brittle design.
- The socio-political network design based on the principle `slavery is a viable principle of economic organization` has been invalidated by the immense and unsustainable Systemic Costs required to maintain it—from vast coercive overheads to the suppression of innovation—rendering it profoundly fragile. The principle is not wrong because of a modern moral judgment; it is a failed engineering principle.

By charting what demonstrably fails, we are not merely learning what to avoid; we are reverse-engineering the hard constraints of a real territory. The Negative Canon functions like a reef chart for inquiry; it is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and prevents a collapse into relativism.

### **4.2 The Apex Network: An Emergent Structure of Viability**

The relentless filtering documented in the Negative Canon is also profoundly constructive, as it progressively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we call the **Apex Network**. It is best understood as a real, structural property discovered through inquiry, not a goal aimed at from the start. Its reality is akin to that of the principles of natural selection: an objective fact about how complex systems behave under selective pressure. It can be visualized as a "fitness landscape" for knowledge systems, where the pressure of pragmatic pushback creates peaks of viability and valleys of failure.

`[FIGURE 2: Conceptual Sketch of a Fitness Landscape]`
`[A 3D landscape with "Viability" on the Y-axis. Peaks are labeled with successful systems like "Germ Theory" and "Relativity." Deep valleys and low plateaus are labeled with failed systems from the Negative Canon, such as "Ptolemaic System," "Lysenkoism," and the "Ming Dynasty Trap."]`

This framework directly addresses the challenge of pluralism. While some domains with tight, universal constraints (like basic physics) may have a single, sharp peak toward which all inquiry is forced to converge, other complex domains may feature multiple, locally stable peaks of comparable viability. The research program this model proposes does not presuppose universal convergence. Rather, it asks an empirical question: for a given domain, does the historical process of pragmatic filtering tend to eliminate all but one design, or does it permit a stable pluralism? The tendency over long historical epochs, however, appears to be toward convergence on core principles, suggesting that many apparent pluralisms are temporary features of our incomplete map, not permanent features of the objective landscape itself.

### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds our fallibilist but realist account of truth, resolving the isolation objection by reframing truth as a status that propositions acquire through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence:** The baseline status. A proposition is coherent *within any specific Shared Network*.
*   **Level 2: Justified Truth:** The highest practically achievable epistemic status. A proposition is justified as true if it is certified by a *Consensus Network* that has itself demonstrated a low and stable degree of systemic brittleness.
*   **Level 1: Objective Truth:** The ultimate, regulative ideal. A proposition is objectively true if its principles are part of the real, emergent **Apex Network**.

This layered framework allows for sharp and non-anachronistic historical judgments. The claim "The sun revolves around the Earth" was contextually coherent (Level 3) within the Ptolemaic network. It never, however, achieved the status of justified truth (Level 2), because the Ptolemaic network itself was demonstrably failing its pragmatic stress test, as evidenced by its rising brittleness.

### **4.4 The Evolving Structure of Knowledge: Convergent Core and Pluralist Frontier**

The state of our inquiry at any given time can be understood as having two distinct epistemic zones.
*   **The Convergent Core:** Domains where relentless pragmatic selection has eliminated all but a single, or functionally identical, set of low-brittleness principles (e.g., the laws of thermodynamics).
*   **The Pluralist Frontier:** Domains of active research where multiple, competing systems may exist with comparably low brittleness, representing a state of epistemic underdetermination (e.g., interpretations of quantum mechanics).

### **4.5 Case Study: A Shared Perceptual Landscape**

To provide an intuitive grasp of how objective structures can emerge from a filtering process, let us move beyond formal domains like science to a simpler question: is there an objective "fitness landscape" for human color preference? At the level of individual choice, answers to "What is your favorite color?" seem entirely subjective. One person favors red for its association with vitality, another green for its calmness. Yet across a vast range of cultures and contexts, a recurring pattern appears: blue often emerges as a common preference. This convergence is not an accident; it is an emergent structural fact that points to an underlying, non-arbitrary landscape of perceptual viability.

In our model, the "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection acting on our biology. Human color vision did not evolve in a vacuum; it was forged by the selective pressures of navigating a world of sky, water, foliage, blood, and fire. The salience of certain wavelengths is the result of countless filtering events in which perceptual systems that efficiently tracked ecologically critical signals were more viable than those that did not. As research on universal color stages suggests (Berlin and Kay 1969), despite cultural variance, human languages tend to converge on a similar hierarchy of basic color terms, reflecting non-arbitrary structures in our shared perceptual apparatus. As Joseph Henrich (2015) argues, culture and genes co-evolve, meaning our perceptual priors are disciplined by the pragmatic costs of misperception (e.g., foraging failures, misidentifying threats).

The observed tendency for blue to emerge as a preference is therefore not merely a statistical curiosity. It is the visible peak on our shared perceptual fitness landscape—a miniature Apex Network. It is not a rule written in a Platonic heaven stating "Blue is the best color." Instead, it is the emergent, structural fact that "associating the color blue with positive, stable conditions" is a highly viable, low-cost default for a species evolved in our specific terrestrial environment. Of course, cultural variation still exists—the **Pluralist Frontier** of our perceptual world—where red might be privileged in one ritual context and white in another. Yet beneath this surface-level pluralism lies a deep, trans-historical attractor. Objectivity here means convergence under constraint, not timeless certainty.

### **4.6 Case Studies in Convergence and Brittleness**

The transition from Newtonian to relativistic physics offers a canonical example of diagnosing a paradigm shift. The Newtonian system, after centuries of viability, began to accumulate catastrophic costs in the late 19th century, manifesting as failed predictions (e.g., Mercury's perihelion) and rising conceptual debt (e.g., the ad-hoc Lorentz-FitzGerald contraction hypothesis). The Einsteinian system proved to be a vastly more effective and resilient solution, paying down this debt and dramatically lowering the systemic costs of physics.

A more contemporary case can be found in the recent history of Artificial Intelligence. This domain provides a clear example of a "brittleness audit" in action. The "AI winter" of the late 20th century can be seen as the collapse of the high-brittleness symbolic AI paradigm, which suffered from a catastrophic rate of ad-hoc modification. The deep learning paradigm that followed proved to be a low-brittleness solution for specific tasks, but it is now showing signs of rising systemic costs:
*   **Massive Energetic Inefficiency:** The computational cost (measured in FLOPs) required for state-of-the-art models is growing at an exponential and unsustainable rate, with some estimates suggesting a 100x or greater escalation for each marginal performance gain.
*   **Accelerating Ad-Hoc Modification:** The need for constant, post-hoc "patches"—from prompt engineering to alignment tweaks—is a clear indicator of mounting conceptual debt. The ratio of papers on such auxiliary modifications to those on foundational architectural advances has increased dramatically from 2020 to 2025.

This illustrates the Pluralist Frontier in action, as rival architectures now compete to become the next low-brittleness solution.

### **4.7 Navigating the Landscape: Fitness Traps and Path Dependence**

This evolutionary model does not imply a simple, linear march of progress. The landscape of viability is complex, and knowledge systems can become stuck in sub-optimal but locally stable states. A system can become locked into a high-brittleness **fitness trap** due to coercive institutions or other contingent historical factors. A slave economy, for instance, is a classic fitness trap: it is objectively brittle in the long run, but it creates path-dependent institutions that make escaping the trap even costlier in the short term.

We can begin to metricize this trap depth. Drawing on cliodynamic analysis (Turchin 2003), a system where the ratio of coercive overheads (e.g., internal security spending) to productive capacity (e.g., R&D, infrastructure) exceeds a certain threshold for a sustained period becomes highly vulnerable to collapse. For example, a historical polity where coercive overheads consumed over 30% of state resources for more than 50 years exhibits a significantly higher probability of fragmentation when faced with an external shock. In Ming China, the immense coercive overheads required to maintain ideological and social stability are estimated to have consumed a vast portion of state capacity, stifling the innovation needed to adapt to novel shocks and leaving it profoundly vulnerable. The persistence of such a system is not a sign of its viability, but a measure of the energy it must expend to resist the structural pressures pushing it toward collapse.

## **5. The Dynamism of the Web: How Knowledge Becomes Entrenched**

Quine’s "Web of Belief" provided a brilliant static anatomy of a knowledge system, but it lacked a corresponding physiology. This section provides that dynamic physiology, detailing the naturalistic process by which a successful discovery migrates from the tentative "periphery" of the web to its load-bearing "core."

### **5.1 From Tentative Hypothesis to Core Principle**

A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the **Conservation of Energy**, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under **bounded rationality**; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of **systemic caching**. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of **Justified Truth (Level 2)**.

### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web. First, it supplies a robust **externalist filter**—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed **learning mechanism**—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **6. Situating the Model: A Naturalistic Synthesis**

This model of inquiry offers a novel synthesis designed to resolve long-standing tensions in epistemology, carving out a unique position as a form of *realist pragmatism*. This section situates the model by clarifying its relationship to other major research programs.

### **6.1 Systemic Externalism and Social Epistemology**

Our model constitutes a form of **Systemic Externalism**. Unlike traditional externalism, which locates reliability in the cognitive processes of an individual, our model scales this insight to the public, historical level. Justification is a property of **beliefs-within-a-proven-system.** This provides an evolutionary grounding for the core insights of social epistemology. Norms like peer review and viewpoint diversity are not a priori ideals but adaptive strategies that have survived because they demonstrably cultivate low-brittleness networks. Our model provides the externalist check of pragmatic viability that purely procedural or consensus-based accounts can lack, a point recently echoed by allies in the field who advocate for what Glenn (2025) has termed "dynamic holism."

### **6.2 Cultural Evolution and Structural Realism**

Our framework refines cultural evolutionary theory by providing a non-circular standard for fitness: long-term pragmatic viability, not mere transmissibility. It also provides a dynamic, naturalistic engine for the core thesis of structural realism. It explains *how* and *why* scientific inquiry is forced to converge on objective, relational structures (the Apex Network) through the brutal, eliminative process of pragmatic selection.

### **6.3 A Realist Corrective for Neopragmatism**

Our model retains the anti-foundationalist spirit of pragmatism while providing a decisive realist corrective to the influential neopragmatism of thinkers like Richard Rorty. The analysis of systemic failure is the non-discursive, often brutal filter that more discourse-focused pragmatisms lack. The collapse of the Soviet Union's Lysenkoist biology provides a stark illustration of this corrective. This failure was not due to a breakdown in conversation—indeed, the 'conversation' was brutally enforced. It was a systemic failure driven by the catastrophic first-order costs of agricultural collapse. Lasting solidarity is not an *alternative* to objectivity; it is an **emergent property** of a low-brittleness network that has successfully adapted to the pragmatic constraints of its environment. This stands in contrast to recent attempts to define a purely "algorithmic truth" based on stable consensus in language models, which our framework would diagnose as a potential coherence trap lacking an externalist check.

## **7. Defending the Model: Addressing Key Challenges**

A philosophical model must be tested against its most difficult cases. This section demonstrates the resilience of our framework by engaging directly with core challenges.

*   **The Challenge from Coherent Fictions:** Our model resolves the "coherence trap" (e.g., a sophisticated conspiracy theory) by insisting on the second, externalist condition: a demonstrated historical track record of low systemic brittleness. Such networks fail this test catastrophically. They can only maintain coherence by incurring massive conceptual debt through an accelerating rate of ad-hoc modification and often require high coercive overheads to maintain ideological purity. Furthermore, such networks are often **epistemically parasitic**: they generate no novel, productive research but exist only to create after-the-fact explanations for the successes of a host network (e.g., mainstream science).

*   **The Challenge from Flawed but Enduring Paradigms:** Our framework distinguishes mere *endurance* from pragmatic *viability*. A system like Ptolemaic cosmology that endures by accumulating immense conceptual debt is a high-cost, brittle system, not a viable one. Its apparent stability is not a sign of health but a direct measure of the systemic costs it must pay to function. Its longevity, therefore, does not justify its principles; it merely represents a long-running, inefficient experiment whose high measured brittleness made it profoundly vulnerable to a more effective competitor.

*   **The Challenge from Incommensurability:** While semantic incommensurability may be real, paradigms can be compared on the meta-level, externalist standard of systemic viability, as gauged by their measured brittleness. A **Kuhnian crisis** is not just a sociological phenomenon; it is the name for the observable state of a network suffering from catastrophically high brittleness. This allows us to rationally compare apparently "incommensurable" paradigms by analyzing their respective systemic health.

*   **Clarifying the Scope: The Macro/Micro Bridge:** While a macro-epistemology, our model connects to individual justification via the concept of **higher-order evidence**. The diagnosed health of a public system provides a powerful defeater (or corroborator) for an individual’s beliefs derived from that system. To make this concrete, consider an agent weighing a claim from a low-brittleness source (like an IPCC report) against one from a high-brittleness source (a denialist documentary). The macro-level diagnosis of the source systems provides a rational basis for trusting the former, even without being an expert on the specific claims.
    To formalize this intuition, we can use a Bayesian framework. An agent’s rational confidence in a claim from a given source, P(Claim|Source), should be treated as a prior probability conditioned by the diagnosed health of the source network. A low-brittleness network (IPCC) warrants a high prior; a high-brittleness network (denialist source) warrants a low one. As Thomas Kelly (2005) argued regarding disagreement, the properties of the source matter. When the agent receives new first-order evidence, *E*, the posterior confidence, P(Claim|Source, E), is updated. This formalizes why the agent should rationally favor the IPCC's claim: even if the denialist source presents a seemingly powerful piece of evidence (a local defeater), the extremely low prior assigned to that network means the agent's posterior confidence in its claims will remain low. The macro-level diagnosis provides a rational, quantitative basis for allocating trust and resisting misleading defeaters.

*   **The Challenge of Defining "Costs" without Subjectivity:** The model is anchored in a tiered framework of costs, moving from foundational, least-contestable bio-social costs (e.g., excess mortality) to more domain-specific epistemic ones (e.g., rising model complexity). Crucially, it diagnoses **cost-shifting**—where a system appears efficient by deferring its costs onto a social or ecological substrate—as a primary indicator of hidden, long-term brittleness.

*   **The Challenge of Self-Application:** This model asks to be judged by its own standards. It offers itself as a more viable research program for epistemology, one that aims to resolve long-standing philosophical anomalies with lower conceptual debt than its rivals. Its justification is not that it is self-evident, but that it constitutes a low-brittleness research program for epistemology itself.

## **8. Conclusion: An Evolutionary Approach to Inquiry**

Quine’s great contribution was to transform the static, foundationalist pyramid of knowledge into a holistic, flexible web. This paper has taken the next logical step: to provide the dynamic physiology for that web. We have framed inquiry not as a project of rational design, but as an evolutionary process of cultivating more resilient public knowledge systems, driven by the selective pressures of real-world costs.

Our framework for assessing systemic brittleness makes this process analyzable. By systematically studying the record of failed systems, we can discern the contours of the Apex Network: the emergent landscape of viable solutions that successful inquiry is forced to discover. The result is a form of Systemic Externalism that resolves long-standing problems in post-Quinean epistemology by grounding justification in the non-discursive filter of systemic consequences.

This model is not presented as a final, complete system, but as the foundation for a progressive research program. Critical future challenges remain, such as fully modeling the role of power asymmetries in creating path-dependent 'fitness traps' and applying the framework to purely aesthetic or mathematical domains. These are not flaws in the model, but precisely the kinds of tractable, empirical questions it is designed to generate.

We began with the challenge of distinguishing viable knowledge from brittle dogma in an era of epistemic crisis. The model we have developed suggests the ultimate arbiter is not the elegance of a theory or the consensus of its adherents, but the trail of consequences it leaves in the world. Dissent, instability, and suffering are not merely political problems; they are epistemic signals that a system is failing its most fundamental pragmatic test. The most pragmatic question—"Is this way of organizing ourselves still working?"—is therefore not a departure from the search for objectivity, but the very heart of it.

## Glossary

### **Part 1: The Core Framework & Philosophical Stance**

**1. Emergent Pragmatic Coherentism (EPC):** The full theoretical framework of the paper, designed to provide a naturalistic account of objectivity that avoids both foundationalism and relativism. Each component of its name is crucial:
- It is **Pragmatic** because its ultimate court of appeal is not abstract reason, but the observable, real-world **costs** generated by a knowledge system when its ideas are put into practice.
- It is **Coherentist** in that it accepts the Quinean insight that propositions are initially justified by their fit within a holistic network. It rejects the idea of isolated, foundational beliefs.
- It is **Emergent** because it argues that objectivity is not a pre-given metaphysical foundation but an **achieved structural property** that arises from a historical, evolutionary process. As brittle, high-cost networks are filtered out by **Pragmatic Pushback**, surviving systems are forced to converge on designs that conform to mind-independent constraints.
- **In Synthesis:** EPC provides the **dynamism** for Quine's static web, explaining how a coherent system is disciplined by an external, pragmatic check, allowing it to learn, evolve, and converge on objective truth.

**2. The Evolutionary Model of Inquiry:** The paper's framing of the project of inquiry, which reframes the entire project from a search for certainty to a form of **epistemic engineering**.
- **Core Idea:** The primary goal of inquiry is not to discover a set of final, incorrigible truths, but to design, build, and maintain more resilient, less brittle public knowledge structures (**Shared Networks**).
- **Methodology:** This model evaluates progress by diagnosing a network's structural health and adaptive efficiency. Progress is the observable, empirical process of engineering networks that demonstrably reduce their systemic costs over time.

**3. Systemic Externalism:** The specific epistemological stance of the model, which synthesizes the strengths of internalism and externalism.
- **Core Claim:** Justification is a **two-level property**. For a proposition to achieve the status of **Justified Truth**, it is not enough for it to cohere within a network (the internalist condition). The **Shared Network itself**, as a public, historical entity, must have demonstrated its reliability by maintaining low **brittleness** against real-world selective pressures. Justification is thus a property of **beliefs-within-a-proven-system**.
- **Philosophical Payoff:** This solves the "isolation problem" for coherentism by adding an external check. It also improves on traditional externalism by shifting the locus of reliability from an individual's opaque cognitive processes to the observable, historical track record of the entire public knowledge system.

**4. Realist Pragmatism:** The model's unique philosophical identity, which unites two often-opposed traditions.
- It is **Pragmatist** in its anti-foundationalism and its focus on inquiry as a fallible, engineering process of solving real-world problems.
- It is staunchly **Realist** in its commitment to the **Apex Network** as a real, emergent structure of viable solutions. The Apex Network is not a social construction; its structure is objectively determined by the mind-independent pragmatic constraints revealed through the historical filtering process.

### **Part 2: The Units of Analysis: How Knowledge is Structured**

**1. Shared Network:** The primary unit of public knowledge and the central object of analysis in EPC. This is the entity that evolves and is subject to pragmatic selection.
- **Definition:** A **Shared Network** is a public, structural system of principles, practices, and validated information (e.g., a scientific discipline, a legal system, a stable craft tradition).
- **Nature and Origin:** It is not merely an aggregate of individual beliefs. Rather, it is an **emergent engineering solution** to a shared coordination problem. When multiple agents face persistent, shared **Pragmatic Pushback**, they are forced to converge on a common set of public concepts and rules, creating a public architecture for collective action.
- **Function:** This is the entity whose structural health and viability can be objectively diagnosed over time by assessing its **brittleness**. It is the vehicle for cumulative, inter-generational knowledge.

**2. The Deflationary Path: Belief, Proposition, and Standing Assertion:** A crucial clarification of the model's naturalistic method, shifting the focus from private mental states to public, functional roles.
- **Belief:** A private, psychological state of an individual agent. It is the raw material from which public propositions are articulated.
- **Proposition:** The public, linguistic *expression* of a belief; a declarative sentence that makes a testable claim. It is a candidate for integration into a Shared Network.
- **Standing Assertion:** A proposition that has been so thoroughly validated through pragmatic testing that it is promoted from being a piece of data *within* the network to being part of the network's core *evaluative structure*. It transitions from *being-tested* to *doing-the-testing*.

**3. Functional Template**
The reusable, generative conceptual patterns that are embedded within propositions and standing assertions.
*   **Definition:** A *Functional Template* is the abstract, action-guiding "schema" or technology within a proposition (e.g., the relational concept `...is caused by...` or the property `...is an infectious disease`).
*   **Function:** These are the functional "genes" of cultural evolution. The long-term evolutionary success or failure of a Shared Network depends on the viability of its core functional templates.

### **Part 3: The Dynamics of Change: How Knowledge Evolves**

**1. Pragmatic Pushback:** The primary causal force driving epistemic evolution in the EPC model.
- **Nature:** It is the non-negotiable, non-discursive sum of the consequences that occur when a **Shared Network's** licensed actions misalign with real-world constraints. This feedback is not an "argument" but a material outcome: a bridge collapses, a treatment fails, a society fragments.
- **Function:** This constant pressure generates the objective, measurable **First-Order Costs** that act as an evolutionary selection filter, forcing networks to adapt or face systemic failure.

**2. A Two-Level Framework of Costs**
This is the set of concepts used to measure a network's viability, shifting evaluation from a binary true/false judgment to a diagnosis of engineering soundness.
- **First-Order Costs (The Symptoms):** The direct, material consequences of a network’s misalignment with reality. These are the objective, observable signals of dysfunction (e.g., failed predictions, excess mortality, resource waste).
- **Systemic Costs (The Underlying Disease):** The secondary, internal costs a network incurs to *manage, suppress, or explain away* its First-Order Costs. They represent non-productive expenditures on internal maintenance rather than adaptation. Diagnosing these hidden costs reveals a network's true fragility. The primary forms are:
    - **Conceptual Debt:** The compounding cost of fragility incurred by adopting flawed, complex "patches" to protect a core principle.
    - **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing dissent.

**3. Systemic Brittleness & Its Indicators**
The central diagnostic concept for assessing a network's health and its vulnerability to future shocks. It is a composite measure of a network's accumulated, hidden **Systemic Costs**, operationalized through several key, measurable indicators or proxies:
- **Rate of Ad-Hoc Modification:** An accelerating need for non-productive "patches" to save a core theory from anomalies.
- **Ratio of Coercion to Production:** The proportion of a system's resources spent on internal control versus productive adaptation.
- **Increasing Model Complexity:** A model requiring more free parameters just to fit existing data without increasing its predictive power.

### **Part 4: The Structure of Objectivity: Truth, Reality, and Progress**

**1. Negative Canon:** The model's empirical and historical anchor for objectivity.
- **Content:** The **Negative Canon** is the robust, evidence-based catalogue of **Shared Networks**, core principles, and architectural designs that have been historically invalidated by their own catastrophic costs (i.e., a chronically high **brittleness** leading to systemic collapse or abandonment). Examples include Ptolemaic astronomy, phlogiston chemistry, and Lysenkoist biology.
- **Function:** It represents our most secure form of objective knowledge—reliable, empirically grounded knowledge of what is structurally unviable. It functions like a reef chart for inquiry, allowing us to **reverse-engineer** the hard constraints of viability by mapping the known hazards.

**2. The Apex Network vs. The Consensus Network**
This entry clarifies the crucial distinction between the objective standard our inquiry aims at (the **Apex Network**) and our current, best approximation of it (the **Consensus Network**).
- **The Apex Network (The Objective Standard):** This is the singular, complete set of all maximally coherent and pragmatically viable principles.
    - **Ontology:** The Apex Network is not a pre-existing metaphysical blueprint but an **emergent structural fact about our world**. Its structure is objectively determined by the mind-independent pragmatic constraints revealed by the historical filtering process.
    - **Role:** It functions as the ultimate, non-negotiable standard for **Objective Truth (Level 1)**.
- **The Consensus Network (Our Best Reconstruction):** This is our current, best, and necessarily fallible reconstruction of the Apex Network's structure.
    - **Definition:** It represents the body of knowledge granted **Justified Truth (Level 2)** status at a given time (e.g., mainstream contemporary science). Its authority derives not from social agreement alone, but from its demonstrated historical track record of maintaining low **brittleness**.
    - **Structure:** Our Consensus Network has two epistemic zones:
        - **The Convergent Core:** Domains where relentless pragmatic filtering has eliminated all but a single, low-brittleness set of principles (e.g., the laws of thermodynamics).
        - **The Pluralist Frontier:** Domains of active research where multiple, competing networks currently exhibit comparably low and stable brittleness (e.g., interpretations of quantum mechanics).

**3. The Three-Level Framework of Truth**
The model's synthesized, procedural account of truth, designed to resolve the tension between fallibilism and realism. It reframes truth as a status earned through a process of **justificatory ascent**.
- **Level 3: Contextual Coherence:** The baseline status. A proposition is coherent within *any* specific **Shared Network**, regardless of its long-term viability.
- **Level 2: Justified Truth:** The highest achievable epistemic status. A proposition is justified as true if it is certified by a **Consensus Network** that *itself* has a demonstrably low and stable **brittleness**.
- **Level 1: Objective Truth:** The ultimate, regulative ideal. A proposition is **objectively true** if its principles are components of the **Apex Network**.

## **References**

Berlin, Brent, and Paul Kay. 1969. *Basic Color Terms: Their Universality and Evolution*. Berkeley: University of California Press.

Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press.

Kelly, Thomas. 2005. “The Epistemic Significance of Disagreement.” In *Oxford Studies in Epistemology, Vol. 1*, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.

Lakatos, Imre. 1970. “Falsification and the Methodology of Scientific Research Programmes.” In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.

Quine, W. V. O. 1951. “Two Dogmas of Empiricism.” *The Philosophical Review* 60 (1): 20–43.

Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press.

Simon, Herbert A. 1972. “Theories of Bounded Rationality.” In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.

Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
