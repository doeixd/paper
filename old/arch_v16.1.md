# **Convergent Coherence: How Failure Forges the Emergent Structure of Objective Truth**

## **Abstract**

Coherentist theories of justification remain vulnerable to the isolation objection: the possibility that a perfectly coherent belief system could be entirely detached from reality. This paper develops Emergent Pragmatic Coherentism (EPC), an externalist framework designed to resolve this challenge by grounding coherence in long-term pragmatic viability. The framework introduces "systemic brittleness" as a diagnostic for assessing a knowledge system's health by tracking the observable costs generated when its propositions are applied. It argues that the selective pressure of these costs forces disparate knowledge systems to converge on a single, maximally coherent system disciplined by mind-independent pragmatic constraints. This failure-driven process reveals an objective structure we term the Apex Network: not a pre-existing truth to be discovered, but a bottom-up emergent structure of maximally shared propositions that has survived historical filtering. This approach yields a form of Systemic Externalism, where a claim’s justification depends on the proven resilience of the public system certifying it. The result is a naturalistic theory that redefines objective truth as alignment with this structure, explaining how Quine’s web of belief is pragmatically revised, and grounding a falsifiable research program for assessing the health of our most critical epistemic systems.

## **1. Introduction: From a Static Web to a Dynamic Process**

Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.

However, any credible theory of knowledge evolution must account for the realities of power, path dependence, and historical contingency. This framework incorporates these factors not as exceptions, but as key variables within the model. The exercise of power to maintain a brittle system, for example, is not a refutation of the model but a primary indicator of that system's non-viability, measured through its high coercive costs. This failure-driven, adaptive process grounds a robust but fallible realism. It explains how our evolving knowledge systems are forced to converge on an emergent structure of workable solutions determined by mind-independent pragmatic constraints. The claim of this framework is therefore probabilistic, not deterministic: it proposes that beneath the surface-level 'noise' of contingency and power, there are underlying structural pressures at work. A system accumulating brittleness—a system’s vulnerability to collapse due to the accumulation of hidden, internal costs, a concept analogous to the notion of fragility developed by Taleb (2012)—is not fated to collapse on a specific date, but it becomes progressively more vulnerable to the very contingent shocks that historians study. This model, therefore, does not offer a deterministic theory of history, but rather proposes the foundation for a falsifiable, probabilistic research program for understanding the structural dynamics that shape the landscape upon which historical events unfold.

To prevent misunderstanding about the framework's scope and ambitions, we must be precise about what this paper does and does not attempt. This is not a foundationalist epistemology that aims to ground all knowledge in indubitable starting points, nor is it a general theory of justification applicable to all domains of inquiry. Rather, it is a specialized framework for understanding the evolution and evaluation of cumulative knowledge systems—those engaged in ongoing, inter-generational projects where claims build upon previous work and where practical consequences provide feedback about systemic performance. The framework applies most directly to domains like empirical science, legal systems, engineering, and public policy, where pragmatic pushback is relatively direct and measurable. Its application to purely theoretical domains like mathematics or formal logic requires additional development and may ultimately prove limited. We present this focus not as a defect but as a feature—the framework's power lies precisely in its focus on knowledge systems where failure and success have observable consequences.

## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

`\[FIGURE 1: Conceptual Flowchart for the Deflationary Path]`
`\[Belief (Private State)] --> \[Articulation into a Proposition (Public Claim)] --> \[Coherence Test] --> \[Integration as Validated Data OR Promotion to Standing Predicate (Public Tool)]`

**Belief → Proposition → Validated Data → Standing Predicate**

#### **2.1.1 From Private Belief to Public Proposition**

The journey begins with *belief*, the raw material of epistemology. As a private psychological state tied to an individual's consciousness, it is analytically inaccessible for a theory of public knowledge. The first step is therefore to isolate its testable, public content as a *proposition*. A proposition is a falsifiable claim that can be articulated, communicated, and collectively assessed.

#### **2.1.2 The Coherence Test**

Next, a candidate proposition must pass a rigorous test for coherence. This is not the thin, formal consistency of logic, but a thick, forward-looking pragmatic assessment—a form of risk analysis. A shared network, as a resource-constrained system, implicitly asks: will integrating this proposition increase or decrease our long-term systemic brittleness? A proposition that successfully passes this test is accepted into the network as validated data—a reliable claim that can be used and cited within the system, awaiting its ultimate functional test.

#### **2.1.3 From Validated Data to Standing Predicate**

Finally, propositions that not only pass the coherence test but do so with exceptional success—by dramatically reducing a network's systemic costs—undergo a profound status change. They are not merely stored as facts; their functional core is promoted and repurposed to become part of the network's core processing architecture.

This process creates what we will call a **Standing Predicate**. A Standing Predicate is the reusable, action-guiding conceptual tool within a proposition that has earned a durable, trusted status. It is the functional "gene" of cultural evolution. For example, once the proposition "Cholera is an infectious disease" proved its immense pragmatic value, its functional component—the predicate `...is an infectious disease`—was promoted. It became a Standing Predicate in the network of medical science.

This Standing Predicate now functions as a durable piece of conceptual technology. Applying it to a new phenomenon activates a rich sub-network of proven diagnostic heuristics, interventional policies, and licensed inferences. The original proposition has transitioned from *being-tested* data to a *tool-that-tests*. This promotion from data to a trusted, standing tool is the first and most crucial step in a network's ability to learn and upgrade its own architecture.

### **2.2 The Units of Analysis: Predicates, Networks, and Replicators**

Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

* **Standing Predicate:** This is the primary unit of cultural-epistemic selection. It is the validated, reusable, and action-guiding conceptual tool within a proposition (e.g., `...is an infectious disease`). Functioning as the generative "genes" of cultural evolution, a Standing Predicate is a highly compressed piece of conceptual technology. When applied, it unpacks a suite of previously validated knowledge, including causal models, diagnostic heuristics, and licensed interventions.
* **Shared Network:** This concept is not a novel theoretical entity but an observable consequence of Quine's holism applied to social groups. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested; a specialized network like germ theory forms a coherent subset of propositions within the broader network of modern medicine, which itself must align with the predicates of empirical science. The emergence of these networks is not a conscious negotiation but a structural necessity. An individual craftsperson whose canoe capsizes will holistically revise their personal web of belief about hydrodynamics; when a group must build a fleet, only the shared principles that lead to non-capsizing canoes can become part of the public, transmissible craft. The Shared Network is the public residue of countless such private, failure-driven revisions under shared pragmatic pressure.

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory, as synthesized for the social sciences by Mesoudi (2011). The network’s abstract informational structure—its core Standing Predicates and their relations—functions as the **replicator**: the "code" that is copied and transmitted. The social group and its institutions (the scientific community, legal system, or society) function as the **interactor**: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This distinction explains how knowledge can persist even when the societies that created it do not; the rediscovery of Roman legal principles during the Renaissance is a prime example of a successful replicator outliving its original interactor. The existence of these countless, independently formed and often nested Shared Networks—all responding to the same landscape of physical and social constraints—supports a crucial inference: they collectively reveal an objective structure that underwrites all successful inquiry. This mosaic of survived predicates, biased toward low-brittleness solutions, anticipates the Apex Network: not a pre-existing truth, but an emergent attractor forged by historical filtering (as detailed in Section 4).

### **2.3 Pragmatic Pushback and Systemic Costs**

A shared network is not a passive library; it is an active system under constant pressure from *pragmatic pushback*—our model’s term for the systemic analogue of what Quine called a "recalcitrant experience" at the periphery of an individual's web of belief. It is the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback is not an argument but a material outcome—a bridge collapses, a treatment fails, a society fragments. This process generates two types of costs.

**First-Order Costs** are the direct, material consequences: failed predictions, wasted resources, environmental degradation, or systemic instability (e.g., excess mortality). These are the objective signals of dysfunction. **Systemic Costs** are the secondary, internal costs a network incurs to *manage, suppress, or explain away* its first-order costs. These non-productive expenditures reveal a network's true fragility. Key forms include:

* **Conceptual Debt:** The compounding fragility incurred by adopting flawed, complex "patches" to protect a core principle.
* **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing dissent. These coercive overheads are the primary mechanism by which power dynamics manifest within our model; the resources spent to maintain a brittle system against internal and external pressures become a direct, measurable indicator of its non-viability. Dissent, in this model, is a critical data stream signaling that a system is generating costs for its members.

It is important to note that "pragmatic pushback" is not limited to direct, material failures. In highly abstract domains, such as theoretical physics or pure mathematics, where direct empirical tests are deferred or unavailable, pushback primarily manifests through the accumulation of Systemic Costs. A research program that requires an accelerating rate of ad-hoc modifications to maintain internal consistency, or that loses its unifying power, is experiencing a powerful form of pragmatic pushback. These epistemic inefficiencies are real costs that render a network brittle and unproductive, even if it has not been directly falsified by an experiment. The framework's diagnostic lens thus applies to all forms of inquiry, measuring viability through either external, material consequences or internal, systemic dysfunction.

### **2.4 Gauging Brittleness: An Empirical Toolkit**

A system's *brittleness* is a measure of its accumulated systemic costs. The following table illustrates potential diagnostic indicators:

| Indicator | Domain | Potential Proxy Metric | Data Sources |
| :--- | :--- | :--- | :--- |
| Rate of Ad-Hoc Modification | Scientific Paradigms | Ratio of auxiliary hypotheses vs. novel predictions | Academic databases |
| Ratio of Coercion to Production | Socio-Political Networks | Internal security vs. R&D budget ratios | Budget data, Seshat Databank |
| Increasing Model Complexity | Computational Systems | Resource escalation for marginal gains | arXiv trends, conference proceedings |

The operationalization of brittleness faces a fundamental circularity problem: measuring systemic costs objectively requires neutral standards for "waste" or "dysfunction," yet establishing such standards appears to require the very epistemic framework our theory aims to provide.

This circularity cannot be eliminated but can be managed through several strategies. First, we anchor measurements in basic biological and physical constraints: demographic collapse, resource depletion, infrastructure failure. These provide relatively theory-neutral indicators of breakdown. Second, we employ comparative rather than absolute measures, comparing brittleness trajectories across similar systems. Third, we require convergent evidence across multiple independent indicators before diagnosing brittleness.

Brittleness assessment remains partially hermeneutic. The framework provides structured tools rather than mechanical algorithms, making judgments more systematic and accountable to evidence without eliminating interpretation. This constrains the framework's ambitions: it offers "structured fallibilism" rather than neutral assessment.

We distinguish degenerative "patches" from progressive hypotheses by assessing explanatory return on investment. Progressive hypotheses offer high returns: small complexity investments yielding novel predictions or unifying phenomena. Degenerative patches offer low returns: high-cost fixes resolving only targeted anomalies while increasing overall complexity. The Higgs boson exemplifies the former—adding complexity but completing theoretical structure with confirmed predictions. Ptolemaic epicycles exemplify the latter—costly modifications saving specific failures without generating insights.

To illustrate how this triangulation methodology works in practice, consider the diagnosis of rising brittleness in a hypothetical research program. We would not rely solely on bibliometric indicators (like an increasing ratio of auxiliary modifications to novel predictions) but would look for convergent evidence across multiple domains: institutional indicators (increasing resource allocation to managing dissent within the research community), empirical indicators (declining predictive accuracy or increasing reliance on immunizing strategies), and social indicators (growing difficulty in training new researchers or public support).

Crucially, this convergence requirement does not eliminate the hermeneutic dimension but constrains it. Any single indicator might be explained away through alternative interpretations, but systematic convergence across independent measures becomes increasingly difficult to dismiss. The methodology thus provides what we might call "constrained interpretation"—structured judgment that remains accountable to multiple streams of evidence.

We acknowledge this falls short of mechanical objectivity, but mechanical objectivity was never the goal. The framework aims to make evaluative judgments more systematic, transparent, and accountable to evidence, not to eliminate judgment entirely.

### **2.5 Two Modalities of Systemic Brittleness**

The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

## **3. The Drivers of Adaptation: The Logic of Systemic Viability**

The framework for assessing systemic health detailed in Section 2 is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems.

### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.

Second, an **instrumental argument**: the framework makes a falsifiable, empirical claim that *networks with a high and rising degree of measured brittleness are statistically more likely to collapse or require radical revision.* From this descriptive claim follows a conditional recommendation: *if* an agent or institution has the goal of ensuring its long-term stability and problem-solving capacity, *then* it has a powerful, evidence-based reason to adopt principles that demonstrably lower its systemic brittleness.

This reframes the paper’s normative language. When this model describes one network as "better" or identifies "epistemic progress," these are not subjective value judgments but technical descriptions of systemic performance. A "better" network is one with lower measured brittleness and thus a higher predicted resilience against failure. Viability is not an optional norm to be adopted; it is a structural precondition for any system that manages to become part of the historical record at all.

### **3.2 Coherence as Forward-Looking Risk Assessment**

Framed by the logic of viability, the test for coherence is a thick, forward-looking *cost-benefit analysis*: a set of heuristics that a resource-constrained system uses to bet on whether adopting a new proposition will increase or decrease its long-term brittleness. The traditional epistemic virtues are the core principles of this practical calculus:

* **Logical Consistency:** A hedge against the infinite future costs of inferential paralysis.
* **Explanatory Power:** A measure of a proposition’s potential return on investment, reducing future inquiry costs by paying down conceptual debt.
* **Simplicity / Parsimony:** A direct measure of systemic overhead; complex propositions increase long-term maintenance costs.
* **Evidential Support:** An assessment of integrative risk; a well-supported claim is a low-risk investment unlikely to trigger a cascade of costly future revisions.

This forward-looking model of coherence also explains how revolutionary science is possible. When a dominant Consensus Network begins to exhibit high and rising systemic brittleness—a state that corresponds to a Kuhnian "crisis"—the cost-benefit analysis for new propositions shifts dramatically. A radical new hypothesis, while having low coherence with the existing network's specific principles, may promise a massive long-term reduction in the systemic costs that are crippling the incumbent paradigm. The network, in effect, makes a high-risk, high-reward bet. The new proposition is not accepted because it fits neatly with the old, failing parts, but because it offers a viable path to restoring low-brittleness for the system as a whole. Coherence, in this dynamic sense, is not a conservative check for conformity, but a pragmatic assessment of a proposition's potential to resolve systemic crisis.

## **4. Convergence and the Emergence of Objective Structures**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We will show how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

Our account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is often of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system—its descent into crippling inefficiency, intellectual stagnation, and institutional decay—provides a clear, non-negotiable data point.

The systematic analysis of these failures allows us to build the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been empirically invalidated by the catastrophic systemic costs they reliably generate. This canon charts failures of both causal and normative alignment:

* **Failures of Causal Alignment** are characteristic of what we term *Epistemic Brittleness*. Systems like scholastic physics, phlogiston chemistry, and Lysenkoist biology entered the canon because their core principles generated catastrophic causal failures, leading to chronically high rates of ad-hoc modification and predictive collapse. They represent failed blueprints for the causal world.
* **Failures of Normative Alignment** are characteristic of *Normative Brittleness*. Systems like chattel slavery or totalitarianism entered the canon because their core principles violated emergent normative facts about human cooperation. Their unsustainability was demonstrated by the immense and ever-rising coercive overheads required to suppress the normative pushback they generated. These are not condemned by a modern moral judgment alone; they are failed engineering principles for a viable social world.

By charting what demonstrably fails, whether in engineering or social organization, we are not merely learning what to avoid; we are effectively reverse-engineering the constraints of a real territory. The Negative Canon functions like a reef chart for inquiry. It is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and prevents a collapse into relativism.

### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

The mechanism that forges this structure is a bottom-up emergence driven by the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question. The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we might expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge. The "pursuit of truth," then, is the practical, fallible project of refining our current Consensus Network to better align with this real, objective structure.

`[FIGURE 2: Conceptual Sketch of a Fitness Landscape]`
`[A 3D landscape with "Viability" on the Y-axis. Peaks are labeled with successful systems like "Germ Theory" and "Relativity." Deep valleys and low plateaus are labeled with failed systems from the Negative Canon, such as "Ptolemaic System," "Lysenkoism," and the "Ming Dynasty Trap."]`

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state; they simply try to solve local problems and reduce costs. The Apex Network is the name we give to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

The Apex Network's status is therefore dual, a distinction critical to our fallibilist realism. Ontologically, it is real: it is the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, however, it remains a regulative ideal. We can never achieve a final, God's-eye view to confirm that our Consensus Network is a perfect map of it; our knowledge of the structure is necessarily incomplete and fallible. Its existence is what grounds our realism and prevents a collapse into relativism, while our epistemic limitations are what make the project of inquiry a permanent and progressive one. It is the necessary concept that makes our comparative judgments of "more" or "less" brittle meaningful.

### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
* **Level 1: Objective Truth.** The ultimate, regulative ideal of the process. A proposition is objectively true if its principles are part of the real, emergent **Apex Network**—the objective structure of viable solutions. While this structure is never fully mapped, it functions as the formal standard that makes our comparative judgments of "more" or "less" brittle meaningful. It is the structure toward which the reduction of systemic costs forces our knowledge systems to converge.

This layered framework avoids a simplistic "Whig history" by recognizing that **Justified Truth** is a historically-situated achievement. Newtonian mechanics earned its Level 2 status by being a maximally low-brittleness system for its problem-space for over two centuries. Its replacement by relativity does not retroactively invalidate that status; it shows the evolutionary process at work, where an expanding problem-space revealed pragmatic constraints that required a new, more viable system. This allows for sharp, non-anachronistic historical judgments: a claim can be justifiably true in its time (Level 2) yet still be objectively false (not Level 1) when judged against the Apex Network from the perspective of a more resilient successor.

### **4.4 The Evolving Structure of Knowledge: Convergent Core and Pluralist Frontier**

The historical process of pragmatic filtering gives our evolving Consensus Networks a discernible structure, which can be understood as having two distinct epistemic zones. This distinction is not about the nature of reality itself, but describes the justificatory status of our claims at a given time.

* **The Convergent Core:** This represents the load-bearing foundations of our current knowledge. It comprises domains where the relentless pressure of pragmatic selection has eliminated all known rival formulations, leaving a single, or functionally identical, set of low-brittleness principles. Principles reside in this core—such as the laws of thermodynamics or the germ theory of disease—not because they are dogmatically held or self-evident, but because all tested alternatives have been relegated to the Negative Canon after generating catastrophically high systemic costs. While no claim is immune to revision in principle, the principles in the Convergent Core are functionally unrevisable in practice, as doing so would require dismantling the most successful and resilient knowledge structures we have ever built. A claim from this core achieves the highest degree of justification we can assign, approaching our standard for Objective Truth (Level 1).
* **The Pluralist Frontier:** This describes the domains of active research where our current evidence is insufficient to decide between multiple, competing, and viable reconstructions of the landscape of viability. Here, rival systems (e.g., different interpretations of quantum mechanics or competing models of consciousness) may co-exist, each with a demonstrably low and stable degree of brittleness. It is crucial to distinguish this constrained, evidence-based pluralism from relativism. The frontier is not an "anything goes" zone; it is a highly restricted space strictly bounded on all sides by the Negative Canon. A system based on phlogiston is not a "viable contender" on the frontier of chemistry; it is a demonstrably failed research program. This pluralism is therefore a sign of epistemic underdetermination—a feature of our map's current limitations, not reality's supposed indifference. This position resonates with pragmatist accounts of functional pluralism (e.g., Price 1992), which treat different conceptual frameworks as tools whose legitimacy is determined by their utility within a specific practice. Within this frontier, the core claims of each viable competing system can be granted the status of Justified Truth (Level 2). This is also the zone where non-epistemic factors, such as institutional power or contingent path dependencies, can play their most significant role, sometimes artificially constraining the range of options explored or creating temporary monopolies on what is considered justified.

### **4.5 Illustrative Cases of Convergence and Brittleness**

The transition from Newtonian to relativistic physics offers a canonical example of this framework's diagnostic application. After centuries of viability, the Newtonian system began to accumulate significant systemic costs in the late 19th century. These manifested as first-order predictive failures, such as its inability to account for the perihelion of Mercury, and as rising conceptual debt in the form of ad-hoc modifications like the Lorentz-FitzGerald contraction hypothesis. The Einsteinian system proved to be a more resilient solution, reducing this conceptual debt and substantially lowering the systemic costs of inquiry in physics.

A more contemporary case can be found in the recent history of artificial intelligence, which illustrates how a brittleness assessment might function in real time. The periodic "AI winters" can be understood as the collapse of high-brittleness paradigms, such as symbolic AI, which suffered from a high rate of ad-hoc modification when faced with novel challenges. While the subsequent deep learning paradigm proved to be a low-brittleness solution for many specific tasks, it may now be showing signs of rising systemic costs. These can be described conceptually as, for example, potentially unsustainable escalations in computational and energy resources for marginal performance gains, or an accelerating research focus on auxiliary, post-hoc modifications rather than on foundational architectural advances. This situation illustrates the Pluralist Frontier in action, as rival architectures might now be seen as competing to become the next low-brittleness solution.

### **4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, are both examples. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability, therefore, requires a balance between low-cost stability and adaptive capacity.

## **5. The Dynamism of the Web: How Knowledge Becomes Entrenched**

Quine’s "Web of Belief" provided a powerful static model of a knowledge system, but it has been criticized for lacking a corresponding dynamic account of its formation and change. This section provides that dynamic physiology, detailing the naturalistic process by which a successful discovery migrates from the tentative "periphery" of the web to its load-bearing "core."

### **5.1 From Tentative Hypothesis to Core Principle**

A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations

This paper has developed what can be termed **Systemic Externalism**—a form of externalist epistemology that locates justification not in individual cognitive processes but in the demonstrated reliability of entire knowledge systems. This section clarifies the framework's position within contemporary epistemology by examining its relationship to four major research programs: coherentist epistemology, social epistemology, evolutionary epistemology, and neopragmatism.

### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

This paper's framework, which we term *Systemic Externalism*, offers a unified externalist solution to this structural problem. It provides the evolutionary backstory for the web's functional structure, arguing that a principle becomes functionally indispensable, as described by Carlson (2015), precisely because it has survived a historical filtering process based on pragmatic consequences. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

### 6.2 Evolutionary Grounding for Social Epistemic Practices

The framework provides a naturalistic foundation for core insights in social epistemology while addressing a persistent problem in that field. Longino (2002) and others have shown that objectivity often emerges from well-structured epistemic communities rather than isolated individuals. Critical discourse norms, peer review, and viewpoint diversity are presented as procedural guarantees of objectivity.

However, purely procedural accounts face what might be called the "parochialism problem": how do we know that a perfectly managed consensus is not simply a stable, shared delusion? If objectivity is secured by following local rules of discourse, what grounds do we have for evaluating those rules themselves?

This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness

The framework contributes to evolutionary epistemology (Campbell, 1974; Bradie, 1986) while avoiding standard problems facing such approaches. Traditional biological models treat beliefs as competing for psychological "survival," but this creates difficulties in defining fitness without circularity—distinguishing genuinely beneficial knowledge from well-adapted "informational viruses."

This framework addresses the circularity problem by providing a hard, non-circular standard for fitness: long-term pragmatic viability as measured by systemic brittleness. The fitness of a principle is not its transmissibility or psychological appeal, but its contribution to the resilience of the knowledge system that hosts it.

This distinction proves diagnostic. Conspiracy theories may achieve high short-term transmissibility through psychological appeal, but they do so by incurring massive conceptual debt, exhibiting accelerating rates of ad-hoc modification, and often requiring high coercive overheads to maintain ideological purity. Their measured brittleness reveals their profound non-viability despite their psychological "fitness."

The framework also addresses evolutionary epistemology's difficulty with the directed nature of human inquiry. Unlike blind Darwinian selection, knowledge systems exhibit Lamarckian-style inheritance of acquired insights through the functional entrenchment of successful solutions in network cores.

It is useful to distinguish our concept of systemic brittleness from related ideas in the philosophy of science, such as Lakatos's (1970) 'degenerative research programmes' and Laudan's (1977) 'problem-solving effectiveness'. While Lakatos provides a brilliant historical description of a degenerating programme—one that relies on ad-hoc hypotheses and fails to make novel predictions—our framework aims to provide the underlying causal engine for this degeneration. Brittleness is a measure of the accumulated systemic costs that cause a programme to become degenerative. It is a diagnostic of a system's structural health, not just a historical description of its output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of risk and resilience. A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.

1. **On Ontology: From Abstract Structures to an Emergent Landscape.** Regarding the first challenge, our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
2. **On Epistemology: From Insight to Selection.** Regarding the second, more difficult challenge, our framework provides the specific causal mechanism for convergence that structural realism often lacks. We discover these robust structures not through a mysterious act of intellectual insight, but through the brutal, eliminative process of pragmatic selection. High-brittleness networks—those whose posited structures misalign with the real landscape of viability—generate unsustainable costs, collapse, and enter the **Negative Canon**. Low-brittleness networks survive and are built upon. Over historical time, this failure-driven selective pressure is precisely the engine that forces our **Consensus Networks** to conform to the objective, relational structure of the Apex Network.

Our framework thus provides a dynamic, evolutionary, and fully naturalized engine for the structural realist's core thesis. It explains *how* and *why* our problem-solving practices are forced to converge on objective structures without appealing to metaphysical mysteries, thereby grounding structural realism in a testable, historical process.

### 6.6 Implications for Contemporary Debates

This framework has implications for several contemporary discussions in epistemology:

**Disagreement**: Following Kelly (2005), the diagnosed brittleness of knowledge systems provides powerful higher-order evidence that should influence how agents respond to disagreement. Claims from low-brittleness networks warrant higher confidence than those from demonstrably brittle sources.

**Testimony**: The framework suggests that testimonial justification depends not only on speaker reliability but on the systemic reliability of the knowledge traditions speakers represent. This provides resources for evaluating competing testimonial sources in an information-rich but epistemically fragmented environment.

**Applied Epistemology**: The brittleness framework offers tools for evaluating knowledge systems in real-time, with applications to science policy, institutional design, and public discourse. It suggests criteria for identifying degenerating research programs before they reach crisis points.

### 6.7 Limitations and Future Directions

The framework developed here operates primarily at the macro-historical level and is best suited to evaluating cumulative knowledge systems with clear practical consequences. Several important limitations deserve acknowledgment:

**Scope**: The framework applies most naturally to domains where pragmatic pushback is relatively direct and measurable. Its application to pure mathematics, logic, or highly theoretical domains requires further development.

**Measurement**: While the paper provides conceptual tools for assessing brittleness, operationalizing these measures in non-question-begging ways remains challenging. The proposed metrics should be understood as heuristic guides for a research program rather than algorithmic solutions.

**Power and Path Dependence**: While the framework acknowledges the role of power in maintaining brittle systems, a fuller account of how coercive mechanisms interact with epistemic selection pressures requires additional development.

These limitations point toward productive future research directions while indicating the framework's current scope and appropriate applications.

### 6.8 A Naturalized vs. Rationalist Procedure

The framework's contribution is best understood as a form of naturalized proceduralism. It shares an affinity with procedural realists, such as the later Putnam, who ground objectivity in the properties of a procedure rather than in direct correspondence with a mind-independent reality. The crucial divergence, however, lies in the nature of that procedure. Where rationalist accounts locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The ultimate arbiter is therefore not the internal coherence of our reasons, but the measurable, non-discursive brittleness of the systems our reasons produce. Our arguments are thus continuously disciplined not merely by other arguments, but by the non-negotiable data of systemic success and failure.

## **7. Defending the Model: Addressing Key Challenges**

A philosophical model is best judged by its ability to resolve the very paradoxes that plague its predecessors. This section demonstrates the resilience of our framework by engaging with a series of classic epistemological challenges. We treat these not as external objections to be deflected, but as core test cases that reveal the explanatory power of analyzing knowledge through the lens of systemic viability.

### **7.1 The Problem of Internal Coherence: Fictions, Paradigms, and the Limits of Isolation**

The most potent challenge to any coherentist model is the "isolation objection"—the possibility of a perfectly self-consistent but factually detached system. This manifests in sophisticated conspiracy theories and incommensurable scientific paradigms famously articulated by Thomas Kuhn (1962). Our model addresses this by introducing an externalist standard based on pragmatic performance, though significant methodological challenges remain.

"Coherent fictions" like conspiracy theories typically exhibit structural features: accelerating ad-hoc modifications to protect core tenets, high maintenance costs through suppression of dissent, and epistemic parasitism—generating no novel research but rationalizing away mainstream successes. Whether these constitute decisive refutation depends on objective measurement, which proves difficult in practice.

Incommensurable paradigms present a different challenge. While direct theoretical comparison may be impossible, certain performance aspects might be compared across paradigm boundaries. The accelerating need for epicycles in Ptolemaic astronomy represents structural dysfunction measurable through formal indicators like complexity-to-prediction ratios, without accepting either paradigm's commitments.

However, paradigms may be optimized for different problems, making direct comparisons misleading. A Kuhnian crisis might reflect rising systemic costs or simply changing research priorities. When paradigms compete within overlapping domains, structural indicators can supplement traditional considerations of empirical adequacy and theoretical virtue.

This reframes certain philosophical impasses as potentially tractable empirical questions, though interpretive challenges remain significant.

### **7.2 The Problem of History: Endurance, Hindsight, and Real-Time Diagnosis**

A second powerful challenge concerns the interpretation of history. If viability is the standard, how do we account for flawed systems that endure for centuries, and how can we apply this standard to live controversies without the distorting benefit of hindsight?

First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model in fact predicts that brittle systems can persist for long periods, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a *confirmation* of it, as it provides a long-running experiment that allows us to observe the high price of insulating a flawed core from pragmatic pushback. Its apparent stability was not a sign of health but a direct measure of the intellectual and institutional energy it had to burn to function, making it profoundly vulnerable to a more efficient competitor.

This leads to the question of real-time application. The goal of this framework is not deterministic prediction but epistemic risk management. Its retrospective analysis of historical cases is not an end in itself; it is the necessary process of calibrating our diagnostic tools. We study known failures like Ptolemaic cosmology to learn the empirical signatures of rising brittleness. Only then can we apply these calibrated tools to live, unresolved debates. This allows us to ask precise, forward-looking questions: Is the exponential rise in computational and energy costs for large language models a sign of a degenerating research program, even as its short-term performance improves? Does the proliferation of ad-hoc 'alignment' fixes represent mounting conceptual debt? A rising trend in a system's brittleness indicators does not prove its core claims are false, but it provides a strong, evidence-based signal that it is becoming a higher-risk, degenerating research program.

### **7.3 The Problem of Scope: From Systemic Health to Individual Belief**

It is crucial to be precise about this model's scope. It is a macro-epistemology, designed to explain the long-term viability of public knowledge systems. It does not primarily aim to solve traditional problems in micro-epistemology, such as Gettier cases or the justification of an individual's perceptual beliefs. Instead, it provides a robust bridge between these levels through the concept of higher-order evidence. The diagnosed health of a public system provides a powerful defeater (or corroborator) for an individual’s beliefs derived from that system.

To formalize this intuition, we can use a Bayesian framework. The diagnosed brittleness of a knowledge system should determine the rational prior probability an agent assigns to any claim from that source. A low-brittleness network (e.g., an IPCC report) warrants a high prior; a high-brittleness network (a denialist documentary) warrants a low one. As Thomas Kelly (2005) argued regarding disagreement, the properties of the source matter. When an agent receives new first-order evidence, E, their posterior confidence is updated via Bayes' rule. This formalizes why an agent should rationally favor the IPCC's claim: even if the denialist source presents a seemingly powerful piece of evidence, the extremely low prior assigned to that network means the agent's posterior confidence in its claims will remain low. The macro-level diagnosis thus provides a rational, quantitative basis for allocating trust.

### **7.4 Defending the Model's Grounding**

Any naturalistic model of objectivity must face ultimate tests of its grounding: can its core metrics be defined objectively, and can the model account for its own epistemic status without circularity? This framework answers these challenges by anchoring itself in empirical analysis and falsifiable claims.

The charge of circularity in cost assessment deserves direct confrontation. Yes, determining what counts as "excess mortality" or "wasted resources" requires background theoretical commitments. The framework does not and cannot eliminate this theory-ladenness entirely. However, it can manage the problem through several strategies that provide sufficient objectivity for its purposes.

First, the framework anchors its assessments in outcomes that register as problems across widely divergent theoretical perspectives. When a bridge collapses, a harvest fails, or a population experiences demographic crisis, these register as failures regardless of one's particular theoretical commitments about optimal bridge design, agricultural policy, or social organization. While the precise interpretation of such failures remains contested, their status as failures is typically not.

Second, the comparative methodology reduces dependence on absolute standards. We need not determine the optimal mortality rate for a population, only whether mortality rates are rising or falling under different policy regimes. Such comparative judgments can often be made robustly even when absolute standards remain contested.

Third, the framework's power lies not in eliminating theoretical commitments but in making them explicit and accountable to systematic evidence. When a system's rising brittleness is diagnosed through convergent evidence across multiple independent indicators, this diagnosis becomes increasingly difficult to dismiss as mere theoretical bias.

The resulting methodology provides what we might call "pragmatic objectivity"—objectivity sufficient for the practical task of evaluating and improving knowledge systems, even if it falls short of view-from-nowhere neutrality.

### **7.5 From Theory to Practice: A Falsifiable Research Program**

The framework's claims are designed to ground a concrete, empirically testable research program. Its core causal hypothesis is a falsifiable prediction: *a network with a high or rising degree of measured brittleness carries a statistically higher probability of collapse or major revision when faced with a comparable external shock.*

Testing this claim requires a rigorous methodology. The first step is to operationalize the indicators of brittleness through quantifiable proxies for systemic cost, such as the ratio of state budgets for internal security versus R&D or the rate of non-generative auxiliary hypotheses in scientific literature. The second step is to apply these metrics in a comparative historical analysis. A significant challenge in this research is to isolate the causal impact of intrinsic brittleness from the noise of historical contingency. To address this, the hypothesis can be tested by analyzing cohorts of systems (e.g., polities, scientific paradigms) that faced similar types of external shocks. Using large-scale databases like the Seshat Databank, researchers could compare the outcomes of systems with different pre-existing brittleness indicators when faced with a comparable shock, thereby statistically controlling for the contingent event itself.

This comparative method provides a well-established procedure for testing the theory's probabilistic claims against complex historical data. The framework is therefore rigorously falsifiable: if broad, methodologically sound historical analysis revealed no statistically significant correlation between the indicators of high systemic cost and subsequent network fragility, the theory’s core causal engine would be severely undermined. We acknowledge that such a research program faces significant challenges, including the operationalization of proxies, the control of confounding variables, and the interpretation of sparse data. The claim is not that this provides a simple algorithm for historical analysis, but that it offers a conceptually coherent and empirically grounded framework for it.

Finally, a crucial component of this program involves moving from retrospective calibration to prospective testing. While historical analysis is essential for identifying and calibrating the indicators of brittleness, the framework's ultimate test lies in its ability to make probabilistic, forward-looking claims. For instance, a diagnosis of rising brittleness in a current research paradigm—such as escalating resource costs for marginal gains—would yield the falsifiable prediction that this paradigm is statistically more likely to be superseded by a more efficient rival architecture in the face of novel challenges. This prospective application is essential for demonstrating that brittleness is a genuine diagnostic tool, not merely a post-hoc explanatory device.

### 7.6 Potential Counterexamples and Boundary Cases

Any framework making claims about knowledge system viability must confront cases that seem to challenge its core predictions. Consider several potential counterexamples:

**Persistent "Brittle" Systems**: Traditional Chinese medicine has persisted for millennia despite lacking the theoretical foundations that our framework would associate with low brittleness. However, this persistence may reflect the framework's distinction between specialized and comprehensive knowledge systems. Traditional medicine succeeded within a limited domain (treating certain symptoms) without needing to provide comprehensive causal explanations. Its brittleness became apparent primarily when it was extended beyond this domain or when more systematic alternatives became available.

**Failed "Viable" Systems**: Newtonian mechanics was demonstrably low-brittleness for centuries before being superseded by relativity. This apparent failure of a viable system illustrates the framework's fallibilist commitment: no system is permanently immune to revision. What the Newtonian case shows is that viability is always relative to a problem-space. As that space expanded to include high-velocity phenomena, indicators of rising brittleness (like the need for increasingly complex auxiliary hypotheses) began to appear.

**The Role of Contingency**: The suppression of genetic science under Stalin might seem to show that political power can maintain brittle systems indefinitely. However, our framework would predict that such suppression imposes measurable costs—in this case, agricultural failures that contributed to famines. The question becomes whether these costs are sustainable in the long term, which the eventual abandonment of Lysenkoist policies suggests they were not.

These cases illustrate both the framework's explanatory resources and its limitations. While it provides tools for understanding systematic patterns in knowledge system evolution, it cannot eliminate the role of contingency, power, and local context in particular historical episodes.

## **8. Conclusion: An Evolutionary Approach to Inquiry**

This paper has developed Emergent Pragmatic Coherentism as a response to the isolation objection facing coherentist theories of justification. By grounding coherence in the long-term viability of knowledge systems rather than internal consistency alone, the framework provides the external constraint that coherentism requires while preserving its holistic insights. The concept of systemic brittleness offers a naturalistic diagnostic tool for evaluating epistemic systems, while the notion of an emergent Apex Network explains how objective knowledge can arise from fallible human practices.

Our framework for assessing systemic brittleness makes this process analyzable. By systematically studying the record of failed systems, we can begin to discern the contours of the Apex Network: the emergent set of maximally convergent, pragmatically indispensable principles that successful inquiry is forced to discover. The result is a form of Systemic Externalism that offers a novel approach for resolving long-standing problems in post-Quinean epistemology by grounding justification in the non-discursive filter of systemic consequences.

This model is not presented as a final, complete system, but as the foundation for a progressive and falsifiable research program. Critical future challenges remain, such as fully modeling the role of power asymmetries in creating path-dependent 'fitness traps' and applying the framework to purely aesthetic or mathematical domains. These are not flaws in the model, but precisely the kinds of tractable, empirical questions it is designed to generate.

We began with the challenge of distinguishing viable knowledge from brittle dogma in an era of epistemic crisis. The model we have developed suggests the ultimate arbiter is not the elegance of a theory or the consensus of its adherents, but the trail of consequences it leaves in the world. While this framework operates at a high level of abstraction, its primary data streams originate from the ground up. As we have argued, systemic costs are ultimately experienced by individuals as suffering, instability, and the frustration of human goals. Dissent, friction, and protest are therefore not merely political problems; they are primary sources of epistemological data about a system's rising brittleness. This framework, in this light, is best understood as a tool for a democratic public to hold its own knowledge-generating systems accountable. By making the indicators of systemic health part of a shared, evidence-based language, it helps us ask the most pragmatic question of all: "Is this way of thinking, this way of organizing ourselves, still working for us?"

## **Glossary**

### **Part 1: The Core Framework \& Philosophical Stance**

**1. Emergent Pragmatic Coherentism (EPC)**
The name for the theoretical framework developed in this paper. It provides a naturalistic account of objectivity that avoids both foundationalism and relativism.

* **Core Logic:** All knowledge begins within a coherentist web but is then subjected to a pragmatic, evolutionary filter. Objectivity is the *emergent result* of this filtering process, not a foundational starting point.

  * It is **Pragmatic** because its ultimate court of appeal is the observable, real-world *costs* generated by a knowledge system when its ideas are put into practice.
  * It is **Coherentist** in that it accepts the Quinean insight that claims are initially justified by their fit within a holistic network, rejecting the idea of isolated, foundational beliefs.
  * It is **Emergent** because it argues that objectivity is not a pre-given metaphysical structure but an *achieved structural property* that arises from a historical process. As brittle, high-cost systems are filtered out, surviving systems are forced to converge on designs that conform to mind-independent constraints.

* **Role in the Paper:** This is the overarching philosophical framework that provides the dynamism for Quine's static web, explaining how a coherent system is disciplined by an external, pragmatic check, allowing it to adapt, evolve, and converge on objective knowledge.

**2. Systemic Externalism**
The specific epistemological stance of the model, which synthesizes internalist and externalist conditions for justification.

* **Core Claim:** Justification is a **two-condition property**. For a proposition to achieve the status of **Justified Truth**, it must meet two conditions: (1) it must cohere with the principles of its certifying **Shared Network** (the internalist condition), and (2) the **Shared Network** itself must be reliable, a status earned through a demonstrated historical capacity to maintain low **Systemic Brittleness** (the externalist condition).
* **Function:** This solves the "isolation problem" for coherentism by adding an external check based on pragmatic performance. It grounds justification in the observable, historical track record of an entire public system.
* **Distinction:** Unlike traditional process reliabilism which focuses on the cognitive processes of an individual, Systemic Externalism locates reliability in the *public, verifiable performance of the knowledge-certifying system*. Justification is a property of *propositions-within-a-proven-system*.

**3. Realist Pragmatism**
The model's philosophical identity, uniting two often-opposed traditions by arguing that being a realist is the most pragmatically effective strategy.

* **Core Synthesis:**

  * It is **Pragmatist** in its anti-foundationalism, its fallibilism, and its focus on inquiry as a problem-solving process whose success is measured by real-world consequences.
  * It is staunchly **Realist** in its commitment to the **Apex Network** as a real, emergent structure of viable solutions whose contours are determined by mind-independent pragmatic constraints.

* **Function:** This synthesis explains *how* a pragmatist inquiry can generate realist outcomes. The relentless, cost-based filtering of our ideas is precisely the mechanism that forces our fallible systems to conform to objective, structural facts about our world.

### **Part 2: The Units of Analysis: How Knowledge is Structured**

**1. Shared Network**

The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**

A clarification of the model's deflationary method, which follows Quine in shifting focus from private mental states to the public, functional roles propositions play within a Shared Network.

* **Belief:** A private, psychological state of an individual agent. It is the raw material from which public claims are articulated but is not itself part of the public network.
* **Proposition:** The public, linguistic expression of a belief. It is the candidate for integration into a Shared Network, where it can be collectively tested against pragmatic pushback.
* **Standing Predicate:** The validated, reusable, action-guiding conceptual tool extracted from a highly successful proposition (e.g., `...is an infectious disease`). It is a core component of a network's processing architecture and the primary unit of cultural-epistemic selection.



### **Part 3: The Dynamics of Change: How Knowledge Evolves**

**1. Pragmatic Pushback**
The primary selective force driving the pragmatic revision of knowledge systems in our model—from individual webs of belief to emergent shared networks. It is the unforgiving interface between ideas and reality, compelling adaptation through material consequences.

* **Definition:** The sum of non-negotiable, non-discursive consequences that arise when principles—from private beliefs to shared networks—are applied to the world. At the individual level, this manifests as personal failures (e.g., a doctor's misdiagnosis based on miasma theory leading to patient harm); at the collective scale, as systemic breakdowns (e.g., a city's sanitation policies misdirecting resources toward odors rather than contaminated pumps).
* **Nature:** This feedback is not an argument or mere cognitive dissonance but a raw, material outcome: a bridge collapses under flawed engineering, a treatment fails amid excess mortality, a society fragments from unjust policies. It operates as reality's amoral filter, hitting individual webs first—Quine's holistic dragnet quivers when a peripheral belief (e.g., a craftsperson's untested heuristic) triggers failure—then cascades to shared networks as agents converge on revisions under common pressures. Whether revising a solitary belief or a public paradigm, pushback enforces mind-independent constraints, pruning delusions without mercy.
* **Function:** This relentless pressure generates objective, measurable costs—first-order (direct losses like wasted resources or lives) and systemic (secondary fixes like conceptual debt or coercive overheads)—that serve as an evolutionary selection filter. For individuals, it prompts holistic revision in their web of belief, minimizing disruption while betting on long-term viability (Section 3.2); for networks, it forces adaptation or collapse, biasing convergence toward low-brittleness structures like the Apex Network. In both cases, costs quantify epistemic health, grounding a falsifiable program for tracking revisions from private tinkering to public resilience (Sections 2.4–2.5).

**2. Systemic Costs: A Two-Level Diagnostic Framework**
The set of concepts used to diagnose a network's health.

* **First-Order Costs (The Symptoms):** The direct, material consequences of a network’s misalignment with its pragmatic environment. These are the objective, observable signals of dysfunction (e.g., failed predictions, excess mortality, resource waste).
* **Systemic Costs (The Underlying Condition):** The secondary, internal costs a network incurs to *manage, suppress, or explain away* its first-order costs. They represent non-productive expenditures on internal maintenance rather than on adaptation. Key forms include:

  * **Conceptual Debt:** The compounding fragility incurred by adopting flawed, complex "patches" to protect a core principle.
  * **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing the dissent that arises from first-order costs.

**3. Systemic Brittleness \& Its Modalities**
The central diagnostic concept for assessing a network's health and its vulnerability to future shocks.

* **Definition:** A system’s *brittleness* is a measure of its accumulated, hidden systemic costs. A high degree of brittleness signals that a system is inefficient, fragile, and a degenerating research program.
* **Distinction:** Brittleness is not the opposite of longevity. A brittle system can endure for a long time by expending massive energy on coercion and conceptual patches. *Viability*, in contrast, is the ability to adapt and solve problems with *low* systemic costs.
* **Modalities:** The framework identifies two primary modalities of failure:

  * **Epistemic Brittleness:** Failure of alignment with the causal structure of the world (e.g., Ptolemaic astronomy).
  * **Normative Brittleness:** Failure of alignment with the constraints on stable human cooperation (e.g., a slave economy).

### **Part 4: The Structure of Objectivity: Truth, Reality, and Progress**

**1. Negative Canon**
The model's empirical and historical anchor for objectivity.

* **Definition:** The evidence-based catalogue of failed predicates, propositions, and entire Shared Networks—including the emergent structures built upon them—that have been historically invalidated by their own catastrophic **Systemic Costs** (e.g., Ptolemaic astronomy, phlogiston chemistry).
* **Function:** This represents our most secure form of objective knowledge: not only knowing what has collapsed, but why it collapsed. It provides a “reef chart” for inquiry, mapping both the exposed wreckage of untenable theories and the hidden hazards of propositions that led them astray. In doing so, it establishes an external boundary that constrains coherence, preventing inquiry from drifting into relativism.

**2. The Apex Network vs. The Consensus Network**
The crucial distinction between the objective structure of viability our inquiry aims at (the Apex Network) and our current, fallible map of it (the Consensus Network).

* **The Apex Network (The Objective Standard):** This is the paper's central realist concept. Its status is dual:

  * **Ontologically:** The Apex Network is the complete set of all maximally coherent and pragmatically viable principles, whose structure is determined by mind-independent pragmatic constraints. It is not a metaphysical blueprint but an *emergent structural fact about our world*, discovered retrospectively through historical filtering. Ontologically, it is real.
  * **Epistemically:** We can never have a final, complete view of this structure. It functions for us as a **regulative ideal** that makes our comparative judgments of brittleness meaningful. **Epistemically, it is an ideal we approximate.**
  * **Function:** It is the ultimate, non-negotiable standard for **Objective Truth** (Level 1).

* **The Consensus Network (Our Best Approximation):** Our current, best, and necessarily fallible reconstruction of the Apex Network's structure (e.g., mainstream contemporary science).

  * **Authority:** Its authority derives not from social agreement alone, but from its demonstrated historical track record of maintaining low **Systemic Brittleness**.
  * **Function:** It is the system that certifies **Justified Truth** (Level 2).

**3. The Three Levels of Truth**
The model's synthesized, procedural account of truth, designed to resolve the tension between fallibilism and realism. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence:** The baseline status. A proposition is coherent *within any specific Shared Network*, regardless of its long-term viability. This explains the internal rationality of failed paradigms but is not sufficient for justification.
* **Level 2: Justified Truth:** The highest practically achievable epistemic status. A proposition is justified as true if it is certified by a **Consensus Network** that has itself demonstrated a low and stable degree of systemic brittleness.
* **Level 1: Objective Truth:** The ultimate, regulative ideal. A proposition is objectively true if its principles are part of the **Apex Network**. This is the standard our inquiry aims to meet, even if we can never be certain we have reached it.



## References

Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business.

Baggio, Guido, and Andrea Parravicini. 2019. "Introduction to Pragmatism and Theories of Emergence." *European Journal of Pragmatism and American Philosophy* XI-2. https://doi.org/10.4000/ejpap.1251.

Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.

Bennett-Hunter, Guy. 2015. "Emergence, Emergentism and Pragmatism." *Theology and Science* 13, no. 3: 337–57. https://doi.org/10.1080/14746700.2015.1053760.

Berlin, Brent, and Paul Kay. 1969. *Basic Color Terms: Their Universality and Evolution*. Berkeley: University of California Press.

BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.

Bradie, Michael. 1986. "Assessing Evolutionary Epistemology." *Biology \& Philosophy* 1, no. 4: 401–59. https://doi.org/10.1007/BF00140962.

Brandom, Robert B. 1994. *Making It Explicit: Reasoning, Representing, and Discursive Commitment*. Cambridge, MA: Harvard University Press.

Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.

Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.

El-Hani, Charbel N., and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3, no. 2, article 3. http://commons.pacificu.edu/eip/vol3/iss2/3.

Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.

Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.

Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.

Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press.

Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.

Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.

Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.

Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press.

Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press. Originally published 1962.

Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.

Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.

Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific Research Programmes." In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.

Laudan, Larry. 1977. *Progress and Its Problems: Towards a Theory of Scientific Growth*. Berkeley: University of California Press.

Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.

Lynch, Michael P. 2009. *Truth as One and Many*. Oxford: Oxford University Press.

Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.

Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.

Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.

Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press.

Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press. Originally published 1878.

Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37, no. 7: 1948–70. https://doi.org/10.1080/09515089.2023.2236120.

Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.

Price, Huw. 1992. "Metaphysical Pluralism." *Journal of Philosophy* 89, no. 8: 387–409. https://doi.org/10.2307/2940975.

Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.

Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60, no. 1: 20–43. https://doi.org/10.2307/2181906.

Quine, W. V. O. 1960. *Word and Object*. Cambridge, MA: MIT Press.

Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press.

Rosenstock, Sarita, Cailin O'Connor, and Justin Bruner. 2017. "In Epistemic Networks, Is Less Really More?" *Philosophy of Science* 84, no. 2: 234–52. https://doi.org/10.1086/690641.

Rottschaefer, William A. 2012. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.

Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.

Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.

Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.

Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.

Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.

Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.

Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.

