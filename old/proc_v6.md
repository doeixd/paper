# **A Procedural and Naturalistic Model of Moral Objectivity**

## **Abstract**

This paper extends **Emergent Pragmatic Coherentism (EPC)**, the framework from *The Architecture of Inquiry*, to develop **Pragmatic Procedural Realism**, a novel metaethical theory grounded in systems theory and historical empirics. Procedural realism, broadly conceived, holds that objectivity arises from correct procedures. Unlike its more familiar Kantian and constructivist variants, which ground objectivity in idealized rational procedures, our model identifies the procedure with the actual, historical process of pragmatic selection. By applying EPC's **Systemic Brittleness Index (SBI)**, we show that normative claims are filtered by the real-world costs generated by their misalignment with pragmatic constraints. This offers a powerful way of reframing the is/ought gap at the level of justification.

The argument proceeds by operationalizing this diagnostic test. We analyze a network’s brittleness by tracking systemic costs—from bio-demographic crises to coercive overheads—to construct a **Negative Canon** of empirically falsified normative principles. This reveals moral progress to be a real, observable process of *systemic debugging*. On this account, moral objectivity is an emergent, procedural fact about which normative architectures prove resilient under pragmatic selection. The result is a robust, fallibilist realism that naturalizes the reference of moral terms and offers a decisive reply to error theory and quasi-realism.

## **1. Introduction: From Epistemic Engineering to Moral Objectivity**

Our previous paper, *The Architecture of Inquiry*, introduced **Emergent Pragmatic Coherentism (EPC)** as a general theory of justification. It argued that inquiry is a project of **epistemic engineering**: the ongoing craft of building more resilient, less brittle public knowledge structures. The viability of these structures, from scientific paradigms to legal systems, can be diagnosed by tracking their **Systemic Brittleness Index (SBI)**—a measure of the real-world costs generated by a network’s misalignment with pragmatic constraints. That paper focused on diagnosing *epistemic brittleness*, where high costs manifest as failed predictions, ad-hoc "patching," and mounting epistemic debt.

This paper extends that diagnostic framework from the descriptive to the normative domain. We argue that the is/ought gap is not a metaphysical chasm but an artifact of a static epistemology that fails to recognize a unified, cost-based mechanism of justification. In a dynamic framework, both "is" and "ought" claims are ultimately subject to the same external filter of pragmatic selection. The same diagnostic toolkit used to track the failure modes of scientific theories can be adapted to track the failure modes of social and ethical systems. We will show how this unified approach dissolves the is/ought problem and grounds a robust, naturalistic form of moral objectivity we call **Pragmatic Procedural Realism**.

Our central thesis is that moral progress is a real, observable, and non-teleological process of **systemic debugging**. By applying the SBI framework to the historical record, we can identify and catalogue demonstrably brittle normative predicates—those that reliably generate catastrophic systemic costs—into a **Negative Canon** of falsified moral ideas. This reveals moral objectivity to be an emergent, procedural fact: moral truths are not discovered in a non-natural realm but are reverse-engineered from the hard-won, empirical data of systemic failure.

The argument will proceed in four stages. First, we will articulate the methodology for this project, operationalizing the SBI for socio-historical analysis by defining the measurable proxies for **normative brittleness**. Second, we will apply this diagnostic toolkit to model moral progress as a process of identifying and replacing high-cost predicates. Third, we will situate Pragmatic Procedural Realism in the metaethical landscape, showing how it provides a powerful naturalistic alternative to both traditional realism and anti-realism. Finally, we will defend the model against core objections, demonstrating its conceptual resilience. The result is a unified theory of inquiry where the pragmatic project of building more viable systems becomes the engine for discovering objective truth in both science and ethics.

Procedural realism, broadly conceived, holds that moral objectivity arises from correct procedures for answering questions, rather than from a direct correspondence to mind-independent metaphysical facts. This tradition, however, contains two distinct streams. The first, rooted in Kantian constructivism, understands the procedure as an idealized process of rational construction, where free and equal agents determine valid moral principles. The second, which this paper develops, understands the procedure as the *actual, empirical, and historical process of pragmatic selection*. We therefore call our specific model **Pragmatic Procedural Realism** to distinguish it from its rationalist counterparts and to emphasize its grounding in the real-world consequences of our normative designs.

### 1.1: The Status of the Project: A Conditional and Descriptive Turn

Before detailing this framework, it is essential to be precise about its scope. This paper does not claim to solve the ultimate grounding problem of normativity or to provide a non-circular answer to the question, 'Why should we value survival?' Instead, it takes a conditional and descriptive turn. Its central claim is this: if one begins from a minimal naturalism where endurance is a de facto constraint on any system, then EPC provides a robust model of procedural objectivity and moral progress. This project does not defend the initial 'if' but demonstrates the explanatory power of the 'then'. Its ambition is not to prove that we ought to play this game, but to provide a systematic description of the rules of the game we are already playing, shifting the focus from a search for metaphysical foundations to a testable model of the evolutionary process by which values are filtered, retained, and come to have authority.

## **2. From Epistemic to Normative Brittleness: A Unified Diagnostic Method**

The EPC framework posits a unified test for the justification of any public knowledge system. A network's claims are justified not only by their internal coherence but by the demonstrated external viability of the network itself. This section operationalizes that external test for the normative domain, adapting the diagnostic toolkit developed in *The Architecture of Inquiry* to measure the structural health of social and ethical systems.

### **2.1 Defining Pragmatic Viability and Normative Brittleness**

The central metric of our framework is the **Systemic Brittleness Index (SBI)**, a measure of a network's vulnerability to future shocks based on the systemic costs it generates. In our previous work, we distinguished two primary modalities of this fragility:

- **Epistemic Brittleness:** The fragility of descriptive networks (e.g., scientific paradigms), diagnosed by proxies like a rising "Patch Velocity" (the accelerating need for ad-hoc hypotheses) and increasing model complexity without a corresponding increase in predictive power.
- **Normative Brittleness:** The fragility of socio-political and ethical networks. While it also generates epistemic debt, it is most acutely diagnosed through proxies measuring social friction and the misallocation of resources toward internal control rather than productive adaptation.

A network's **Pragmatic Viability** is therefore defined as the inverse of its brittleness: it is the demonstrated, historical capacity of a system to maintain a low and stable SBI over time. This distinction is crucial for immunizing our theory against the "might makes right" objection. A brutal empire that *endures* for a millennium through high-cost coercion is a textbook example of a high-brittleness system. Its longevity is a measure of the immense energy it must burn to manage its self-inflicted instability, not a sign of its viability. A viable network, by contrast, is efficient, resilient, and maintains its structure with low systemic costs.

To be precise about the unit of selection, this framework adopts a distinction from generalized evolutionary theory. A network's informational structure—its core normative predicates and their relations—functions as the replicator: the abstract code that is transmitted over time. The social group and its institutions serve as the interactor: the physical vessel through which the code is expressed and tested. A network 'survives' by successfully propagating its core informational principles, which can occur even if the original interactor collapses; the rediscovery of Roman legal principles during the Renaissance is a prime example. This distinction allows our systems-level analysis to avoid the pitfalls of naive group selection and focus on the long-term viability of the normative code itself.

### **2.2 A Causal Hierarchy of Costs: The Diagnostic Method**

While the general categories of First-Order and Systemic Costs were defined in *The Architecture of Inquiry*, their application to the normative domain requires a crucial methodological principle to avoid ideological circularity: the principle of **causal priority**. This principle establishes a hierarchy for diagnosing normative brittleness, moving from objective bio-social consequences to more abstract ideological claims. This is not a normative claim about which suffering matters more, but a descriptive, systems-dynamic claim about what makes a network unstable and drives its evolution.

1. **First-Order Costs (Causally Primary):** These are the direct, material signatures of a network's conflict with the constitutive **Drive to Endure**. They are the most fundamental indicators, measurable through objective proxies *without reference to a network's internal values*. Key proxies include:
    - **Bio-demographic Data:** Excess mortality, morbidity, malnutrition, and other bio-indicators of systemic stress, measurable via demographic and bioarchaeological records.
    - **Coercive Overheads:** Resources expended on non-productive internal control. The primary proxy is the **Coercion Ratio**: the proportion of a society's total resources (e.g., GDP or labor) allocated to internal security and suppression versus productive capacity (e.g., infrastructure, public health).
2. **Second-Order Costs (Causally Subordinate):** These are more abstract, ideologically-framed costs contingent on a particular sub-network's goals, such as threats to a cultural identity, religious orthodoxy, or an economic model. A network that must consistently pay immense **First-Order Costs** (e.g., through mass coercion and systemic violence) to shield itself from potential **Second-Order Costs** is demonstrating a critical, high-brittleness design flaw.
3. **Systemic Costs (The Management Overhead):** As in the general model, these are the informational costs a network must pay to manage the friction generated by its First-Order Costs. This is where ideology becomes a diagnostic indicator. Proxies include **Information Suppression Costs**: resources dedicated to censorship, propaganda, and the persecution of dissenters, often measurable by innovation lags in critical sectors.

This cost hierarchy is not static; it operates within a dynamic feedback loop that bridges the micro and macro levels. Persistent individual friction—the direct, personal experience of **First-Order Costs**—generates withdrawal of support, defection, and resistance at the micro-level. This aggregation of local responses starves the network's institutional carriers (the macro-level) of the legitimacy and resources they need to function. A network ultimately fails when the systemic costs of maintaining its core institutions exceed the society’s capacity to pay them, leading to collapse or radical transformation. This causal mechanism is key: networks predicated on norms that generate high First-Order Costs must pay compounding Systemic Costs to manage the resulting dissent, making their high SBI a clear, diagnostic signal of their non-viability.

### **2.3 The Primary Tool: Constructing the Negative Canon**

Following Karl Popper, our most reliable objective knowledge is of what is demonstrably false or unworkable. The primary empirical project of this methodology is therefore a negative one: the construction of a **Negative Canon**. This is a robust, cross-cultural, and evidence-based catalogue of normative predicates that have been empirically falsified by the catastrophic costs they reliably generate across multiple historical contexts.

The predicate `slavery is an acceptable organizing principle` is a core entry in this canon. This claim is not based on modern moral sentiment but on the overwhelming historical evidence that networks built on this predicate consistently exhibit a pathologically high **Coercion Ratio**, suffer from endemic **First-Order Costs** of violence, and experience long-term economic and innovative stagnation (Patterson 1982; Acemoglu & Robinson 2012). By meticulously mapping these failures, we empirically chart the boundaries of viable social design.

This diagnostic framework provides a purely instrumental, non-moralistic justification for the central insight of **standpoint epistemology**. The testimony of the marginalized is not treated as privileged out of a prior commitment to social justice, but because it is a **privileged stream of diagnostic data**. Those agents experiencing the highest First-Order Costs function as the most sensitive detectors of a system's impending failure. Their dissent is not mere noise to be suppressed; it is the primary "check engine" light signaling that a network’s core predicates are dangerously misaligned with reality.

### **2.4 Falsification Conditions**

This framework is an empirical hypothesis and must be falsifiable. Its central causal claim is that a predictable, statistically significant relationship exists between a network's internal systemic costs and its long-term fragility. The framework would be falsified if broad, cross-cultural historical analysis revealed any of the following:

1. **No Correlation:** No statistically significant correlation exists between high First-Order Costs (e.g., internal violence) and high Systemic Costs (e.g., a high Coercion Ratio).
2. **The Superiority of High-Cost Systems:** High-cost, coercive networks are shown to be *more* innovative, adaptable, and resilient to external shocks over the long term than low-cost, cooperative networks when controlling for confounding variables.
3. **The Failure of the Negative Canon:** Predicates in the proposed Negative Canon (e.g., `slavery is acceptable`) are found in multiple, controlled historical comparisons to *enhance* a network's long-term resilience and adaptive efficiency.

If the historical record showed high-friction, coercive systems to be just as resilient as low-friction ones, the model's central causal claim would be broken, and the theory itself would be falsified.

### **2.5: Relationship to the Social Sciences: Building on Empirical Foundations**

The diagnostic toolkit proposed here is not built in a vacuum. It is intended to build directly upon the rich empirical work of the social sciences. Concepts like a high 'Coercion Ratio' or 'First-Order Costs' are philosophical abstractions designed to be operationalized using the concrete data and methods developed in fields like political sociology, cliodynamics, and economics.

The specific contribution of this philosophical project is therefore not to replace established and useful concepts like 'legitimacy crisis' or 'state capacity', but rather to ask a different kind of question about the data they generate. The core philosophical hypothesis of this paper is that the same underlying dynamic—a misalignment between a network's core principles and pragmatic constraints—might help explain the failure modes of both normative systems (like societies) and descriptive systems (like scientific paradigms).

By suggesting a common causal engine for these different types of systemic failure, this framework aims to explore a naturalistic basis for a general theory of justification that could span the traditional is/ought divide. This places the project at the intersection of epistemology and social science, asking a distinctly philosophical question: what can the empirical patterns of systemic failure, so richly documented by historians and social scientists, teach us about the structure of objectivity itself?

## **3. Moral Progress as Systemic Debugging**

With the diagnostic method for normative brittleness established, we can now apply it to model moral objectivity. Emergent Pragmatic Coherentism reframes moral inquiry not as a search for non-natural properties, but as a fallible, empirical project of engineering more viable systems of social cooperation. This section will show how this engineering perspective provides a robust, non-teleological account of moral progress.

### **3.1 A Non-Teleological Model of Progress**

EPC models moral progress as a process of **systemic debugging**: the identification and removal of high-cost, brittle predicates from a society’s operative normative network. This process is not a march toward a pre-ordained utopian endpoint (teleology), but a backward-looking, problem-solving endeavor of learning from and correcting catastrophic failures. Progress is the empirically observable reduction of a network’s **Systemic Brittleness Index (SBI)** over time. A moral change constitutes *progress* if the successor network demonstrates a measurably lower SBI—generating fewer **First-Order Costs** and requiring lower **Systemic Costs** for its maintenance—than the network it replaced.

### **3.2 Paradigm Case: The Abolition of Slavery**

The abolition of chattel slavery is a paradigm case of systemic debugging. The claim that this was objective moral progress is not based on an appeal to modern sentiment, but on a pragmatic diagnosis of the predicate `slavery is an acceptable organizing principle` as a catastrophic and unsustainable design flaw.

- **The Bug:** The predicate was a core component of the normative architecture of many historical societies.
- **The Pragmatic Costs:** As predicted by our methodology, this predicate generated immense and unsustainable costs. It required a pathologically high **Coercion Ratio**, diverting vast societal resources to surveillance, patrols, and the violent suppression of dissent. It also generated catastrophic **First-Order Costs**, from the endemic violence needed to maintain the system to the constant, systemic risk of mass revolt. Furthermore, by suppressing the human capital and innovative potential of a huge portion of the population, it incurred profound long-term economic and informational costs, rendering the entire system fragile.
- **The Debugging Process:** The moral arguments of abolitionists were not just appeals to a novel emotion. They were, in effect, arguments that the incumbent normative network was demonstrably inefficient, brittle, and generating unsustainable pragmatic costs. The replacement predicate—`slavery is wrong`—succeeded not because it was metaphysically "truer," but because it proposed a solution that would dramatically lower the network's long-term SBI.
- **The Result:** The eventual triumph of abolition was an act of engineering a more viable social architecture. A demonstrably failed organizing principle, a predicate firmly in the **Negative Canon**, was identified and removed. We can state with empirically grounded confidence that this was not just a change, but progress, because the resulting network, while still imperfect, was significantly less brittle and more pragmatically viable.

### **3.3 Analyzing a Complex Case: The Status of Women**

The EPC framework's power is not limited to historically settled cases. It provides a diagnostic lens for analyzing complex and ongoing moral debates. Consider the historical shift away from patriarchal norms, which can be modeled as the debugging of the predicate `women's roles are confined to the private and subordinate sphere`.

The long-term diagnosis reveals this predicate to be profoundly inefficient. Its costs include the massive economic deadweight loss of excluding half the population from the formal economy and public life; the severe **informational cost** of silencing female perspectives in collective problem-solving; and the high **coercive costs** (both formal and informal) required to enforce these rigid social roles.

The transition to a more egalitarian network is a complex debugging process. It incurs significant short-term friction costs, such as social and political conflict over traditional norms. However, the EPC framework analyzes this not as a failure, but as a high-cost "investment" made to pay down the immense systemic debt of the old architecture. The empirical wager of feminist critique is that a network that fully utilizes the cognitive and economic resources of its entire population will, in the long run, be vastly more innovative, resilient, and pragmatically viable—that is, it will have a much lower SBI. This reframes the debate from a clash of incommensurable values to a tractable, empirical question about the relative engineering soundness of different social designs.

### **3.4 The Structure of Moral Knowledge: Core and Periphery**

This debugging model does not predict a single, dogmatically absolute moral code. Instead, it predicts an emergent moral landscape with two distinct zones, whose boundaries are an ongoing subject of empirical inquiry.

1. **The Convergent Core:** This consists of normative predicates that have been independently discovered and retained across diverse cultures. We identify these core principles not only by elimination via the Negative Canon, but also through a positive methodology: the Test of Independent Convergence. When we observe a specific norm (e.g., reciprocity) emerging repeatedly in maximally different historical and environmental contexts, it provides strong evidence that it is not a cultural accident but the discovery of a non-negotiable, low-brittleness 'engineering principle' for any viable human society.
2. **The Pluralist Periphery:** This accounts for legitimate and persistent cultural diversity without collapsing into relativism. It describes domains where multiple, distinct, yet *equally viable* normative solutions may exist. For example, different models of economic organization (e.g., a regulated welfare-state capitalism vs. a robust social democracy) might represent two different but comparably low-brittleness strategies for managing a complex modern society. The Negative Canon defines the hard boundary of this pluralism: a cultural practice is not a viable option if it relies on predicates (like those supporting honor killings or chattel slavery) that have been empirically falsified by their catastrophic costs.

### **3.5 Methodological Principles and Challenges**

The application of this diagnostic framework is not a simple algorithm but a complex, probabilistic, and ongoing research program. To prevent the SBI from becoming a merely retrospective, "just-so story," its application must adhere to specific methodological principles that account for the complexities of historical analysis.

First, we must maintain **methodological humility**. A network’s failure can result from many **confounding variables** beyond its normative code, such as climate shocks or foreign invasion. The framework's power, therefore, does not lie in single-case causal proofs. It is grounded in **comparative historical inference**, identifying robust patterns where specific normative predicates consistently correlate with high brittleness across different contexts (as in the work of Tainter 1988 or Turchin 2003).

Second, the diagnostic power of the SBI resides not in any single proxy but in analyzing the **correlation of trends across multiple, independent lines of evidence.** A diagnosis of rising brittleness is therefore akin to a medical diagnosis of a syndrome; a single symptom (e.g., a short-term drop in GDP) can be misleading, but a *constellation of correlated symptoms*—such as a rising **Coercion Ratio**, stagnating innovation rates, and negative **bio-demographic indicators**—provides a robust, empirical signal of underlying systemic disease. A warning from one proxy requires investigation; a consistent, negative trend across several proxies constitutes a strong diagnosis.

Third, this comparative approach is essential for addressing a key methodological challenge: the **reference class problem**. A diagnosis of a system’s brittleness can be sensitive to the group of other systems it is compared against; an ancient empire might look fragile compared to a modern industrial state but remarkably resilient when compared to its contemporaneous rivals. To avoid anachronism, the pragmatic test must rely on context-sensitive comparisons, primarily evaluating a network against other systems that operate under similar pragmatic constraints (e.g., comparable levels of technology, similar ecological pressures, and direct competitors). The goal is not to judge a historical system by our standards, but to understand how it performed within its own landscape of viability.

Fourth, specific research designs can be employed to **mitigate confounds**. To isolate normative effects from exogenous shocks, a researcher could, for instance, employ **process-tracing techniques** (as in Bennett and Checkel 2014) alongside **matched-pair comparisons** across contemporaneous societies. In analyzing patriarchal brittleness, one could trace causal chains from gender norms to **First-Order Costs** while holding constant variables like technological access by comparing two societies that faced the same external shock. This elevates comparative inference from mere pattern-spotting to probabilistic causal attribution, enhancing the framework's falsifiability.

Finally, the framework operates under strict **scope conditions** and confronts an ongoing empirical question. The historical record is biased toward large, complex societies, meaning the **Negative Canon** is most reliably built from their failure modes. This defines a crucial limit: the EPC framework is most powerful for diagnosing the viability of normative architectures for large-scale societies. This is a feature, not a bug, that keeps the theory from over-reaching and respects the limits of its evidentiary base. Furthermore, a crucial challenge lies in the relative weighting of the SBI's proxies. This framework does not propose a universal, a priori formula for this. Instead, determining the relative predictive power of different costs is a central empirical question for the research program itself. Comparative analysis might reveal, for instance, that while economic costs are volatile, a rising Coercion Ratio is a more reliable long-term predictor of state failure across different system types. This underscores that a diagnosis must rely on the correlated trend across the entire dashboard of indicators, a task that requires careful, context-sensitive historical analysis.

## 4. Pragmatic Procedural Realism: Situating a Naturalistic Alternative

The framework of Emergent Pragmatic Coherentism, when applied to the normative domain, yields a novel metaethical position we call **Pragmatic Procedural Realism**. This name is chosen to situate our view within the broader family of procedural realisms while highlighting its unique, naturalistic engine. This section will first distinguish our pragmatic approach from the more established rationalist stream of proceduralism, and then show how this empirical grounding provides a powerful naturalistic response to the most significant anti-realist challenges.

As noted, procedural realism contains two primary streams. The first is **Rationalist Proceduralism**, exemplified by Kantian constructivism (e.g., Korsgaard 1996), which grounds normativity in the internal coherence of our own attitudes or the structure of rational agency. For this tradition, the "procedure" is an idealized one of self-legislation or rational agreement. The second stream, which EPC makes possible, is **Pragmatic Procedural Realism**. Here, the procedure is externalist, empirical, and historical: the relentless filtering of normative systems by the real-world costs of their application. Our claim to objectivity is not grounded in the structure of reason, but in the emergent, structural facts about which social architectures prove resilient against the selective pressures of reality.

### **4.1 A Form of Pragmatic, Externalist Realism**

Pragmatic Procedural Realism is a form of moral realism, but its claim to objectivity is not grounded in mysterious non-natural properties or a thick teleological account of human flourishing. It is:

- **Realist** in positing that there are objective, mind-independent truths about the viability of normative systems. The claim that `slavery is wrong` refers to a real, structural fact about that predicate's profound incoherence with the **Apex Network**, the emergent, mind-independent structure of viable predicates.
- **Procedural** in that these moral truths are not static properties but emergent, relational facts discovered through a historical, empirical process. The "truth-makers" for moral claims are the objective facts about which networks prove resilient (maintain a low SBI) against the constraints of pragmatic selection.
- **Externalist** in that the justification for a moral claim rests not on an agent's internal coherence or a culture's consensus, but on the demonstrated, historical track record of the entire public system.

### **4.2 The Apex Network: The Ontology of an Emergent Standard**

The entire architecture of our procedural objectivity rests on the concept of the **Apex Network**. As this is the ultimate standard for moral truth in our system, its ontological status must be defended against a crucial objection: that it is either a merely contingent historical product (and thus not a stable standard) or a mysterious, quasi-Platonic blueprint (and thus not naturalistic). The solution lies in a precise understanding of its nature as an **emergent, procedural object.**

The Apex Network is the complete, trans-historical set of all maximally coherent and pragmatically viable normative predicates. To clarify this, we must return to the core analogy from *The Architecture of Inquiry*: the Apex Network's reality is akin to the shape of a canyon carved by a river.

1.  **vs. The "Contingent Product" Objection:** The canyon's shape is indeed the product of a contingent historical process (the specific path of a river over time). However, it is not *arbitrary*. Its final form is rigorously determined by the interaction between that process and the mind-independent, non-negotiable constraints of the landscape—the hardness of the rock, the laws of erosion, and gravity. Similarly, the **Apex Network** is the emergent "shape" of viability, forged by the contingent river of human history. Its structure, however, is not arbitrary; it is objectively determined by the non-negotiable "bedrock" of pragmatic constraints (e.g., the logic of cooperation, bio-demographic limits). The historical process *reveals and gives form to* these constraints.

2.  **vs. The "Platonic Blueprint" Objection:** The canyon's shape was not a pre-existing design that the river was trying to follow. It did not exist before the procedure that created it. Likewise, the **Apex Network** is not a pre-ordained utopian endpoint or a metaphysical blueprint waiting in a non-natural realm. It is a **retrospectively discovered structural fact**. We infer its properties by studying the "canyon walls"—the vast, empirical data of systemic failures catalogued in our **Negative Canon**.

The Apex Network's status as an objective standard comes from its nature as a **procedurally-generated object that achieves functional independence.** Once the canyon is carved, it becomes a real, objective, and mind-independent feature of the landscape. It now functions as a hard constraint on any new water that flows through it. In the same way, the Apex Network, as the cumulative, time-tested informational structure of what works, becomes a de facto objective standard. Its structure is the singular, convergent solution space left after the relentless filtering of unviable alternatives. It is the real and mind-independent *product* of a procedure, which then functions as the objective *standard* for that procedure. This allows us to maintain a robust, fallibilist realism without appealing to non-natural properties.

Finally, it is crucial to be precise about the scope of this realism. The Apex Network, as inferred from the data of human history, represents the landscape of viable solutions for *complex, social, cooperative primates like us*, with our specific evolutionary heritage and set of pragmatic constraints. The framework does not, and cannot, make claims about the normative truths for hypothetical, non-human intelligences with a completely different biological and social architecture. This is not a concession to relativism but a mark of empirical discipline. It is a form of **species-specific objectivity**. The claim that `slavery is wrong` refers to a real, objective fact about viable social design for human societies, grounded in the entirety of our available evidence. To make claims beyond that would be to abandon the naturalistic, evidence-based methodology that grounds the entire project.

### **4.3 Answering the Anti-Realist Challenge**

This architecture provides a powerful, unified response to the canonical anti-realist arguments.

- **vs. Error Theory (Mackie 1977; Joyce 2001):** We agree with the error theorist's "argument from queerness" that there are no strange, non-natural properties. However, we deny the conclusion that all positive moral claims are false. EPC diagnoses this as a case of **successful but mis-glossed reference**. Our moral discourse has been successfully, if imperfectly, tracking real, procedural facts about the viability of social arrangements all along. The "wrongness" of slavery is not a queer property but a relational, empirical property of a predicate that reliably generates catastrophic **First-Order Costs**. Our framework naturalizes the reference of our moral terms, it does not eliminate them.
- **vs. Non-Cognitivism and "Quasi-Quasi-Realism" (Blackburn 1993; Streumer 2025):** This Pragmatic Procedural Realism can be brought into sharp relief by contrasting it with contemporary projects that defend realism at the semantic level. Bart Streumer, for instance, argues that the truth of a normative judgment is a descriptive fact about the attitudes of the person *assessing* the judgment. This move elegantly solves disagreement for an assessor, grounding objectivity in their stable context. EPC shares this multi-level structure but operates on an entirely different scale. While a predicate like "slavery is acceptable" can have *contextual truth* relative to the local network of a dominant group, EPC argues that when we make objective moral claims, we appeal to a higher standard: that predicate's coherence with our best model of the **Apex Network**. The truth-maker for "slavery is objectively wrong" is therefore not a psychological fact about an assessor's present disapproval, but a robust, empirical fact about the catastrophic costs and systemic fragility that such a predicate imposes on any network over time. Streumer's project brilliantly vindicates the realist-sounding *grammar* of our moral talk; EPC identifies the objective, evolutionary *filter* that makes some moral systems robustly viable and others demonstrable failures.
- **vs. Evolutionary Debunking Arguments (Street 2006):** Sharon Street's "Darwinian Dilemma" argues that since evolution shaped our moral beliefs for adaptiveness, their alignment with any independent moral truth would be a wild coincidence. Pragmatic Procedural Realism resolves this dilemma by collapsing one of its horns. For us, **pragmatic viability is not a coincidental correlate of moral truth; it is the truth-maker itself.** The predicates that enhance a network's long-term resilience are procedurally "better" precisely *because* they are better adapted to the constraints of the **Apex Network**. Evolution is not a distorting influence that the realist must explain away; it is the very filtering process that grounds objectivity.

### 4.3.1 vs. Constructivism: An Externalist vs. Internalist Procedure

EPC’s systems-level externalism distinguishes it from other influential naturalist rivals.

- **vs. Cornell Realism (Railton 1986):** Unlike agent-centric realisms that ground moral facts in an individual's "objective interests" or a synchronic "homeostatic property cluster" of human goods, EPC's unit of evaluation is the **diachronic, multi-generational viability of the informational network itself.** The objective interests of a tyrant are irrelevant to the assessment; the socio-political network that licensed his predicates was an objective, evolutionary failure because of the systemic friction it generated.
- **vs. Neo-Aristotelianism (Foot 2001):** The Neo-Aristotelian project grounds normativity in a "thick," teleological concept of species-specific flourishing. EPC, by contrast, uses the "thin," procedural standard of viability. This allows it to robustly account for the **Pluralist Periphery**, where multiple, distinct forms of life may prove equally viable without having to conform to a single, contentious model of "human goodness."
- **vs. Constitutivist Constructivism (Korsgaard 1996):** Where a constructivist grounds normativity in the internal coherence of our own attitudes or the structure of rational agency, EPC insists on an external check. The ultimate arbiter is not internal consistency but the network's external relationship with the selective pressures of reality. This provides EPC's definitive answer to the "ideally coherent Caligula": Caligula's internal coherence is irrelevant. The network that produced and sustained such a predicate was an **objective, pragmatic, evolutionary failure**, judged not by our disapproval, but by the immense, real-world systemic costs it generated.
*   **vs. Putnam's Pragmatist Proceduralism:** The philosopher Hilary Putnam, in his dialogue with Habermas, developed a pragmatist ethic that has also been characterized as a "procedural realism" (Gil Martín & Encabo 2008). While our project shares Putnam's overarching goal of carving a middle path between substantive moral realism and skepticism, our conception of the core 'procedure' is fundamentally different. The Putnamian procedure remains tied to the norms of idealized rational inquiry and discourse. **Pragmatic Procedural Realism**, by contrast, naturalizes the procedure entirely. The ultimate arbiter is not what we would agree to under ideal conditions, but which systems in fact survive the harsh, empirical filter of historical selection, as diagnosed by the SBI. Our project can thus be seen as providing a fully externalist and systems-theoretic engine for the proceduralist impulse found in the American pragmatist tradition.

### **4.4 The Architecture of Normativity: A Three-Level Account of 'Ought'**

The EPC framework’s theory of truth—with its distinction between contextual, justified, and objective claims—provides a direct and powerful architecture for understanding the status of normative judgments. It resolves the classic tension between moral relativism and moral objectivity by showing how they operate at different levels of a unified system. "Right," "wrong," and the "ought" that commands them are not monolithic concepts; they earn their authority through a process of justificatory ascent.

#### **Level 3: Contextual Rightness (The 'Ought' of Coherence)**
At the baseline level, normativity is a function of a network's internal rules. This is the realm of cultural relativity and social convention.

-   **Right/Wrong:** An action is **Contextually Right** if it conforms to the operative predicates of a specific **Shared Network**. In a society structured by a rigid caste system, for instance, adhering to caste-based rules of deference would be "right" *within that network's logic*. It is a matter of procedural correctness, not objective justification.
-   **The 'Ought':** The "ought" at this level is an **'Ought' of Coherence**. It is a command internal to the system: "If you are a member of this network, you ought to follow its rules." This "ought" has binding force on its members but carries no objective, external authority. It explains how actions we now find abhorrent could have been considered mandatory, but it is also the classic "coherence trap" that EPC's externalist check is designed to overcome.

#### **Level 2: Justified Rightness (The 'Ought' of Viability)**
This is the highest achievable epistemic status for our moral judgments and the source of our most robust, action-guiding "oughts." Justification here is external, empirical, and diagnostic.

-   **Right/Wrong:** A principle or action is **Justifiedly Right** if it is prescribed by a network with a demonstrably low and stable **Systemic Brittleness Index (SBI)**. Conversely, it is **Justifiedly Wrong** if the historical and empirical evidence shows that it reliably generates catastrophic **First-Order Costs** and belongs in the **Negative Canon**. The claim that "slavery is wrong" is justified at this level because the predicate `slavery is acceptable` is a demonstrably high-SBI, failed engineering principle.
-   **The 'Ought':** The "ought" at this level is the **'Ought' of Viability**. This is the core, instrumental "ought" of the EPC framework, as detailed in Section 5.4. It is a wide-scope, strategic command grounded in the evidence of what works: "If our collective goal is to persist as a resilient, low-cost cooperative system, then we ought to adopt these low-brittleness principles and avoid those in the Negative Canon." Its authority comes not from metaphysical fiat, but from the immense weight of historical evidence about the consequences of different social designs.

#### **Level 1: Objective Rightness (The 'Ought' of the Apex Network)**
This is the ultimate, regulative ideal of the ethical project. It represents the formal standard that grounds the comparative judgments made at Level 2.

-   **Right/Wrong:** A principle is **Objectively Right** if it is a core, load-bearing component of the **Apex Network**—the hypothetical, complete structure of maximally viable predicates.
-   **The 'Ought':** The "ought" at this level is the **'Ought' of Optimal Design**. It represents the commands that would issue from a perfect understanding of the principles of viable human cooperation. While we can never be certain we have grasped this "ought" in its entirety, it functions as the non-arbitrary, mind-independent standard toward which our Level 2 engineering project is aimed. Fundamental principles like reciprocity, which pass the Test of Independent Convergence, are our strongest candidates for being certified by this ultimate "ought."

This layered architecture allows Pragmatic Procedural Realism to give a nuanced answer to the question of objectivity. Moral judgments are indeed **relative** at Level 3, accounting for the reality of cultural diversity. But they are robustly **objective** at Levels 2 and 1, grounding our ability to identify moral progress, condemn failed systems, and engage in a shared, evidence-based project of building a more viable world. When we make a serious moral claim like "genocide is wrong," we are not merely stating a Level 3 preference of our culture. We are making a Level 2 justified claim that this predicate belongs in the Negative Canon, and we are making a Level 1 realist wager that it is fundamentally incoherent with the objective structure of the Apex Network.

### **4.5 A Comparative Summary**

The following table summarizes Pragmatic Procedural Realism's unique position:

| View | Truth-makers | Unit of Evaluation | Method | Objectivity |
| --- | --- | --- | --- | --- |
| **Pragmatic Procedural Realism (EPC)** | External viability facts (low SBI), grounded in the Apex Network | Informational network (Replicator) | Comparative, failure-driven empirics | Procedural, fallibilist, externalist |
| **Quasi-realism** | Attitude-dependent projection | Attitudes/practices | Semantic explanation | Deflated, grammar-vindicating |
| **Cornell Realism** | Natural properties of human good | Agent interests | Reflective equilibrium + science | Robust, agent-centered |
| **Neo-Aristotelianism** | Flourishing of organism | Life-form | Teleological evaluation | Robust, teleological |
| **Constructivism** | Idealized procedures / structure of agency | Rational agents | Hypothetical agreement / self-constitution via idealized rational procedures | Procedural, internalist |

### 4.6 The Macro/Micro Bridge: A Complementary Role for Autonomy

This externalist, systems-level account does not seek to replace or invalidate internalist, agent-centered accounts of moral obligation, such as those found in the Kantian tradition. Rather, it aims to provide the macro-level boundary conditions that explain how a stable social practice of autonomous reason-giving is possible at all. A network of Kantian agents still constitutes a social system that must be pragmatically viable to endure.
Our central claim is that the normative principles that would be derived from a procedure of autonomous reason (e.g., respect for persons as ends in themselves) are not only rationally necessary but are also superb, low-brittleness engineering principles. A network that systematically violates the dignity of its members is one that will require a pathologically high Coercion Ratio and will suffer from catastrophic First-Order Costs. In this view, the externalist story of pragmatic selection explains how history filters out those social architectures that are too brittle to sustain a community in which the internalist experience of autonomous moral judgment can flourish. The two accounts are not rivals; they are complementary explanations at different levels of analysis.

A subtle but important question concerns the causal relationship between these two levels of analysis. This framework does not suggest that pragmatic selection is a one-way street, blindly culling ideas proposed by reason. Instead, the relationship is best understood as a long-term dialogue. Rational, internalist procedures, like those described by the Kantian tradition, are powerful engines for generating novel hypotheses about better ways to organize ourselves. The externalist, pragmatic filter of historical selection then acts as the ultimate, non-negotiable arbiter of these hypotheses, preserving those that prove to be not only elegant in theory but viable in practice. In this sense, pragmatic viability is not the *source* of moral insight, but it is the ultimate, non-negotiable *editor* of it, ensuring that our normative systems are accountable to the world of consequences.

## **5. Defending Pragmatic Procedural Realism: Objections and Replies**

With the framework for Pragmatic Procedural Realism fully articulated, we must now test its resilience against the most pressing critical objections. The model's ability to provide robust, non-circular answers to these challenges strengthens its claim to be a coherent, naturalistic framework for moral objectivity.

### **5.1 Objection: The Stability of Evil ("Might Makes Right")**

The most common objection to any pragmatic ethical theory is that it collapses into "might makes right," justifying the practices of any oppressive society that manages to endure. Hasn't a millennium-old empire "won" the pragmatic test, thereby making its oppressive predicates objectively viable by our own standards?

This objection rests on a crucial error the EPC framework is designed to correct: it mistakes **mere endurance** for the richer, more demanding concept of **pragmatic viability**. As defined in Section 2, viability is a measure of a system's ability to maintain its structure with a low **Systemic Brittleness Index (SBI)**. An oppressive state that persists through immense coercion is not viable; it is a high-cost, high-brittleness system. Its longevity is not a sign of strength but a measure of the immense energy (a high **Coercion Ratio**) it must burn to manage its self-inflicted instability.

Such systems are often functionally parasitic, their endurance subsidized by external extraction that masks internal normative decay. This allows us to formulate a set of present-tense **Diagnostic Heuristics**. A network's apparent stability is presumptively parasitic rather than viable when we observe that: (a) its budget for internal control (the **Coercion Ratio**) persistently exceeds investment in productive public goods; (b) its innovation and learning rates lag significantly relative to peers; and (c) its suppression of dissent intensifies without any corresponding reduction in First-Order Costs. These are the empirical signatures of a high-brittleness system on a path toward failure. Their endurance does not vindicate their predicates; it merely makes them long-running, inefficient experiments whose wreckage provides the most unambiguous data for our **Negative Canon** and our fallible map of the **Apex Network**.

### **5.2 Objection: The Power of Ideology**

A more sophisticated objection concedes that oppressive systems generate pragmatic costs but argues that **ideology** can co-opt the revision process. Can't a network create powerful "patch" predicates (e.g., "your suffering is a divine trial") that convince agents to endure failure rather than revise the network's core?

This objection correctly identifies a key tactic in network evolution, but it mistakes a symptom of brittleness for a solution to it. Ideological patches are a form of **normative patching**, functionally identical to the ad-hoc hypotheses that create **Epistemic Debt** in a failing scientific paradigm. They are never pragmatically free. A predicate like "the state is infallible" is a low-eROI (Epistemic Return on Investment) patch—a high-cost addition that makes no novel predictions but merely insulates a failing predicate—designed to suppress data about First-Order Costs. To maintain it, the network must pay compounding **Systemic Costs**: a vast apparatus of propaganda, surveillance, and coercion. The very *need* for this ever-growing layer of ideological insulation is a primary diagnostic indicator of a rising SBI. It provides a powerful, present-tense epistemic reason to be deeply skeptical of any network that requires immense energy expenditure to explain away suffering. A viable network does not need to spend a third of its budget convincing its citizens that they are not suffering.

Historical cases illustrate this brittleness in action. The Soviet Union's ideological apparatus—framing gulag labor as "re-education" and famines as "temporary setbacks"—served as a massive normative patch to sustain predicates like "state ownership is infallible." Yet, it demanded exponential Systemic Costs: a sprawling KGB network, pervasive censorship, and purges that consumed up to 20% of GDP equivalents in coercive overhead ( Conquest 1990). The patch delayed but did not avert collapse, as the underlying First-Order Costs (e.g., demographic crashes from the Holodomor) eroded legitimacy. This exemplifies how ideological insulation, far from resolving friction, amplifies it, providing a diagnostic heuristic for present-day networks reliant on similar "explanatory" infrastructures.

### **5.3 Objection: The Asymmetry of Testing**

A third objection targets the claim of a unified test for all predicates. The test for an empirical claim ("water boils at 100°C") is rapid and decisive, while the test for a moral claim ("slavery is unacceptable") is slow, diffuse, and unfolds over generations. Is this asymmetry so great as to render the claim of a unified test meaningless?

This observed asymmetry is not a flaw in the model but a **direct and crucial prediction of it**. EPC posits a unified pragmatic *filter*, while fully acknowledging that the *complexity of the system being tested* determines the timescale and texture of the feedback. An empirical predicate about a simple physical system produces rapid, clear feedback loops. A moral predicate is a hypothesis about structuring a complex adaptive social system, which has countless interacting variables and feedback loops that are distributed, delayed, and holistic. The difference in the velocity and clarity of the feedback does not imply a difference in the kind of the ultimate arbiter. It simply explains why moral knowledge is so much harder-won—requiring the aggregation of vast amounts of historical data—and underscores that our moral claims are, at their best, fallible attempts to map the real territory of the **Apex Network**.

### **5.4 Objection: The Grounding Problem Revisited**

The final and most fundamental challenge is that the theory appears circular. By making viability the ultimate standard, does it not smuggle in a normative commitment to endurance? Why *ought* we to care about what survives the pragmatic filter?

This objection misunderstands the model's descriptive and conditional nature. The reply is a two-part defense that separates the grounding of the justificatory *arena* from the source of *normative force* within it.

First, the Constitutive Defense: The model is anchored not by a chosen value but by what we can call the Constitutive Condition of Persistence—a minimal, structural precondition for any informational system (a replicator) to propagate over time. This condition functions as a system’s de facto Drive to Endure. The status of this condition is not normative but procedural-transcendental. It is a claim about the conditions of possibility for our inquiry: for a normative predicate to become an object of historical analysis, the network that carried it must have persisted long enough to leave a data trace. Like gravity for an architect, this condition is the non-negotiable filter through which all informational blueprints, including moral ones, must pass to become part of the historical record we can analyze. Persistence thus functions as an inescapable methodological filter on our data set, not a substantive value we are smuggling into the system. It grounds our inquiry into the structure of the Apex Network.

Second, with this non-normative arena established, the **Instrumental Defense** provides the conditional normative force. The "ought" that emerges from EPC is a wide-scope, system-level, and strategic one:

> If a community has a de facto goal of persisting and solving problems (which any ongoing society does), then it has a powerful, evidence-based reason to adopt normative predicates that align with the Apex Network and to avoid those catalogued in the Negative Canon, as this is the most effective strategy for lowering its SBI and satisfying its constitutive Drive to Endure.
> 

This "ought" is purely instrumental. It does not command commitment from an external, metaphysical source. It is a piece of strategic advice, mapping the predictable consequences of different design choices for any system operating within the inescapable constraints of reality. This two-part defense provides a non-circular and robust grounding for the entire framework.

To address potential counterexamples, consider subcultures driven by non-endurant goals, such as ascetic traditions that prioritize transcendence over persistence or apocalyptic ideologies that embrace collapse. EPC does not deny the psychological or cultural reality of such drives but observes that they leave no durable informational trace in the historical record—precisely because they conflict with the constitutive mechanics of replicator propagation. The "Drive to Endure" is thus not a substantive value but a structural filter: only networks that incidentally align with endurance mechanisms (e.g., biological imperatives for survival and reproduction) persist long enough to generate the cross-cultural data from which we infer the Apex Network. This sidesteps circularity by grounding the arena in observable evolutionary dynamics rather than prescriptive ethics.

It is important to clarify that this instrumental "ought" is not intended to provide a lonely egoist with a definitive reason to care about others in every specific instance. Rather, its primary function is to ground a form of **public reason**. It provides a shared, non-metaphysical, and evidence-based standard to which a community can appeal when adjudicating between rival normative systems and justifying its core principles to its members.

### 5.5 Objection: The Problem of Justified Revolution

A final, powerful objection is that this framework appears inherently conservative, privileging stability over justice. A revolution to overthrow a tyrannical but stable regime intentionally generates a massive, short-term spike in First-Order Costs like violence and instability. By this logic, would a just revolution not be diagnosed as a catastrophic failure?

This objection highlights the importance of analyzing systemic costs over the long term and accounting for a system's hidden debts. A revolutionary movement does not generate costs from nothing; it is often a response to a regime that is already carrying immense, if suppressed, **systemic debt**. The apparent "stability" of the old regime is often an illusion, maintained by a pathologically high **Coercion Ratio** and the violent deferral of catastrophic **First-Order Costs** onto a marginalized population.

The violence and instability of a revolution can therefore be understood not as a sign of the new system’s brittleness, but as the sudden, high-cost "payment" of the old system's accumulated debt. The empirical wager of the revolutionaries is that this immense short-term expenditure, and the associated spike in the SBI, will be justified by the creation of a successor network with a dramatically lower and more sustainable long-term SBI. The framework thus does not condemn all revolution, but reframes it as a high-risk, high-cost attempt at radical systemic debugging.

## **6. Conclusion: Inquiry as a Pragmatic Project**

This paper began with one of philosophy's oldest chasms: the gap between "is" and "ought." We have argued that this chasm is not a feature of reality but an artifact of a static epistemology. It is dissolved by a unified, dynamic theory of inquiry—**Emergent Pragmatic Coherentism (EPC)**—and its application to ethics, **Pragmatic Procedural Realism**. The goal of this unified project, whether in science or ethics, is the construction of ever more viable maps for navigating a shared reality.

Pragmatic Procedural Realism reframes moral claims as testable hypotheses about viable social design. Their justification is not metaphysical but diagnostic. We assess the **normative brittleness** of a social system by tracking its **Systemic Brittleness Index (SBI)**—a measure of the real-world costs generated by its core principles. This empirical approach allows us to construct a **Negative Canon** of normative predicates that have been decisively falsified by the historical record of systemic failure.

This framework offers a naturalistic model for a robust, procedural objectivity. We can be humble in knowing that our current maps are imperfect, yet we can be confident that moral progress is a real, observable phenomenon. We can state with an objectivity grounded in empirical evidence that abolishing slavery was not a mere change of opinion; it was a profound act of **systemic debugging**—the identification and removal of a demonstrably high-SBI predicate from our societal code.

Progress occurs when we treat suffering, dissent, and instability not as mere political problems to be managed, but as primary **epistemological data**. They are the "check engine" light for society, the clearest signals of a rising SBI and a misalignment between our normative network and the constraints of reality. This gives us a powerful, pragmatic reason to listen to the marginalized, as they possess the most crucial information about where our collective map is wrong.

The authority of an "ought," therefore, is not found in a mysterious metaphysical foundation or a non-natural realm of values. It lies instead in the immense, procedural weight of historical evidence. An "ought," on this view, is a time-tested, low-brittleness predicate coherent with our most pragmatically resilient networks—a hard-won empirical signal that it represents a viable strategy for the shared project of human endurance. Pragmatic Procedural Realism reframes moral philosophy from a search for ultimate foundations to the ongoing, fallible craft of pragmatic navigation: the art of steering by the light of humanity's most enduring successes and the hard-won lessons from the wreckage of its failures.

It is important, finally, to be clear about the specific contribution this framework aims to make. This model, grounded in the analysis of systemic costs and viability, is primarily a theory of the "floor" of normativity—the necessary, pragmatic foundations upon which any successful social system must be built. It seeks to explain the boundary conditions that separate viable social arrangements from unviable ones. It does not, by itself, provide a complete account of the "ceiling"—the full range of human flourishing, aesthetic values, or supererogatory actions that a viable society might choose to pursue. The central claim is that a society cannot begin to build a durable ceiling until it has secured a stable floor. By providing a naturalistic account of the hard-won principles of systemic viability, this project hopes to offer a solid, empirical foundation upon which other normative theories of the good life can more securely build.

## **Glossary**

This glossary is organized to reflect the architecture of the theory itself, moving from the overarching metaethical position to the diagnostic tools and finally to the resulting structure of moral knowledge.

### **Part 1: The Core Metaethical Stance**

**Pragmatic Procedural Realism**
*   **Core Definition:** The metaethical theory of this paper. It is a form of realism because it holds that there are objective moral truths. It is *procedural* because these truths are not based on mind-independent metaphysical facts, but are emergent properties of a correct procedure.
*   **The Procedure:** Unlike rationalist proceduralisms that rely on idealized thought experiments (e.g., Kantian constructivism), this theory identifies the procedure with the *actual, empirical, and historical process of pragmatic selection.*
*   **The Truth-Maker:** The "truth-maker" for a moral claim is a real, relational, and procedural fact: the objective viability (i.e., the maintenance of a low **SBI**) of the social system that instantiates that principle over the long term, as judged against the constraints of the **Apex Network**.

### **Part 2: The Diagnostic Engine & Method**

**Normative Brittleness**
*   **Core Definition:** A system's vulnerability to internal collapse or external shocks, caused by a misalignment between its core normative principles and pragmatic constraints.
*   **Function:** The primary diagnostic target of the **Pragmatic Test**. It is the normative counterpart to the *epistemic brittleness* found in failing scientific paradigms and is the key to a unified theory of justification.
*   **Key Indicators:** Diagnosed by tracking a correlated set of proxies for high systemic costs, such as a high **Coercion Ratio** (resources spent on internal suppression) and catastrophic **First-Order Costs** (e.g., excess mortality, systemic instability).

**Pragmatic Viability**
*   **Core Definition:** A measure of a system's demonstrated resilience and adaptive efficiency, evidenced by its historical capacity to solve problems and absorb shocks with low systemic costs.
*   **Function:** The objective, external standard of success against which normative systems are evaluated by the **Pragmatic Test**. It is the inverse of **Normative Brittleness**.
*   **Crucial Contrast:** Must be distinguished from *mere endurance*. An oppressive empire that survives for centuries through brutal force is a high-brittleness, low-viability system. Viability is a measure of efficiency, not just longevity.

**The Pragmatic Test**
*   **Core Definition:** The overall empirical methodology that applies the EPC framework to diagnose **Normative Brittleness**.
*   **Function:** It is a research program that analyzes the historical record to assess the long-term costs of normative principles. Its primary empirical output is the construction of the **Negative Canon**, which in turn allows for the fallible mapping of the **Apex Network**.

### **Part 3: The Structure of Moral Knowledge**

**Negative Canon**
*   **Core Definition:** A robust, evidence-based catalogue of normative principles (e.g., `slavery is an acceptable organizing principle`) that have been empirically falsified by their demonstrated, cross-cultural tendency to generate catastrophic costs and a chronically high **Systemic Brittleness Index (SBI)**.
*   **Function:** Represents our most secure form of objective moral knowledge—knowledge of what is demonstrably unworkable. It provides the hard, non-negotiable boundary for any viable moral system, preventing a collapse into relativism. It is the "reef chart" for moral inquiry, mapped from the wreckage of past failures.

**The Convergent Core & The Pluralist Periphery**
This pair of concepts describes the structure of our evolving "map" of viable moral principles.
*   **The Convergent Core:** The set of normative principles (e.g., norms of reciprocity) that have been independently discovered and retained across diverse cultures. Their repeated emergence suggests they are stable, low-cost solutions to universal human coordination problems—the "settled territory" of our moral map.
*   **The Pluralist Periphery:** The domain where multiple, distinct, yet demonstrably viable normative solutions may exist (e.g., different but equally low-brittleness models of social organization). This concept accounts for legitimate cultural diversity and moral disagreement, representing the "contested frontier" of our moral map.
*   **The Boundary Condition:** The **Negative Canon** provides the non-negotiable outer boundary for the Pluralist Periphery. A cultural practice is not a "viable alternative" if it relies on principles that have already been empirically falsified by their catastrophic costs.

### **References**

Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business.

Axelrod, Robert. 1984. *The Evolution of Cooperation*. New York: Basic Books.

Bagnoli, Carla, ed. 2013. Constructivism in Ethics. Cambridge: Cambridge University Press.

Bennett, Andrew, and Jeffrey T. Checkel, eds. 2014. Process Tracing: From Metaphor to Analytic Tool. Cambridge University Press. 

Blackburn, Simon. 1993. *Essays in Quasi-Realism*. New York: Oxford University Press.

Boyd, Richard N. 1988. “How to Be a Moral Realist." In Essays on Moral Realism, edited by Geoffrey Sayre-McCord, 181–228. Ithaca, NY: Cornell University Press.

Buchanan, Allen, and Russell Powell. 2018. *The Evolution of Moral Progress: A Biocultural Theory*. New York: Oxford University Press.

Conquest, Robert. 1990. The Great Terror: A Reassessment. Oxford University Press.

Dewey, John. (1929) 1960. *The Quest for Certainty: A Study of the Relation of Knowledge and Action*. New York: Capricorn Books.

Foot, Philippa. 2001. *Natural Goodness*. Oxford: Clarendon Press.

Gil Martín, Francisco, and Jesús Encabo. 2008. “Truth and Moral Objectivity: Procedural Realism in Putnam's Pragmatism.” Poznan Studies in the Philosophy of the Sciences and the Humanities 95: 265–285.

Glenn, Patrick. n.d. *The Architecture of Inquiry: A Pragmatic and Naturalistic Account of Objectivity*. PhilPapers. Accessed September 11, 2025. https://philpapers.org/rec/GLETAO.

Heidler, Raphaela, et al. 2019. "Bayesian Analysis for Historians." Historical Methods 52(3): 143–162.

Habermas, Jürgen. 1990. Moral Consciousness and Communicative Action. Translated by Christian Lenhardt and Shierry Weber Nicholsen. Cambridge, MA: MIT Press.

Harding, Sandra G. 2004. *The Feminist Standpoint Theory Reader*. New York: Routledge.

Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press.

Holling, C. S. 1973. “Resilience and Stability of Ecological Systems.” *Annual Review of Ecology and Systematics* 4: 1–23.

Joyce, Richard. 2001. The Myth of Morality. Cambridge: Cambridge University Press.

Kitcher, Philip. 2011. The Ethical Project. Cambridge, MA: Harvard University Press.

Korsgaard, Christine M. 1996. The Sources of Normativity. Cambridge: Cambridge University Press.

Mackie, J. L. 1977. *Ethics: Inventing Right and Wrong*. London: Penguin Books.

Patterson, Orlando. 1982. *Slavery and Social Death: A Comparative Study*. Cambridge, MA: Harvard University Press.

Popper, Karl. (1934) 1959. *The Logic of Scientific Discovery*. London: Hutchinson.

Putnam, Hilary. 2002. The Collapse of the Fact/Value Dichotomy and Other Essays. Cambridge, MA: Harvard University Press.

Quine, W. V. O. 1951. “Two Dogmas of Empiricism.” *The Philosophical Review* 60 (1): 20–43.

Railton, Peter. 1986. “Moral Realism.” *The Philosophical Review* 95 (2): 163–207.

Rawls, John. 1971. A Theory of Justice. Cambridge, MA: Harvard University Press.

Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press.

Scott, James C. 1998. Seeing Like a State: How Certain Schemes to Improve the Human Condition Have Failed. New Haven, CT: Yale University Press.

Street, Sharon. 2006. “A Darwinian Dilemma for Realist Theories of Value.” *Philosophical Studies* 127 (1): 109–66.

Streumer, Bart. 2025. "Quasi-Realism for Realists." Philosophers' Imprint 25 (10): 1–17.

Tainter, Joseph A. 1988. *The Collapse of Complex Societies*. Cambridge: Cambridge University Press.

Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.