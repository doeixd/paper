This paper proposes **Emergent Pragmatic Coherentism (EPC)**, a descriptive model of moral epistemology designed to re-frame, rather than solve, the is/ought problem. Building on Quine’s holism, EPC models knowledge as an emergent hierarchy of shared “networks of predicates” scaling up from the individual “web of belief.” Within this architecture, truth is treated deflationarily: not as correspondence with ultimate reality, but as a functional label for robust coherence within a pragmatically-tested network. Crucially, EPC disciplines this network-relativity through an external, selective filter: evolutionary pragmatic selection, the relentless pressure exerted by reality itself. Networks incurring high systemic costs collapse over time, while resilient networks propagate. This process grounds a form of procedural objectivity: truth is internal to networks, but networks themselves can be objectively ranked by their long-term viability. The ultimate regulative standard is a real, emergent historical object—the Apex Network—understood not as a metaphysical ideal but as the cumulative record of predicates that have survived humanity’s long encounter with reality. EPC’s central claim is conditional: if we begin from minimal naturalism, where endurance is a non-negotiable constraint, then EPC provides the best available account of moral progress—not from metaphysical fiat, but from the empirical lessons encoded in our most resilient social and epistemic structures.

# Extending Quine's Web: A Pragmatic and Emergent Model of Moral Knowledge

### Abstract

This initial network-relativity is disciplined by a procedural and objective process of evolutionary pragmatic selection. The model posits that networks are continuously filtered by the constraints of reality, and that unviable networks—those generating high degrees of systemic friction, or pragmatic pushback—are abandoned over time. Consequently, while 'truth' is a property internal to a network, the networks themselves can be objectively ranked by their long-term pragmatic viability. The ultimate standard for this ranking is a theoretical, emergent object this paper terms the Apex Network.

EPC's central claim is conditional: if we begin from a minimal naturalism where endurance is a de facto constraint, then EPC provides a robust model of procedural objectivity. The paper does not seek to solve the ultimate grounding problem but to describe the system that in fact adjudicates our normative claims. It offers a naturalistic model of moral progress, locating the source of normative authority not in a metaphysical foundation, but in the cumulative, hard-won lessons encoded in humanity's most resilient social and epistemic structures. EPC’s contribution is to naturalize moral epistemology by unifying descriptive and normative claims under one selection process, extending Quine’s holism into the moral domain.

---

## 1. Introduction: Modeling the Court of Pragmatic Selection

For centuries, philosophy has wrestled with the apparent gap between claims of fact (“is”) and claims of value (“ought”). EPC suggests that this gap is not a permanent chasm but an artifact of foundationalist epistemology, which artificially segregates descriptive and normative claims. Rather than building a speculative “bridge” across the gap, this paper describes the system we already use to adjudicate both: the evolutionary court of pragmatic selection. On this view, facts and values are tested by the same filter—the unforgiving feedback of reality—and the authority of an “ought” emerges not from metaphysical foundations but from survival-tested coherence.

This paper presents such a model: Emergent Pragmatic Coherentism (EPC). At its core, EPC is a systematic extrapolation of Quinean holism. It begins with the premise that individuals navigate reality using a “web of belief,” a coherent but fallible network of claims. Because all agents face the constraints of a shared reality, these individual webs inevitably overlap where they prove pragmatically successful, giving rise to an emergent hierarchy of shared networks.
EPC models this entire system as an evolutionary process. A claim is 'true' in a deflationary sense: it is coherent within a given network. However, the networks themselves are subject to a ruthless, external filter this paper terms evolutionary pragmatic selection. Networks that generate high degrees of systemic friction, or pragmatic pushback, are less viable and tend to be abandoned over time. The ultimate standard for this process is a theoretical, emergent object—the maximal set of historically viable predicates—termed the Apex Network. Our access to this standard is primarily negative, through the construction of an evidence-based Negative Canon of failed systems.

The argument will proceed by first detailing the core architecture of the EPC model: the bottom-up emergence of shared networks, the evolutionary engine of pragmatic selection, and the negative, historical methodology by which we approximate a standard of objectivity. The paper will then demonstrate the framework's explanatory power by applying its diagnostic tools throughout to a primary historical case study: the abolition of slavery. With the model fully articulated and illustrated, the argument will turn to defending it against the most pressing philosophical objections and, finally, situating it in the contemporary landscape by distinguishing it from its major rivals.

### 1.1 The Status of the Project: A Conditional and Descriptive Turn

It is essential to be precise about scope. EPC does not claim to solve the ultimate problem of grounding normativity, nor to provide a non-circular answer to “why value survival?” Instead, it takes a conditional, descriptive turn. Its claim is modest but powerful: if we assume minimal naturalism—where endurance is the de facto constraint on any system—then EPC provides a robust account of procedural objectivity and moral progress. This conditionality is not a weakness but a methodological strength: by bracketing metaphysical ultimates, EPC avoids endless regress and instead models the actual process by which our normative claims are tested and revised. It does not provide a non-circular answer to the question, "Why should I value pragmatic viability?"

Instead, EPC is a conditional and descriptive model of moral epistemology. Its central claim is this: if one begins from a minimal naturalism where endurance is a de facto constraint on any system, then EPC provides a robust model of procedural objectivity and moral progress. The paper does not defend the initial 'if'—a task for a different metaethical project—but rather demonstrates the explanatory power of the 'then'.

Its ambition is not to prove that we *ought* to play this game, but to provide the first systematic description of the rules of the game we are already playing. It shifts the focus from a futile search for metaphysical foundations to the construction of a testable model of the evolutionary process by which values are filtered, retained, and come to have authority. It describes the court of pragmatic selection; it does not attempt to justify that court's ultimate jurisdiction.

### 1.2 Truth, Viability, and the Pragmatic 'Ought'

Within this descriptive framework, EPC employs a deflationary account of truth. 'Truth' is not a claim of correspondence with ultimate reality, but a functional label for a predicate’s robust coherence within a specified, pragmatically-tested network. A critic might then ask: why should this internal property of 'truth' have any normative force?

The answer lies in the evolutionary origin of the network itself. A shared network is not a logic puzzle; it is a tool for survival forged in the crucible of history. The descriptive fact that a predicate is 'true' (i.e., coherent with one of our most successful, time-tested networks) is therefore a powerful empirical signal. It indicates that the predicate is an integral component of a highly viable strategy for navigating reality.

This reconnects the 'is' and the 'ought' in a procedural way. The 'is' of a predicate's truth-value serves as a reliable guide to the pragmatic 'ought', precisely because that network's coherence was itself forged by the unforgiving 'is' of reality's feedback. This approach bypasses the traditional is/ought problem not by grounding 'ought' in a metaphysical fact, but by treating it as an evidence-based recommendation derived from the descriptive facts of survival. In this, it shares a goal with naturalistic projects like those of Midgley or Curry, which seek to dissolve the fallacy by showing that normative claims are already embedded in the empirical project of navigating a constrained reality. To trust what is 'true' in our best networks is therefore not an appeal to correspondence with a hidden reality, but a pragmatic wager: the hard-won lessons of history are more reliable guides than our untested intuitions. This reframes the is/ought problem procedurally: the “is” of a predicate’s truth-value signals the pragmatic “ought,” because coherence itself has already been stress-tested against reality’s feedback. EPC therefore grounds normativity not in metaphysics but in the cumulative survival data of humanity’s most resilient networks.

## 2. The Architecture of Emergence: From Individual Webs to Shared Networks

The foundation of Emergent Pragmatic Coherentism is a systematic extrapolation of W.V.O. Quine’s holism. Just as Quine’s critique of the analytic/synthetic distinction collapsed the wall between truths of logic and truths of fact, EPC uses his holistic framework to re-frame the distinction between facts and values. Quine famously pictured an individual's knowledge as a “web of belief”—a contextual, coherent network that serves as the arena in which truth-claims are made and adjudicated.¹ While a brilliant metaphor, it remains fundamentally psychological. The task for a social epistemology is to model the structural nature of our shared knowledge.

EPC argues that this transition from the private to the shared is not a puzzle to be solved, but an inevitable, bottom-up process of emergence. The model rests on two basic premises:

1. Every individual agent navigates the world using their own Quinean web of beliefs.
2. Every individual agent is subject to the pragmatic constraints of a shared reality.

From these premises, a third fact follows with structural necessity: wherever agents successfully interact with reality, their individual webs are forced to overlap. Crucially, this intersection is not a conscious negotiation or top-down agreement, but an instantaneous, bottom-up emergence. As soon as two agents align around a successful practice (building a canoe, trading food, coordinating defense), their individual webs have already been pragmatically updated and partially fused. Shared networks are emergent rather than designed: they form through countless micro-adjustments by individuals responding to friction at the boundary with reality. When two people build a canoe, each recalibrates their own web in light of failures, materials, and cooperation. These adjustments converge, producing a shared predicate-network. Crucially, the process is **bounded**: networks cannot drift arbitrarily, since only those recalibrations that succeed under external constraints—buoyancy, weather, endurance of materials—persist. The objectivity of the network is thus not imposed top-down but flows from the eliminative pressure of reality itself.

The coherent set of predicates shared by the canoe-builders is a miniature emergent network. This process scales dynamically across all levels of social organization. A scientific community, a legal system, or an entire culture are simply vastly more complex versions of the same phenomenon. These emergent, shared networks are not centrally designed or consciously ratified. They are the inevitable, contextual byproducts of countless individual webs being continuously filtered and revised against a shared reality. Because individuals are always adjusting their own webs in real time under pragmatic pressure, overlap emerges instantly wherever coherence is rewarded. This bottom-up mechanism explains why durable norms often appear without formal coordination—they are the structural consequence of convergence under common constraints. Their existence is a matter of structural necessity, not consensus.

To analyze this emergent reality, EPC makes a crucial shift from the psychological language of a "web of belief" to the more formal concept of a “network of predicates.” A predicate is the logical structure of a claim that says something about a subject—for instance, the propositional content of “…is wrong,” “…boils at 100°C,” or “…is a viable social strategy.” This move is crucial for three reasons. First, it shifts the analysis from the private and mental to the shared and structural. Second, it makes the formal commensurability of all claims explicit, preparing the ground for a unified test of viability.

Third, and most crucially, it operationalizes the paper's deflationary and coherentist account of truth. On this view, 'truth' is not a claim of correspondence with a noumenal reality, but a functional label. It is a relational property that signifies a predicate's robust coherence within a specified network—a network that has itself been forged by the bottom-up, emergent process. A predicate is therefore 'true' relative to a network, and its truth-value can change the moment that network is pragmatically revised in response to friction. This dynamic updating is not optional but constitutive: coherence shifts whenever agents encounter pushback, making truth a continuously recalibrated property of networks. Far from simple relativism, this context-sensitive dynamism is the conceptual ground for EPC’s account of procedural objectivity: objectivity emerges not from static correspondence, but from the convergent survival of networks forged in the bottom-up crucible of shared reality.

## 3. The Engine of Selection: Pragmatic Pushback and System Viability

What prevents the emergent networks described in Section 2 from being arbitrary, self-justifying constructs? The answer is the constant, bottom-up filter of evolutionary pragmatic selection. This is not a slow or occasional tribunal, but a continuous and immediate process: as soon as a predicate misfires against reality, individuals experience friction, update their webs, and shift alignment. The aggregate of these countless local adjustments is the instant contextual evolution of the shared network. Selection is therefore not external policing but an internal, ever-present discipline exerted by reality’s feedback. This section will detail the causal model of this mechanism, showing how a set of minimal premises entails both the bottom-up emergence of shared networks and the selective, non-foundationalist filter that continuously refines them.

To be precise, the “network of predicates,” as a unit of selection, is not a disembodied set of ideas. It is an informational structure whose viability is tested through its physical instantiation. We must distinguish between the network's active instantiation—its expression in a society’s living institutions, laws, and norms—and its latent persistence. A network's institutional carriers can collapse, yet the informational code—predicates—can survive in texts, oral traditions, or cultural memory, capable of being re-instantiated under different historical conditions. The rediscovery of Roman legal principles during the Renaissance is a prime example of a latent network being reactivated. The ultimate unit of selection is therefore the informational code itself, judged by its capacity to successfully organize a society over time. The failure of a specific instantiation (e.g., the Western Roman Empire) is a powerful data point against its viability, but the ultimate endurance of the code itself is the final measure. This process bridges the micro and macro levels: persistent individual friction (micro) generates withdrawal of support, defection, and resistance, which starves the institutions (macro) of the legitimacy and resources they need to function. A network ultimately fails when the First-Order costs of maintaining its core institutions—through coercion and ideological repair—exceed the society’s capacity to pay them, leading to systemic collapse or radical transformation.

The model rests on two minimal, empirically observable premises: (1) that agents exist within a world with hard constraints, and (2) that they are animated by a suite of persistent, evolved drives to navigate these constraints. Within this suite, one drive has a unique philosophical status: the ‘drive to endure.’ This is not a thick, value-laden concept of "flourishing," but the most minimal, biologically-grounded precondition for any system's continued existence: the persistence of its structure and core information over time.

The causal primacy of this drive can be seen by examining networks organized around apparently different goals (e.g., a religious community focused on otherworldly salvation or a warrior society focused on glorious sacrifice). The EPC model does not deny the sincerity or power of these alternative drives. Instead, it analyzes them at a systems level, focusing on the conditions required for the propagation of the network’s informational structure (its beliefs and values) over time.

Consider a network whose highest stated value is glorious death in battle. For this network’s code to be transmitted to a new generation, it requires a substrate of adherents who survive—warriors who succeed, non-combatants who raise children, and an economy that can produce weapons and food. If the network is *too* effective at realizing its stated value, it eliminates its own carriers and its informational structure vanishes from the historical record. Similarly, a religious network focused on the afterlife must still solve the pragmatic problems of earthly existence—health, social cohesion, resource management—to maintain the institutional and demographic stability required to propagate its beliefs through time. Its otherworldly predicates are parasitic on a set of unstated but necessary pragmatic predicates.

The drive to endure retains its primary, foundational status not because it is the only drive agents possess, but because it is the ultimate selective filter for the informational systems themselves. Endurance is the non-negotiable condition for the multi-generational transmission of *any* set of values, including those that might seem to devalue endurance.

This minimal definition is what gives the drive its unique, non-moral status. All other potential drives (e.g., for purity, glory, or divine submission) are substantive claims about how to live; they are predicates whose long-term pragmatic consequences are to be evaluated. The drive to endure, by contrast, functions as the constitutive condition of inquiry itself. Endurance is not a norm we choose, but a filter imposed by reality. Networks that undermine their own persistence simply vanish; they never enter into the historical record of ongoing inquiry. To call endurance ‘constitutive’ is to note that it is the boundary condition for all epistemic projects. No one legislates it from above; it emerges automatically, bottom-up, from the eliminative pressure reality applies to fragile designs. It is the pre-rational, biological "is" that makes the testing of any predicate-network a necessary and meaningful activity. While its precise boundaries are a valid site of philosophical contention, the project of navigating reality via a network of beliefs necessarily presupposes a commitment to persistence. To question this drive's primacy is a form of pragmatic self-contradiction, as the act of questioning is performed by a system already engaged in the project of enduring.

These premises explain not only why shared networks form (as a convergent solution to shared problems), but also how they are filtered. The selective pressure originates at the individual level as pragmatic pushback. When an agent's actions, guided by a predicate, clash with the constraints of reality, they generate friction—a personal experience of dysfunction. When a predicate is misaligned with shared conditions of reality, individuals will independently and repeatedly experience friction. These experiences aggregate instantly into systemic signals: widespread suffering, dissent, inefficiency, or brittleness. The emergent network “knows” this in real time, not because anyone has consciously tallied costs, but because its constituent agents are already updating their webs in response.

It is this large-scale aggregation of individual friction that constitutes systemic pragmatic pushback: the holistic, emergent consequence of a network's widespread adoption. This is not a vague feeling, but a phenomenon with objective, empirical signatures, such as:

- **Persistence and Breadth:** Systemic friction that recurs across generations and cultural contexts.
- **Systemic Cost:** The immense and ongoing investment of energy (e.g., in coercion and ideological maintenance) required to manage the friction a predicate generates.

This persistent, costly friction is the selective pressure that drives network evolution. It is a bottom-up, emergent phenomenon: individuals, experiencing friction, independently revise their personal networks. The mass aggregation of these local recalibrations causes flawed predicates to decay and more viable alternatives to propagate, thus evolving the shared network. The crucial point is that the data signaling a predicate's incoherence is generated most intensely at the points of greatest friction. Therefore, suffering and dissent are not mere moral problems; they are primary epistemological signals of a network's pragmatic failure. To prevent a circular diagnosis, however, we must turn to the causal mechanism of that failure.

### **3.1 The Causal Mechanism: A Hierarchy of Pragmatic Costs**

To ground the analysis of systemic friction in a non-circular, non-ideological metric, the EPC model specifies a causal hierarchy of pragmatic costs.

- **First-Order Costs:** These are the direct, material signatures of a network's conflict with the constitutive drive to endure. They can be identified through objective metrics: elevated mortality rates, systemic violence, resource depletion, and the immense, ongoing energy expenditure required for mass coercion. These are not abstract "harms" but the measurable, physical consequences of a system generating friction against the biological reality of its agents.
- **Second-Order Costs:** These are more abstract, ideologically-framed costs contingent on a particular sub-network's survival, such as threats to a cultural identity or the disruption of an existing economic model.

The central principle is that **First-Order costs have causal and diagnostic priority.** This is not a normative claim about which forms of suffering matter more; it is a descriptive, systems-dynamic claim about what makes a network unstable. High mortality rates and massive energy expenditures on coercion are objective, material drags on a system's resilience, *regardless of how the dominant group interprets them*. A network that must constantly pay immense First-Order costs to shield itself from potential Second-Order costs is demonstrating a critical design flaw. Its long endurance is not evidence of its viability, but a measure of the energy it must burn to manage its own self-inflicted instability.

This dynamic creates a fatal feedback loop termed **Information Cost**. By systematically suppressing the dissent of those experiencing the most intense First-Order costs, a network blinds itself to the most critical data signaling its own failures. This self-induced epistemic brittleness makes the network profoundly vulnerable to novel challenges and provides a pragmatic justification for treating the testimony of the marginalized as crucial diagnostic data. Their dissent is not just morally relevant but epistemically decisive: it is the localized signal of systemic friction, the bottom-up evidence that predicates are failing. Because emergence is contextual and instant, ignoring these signals accelerates collapse by blinding the network to the very updates it needs to survive.

### **3.2 A Note on Asymmetry and a Brief Case Study**

This framework predicts the vast asymmetry between the testing of empirical and moral predicates. An empirical predicate like “…boils at 100°C” is a hypothesis about a simple system with short, decisive feedback loops. A moral predicate like “…is unjust” is a hypothesis about structuring a complex adaptive social system, where feedback is diffuse, holistic, and unfolds over generations. This difference in velocity explains why moral knowledge is so much harder-won.

Consider the predicate “slavery is acceptable,” a network whose failure serves as a paradigmatic case study for EPC's diagnostic toolkit. The analysis begins with its immense and undeniable **First-Order Costs**: the elevated mortality, malnutrition, and systemic violence inflicted upon the enslaved, all measurable in the demographic and bioarchaeological record. These costs were the direct source of persistent pragmatic pushback in the form of resistance, from work slowdowns to open revolt. This friction, in turn, generated massive, quantifiable **Systemic Costs** for the dominant network. It necessitated a vast expenditure of energy on a coercive apparatus (slave patrols, overseers, punitive legal codes) that diverted resources from more productive enterprise. Furthermore, to manage the cognitive dissonance required, the network incurred a crippling **Information Cost**, systematically suppressing the testimony of the enslaved—the most crucial data signaling the system's failures—and investing heavily in ideological "patches" to justify its own internal contradictions. Abolition was therefore not merely a shift in sentiment; it was the eventual triumph of a predicate ('slavery is wrong') that proposed a more viable, lower-friction, and computationally efficient model for social organization, one that drastically reduced the deadweight loss of coercion and information suppression.

### 3.3 The Scope and Operationalization of the Model

It must be stated clearly, in agreement with the sharpest critiques, that EPC does not offer a simple algorithm for producing a quantitative "viability score."  A network might fail not from its own internal friction, but from sheer historical bad luck—a crippling pandemic, a sudden climate shock, or conquest by a technologically superior neighbor. Isolating the causal effect of a single predicate from this "historical noise". The empirical project of assessing the total First-Order costs of a complex historical network is immense. Furthermore, isolating the causal effect of a single predicate from the noise of countless historical, environmental, and technological variables is a profound methodological challenge. A naive attempt to prove that ‘slavery caused the empire to fall’ would be rightly dismissed as simplistic.

The purpose of this framework is therefore not to provide a final calculation but a diagnostic toolkit and a philosophical research program. It disciplines the analysis by identifying what we should be measuring, and by shifting the burden of proof from direct causal attribution to robust comparative inference. EPC’s methodology is designed precisely to filter out this noise over the long run. The analytical power comes not from a single case study, which might be an anomaly, but from identifying robust patterns across multiple, independent instances that vary in their exposure to confounding variables. If networks predicated on chattel slavery consistently exhibit high coercive costs and internal fragility across different continents, centuries, and technological contexts, we gain high confidence that the predicate itself is a primary source of pragmatic failure, not merely a correlate of historical misfortune. The goal is not to prove that a predicate was the *sole cause* of failure, but to demonstrate that networks organized around it consistently and predictably exhibit higher systemic costs and greater fragility than comparable networks organized around alternative predicates. This approach, common in fields that study complex systems like epidemiology or cliodynamics (e.g., Tainter 1988; Turchin 2003), relies on identifying patterns across multiple case studies to build a robust, evidence-based inference about a network’s design viability.

To move beyond a thought experiment, EPC points toward specific, objective criteria that can be investigated by historians, archaeologists, and economists. A research program for EPC would compare networks by tracking empirical signatures of systemic friction, including:

- **Demographic Indicators:** Comparing excess mortality rates, morbidity, population instability (e.g., collapse or mass flight), and bioarchaeological data (e.g., skeletal markers of malnutrition and violence) between societies organized around different core predicates.
- **Energetic & Economic Indicators:** Measuring the ratio of societal resources allocated to coercive apparatus (internal security, slave patrols, ideological enforcement) versus productive infrastructure (agriculture, trade, education). A network that must spend a vast and increasing share of its energy merely to suppress internal dissent is demonstrably inefficient.
- **Institutional Indicators:** Tracking the frequency of internal conflict, the stability of legal systems, and the costs of information suppression. A network requiring high "Information Costs"—by censoring dissent and punishing heretics—blinds itself to crucial feedback, a quantifiable vulnerability.

Crucially, these metrics are not cherry-picked to confirm a pre-existing moral intuition. They are the objective signatures any system struggling against the endurance filter will necessarily exhibit. For example, a comparative analysis of apartheid South Africa would reveal pathologies across all three categories: elevated mortality in townships, massive security expenditures that crippled the broader economy, and constant internal resistance requiring ever more costly suppression. The model's claim is falsifiable: if a society predicated on, for instance, "slavery is acceptable" could be shown to consistently exhibit greater long-term stability, lower coercive costs, and higher demographic resilience than more cooperative societies under comparable conditions, the EPC hypothesis would be severely damaged. The goal is not a perfect calculation, but a **disciplined and defensible inference** about a network's design viability.

## 4. The Architecture of Objectivity: A Methodological Account

The EPC model avoids simple relativism by demonstrating how a robust, fallibilist objectivity can emerge from a system of pragmatic selection. This section will detail that architecture. It is an account of objectivity grounded not in access to a final truth, but in a rigorous, public, and fallible methodology for inquiry, built upon the bottom-up, emergent hierarchy of contextually coherent networks that is the inevitable result of agents confronting a shared reality. While truth is relative to network level, networks themselves are objectively ranked by pragmatic viability. This creates a structured landscape where individual beliefs can be locally true but globally false when measured against the historical record of what survives.

To frame the following methodological account, it is crucial to resolve an apparent tension in EPC’s theory of truth. On one hand, 'truth' is a functional label for coherence within any given network, which entails that a predicate like “slavery is acceptable” can possess *contextual truth* within the dominant sub-network of a functioning slave society. On the other hand, EPC claims a robust form of procedural objectivity. The resolution lies in a two-level account of truth, recognizing that our objective truth-claims and everyday invocations of 'truth' implicitly appeal to a higher standard. This standard is not coherence with just *any* network, but with the most pragmatically viable network to which we have epistemic access—our best and most fallible approximation of the Apex Network.

From this wider, evidence-based perspective, the predicate “slavery is acceptable” is objectively false. Its falsehood is grounded not in a metaphysical fiat or a non-natural property, but in the robust, empirical conclusion that networks predicated on chattel slavery consistently collapse under the weight of their own catastrophic First-Order costs. This distinction is what allows EPC to escape relativism without metaphysical appeals. While truth remains a property internal to a network, the networks themselves are subject to an external, objective ranking based on the non-negotiable metric of long-term viability. This ranking provides the fallible but real basis for our claims of moral progress.

---

### 4.1 The Status of the Apex Network: An Objective, Emergent Fact

The entire architecture of EPC’s procedural objectivity rests on the concept of the Apex Network. To defend this concept against the charge that it is merely a functional equivalent to an unknowable Platonic Form, we must be rigorously precise about its status. The core of the account lies in a crucial distinction: the difference between **the Apex Network as a real, objective, and mind-independent standard** and our **epistemic access to it, which is necessarily indirect, fallible, and scientific.** The Apex Network is not fallible; our models of it are.

First, we must establish why such an object must necessarily exist, based on the model’s core premises. EPC begins with individual agents, each navigating reality with their own Quinean web of belief. Because every agent is subject to the unforgiving pragmatic constraints of a shared reality, their individual webs are constantly being updated to reduce friction. When two agents successfully cooperate—or even just coexist—their webs are forced to overlap. This convergence is not a choice; it is a structural consequence of interacting with the same set of real-world problems. Now, extrapolate this bottom-up process across all of humanity, throughout all of history. At every moment, countless individual webs are being filtered and forced into partial alignment by reality’s feedback. Given this constant, system-wide process of pragmatic convergence, the existence of a **maximal, coherent, and shared set of pragmatically successful predicates is a structural necessity.** This emergent, historical object—the cumulative, time-tested informational structure forged in the crucible of humanity’s collective, filtered experience—is what EPC terms the Apex Network.

Its ontological status, therefore, is that of an **emergent, structural fact** about our world. The most precise analogy is to the laws of engineering or hydrodynamics. These objective principles are not a perfect "Form of the Cathedral," but a set of real-world constraints on viable design. The Apex Network is the socio-dynamic equivalent: it is the set of "design principles" for a social system that is maximally resilient, stable, and efficient in navigating the real-world constraints of human biology, psychology, game theory, and our shared physical environment. This set of principles is the real, causally effective filter that has been shaping the bottom-up emergence of shared networks all along, whether we correctly understand it or not.

Having established its necessary existence and ontological status, the challenge becomes epistemological. Our access is never direct or final. Instead, we construct a **fallible scientific model** of it—our best and most current map of this unforgiving territory. The construction of this map is a fundamentally historical, empirical, and Popperian-spirited project. We learn the principles of successful design primarily by studying the wreckage of failures. This is the central function of the **Negative Canon**: our robust, evidence-based catalogue of predicates (e.g., “slavery is acceptable,” “political power requires no consent from the governed”) that have been empirically demonstrated to generate catastrophic First-Order Costs across diverse contexts. Each failed empire is a falsified experiment, providing crucial data points for our model. By charting these verified hazards, we are mapping the boundaries of the viable design space. This negative methodology yields positive knowledge: knowledge of what is objectively unviable constitutes positive, albeit incomplete, knowledge of the constraints that any viable system must satisfy.

This framework grounds a robust, fallibilist objectivity and gives precise meaning to our two-level account of truth.

- A predicate is **objectively true** if it is coherent with the Apex Network itself—that is, if it is a component of this maximally viable, emergent structure.
- A predicate is **epistemically justified** for us, at a given time, if it is coherent with our best and most evidence-based model of the Apex Network.

This distinction allows us to account for moral error and progress without contradiction. A predicate like "slavery is acceptable" could have been *contextually coherent* within the dominant network of a slave society. However, it was and is **objectively false** because it is profoundly incoherent with the real, mind-independent Apex Network. Our modern claim to its objective falsehood is not an assertion of cultural preference; it is a highly-warranted scientific hypothesis based on a model built from overwhelming evidence of systemic failure. Moral progress is the process of debugging our societal code by updating our shared networks to better align with our ever-improving map.

The humility of EPC comes from acknowledging that our map is always provisional. The confidence comes from recognizing that we are, in fact, mapping a real and necessarily existent territory. The objective existence of the Apex Network as the ultimate selective filter is what saves the project from relativism, while our fallible access is what disciplines it into a humble, ongoing scientific inquiry.

### 4.2 The Methodology of Objective Inquiry: Charting the Wreckage

Our connection to this regulative ideal is purely methodological. The procedure is fundamentally negative and historical.

First is the Principle of Negative Universalism. Our most reliable knowledge is of failure. The project is not to positively define a utopia, but to build an evidence-based Negative Canon: a robust, cross-cultural catalogue of predicates (e.g., “slavery is acceptable”) that have been empirically demonstrated to generate catastrophic First-Order costs. Crucially, this record is built from the lived experience of those who bear those costs. Oppressed and marginalized individuals are not external to the system but constitutive parts of the shared network. Their dissent, suffering, and resistance provide the most sensitive diagnostic signals of systemic friction. Far from being erased, their webs of belief continuously contribute to the aggregate updates that revise the shared network.

Second is the Test of Independent Convergence under Varying Conditions, the primary tool for distinguishing a structural necessity from a contingent cultural artifact. This test moves beyond simply observing that successful societies share a trait; it actively seeks to falsify the hypothesis that the trait is a mere historical accident. The procedure involves identifying core predicates or functional norms and testing their recurrence across networks that have endured under maximally different environmental, technological, and historical conditions.

For example, the emergence of reciprocity norms in hunter-gatherer societies in arid deserts, agrarian societies in fertile river valleys, and maritime trading societies provides powerful evidence. The vastly different external conditions (geography, subsistence strategy, population density) make it highly improbable that the convergence on reciprocity is an accident. Instead, it suggests that reciprocity is a necessary, convergent solution to a universal set of coordination problems inherent in any enduring human social system. This test becomes more robust still when it aligns with formal models, such as game theory's demonstration that strategies like tit-for-tat are objectively superior for stabilizing cooperation in repeated interactions.

This methodology actively guards against "just-so" stories by demanding that we account for variance. If a predicate only appears in, for example, large-scale agrarian empires, we have reason to believe it is a contingent adaptation to that specific context, not a candidate for the Convergent Core. By systematically cross-referencing history, anthropology, and formal modeling, we build a case for a predicate's pragmatic necessity that is robust, evidence-based, and, crucially, falsifiable. The authority of such a predicate comes not from its presence in a single "winning" culture, but from its repeated discovery as a necessary solution to a universal coordination problem.

A critic might rightly argue that such convergence may merely reflect a contingent, shared human psychology. EPC fully accepts this, as it is irrelevant to the pragmatic project. The theory offers a model for building the most viable maps for the territory we actually inhabit, and that territory includes the unchangeable constraints of our own evolved psychology. EPC's naturalism lies in treating these evolved 'givens' as the non-negotiable starting point.

The convergence test becomes more robust when we focus on functional solutions to universal coordination problems. That multiple isolated cultures develop reciprocity norms isn't mere psychological coincidence—it's repeated discovery of a mathematical necessity for stable cooperation. Game theory shows certain strategies are objectively superior for repeated interactions. Cultural convergence on these solutions is evidence of their logical necessity, not human bias.

The normative force of our inquiry thus comes not from a perfect image of a safe harbor, but from a trans-generational nautical chart pieced together from the records of countless failed voyages. To ask why one should care about this model is like a new captain asking why they should care about a chart of known hazards: it is the most reliable guide we have for avoiding catastrophic failure. This is the essence of moral progress in EPC: the difficult, empirical process of adding to this shared chart of known failures.

Our negative methodology allows us to make increasingly confident truth claims as we move up the hierarchy. While individual networks may contain locally coherent but globally false predicates, the Apex Network represents our best approximation of universally viable truths.

To distinguish genuine pragmatic constraints from mere psychological bias, EPC employs a crucial filtering mechanism: the **Test of Functional Necessity**. This examines whether convergent norms solve specific coordination problems that any enduring social system must address, regardless of psychological makeup. For example, some form of reciprocity norm emerges across cultures not merely because humans happen to value fairness, but because groups lacking such norms face systematic collective action failures that threaten their persistence. The universality of incest taboos reflects not shared psychological quirks but the biological reality that inbreeding depression threatens group viability. These norms persist because they solve real, objective problems that any enduring system must address. Crucially, this test can distinguish between contingent human preferences (which vary widely) and structural requirements (which converge because reality constrains viable solutions).

### 4.3 The Structure of Objective Knowledge: The Core and Periphery

This architecture does not predict a single, uniquely correct answer for every moral question. Instead, it predicts a structured landscape: a “Convergent Core” of universally viable solutions surrounded by a “Pluralist Periphery” of multiple workable options. The Convergent Core is reached not only by dominant groups but through the continuous input of marginalized perspectives. This aligns with standpoint epistemology, which posits that marginalized agents hold epistemic advantages in moral knowledge due to their acute experience of systemic friction. In EPC, such perspectives are not supplementary but constitutive, providing privileged data for refining networks and demarcating the Pluralist Periphery from failed predicates. Because oppressed individuals experience systemic friction most acutely, their networks often supply the decisive evidence that certain predicates (“slavery is acceptable,” “women are subordinate”) belong in the Negative Canon. Thus, convergence emerges from the bottom up, shaped by the whole population of interacting networks—not only the powerful. This structure is a direct consequence of the theory: convergence is expected where universal problems have a narrow range of viable solutions, while pluralism is expected where multiple, equally stable solutions exist.

It is crucial to note that the precise boundary between these zones is an ongoing empirical question, not an a priori declaration. The framework's strength is not in providing pre-ordained answers, but in offering a fallible, evidence-based procedure for discovering where objective constraints are tight and where they are loose. This distinction allows EPC to incorporate the insights of thinkers like Sam Harris while avoiding the pitfalls of a simplistic consequentialism. The concept of the Pluralist Periphery explains why there may be multiple, equally stable peaks on the moral landscape, allowing EPC to account for legitimate cultural disagreements without collapsing into relativism. What prevents relativism is precisely the negative filtering role of systemic failure signals—signals disproportionately supplied by those marginalized within a network. Their webs constrain which cultural variations are truly stable and which are simply subsidized failures waiting to collapse. In this way, dissent and oppression are not noise but epistemically privileged inputs into the shared, evolving chart of viable moral options.

This framework also explains persistent and legitimate moral disagreement. On a contentious issue like abortion, for example, there may be no single, stable predicate yet present in the global shared network. This is not a failure of the model, but rather a snapshot of the bottom-up evolutionary process at work. It shows a domain where multiple competing predicates are still being tested, and where the long-term pragmatic costs and viability of the networks they anchor are still unfolding.

More clearly, the Pluralist Periphery can be illustrated by comparing different but enduring models of political economy. For instance, a well-regulated capitalist system and a robust social democracy represent distinct but viable strategies for organizing a complex society. Both have demonstrated the capacity to endure for generations under modern conditions, and neither generates the kind of catastrophic First-Order costs that would place them in the Negative Canon. They differ significantly in their Second-Order costs and benefits, particularly in how they balance the values of individual liberty and social equality. Within the EPC framework, this disagreement is not a sign that one must be objectively "wrong," but rather that both exist within the Pluralist Periphery of workable solutions. They represent different, stable peaks on the moral landscape, and the ongoing political debate between them is a form of pragmatic negotiation over which set of Second-Order costs a society prefers to bear, bounded by the hard constraints of the Convergent Core.

## **5. Defending the Model: Four Critical Objections**

This section addresses some of the most pressing objections to the EPC model. Demonstrating the theory’s resilience against these challenges strengthens the case for its adoption as a viable framework for a naturalistic moral epistemology.

### **5.1 Objection: The Stability of Evil**

**Objection:** A technologically advanced, oppressive society endures for millennia. Hasn't its network "won" the pragmatic test, thereby making its oppressive predicates objectively true by EPC's standards?

**Reply:** This objection rests on two crucial errors: it mistakes mere endurance for pragmatic viability, and it misidentifies the unit of selection. Moreover, it echoes longstanding critiques of evolutionary ethics, such as those leveled against Social Darwinism for conflating descriptive adaptation with normative justification. EPC avoids this by remaining strictly descriptive and conditional: it models procedural objectivity from viability patterns without prescribing that survival equates to moral goodness, unlike Spencer's progressivist claims. Historical 'stable evils' thus confirm EPC's predictions of eventual collapse due to First-Order costs, not counterexamples. First, the critic mistakes a dominant sub-network for the entire emergent social system. The "stable evil" society is not one coherent network; it is a system defined by the costly conflict between the oppressive network of the dominant group and the resistant networks of the subjugated. The predicates of the subjugated are an ineliminable part of the total emergent structure, and their persistent rejection of the dominant norms is the primary source of the system's friction.

Second, the objection underestimates pragmatic cost by conflating temporary stability with long-term viability. Within the EPC model, "long-term" is not a fixed chronological duration but a functional timescale: it is the period required for a network's internal First-Order costs to compound, drain its energetic reserves, and render it brittle in the face of novel shocks. The apparent stability of an oppressive empire for centuries is often an artifact of external subsidies (e.g., wealth from conquest, resource extraction) that temporarily mask this underlying decay.  Historical analysis reveals that seemingly "stable" oppressive systems follow predictable decay patterns: (1) Escalating coercive costs as resistance adapts; (2) Information degradation as suppression blocks crucial feedback; (3) Institutional brittleness as adaptive capacity atrophies; (4) Elite defection as maintenance costs exceed benefits. The Roman Empire's slave system, medieval serfdom, and apartheid all exhibited these phases. Even when such systems endure for centuries, they demonstrate exactly what EPC predicts: networks requiring massive, ongoing energy expenditure to suppress their own internal contradictions are fundamentally unstable. Their "success" lies not in genuine viability but in temporarily accessing sufficient resources (through conquest, extraction, or technological advantage) to subsidize their core dysfunction. When those external subsidies disappear—as they inevitably do—the systems collapse rapidly, revealing their underlying fragility. In this sense, oppressive empires are not counterexamples to EPC, but paradigmatic confirmations: their longevity is not evidence of strength but of massive external subsidies spent to manage internal contradictions. Their collapse is not a surprise—it is the predictable endpoint of predicates that burn through resources to suppress dissent faster than they can generate resilience. In this sense, oppressive empires are not counterexamples to EPC, but paradigmatic confirmations: their very longevity demonstrates the staggering energy cost of maintaining predicates misaligned with reality. The "stable evil" society is "successful" only in its ability to extract the energy required to manage its own self-inflicted instability. This constant energy drain makes the system brittle and less adaptable to novel challenges. Its endurance does not make its predicates true; it merely makes it a long-running failed experiment whose wreckage we chart to build a better map.

### **5.2 Objection: The Power of Ideology**

**Objection:** Ideology can co-opt the revision process. A network can create "patch" predicates (e.g., “your suffering is a noble trial”) that convince agents to endure failure rather than revise the core of the network.

**Reply:** This objection correctly identifies a key tactic in the attrition between predicates. Yet ideological patches are never pragmatically free. They introduce cascading costs: not only second-order incoherence, but also measurable First-Order drains in the form of surveillance, propaganda, and coercion apparatuses. These patches create brittle systems that depend on constant inputs of energy to maintain illusions. EPC predicts that such networks can mask dysfunction in the short run, but at the price of compounding their long-term fragility. The very need for costly ideological insulation is itself an epistemic red flag: it signals predicates already misaligned with reality. While ideology can mask the *symptoms* of pragmatic pushback, it cannot eliminate the underlying dysfunction.

This highlights the theory's crucial short-term diagnostic force. While the ultimate filter is long-term, EPC provides an urgent, present-tense criterion: **we have a powerful pragmatic reason to be deeply skeptical of any network that requires costly ideological patches to function.** The presence of these defense mechanisms is a primary epistemological signal that the network's core predicates are misaligned with reality. It gives us a reason to listen to dissenters *now*, as they are the canaries in the coal mine of long-term systemic failure.

### **5.3 Objection: The Asymmetry of Testing**

**Objection:** The test for an empirical claim (“water boils at 100°C”) is immediate, while the test for a moral claim (“slavery is unacceptable”) is slow and contested. This asymmetry is so great that it renders the claim of a unified test meaningless.

**Reply:** This asymmetry is not a weakness of EPC but a direct prediction of it. The model posits a unified principle of justification—pragmatic viability—while fully acknowledging plural timescales and textures of testing. Simple physical systems yield rapid feedback; complex adaptive social systems yield slow, distributed signals across generations. What unifies them is not speed but filter: both are disciplined by reality’s unforgiving feedback. The lag in moral testing is exactly what the model expects, and explains why moral knowledge is harder won but no less objective. The nature of the pragmatic pushback is proportional to the complexity of the system being mapped. An empirical predicate about a simple physical system has short, decisive feedback loops. A moral predicate about structuring a complex adaptive social system has long, diffuse, and contested feedback loops that unfold over generations. This difference in velocity and texture explains why moral knowledge is so much harder-won, without undermining the unified framework.

### **5.4 Objection: The Grounding Problem in Three Forms**

The most fundamental challenge to EPC can be posed in three interconnected ways: **(a)** that the theory is circular, resting on a smuggled normative premise (the "drive to endure"); **(b)** that it still requires us to *care* about what survives, a normative commitment it cannot justify; and **(c)** that its deflationary truth severs the link between what is 'true' and what we 'ought' to do. These are all facets of the same grounding problem.

**Reply:** The EPC model answers all three facets of this objection with its explicit **descriptive and conditional turn**, as outlined in Section 1.1.

First, the model is not circular: it does not attempt to justify endurance normatively. Endurance is not a smuggled value but a transcendental condition of inquiry. A system that does not persist cannot evaluate, revise, or remember. Thus the “drive to endure” is not a moral claim within the game, but the constitutive rule of the game itself. We need not choose survival any more than we choose gravity; it is the brute filter through which all networks are already passing. EPC’s modest but powerful claim is simply this: if one is engaged in inquiry at all, one is already presupposing endurance. The system's anchor is not a value we choose, but a brute fact of selection we observe. We do not need to *value* survival for it to be the non-negotiable constraint, any more than we need to *value* gravity for it to determine which structures stand and which fall.

Second, this answers the question of why we should *care*. EPC's claim is conditional: *if* an agent or system has a de facto commitment to enduring, *then* EPC describes the process by which more viable strategies are discovered and encoded. The theory does not command this commitment; it simply describes the consequences for systems that have it versus those that do not.

Finally, this reconnects 'truth' and the pragmatic 'ought' through a purely instrumental and conditional logic. The theory does not make the normative claim that "what survives is what is good." It makes the descriptive claim that our most enduring networks are vast repositories of data on what has survived. The key insight is that the "drive to endure" is not a normative commitment we choose, but the constitutive constraint that makes normative inquiry possible at all. Just as formal logic presupposes the law of non-contradiction without deriving it from more basic premises, EPC identifies endurance as the transcendental condition that makes the evaluation of any normative system meaningful. A system that systematically undermines its own persistence cannot coherently evaluate anything, including alternative values like justice or beauty. Therefore, the normative force is conditional but non-arbitrary: if an agent engages in normative inquiry at all, then they are already presupposing the framework within which 'true' predicates (coherent with time-tested networks) provide the most rational pragmatic guidance. The "ought" is not a moral command, but a piece of evidence-based, strategic advice for a system that presupposes its own desire to endure. This conditional framing also distinguishes EPC from error theories (e.g., Mackie or Joyce, cited herein), which deny objective moral facts altogether; EPC posits procedural facts about viability as objective, albeit fallible, guides.

### 5.4.1 The Transcendental Status of Endurance

The authority of the endurance filter is thus physical, not moral. It is a constitutive background condition, analogous not to a chosen value like "justice," but to a physical constraint like gravity. An architect does not need to normatively value gravity to be bound by its rules; any design that ignores gravity is not a viable alternative, but simply a failure. Gravity is the non-negotiable filter through which all blueprints pass.

Similarly, EPC does not argue that one ought to value a system’s endurance. It makes the descriptive claim that persistence-over-time is the de facto selective filter for any information-bearing system, be it a genome, a species, or a network of predicates. A network that fails this filter does not lose a moral argument; it ceases to exist. The object of selection is the informational structure of the network itself. While this has consequences for the individuals within it, the primary filter acts on the viability of the shared code.

Therefore, to question endurance’s primacy is not to propose an alternative value, but to misunderstand the frame of inquiry. It is a pragmatic self-contradiction, as the act of questioning is performed by a system (an agent, a language, a culture) already engaged in the project of enduring through time. EPC’s claim is not circular because it doesn't justify endurance; it simply points out that it is the arena in which all justification takes place.

### 5.5 Objection: The Status of the Apex Network

**Objection:** The Apex Network is epistemologically problematic. As a standard that is fundamentally inaccessible, it is functionally equivalent to an unknowable Platonic form.

**Reply:** This objection conflates epistemology (our access) with ontology (what exists). EPC posits the Apex Network not as a metaphysical ideal but as a structural byproduct of reality’s filtering. Its existence is no more mysterious than the fact that, among countless organisms, some persist while others go extinct. Our access is partial and fallible, but this is the standard epistemic condition for studying any large-scale emergent system—ecosystems, economies, even the climate. That we cannot see the whole does not erase the object; it disciplines us to triangulate using convergent evidence from history, anthropology, and systems dynamics. Its existence is an inevitable structural consequence of the model's premises. The challenge is purely epistemological: our access to this real object is incomplete and fallible. However, this is the standard condition for inquiry into any large-scale emergent system, from an ecosystem to "the global economy." We cannot perceive these systems directly, but we can build and test fallible models of them.

The existence of the Apex Network is what grounds the possibility of making objective, truth-apt claims: a statement is objectively true if it is coherent with the Apex Network and objectively false if it is not. Our epistemic humility comes from acknowledging that we can never be 100% certain that our current map of this network is perfect. Our confidence in any truth claim is therefore proportional to the strength of our evidence—primarily, the long-term, cross-cultural data gathered via our "nautical chart" of failed systems. Our claim to objectivity rests not on a perfect vision of the safe harbor, but on the undeniable reality of the historical wreckage.

## **6. Situating the Model: A Unified Alternative**

Emergent Pragmatic Coherentism synthesizes insights from several traditions to occupy a unique position in the philosophical landscape. This section will situate the EPC model by contrasting it with major rival accounts in metaethics and epistemology, demonstrating its distinct advantages.

### **6.1 A Form of Procedural, Pragmatic Realism**

First, EPC is a form of realism without metaphysical mystery. It is realist in that it claims there are objective truths about which normative systems are superior—but unlike Non-Natural Realism, it grounds these truths in observable patterns of viability, not in positing sui generis moral properties. Unlike constructivist approaches, EPC does not rest on the coherence of our attitudes alone, but on their exposure to reality’s external filter. Its objectivity is therefore procedural: grounded in the evolutionary dynamics of pragmatic selection, not in metaphysical fiat or conceptual analysis. The truth-makers for moral claims are not mysterious, non-natural properties (as in Non-Natural Realism) nor are they reducible to an analysis of our moral concepts (as in Michael Smith's analytic realism). Rather, the truth-makers are the objective, empirical facts about which networks prove most viable against the real-world constraints of pragmatic selection. The validity of a moral predicate is thus an emergent, relational property, much as the ‘fitness’ of an organism is a real, objective property of its relationship with an environment. To put it plainly: 'truth' is a property internal to a network’s coherence, while **viability** is an objective property of that network’s external relationship with reality.

This procedural, systems-level realism can be brought into sharp relief by contrasting it with contemporary projects that defend reductive realism at the semantic level. Bart Streumer, for instance, argues that a reductive realist can answer the charge of relativism by adopting the quasi-realist's toolkit, particularly a form of "assessor relativism" (Streumer 2025). On this view, the truth of a normative judgment like "Lying is wrong" is a descriptive fact about the attitudes of the person *assessing* the judgment. This move elegantly solves the problem of disagreement for an individual assessor, grounding objectivity in the stable context of assessment rather than the variable context of the speaker.

EPC shares this multi-level structure but operates on an entirely different scale. Like Streumer's view, EPC acknowledges a form of contextual truth: a predicate like "slavery is acceptable" can be true *relative to the local network* of a dominant group in a slave society. However, EPC's crucial move is not semantic but epistemic and empirical. It argues that when we make objective moral claims, we are implicitly appealing to a higher standard than our local network's coherence. We are making a fallible claim about that predicate's coherence within our best approximation of the **Apex Network**—the maximal set of historically viable predicates. The truth-maker for "slavery is objectively wrong" is therefore not a psychological fact about an assessor's present disapproval, but a robust, empirical fact about the catastrophic First-Order costs and systemic fragility that such a predicate imposes on any network over time. Streumer's project vindicates the realist-sounding *grammar* of our moral talk; EPC identifies the objective, evolutionary *filter* that makes some moral systems robustly viable and others demonstrable failures.

This distinction allows EPC to provide a powerful, naturalistic account of the very same phenomena that motivate quasi-realist strategies. Streumer, following quasi-realists, seeks to vindicate our intuitions that moral mistakes are possible, that disagreement entails that one party is wrong, and that moral truths are mind-independent. EPC explains these as follows:

1. **Possibility of Mistake:** An agent can be mistaken in two ways. They can hold a belief that is incoherent with their own sub-network (a simple error), or, more profoundly, their entire sub-network's "truth" can be an objective, pragmatic failure because it is radically misaligned with the Apex Network.
2. **Disagreement:** When two agents from conflicting sub-networks disagree, EPC does not leave them at a relativistic impasse. It provides an external, objective court of appeal: the historical record of pragmatic viability. Their disagreement is a real one about which strategy for social organization better avoids systemic collapse, a question that can be investigated empirically.
3. **Mind-Independence:** Crucially, EPC grounds the intuition that "slavery would have been wrong even if no one thought so" not in an attitude, but in an objective fact about the world. The systemic friction and immense First-Order costs generated by slavery are real, mind-independent consequences of that system's design, which exist regardless of whether anyone at the time disapproves of or even recognizes them.

Thus, while Streumer's "quasi-quasi-realism" offers a brilliant account of how the logic of assessment can mimic objectivity, EPC proposes that this logic is a reflection of a deeper, procedural objectivity governed by the unforgiving feedback of reality itself.

This specific type of procedural realism places EPC firmly within the tradition of **pragmatic naturalism**. It distinguishes itself, however, by providing a more rigorous philosophical architecture with three distinct contributions: (1) a formal emergent structure (the **nested hierarchy of networks**); (2) a causal, rather than purely normative, filter (the **hierarchy of pragmatic costs**); and (3) a naturalistic basis for fallibilist objectivity (the **Apex Network**). The necessity for such a rigorous architecture is vividly illustrated by the philosophical dead ends encountered by more intuitive, "folk" versions of pragmatic naturalism, which often struggle to escape relativism without a formal model of procedural objectivity.

### **6.2 A Completion of the Quinean Project**

Second, the EPC framework should be understood as the systematic completion of the **Quinean project**, itself a direct heir to **American Pragmatism**. Quine’s demolition of the analytic/synthetic distinction unified all descriptive claims within a single pragmatic web. EPC extends this holism one decisive step further: it shows that normative claims are subject to the same filtering process. Where Quine mapped the static architecture of an individual’s web of belief, EPC identifies the dynamic, bottom-up mechanism that evolves these webs into shared networks and continuously revises them under pragmatic pushback. In doing so, it completes the Quinean project by dissolving the last firewall—between fact and value—without resorting to metaphysical shortcuts. It inherits Quine’s core commitments—fallibilism, pragmatism, and the rejection of a priori firewalls—but where Quine described the static architecture of an individual’s web, EPC identifies the missing piece: the **dynamic, evolutionary engine** of pragmatic selection. It fulfills John Dewey’s project of dissolving dualisms and provides a naturalistic answer to C.S. Peirce’s “end of inquiry”: the Apex Network is not an ideal limit, but the messy, emergent result of what has *actually survived* our collective experiment. This realism distinguishes EPC from the anti-representationalism of neopragmatists like Richard Rorty. Where Rorty replaces objectivity with solidarity, EPC argues that lasting solidarity is an **emergent property** of a network that has achieved objective pragmatic success.

This synthesis of holism and evolution also distinguishes EPC from other non-foundationalist epistemologies. While EPC shares affinities with Haack’s Foundherentism in rejecting strict foundationalism, the models diverge in what disciplines coherence. Foundherentism relies on the interplay of experiential input and mutual support, but does not posit an external, eliminative dynamic. EPC strengthens the account by locating that discipline in reality’s selective pressures: networks incurring systemic costs collapse, regardless of their internal coherence. This grounds EPC’s objectivity not in conceptual analysis but in observable survival patterns. In this respect, EPC occupies a middle ground: more naturalistic and externalist than Foundherentism, while avoiding the metaphysical commitments of non-natural realism. And while it shares a language with **Network Epistemology**, its focus is on the **diachronic, evolutionary dynamics** that forge and destroy networks, not on the static analysis of belief-linkages.

### **6.3 A Realist Alternative to Constructivism and Expressivism**

Finally, while EPC shares constructivism’s insight that moral systems are human-built, it departs decisively by locating validity not in coherence alone but in long-term viability under reality’s pressures. Unlike constructivism, it does not stop at “we built it, therefore it binds us”; instead, it subjects constructions to elimination by pragmatic selection. Similarly, unlike expressivism, EPC does not reduce normative claims to projections of sentiment; it explains why some sentiments propagate and stabilize while others collapse. For error theorists like Mackie or Joyce (cited), who argue moral claims lack truth-aptness, EPC offers a conditional alternative: if endurance is presupposed, then moral predicates gain procedural truth via pragmatic viability, reframing error as network-specific failure rather than global illusion. Non-naturalists might view this as insufficiently foundational, but EPC's descriptive turn invites them to test their systems empirically against the same filter, without claiming to resolve ultimate metaphysics. EPC therefore offers a realist account of moral objectivity that is both stance-independent and naturalistic: truths about norms are emergent properties of which networks survive, not of how we feel about them.

It avoids the pitfalls of some forms of **Evolutionary Ethics** that attempt to derive normative content directly from our evolved psychological dispositions. EPC makes no such direct derivation; the test of a predicate is not its origin, but its long-term performance. This also distinguishes it from **Norm-Expressivism** (Gibbard 1990). While Gibbard provides a brilliant naturalistic account of the psychological function of normative language, EPC offers a theory of how these normative systems are themselves subject to an objective, external, mind-independent selective pressure. It can explain not just why we have norms, but why some normative systems fail.

This external, systems-level focus draws a sharp distinction between EPC and other influential naturalist or coherentist rivals. It is most clearly seen when contrasted with **Neo-Aristotelian Naturalism** (Foot 2001). While both are forms of naturalism, they operate on different scales and with different evaluative metrics. The Neo-Aristotelian project grounds normativity in a thick, teleological concept of species-specific flourishing; a good human is one who excels at the life characteristic of the human life-form. The unit of evaluation is, ultimately, the organism. EPC, by contrast, shifts the unit of selection from the individual organism to the **informational structure of the network itself**. Its metric is not a contentious, pre-defined concept of "flourishing," but the thinner, procedural, and non-teleological standard of multi-generational viability. This move provides a crucial advantage: it allows EPC to robustly account for the Pluralist Periphery, where multiple, distinct forms of life can be judged equally viable so long as they do not generate catastrophic First-Order costs. Where the Neo-Aristotelian might be forced to judge which of these forms best instantiates human goodness, EPC remains agnostic, testing only the systemic resilience of the code that underwrites them. The question is not "Does this practice contribute to human flourishing?" but rather "Does the network predicated on this practice demonstrate long-term stability under pragmatic pressure?"

This same externalist principle distinguishes EPC from a more conventional **Constructivism**. A purely constructivist theory, whether Kantian or Humean, ultimately grounds normativity in the coherence of our own attitudes. This is most clearly articulated in the constitutivism of thinkers like Christine Korsgaard, who grounds normativity in the self-reflective structure of rational agency (Korsgaard 1996). For EPC, this internal coherence is insufficient. The ultimate arbiter of a network's validity is not its internal logical structure but its **external relationship with the selective pressures of reality.**

This provides EPC's definitive answer to Sharon Street's "ideally coherent Caligula" (Street 2006). A constructivist might struggle to condemn Caligula if his monstrous attitudes are perfectly coherent. For EPC, Caligula's internal coherence is irrelevant. The network that produced and sustained such a predicate was an **objective, pragmatic, evolutionary failure**, judged not by our disapproval, but by the immense, real-world systemic friction it generated. This stance-independence is what makes the theory a form of realism and allows it to suggest that the ingenious project of **quasi-realism** (Blackburn 1993) is a brilliant solution to a problem that a more sophisticated, realist-compatible epistemology like EPC dissolves from the outset.

### 6.4 The Dynamics of Moral Innovation

EPC privileges neither tradition nor novelty but demonstrated functionality. Established norms have no intrinsic authority; they persist only if they continue to pass reality’s filter. Moral innovations face the same test: many fail, but those that resolve entrenched systemic friction spread and stabilize. Abolitionism, for example, succeeded not because it was sentimentally compelling but because it alleviated immense First-Order costs that slavery generated. EPC therefore captures both conservatism and revolution within a single logic: every predicate, old or new, must prove its viability against the same bottom-up, pragmatic selection process. Abolitionism initially appeared 'untested' but succeeded because slavery generated massive First-Order costs that abolition resolved. The framework's bias isn't toward tradition but toward demonstrated functionality. Revolutionary moral insights that reduce systemic friction will, by definition, propagate and become part of the Apex Network.

## **7. Conclusion: Inquiry as a Pragmatic Project**

Emergent Pragmatic Coherentism ultimately argues that **morality is a project, not a given.** It offers a descriptive model of all inquiry—scientific, political, and ethical—as part of the same fundamental human endeavor: the multi-generational effort to construct the most viable maps for navigating our shared existence. On this view, an "is" is a predicate about how the world appears to function, and an "ought" is a time-tested predicate about how best to act within it. Both are adjudicated in the same ultimate court of long-term pragmatic selection.

This framework is not a solution to the ultimate philosophical problem of grounding normativity. Instead, by taking a descriptive turn, it provides a powerful, naturalistic model for a form of **procedural objectivity**. It allows us to be both **humble and hopeful.** We are humble in knowing that our current maps are imperfect and that our access to the lessons of history is fallible. Yet we can be hopeful, because moral progress is a real, observable phenomenon. We can state with confidence that abolishing slavery was not a mere change of opinion; it was a profound act of **debugging our societal code**. It was the identification and removal of a predicate from our **Negative Canon** of demonstrably failed systems.

Progress occurs when we treat suffering, dissent, and instability not as mere political problems to be managed, but as **primary epistemological data**—the "check engine" light for society. This gives us a powerful, pragmatic reason to listen to the marginalized, as they possess the most crucial information about where our collective map is wrong.

The authority of an “ought” is therefore not found in a metaphysical foundation or the structure of rational agency. It is located in the immense, objective, and procedural weight of the lessons we have collectively learned from what has, and has not, survived our species’ long and often brutal encounter with reality. An "ought," on this view, is simply a predicate that is 'true' within our most pragmatically resilient network—a hard-won empirical signal that it is a viable strategy for the shared, enduring project of human cooperation.

The conditionality isn't a weakness but a methodological strength. By explicitly bracketing ultimate metaphysical questions, EPC provides what philosophy most needs: a research program rather than dogma. Whether one is a Kantian, utilitarian, or virtue ethicist, all must confront the empirical question: do the systems advocated by their theory actually work? EPC does not deliver final answers, but it does provide the conceptual and methodological tools for approaching that question with greater objectivity. It reframes moral philosophy from the search for ultimate foundations to the ongoing, fallible craft of building the best maps we can from the wreckage of failed systems.

---

### **References**

Blackburn, Simon. *Essays in Quasi-Realism*. Oxford University Press, 1993.

Dewey, John. *The Quest for Certainty: A Study of the Relation of Knowledge and Action*. Minton, Balch & Company, 1929.

Haack, Susan. *Evidence and Inquiry: A Pragmatist Reconstruction of Epistemology*. 2nd ed. Prometheus Books, 2009.

Harris, Sam. *The Moral Landscape: How Science Can Determine Human Values*. Free Press, 2010.

James, William. *Pragmatism: A New Name for Some Old Ways of Thinking*. Longmans, Green, and Co., 1907.

Joyce, Richard. *The Evolution of Morality*. MIT Press, 2006.

Kitcher, Philip. *The Ethical Project*. Harvard University Press, 2011.

Korsgaard, Christine. *The Sources of Normativity*. Cambridge University Press, 1996.

O’Connor, Alex (CosmicSkeptic). “Why Ethics Are Subjective - Destiny” *YouTube*, May 29, 2021. Video, 1:17:34. https://www.youtube.com/watch?v=Izg97iGcTk4.

O'Connor, Cailin. *The Origins of Unfairness: Social Categories and Institutional Inertia*. Oxford University Press, 2019.

Peirce, Charles Sanders. “How to Make Our Ideas Clear.” *Popular Science Monthly* 12 (January 1878): 286–302.

Putnam, Hilary. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Harvard University Press, 2002.

Quine, W.V.O. “Two Dogmas of Empiricism.” *The Philosophical Review* 60, no. 1 (1951): 20–43.

Quine, W.V.O. *Word and Object*. MIT Press, 1960.

Rorty, Richard. *Contingency, Irony, and Solidarity*. Cambridge University Press, 1989.

Sellars, Wilfrid. *Empiricism and the Philosophy of Mind*. Harvard University Press, 1997.

Street, Sharon. “A Darwinian Dilemma for Realist Theories of Value.” *Philosophical Studies* 127, no. 1 (2006): 109–66.

Robson, Ellie. "Mary Midgley's Meta-Ethics and Neo-Aristotelian Naturalism." *British Journal for the History of Philosophy* (2024). https://doi.org/10.1080/09608788.2024.2389863.

Curry, Oliver Scott. "Who's Afraid of the Naturalistic Fallacy?" *Evolutionary Psychology* 4 (2006): 234-247. (Republished/updated version available on ResearchGate as of March 4, 2025: https://www.researchgate.net/publication/287952927_Whos_Afraid_of_the_Naturalistic_Fallacy).

Dular, Nicole. "Standpoint Moral Epistemology: The Epistemic Advantage Thesis." *Philosophical Studies* 181, no. 8 (2024): 1813-1835. https://doi.org/10.1007/s11098-023-02072-9.

Fleisher, Will. "A Defense of Endorsement." In *Philosophy with Attitude*, edited by Mark Walker and Sanford Goldberg. Oxford: Oxford University Press, forthcoming. (PhilArchive upload: May 20, 2024, https://philarchive.org/rec/FLEADO-4).

Okasha, Samir. "On the Very Idea of Biological Individuality." *British Journal for the Philosophy of Science* (2023). https://doi.org/10.1086/728048

Streumer, Bart. "Quasi-Realism for Realists." *Philosophers' Imprint* 25, no. 10 (2025): 1–17.

Hume, David. *A Treatise of Human Nature*. Edited by L.A. Selby-Bigge, 2nd ed. revised by P.H. Nidditch. Oxford: Clarendon Press, 1978. (Originally published 1739–40).

Foot, Philippa. *Natural Goodness*. Oxford University Press, 2001.

Popper, Karl R. *The Logic of Scientific Discovery*. Hutchinson & Co., 1959. (Originally published as *Logik der Forschung*, 1934).

---

## Glossary

To ensure precision in this extrapolation of Quinean holism, several core terms warrant formal definition, grounded in the model's emergent and pragmatic architecture.

- **Network of Predicates**: The structural unit of shared knowledge, emerging bottom-up from overlapping individual webs; it comprises propositional claims (e.g., '...is wrong' or '...boils at 100°C') that cohere functionally for navigating reality.
- **Pragmatic Pushback (or Systemic Friction)**: The immediate, experiential feedback from reality when a predicate misaligns with constraints, manifesting as individual dysfunction (e.g., suffering, inefficiency) that aggregates into systemic signals like dissent or resource depletion. This is not a top-down judgment but a bottom-up, causal consequence of network-reality mismatch.
- **Drive to Endure**: The minimal, constitutive precondition for any system's persistence, understood biologically as the transmission of informational structure (genetic, cultural, or epistemic) over time. It encompasses both genetic survival and memetic propagation, functioning not as a chosen value but as the transcendental filter imposed by reality—any inquiry presupposes it, as non-enduring systems vanish from the record.
- **Truth**: A functional label for a predicate's robust coherence within a specified network. This yields 'contextual truth' relative to any viable network, but in objective discourse, 'truth' implicitly refers to coherence with the Apex Network—the cumulative, historically resilient set of predicates that best approximates procedural objectivity.