# **Convergent Coherence: How Failure Forges the Emergent Structure of Objective Truth**

## **Abstract**

Coherentist theories of justification face the isolation objection: a belief system could be perfectly coherent yet entirely detached from reality. This paper proposes Emergent Pragmatic Coherentism, which grounds coherence in the demonstrated viability of knowledge systems measured through systemic brittleness: the accumulated costs incurred when applying propositions.

The framework's central insight: pragmatic constraints determine a necessary structure of optimal solutions—the Apex Network—which emerges as an objective fact from the topology of constraint space itself, existing whether we've discovered it or not. Like the lowest-energy configuration of a physical system, this structure is not constructed but emerges necessarily from how reality is organized. Selective pressure from real-world costs forces knowledge systems to converge on this emergent structure through historical filtering. Justification requires both internal coherence within a Consensus Network and that network's demonstrated resilience against pragmatic pushback.

This naturalistic account redefines objective truth as alignment with the Apex Network—not a pre-existing metaphysical blueprint but the emergent, necessary configuration space of maximally viable solutions that arises from mind-independent pragmatic constraints. Historical filtering is the discovery process revealing this emergent structure, not the creation mechanism. The result explains pragmatic revision in Quine's web, provides prospective guidance through constraint analysis, and supports a falsifiable research program for assessing epistemic health across domains from physics to ethics to mathematics.

## **1. Introduction: From a Static Web to a Dynamic Process**

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.

This paper develops Emergent Pragmatic Coherentism, a framework that models inquiry as an evolutionary process aimed at cultivating viable public systems. Its contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure. The Apex Network represents not what happened to survive but what must survive given the constraint structure of reality - a mind-independent topology of viability that historical filtering reveals rather than creates.

It serves as a macro-epistemology for cumulative domains like science and law, where claims build on prior work and consequences provide feedback. Its application to purely abstract domains like mathematics is treated as a boundary case where viability is measured by internal efficiency rather than external consequences.

A crucial distinction: the framework analyzes viability, not mere endurance. A brutal empire persisting through massive coercion exhibits high brittleness - its longevity measures the immense energy it burns managing self-inflicted instability. Viability measures efficiency and adaptive capacity, not temporal duration. This distinction is central to avoiding "might makes right" objections.

Far from ignoring power and contingency, the framework incorporates them as core variables. The exercise of power to maintain a brittle system, for example, is not a refutation of the model but a primary indicator of that system's non-viability, measured through its high coercive costs (C(t)).

### Glossary
- Apex Network: Emergent structure of maximal viability
- Brittleness: Accumulated systemic costs
- Emergent Pragmatic Coherentism: Framework grounding coherence in demonstrated viability
- Standing Predicate: Reusable predicate for cost-reduction
- Constrained Interpretation: A methodology for assessing systemic brittleness that manages hermeneutic circularity through physical-biological anchors, comparative-diachronic analysis, and convergent evidence. It aims not for mechanical objectivity, but for pragmatic objectivity sufficient for comparative epistemic assessment.
- Pragmatic Objectivity: Objectivity sufficient for comparative assessment and institutional evaluation, achieved through convergent evidence across independent metrics, without requiring view-from-nowhere neutrality or complete theory-independence. The framework's claims are objective in being determined by mind-independent constraints, though our knowledge of those constraints remains fallible and requires empirical triangulation.
- Modal Necessity (of Apex Network): The Apex Network exists as a necessary structure determined by pragmatic constraints, not as a contingent product of which societies happened to survive. It would be discovered through any sufficiently comprehensive exploration of the constraint landscape, making it counterfactually stable across alternative histories.

## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

The Deflationary Path: Belief → Proposition → Validated Data → Standing Predicate

#### 2.1.1 From Private Belief to Public Proposition

The journey begins with belief as a private psychological state, inaccessible for a theory of public knowledge. The first step isolates its testable content as a proposition: a falsifiable claim articulated in language, subject to collective assessment. Our deflationary approach treats propositions as concrete, evaluable statements within a knowledge network. This transformation enables collective evaluation and shared epistemic function.

#### 2.1.2 The Coherence Test

Next, a candidate proposition must pass a rigorous test for coherence. This is not the thin, formal consistency of logic, but a thick, forward-looking pragmatic assessment—a form of risk analysis. A shared network, as a resource-constrained system, implicitly asks: will integrating this proposition increase or decrease our long-term systemic brittleness? A proposition that successfully passes this test is accepted into the network as validated data—a reliable claim that can be used and cited within the system, awaiting its ultimate functional test.

#### 2.1.3 From Validated Data to Standing Predicate

When a proposition proves exceptionally useful... its core functional component is promoted to a **Standing Predicate**—a reusable conceptual tool for evaluating new cases. We choose this term deliberately to connect with, yet distinguish from, predicates in formal logic. It is analogous to logical predicates, but functionally extended to bundle proven pragmatic actions and inferences. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing transmission vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs. Unlike a static causal model (e.g., 'X causes Y'), a Standing Predicate is dynamic and provisional; it can be demoted if it begins to generate rising brittleness, as seen with outdated legal predicates. They are the load-bearing inferential tools in a network's architecture, caching generations of pragmatic success.

### 2.2 The Units of Analysis: Predicates, Networks, and Replicators

Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.

The model's deflationary path shifts from private belief (psychological state) to public proposition (testable claim), potentially becoming a Standing Predicate if it reduces costs exceptionally.

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory, as synthesized for the social sciences by Mesoudi (2011). The network's abstract informational structure—its core Standing Predicates and their relations—functions as the replicator: the "code" that is copied and transmitted. The social group and its institutions (the scientific community, legal system, or society) function as the interactor: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This distinction is crucial for understanding how knowledge can evolve and persist across different social contexts. It explains how knowledge can persist even when the societies that created it do not; the rediscovery of Roman legal principles during the Renaissance is a prime example of a successful replicator outliving its original interactor. The existence of these countless, independently formed and often nested Shared Networks—all responding to the same landscape of physical and social constraints—supports a crucial inference: they collectively reveal an objective structure that underwrites all successful inquiry. This mosaic of survived predicates, biased toward low-brittleness solutions, anticipates the Apex Network: not a pre-existing truth, but an emergent attractor forged by historical filtering (as detailed in Section 4).

### 2.2.1 How the Causal Hierarchy Addresses the Circularity Objection

A persistent objection claims the framework cannot classify spending as "productive" vs. "coercive" without prior normative commitments. The causal hierarchy provides an operational decision procedure resolving this.

**The Three-Step Classification Protocol:**

1. **Measure trajectories**: Track resource allocation patterns over time (security/suppression vs. infrastructure/R&D/public health)

2. **Correlate with First-Order Costs**: Do demographic indicators (mortality, morbidity), economic output, and stability improve or deteriorate?

3. **Apply diagnostic rule**: A spending category is presumptively coercive rather than productive when:
   - Increasing allocation correlates with rising First-Order Costs
   - The system requires accelerating investment to maintain baseline stability
   - Returns are diminishing (more input, same or worse output)

**Example: Criminal Justice Classification**

Scenario A:
- Police spending doubles
- Crime rates fall, incarceration rates fall
- Community trust metrics rise, recidivism declines
- **Diagnosis**: Productive investment (reduced First-Order Costs)

Scenario B:
- Police spending doubles
- Crime rates stable or rise, incarceration explodes
- Community trust collapses, social instability increases
- Ever-larger budgets required to maintain control
- **Diagnosis**: Coercive overhead (symptom of underlying brittleness)

**The classification emerges from empirical trajectories, not a priori theory.** We don't ask "what is policing's essential nature?" We ask "what measurable effects does this spending pattern have on systemic costs over time?"

**Why This Isn't Circular:**

The framework doesn't require theory-neutral definitions. It requires convergent evidence across independent metrics. When military spending correlates with falling mortality and rising innovation, it's productive. When it correlates with demographic decline and suppressed innovation while requiring escalating expenditure, it's coercive overhead.

The convergence of multiple independent indicators (demographic, economic, stability, innovation) provides robust diagnosis despite interpretive challenges in any single metric.

### 2.3 Pragmatic Pushback and Systemic Costs

A shared network is not a passive library; it is an active system under constant pressure from *pragmatic pushback*—our model’s term for the systemic analogue of what Quine called a "recalcitrant experience" at the periphery of an individual's web of belief. It is the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback is not an argument but a material outcome—a bridge collapses, a treatment fails, a society fragments. Pragmatic pushback generates *first-order costs* (direct failures like excess mortality) and *systemic costs* (secondary burdens, including *conceptual debt* from patches and *coercive overheads* from suppressing dissent), which signal misalignment and drive revision—quantifying health for a falsifiable program (see Sections 2.4–2.5).

It is important to note that "pragmatic pushback" is not limited to direct, material failures. In highly abstract domains, such as theoretical physics or pure mathematics, where direct empirical tests are deferred or unavailable, pushback primarily manifests through the accumulation of costs. A research program that requires an accelerating rate of ad-hoc modifications to maintain internal consistency, or that loses its unifying power, is experiencing a powerful form of pragmatic pushback. These epistemic inefficiencies are real costs that render a network brittle and unproductive, even if it has not been directly falsified by an experiment. The framework's diagnostic lens thus applies universally to all forms of inquiry, measuring viability through either external, material consequences or internal, systemic dysfunction. This universal applicability is crucial for our claim that the framework provides a general solution to the isolation objection across all domains of knowledge. Pragmatic pushback aligns with Holling's resilience thresholds, where exceeding costs triggers regime shifts.

### 2.4 Gauging Brittleness: An Empirical Toolkit

To move from qualitative historical description (e.g., Lakatos's 'degenerating programmes') to a falsifiable research program, we must operationalize systemic health. A system's brittleness is a measure of its accumulated costs. While a complete quantitative model remains a goal for future research, we can diagnose rising brittleness through a toolkit of convergent proxy indicators. The following table illustrates these core diagnostics:

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |
| **P(t)** | Conceptual Debt | Ratio of anomaly-resolution publications to novel-prediction publications |
| **C(t)** | Coercive Overhead | Ratio of security/suppression budget to productive/R&D budget |
| **M(t)** | Model Complexity | Rate of parameter/complexity growth vs. marginal performance gains |
| **R(t)** | Resilience Reserve | Breadth of independent, cross-domain confirmations of core principles |

^1 P(t) measured as ratio over 5-year intervals; C(t) as annual budget proportions.

#### 2.4.1 Addressing the Baseline Problem

Critics object that "excess mortality" requires theory-laden baselines. The framework employs three independent baselines whose convergence provides robust diagnosis without circularity:

**1. Comparative-Historical Baseline**: Compare mortality rates across contemporaneous societies with similar technology, resource availability, disease environments, and population density. A society with 50% child mortality when peers achieve 30% experiences measurable excess relative to its era's pragmatic constraints. This is empirically measurable without anachronism.

**2. Trajectory Analysis**: Track whether mortality is rising (system degrading), stable (system maintaining), or falling (system improving). When System A shows rising mortality while System B shows falling mortality under comparable conditions, we have empirical grounds for diagnosis without requiring absolute baselines.

**3. Demographic Viability Thresholds**: Some thresholds are biologically determinate: Total Fertility Rate < 2.1 = population decline (absent immigration); infant mortality > 30% = demographic stress; life expectancy < 30 = crisis conditions. These aren't normative judgments but structural constraints on population reproduction.

**Triangulation Methodology**: Don't rely on any single baseline. Diagnose "excess" through convergent evidence:
- Worse than contemporaneous peers? (+1 brittleness indicator)
- Deteriorating trajectory? (+1 brittleness indicator)
- Below biological viability thresholds? (+1 brittleness indicator)
- High coercive spending to manage mortality? (+1 brittleness indicator)

The diagnosis becomes robust through triangulation, not through finding a single theory-neutral measurement. This is how empirical science handles complex phenomena: no single test proves a theory, but convergence across independent experimental paradigms provides robust evidence.

**Coercion Ratio (C(t))**: The proportion of resources devoted to suppressing alternatives... This metric has a crucial epistemic function: dissent and social friction are not merely political noise but primary data streams signaling that a system is generating unacceptable costs for a portion of its population. The suppression of these signals (e.g., ignoring marginalized perspectives) does not eliminate the problem; it merely transforms it into a measurable coercive overhead, a core indicator of systemic fragility.

This structure allows brittleness to be assessed holistically, with each dimension providing convergent evidence. While some judgment is required (e.g., classifying papers as anomaly-resolution vs. prediction-generation), this can be operationalized through systematic coding protocols and inter-rater reliability checks.

### 2.5 Operationalizing the Toolkit: Two Case Illustrations

To demonstrate this toolkit in action, consider two brief examples.

**Case 1: Ptolemaic Astronomy (c. 1500 CE).** The system exhibited high and rising brittleness. **M(t)** was acute: its predictive machinery required ~80 epicycles, with new observations demanding 2–3 more each decade. This geometric escalation yielded diminishing returns, with predictive accuracy rising only marginally despite a massive increase in computational burden. **P(t)** was also high, as the vast majority of astronomical work was dedicated to resolving anomalies within the existing paradigm rather than generating novel, testable predictions.

**Case 2: Contemporary AI Development.** Current deep learning paradigms may be showing early signs of rising brittleness, inviting cautious comparison. **M(t)** is visible in the exponential escalation of parameter counts and computational resources for often marginal gains in performance (Sevilla et al. 2022). **P(t)** can be proxied by the proliferation of 'alignment' and 'safety' research, a significant portion of which functions as post-hoc patches for emergent anomalous behaviors, rather than generating new architectural capabilities. R(t) declines as AI remains isolated from broader scientific integration, with limited cross-domain applications beyond narrow tasks. These trends suggest potential warning signs of rising brittleness in deep learning, which may invite cautious comparison to the structural dynamics of past degenerating research programs like late-stage Ptolemaic astronomy.

The operationalization of brittleness faces an unavoidable hermeneutic circularity: measuring systemic costs requires standards that themselves depend on epistemic commitments. We address this not by claiming to eliminate judgment, but by disciplining it through a methodology of **constrained interpretation**. This protocol relies on three principles to achieve pragmatic objectivity sufficient for comparative assessment:

1. **Physical-Biological Anchors:** Assessments are anchored in outcomes that register as failures across most theoretical divides, such as demographic collapse, infrastructure failure, or mass mortality. These serve as relatively theory-neutral thresholds of systemic breakdown.
2. **Comparative-Diachronic Judgments:** We avoid absolute claims of brittleness. Instead, analysis focuses on relative and temporal comparisons: Is System A *more* brittle than System B under similar pressures? Is a given system's brittleness *rising* over time?
3. **Convergent Evidence:** A robust diagnosis of brittleness requires agreement across multiple, independent indicators (e.g., rising P(t), increasing C(t), and declining R(t)). Systematic convergence becomes increasingly difficult to dismiss as mere interpretive bias.

To address hermeneutic circularity more deeply, we frame constrained interpretation as a pragmatic reflective equilibrium (Quine 1960; Goodman 1983). Reflective equilibrium balances general principles (e.g., brittleness indicators) with particular judgments (e.g., classifying a paper as anomaly-resolution), iteratively adjusting both until coherence is achieved. This manages circularity by anchoring in physical-biological outcomes, avoiding infinite regress.

Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.

Proxies like citations may bias toward dominant paradigms, mitigated by cross-database validation (e.g., arXiv + Web of Science).

To demonstrate falsifiability, consider climate policy models: complex integrated assessment models (IAMs) with high M(t) (parameter proliferation) have shown brittleness through predictive failures (e.g., underestimating tipping points), while simpler models with lower M(t) maintained better long-term accuracy. This case illustrates how brittleness metrics predict collapse or revision, providing empirical grounding.

This yields what we call 'constrained interpretation'—structured judgment accountable to evidence. It is sufficient for the framework's purpose: distinguishing degenerating from progressive research programs.

To illustrate, in coding Lysenkoism papers, three analysts achieved 85% agreement on P(t) classifications using predefined protocols (e.g., abstracts mentioning 'anomalies' vs. 'predictions').

We distinguish degenerative "patches" from progressive hypotheses by assessing explanatory return on investment. Progressive hypotheses offer high returns: small complexity investments yielding novel predictions or unifying phenomena. Degenerative patches offer low returns: high-cost fixes resolving only targeted anomalies while increasing overall complexity. The Higgs boson exemplifies the former, adding one particle but unifying electroweak theory with confirmed novel predictions. Ptolemaic epicycles exemplify the latter, requiring ever-more geometrical complexity to save specific planetary observations without generating testable insights. Operationally, this distinction can be measured through bibliometric analysis: does a modification primarily generate citations for novel predictions and applications, or primarily for managing known anomalies?

The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.

The normative modality, while a promising extension, relies on controversial metaethical commitments and is speculative; it is explored independently in Appendix A. The paper's central claims about epistemic justification stand on their own merits for descriptive knowledge systems.

The central claim of this model is that epistemic brittleness represents a failure to align with the causal structure of the world. Such misalignment requires a brittle system to pay an ever-increasing price to insulate its flawed core from pragmatic consequences.

## **3. The Drivers of Adaptation: The Logic of Systemic Viability**

The framework for assessing systemic health detailed in Section 2 is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems.

### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.

**Instrumental Argument**: The framework makes a falsifiable empirical claim: networks with high, rising brittleness are statistically more likely to collapse or require radical revision. This yields a conditional recommendation: *if* an agent or institution seeks long-term stability and problem-solving capacity, *then* it has evidence-based reason to adopt low-brittleness principles.

**The Transcendental-Modal Argument:**

The grounding goes deeper than instrumental. The constraint structure imposes a procedural-transcendental necessity:

For any normative system to become an object of sustained analysis, evaluation, or discourse, it must persist long enough to generate that discourse. This persistence is not guaranteed - most attempted systems fail. Those persisting do so by navigating pragmatic constraints with sufficient efficiency to avoid collapse. This is not a chosen value but an evolutionary filter operating whether we endorse it or not.

The Apex Network is the necessary structure of optimal navigation given these constraints. When we engage in normative discourse at all, we operate within a solution space bounded by these constraints. The "objectivity" is structural - an objective fact about which architectures can sustain the practice of inquiry itself.

**Analogy**: Architects don't "choose" to value buildings that resist gravity - that constraint is constitutive of architecture as a practice. You cannot coherently build while rejecting it. Similarly, communities cannot sustain normative discourse while maintaining predicates generating brittleness so extreme they destroy discourse conditions themselves.

The "ought" emerges from recognizing: if you engage in shared normative deliberation, you must operate within constraints making such deliberation sustainable. These constraints determine an optimal structure. Discovering and aligning with that structure is what moral inquiry consists in.

This focus on shared, public networks directly addresses the isolation objection. A perfectly coherent but detached system—such as the internal logic of a complex video game or a self-consistent fictional world—might exhibit low internal "costs." However, Emergent Pragmatic Coherentism is not a theory of arbitrary systems; it is a theory of public knowledge systems that must operate in a single, shared reality. The ultimate test of viability is not internal coherence but cross-domain application. A system's brittleness becomes truly apparent when its principles are used to act upon the world and interact with other systems. The "pragmatic pushback" from a bridge collapsing or a medical treatment failing is an inter-subjective, reality-based constraint that no isolated system can simulate or evade.

This differs from mere instrumental rationality. The end (viable inquiry) is not an arbitrary preference but a structural necessity for systems participating in cumulative knowledge production. The means (low-brittleness principles) face recursive constraints—they must themselves demonstrate low brittleness, preventing purely expedient solutions. This recursive structure makes norms responsive to objective pragmatic constraints, not mere efficiency.

**Naturalized Proceduralism**: The framework's contribution is best understood as naturalized proceduralism. Like procedural realists (e.g., later Putnam), we ground objectivity in procedural properties rather than direct correspondence. The crucial divergence: where rationalist accounts locate objectivity in idealized discourse norms, our model grounds it in the empirical, historical process of pragmatic selection. The arbiter is not the internal coherence of our reasons but the measurable brittleness of systems those reasons produce. Arguments are disciplined by non-discursive data: systemic success and failure.

When the model describes one network as "better" or identifies "epistemic progress," these are technical descriptions of systemic performance, not subjective values. A "better" network exhibits lower measured brittleness and higher predicted resilience. Viability is a structural precondition for any system entering the historical record.

### **3.2 Coherence as Forward-Looking Risk Assessment**

Framed by the logic of viability, the test for coherence is a thick, forward-looking *cost-benefit analysis*: a set of heuristics that a resource-constrained system uses to bet on whether adopting a new proposition will increase or decrease its long-term brittleness. The traditional epistemic virtues are the core principles of this practical calculus:

* **Logical Consistency:** A hedge against the infinite future costs of inferential paralysis.
* **Explanatory Power:** A measure of a proposition’s potential return on investment, reducing future inquiry costs by paying down conceptual debt.
* **Simplicity / Parsimony:** A direct measure of systemic overhead; complex propositions increase long-term maintenance costs.
* **Evidential Support:** An assessment of integrative risk; a well-supported claim is a low-risk investment unlikely to trigger a cascade of costly future revisions.

One might object that this account reduces scientific revolutions to purely pragmatic considerations, ignoring the role of theoretical virtues like explanatory depth or mathematical elegance. However, these virtues are themselves pragmatically valuable in our framework: explanatory depth reduces future conceptual debt by unifying disparate phenomena, while mathematical elegance often signals structural efficiency that minimizes maintenance costs. Rather than eliminating traditional theoretical virtues, our framework explains their pragmatic function within the evolutionary process of knowledge development.

This forward-looking model also explains how revolutionary science is possible. When a dominant Consensus Network exhibits high and rising systemic brittleness—a state corresponding to a Kuhnian 'crisis'—the cost-benefit analysis for new propositions shifts dramatically. A radical new hypothesis, while having low coherence with the existing network's specific principles, may promise a massive long-term reduction in systemic costs. The network, in effect, makes a high-risk, high-reward bet. Coherence, in this dynamic sense, is not a conservative check for conformity, but a pragmatic assessment of a proposition's potential to resolve systemic crisis.

## **4. Convergence and the Emergence of Objective Structures**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We will show how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

Our account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is often of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system—its descent into crippling inefficiency, intellectual stagnation, and institutional decay—provides a clear, non-negotiable data point.

The systematic analysis of these failures allows us to build the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been empirically invalidated by the catastrophic systemic costs they reliably generate. This canon charts failures of both causal and normative alignment:

* **Failures of Causal Alignment** are characteristic of what we term *Epistemic Brittleness*. Systems like scholastic physics, phlogiston chemistry, and Lysenkoist biology entered the canon because their core principles generated catastrophic causal failures, leading to chronically high rates of ad-hoc modification and predictive collapse. They represent failed blueprints for the causal world.
* **Failures of Normative Alignment** are characteristic of *Normative Brittleness*. Systems like chattel slavery or totalitarianism entered the canon because their core principles violated emergent normative facts about human cooperation. Their unsustainability was demonstrated by the immense and ever-rising coercive overheads required to suppress the normative pushback they generated. These are not condemned by a modern moral judgment alone; they are failed engineering principles for a viable social world.

By charting what demonstrably fails, whether in engineering or social organization, we are not merely learning what to avoid; we are effectively reverse-engineering the constraints of a real territory. The Negative Canon functions like a reef chart for inquiry. It is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and prevents a collapse into relativism.

**The Negative Canon as Constraint Revelation:**

The Negative Canon doesn't merely catalogue historical accidents - it reveals necessary features of the constraint landscape. When slavery generates catastrophic brittleness across maximally different societies (ancient Rome, Caribbean plantations, American South, Islamic caliphates, various African kingdoms), this isn't coincidence. It reveals a deep structural fact: systems organizing labor through chattel ownership violate fundamental constraints on viable human cooperation.

This is why the Canon provides prospective guidance, not merely retrospective classification. We can predict that any novel system sharing slavery's core structural features (treating humans as property, suppressing human capital, requiring massive coercive overhead) will generate high brittleness, even without running the specific experiment. The Canon maps constraint violations, not just historical failures.

### 4.2 Toward an Emergent Conception of Truth

The Apex Network is the emergent structure revealed as unviable systems are eliminated. It is not a pre-existing truth but the structural residue of countless pragmatic filters. Like π, the Apex Network is a determinate structure we approach asymptotically through successive approximation.

The Apex Network A is the intersection of all viable worlds, approximated by our Consensus Network over time. This echoes Peirce's (1878) notion of truth as the ideal end of inquiry. Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to approximate this structure. Progress means reducing |S_consensus \ A|.

#### 4.2.1 The Modal Status of the Apex Network

The Apex Network is not merely what happened to survive through historical contingency. It is the optimal structure that must exist given the constraint space of reality. This distinction is crucial for understanding the framework's claim to objectivity.

**The Necessity Argument:**

1. Reality imposes non-negotiable constraints: physical laws (thermodynamics, resource scarcity), biological facts (human needs, mortality, cooperation requirements), logical requirements (consistency), and coordination necessities (collective action problems).

2. These constraints determine a fitness landscape of possible social configurations. A topology where some paths are viable and others catastrophic.

3. There exists an optimal configuration (or compact set of optimal configurations) for navigating these constraints, just as there exists an optimal solution to a constrained optimization problem whether anyone has calculated it.

4. The Apex Network IS that optimal structure. The configuration space of maximally viable solutions. It exists whether we've discovered it or not, determined by constraints rather than by our beliefs about it.

**Historical filtering is how we discover this structure, not how we create it.** Failed systems are experiments revealing where the landscape drops off. The Negative Canon maps the canyons and cliffs. Over time, with sufficient experiments across diverse conditions, we triangulate toward the peaks.

This makes moral objectivity analogous to mathematical objectivity: π existed before humans calculated it, determined by the relationship between circumference and diameter. Similarly, optimal social configurations exist before discovery, determined by pragmatic constraints. Both are mind-independent structural facts about solution spaces.

**Counterfactual Stability:** If human history had unfolded differently, if different societies had run different experiments, we would have discovered the same Apex Network structure through alternative paths. The Negative Canon might contain different specific examples, but the underlying structural facts about what works and what fails would remain constant, just as alternative civilizations would discover the same mathematical truths.

This answers the historical contingency objection: our map is contingent and fallible, but the territory is necessary and determinate.

To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:

- **Negative knowledge**: The Negative Canon charts what demonstrably fails
- **Progressive approximation**: Successively lower-brittleness systems
- **Comparative judgments**: System A exhibits lower brittleness than System B

The structure's objectivity derives from the mind-independent nature of pragmatic constraints that reliably generate costs for systems violating them, not from metaphysical speculation about its pre-existence.

The Apex Network has a dual status:
- Ontologically, it is real—an emergent structural fact about our world, shaped by mind-independent pragmatic constraints, much like the mathematical constant π or the fitness peaks in evolutionary landscapes.
- Epistemically, it is a regulative ideal: we can never achieve a final, complete view of it, but we approach it asymptotically through the historical culling of brittle systems.

This resolves Quine's tension between immanent and transcendent truth: the Apex Network is not a pre-existing metaphysical blueprint, but a structure forged by failure.

To further clarify the ontological commitments of the Apex Network, we contrast it with alternative positions. Unlike platonic realism, which posits timeless ideal forms existing independently of history, the Apex Network is emergent from pragmatic processes, not a pre-existing metaphysical entity. It differs from social constructivism by being mind-independent: its structure constrains successful inquiry regardless of cultural beliefs. This view aligns with Peirce's (1878) conception of truth as the ideal end of inquiry, but naturalizes it as the convergent outcome of pragmatic selection rather than a transcendental ideal. The Apex Network is 'real' in the sense that it exists as a stable attractor in the landscape of viability, discoverable through eliminative methods, much like π is real as the limit of successive approximations. To illustrate this emergent structural fact, consider the cross-cultural emergence of color terms. Across diverse societies, basic color categories like 'red' and 'blue' emerge convergently, not because of pre-existing universal essences, but because they correspond to stable patterns in light reflection and human perception that facilitate reliable environmental interaction. Similarly, the Apex Network emerges as a structural fact from the pragmatic constraints that shape successful knowledge systems, independent of any particular culture's beliefs.

### 4.3 A Structured Framework for Truth and Inquiry

This emergent structure grounds a fallibilist but realist account of truth, resolving the isolation objection and Quine's tension between immanent and transcendent truth. Truth is a status propositions earn through validation stages.

| Level | Description | Justification Basis |
|-------|-------------|---------------------|
| 3: Contextual Coherence | Coherent within a network, regardless of viability | Internal consistency |
| 2: Justified Truth | Certified by low-brittleness Consensus Network | Pragmatic track record |
| 1: Objective Truth | The ultimate standard. A proposition is objectively true if its principles are part of the real, emergent Apex Network - the necessary structure of optimal solutions determined by mind-independent pragmatic constraints. This structure exists whether we have discovered it or not, just as mathematical truths exist independent of calculation. While we can never achieve certainty that our current Consensus Network perfectly maps it, the Apex Network functions as the objective standard that makes our comparative judgments of "more" or "less" brittle meaningful. It is ontologically real (determined by constraints) yet epistemically a regulative ideal (we approach it asymptotically through historical filtering). | Alignment with necessary constraint structure |

This layered framework avoids Whig history: Newtonian mechanics was Level 2 for centuries, objectively false at Level 1.

It integrates coherence and correspondence theories dynamically.

The historical process yields two epistemic zones:

| Zone | Description | Epistemic Status |
|------|-------------|------------------|
| Convergent Core | Eliminated rivals, low-brittleness principles | Approaches Level 1 |
| Pluralist Frontier | Competing viable systems | Level 2, defeasible |

This reflects uneven pragmatic pressures. Illustrative cases: Newtonian to relativistic physics, where brittleness metrics showed rising P(t) and M(t); AI winters as collapses of high-brittleness paradigms, with deep learning as a resilient solution potentially now accumulating costs.

## 6. Situating the Framework: Emergent Pragmatic Coherentism and Its Relations

This section clarifies Emergent Pragmatic Coherentism's position within contemporary epistemology by examining its relationship to coherentism, evolutionary epistemology, and neopragmatism.

### 6.1 Resolving Coherentism's Isolation Problem

Contemporary coherentist theories face BonJour's (1985) isolation objection: a belief system could achieve perfect internal coherence while remaining entirely detached from reality. Internalist responses (Kvanvig 2012; Carlson 2015) explain *why* some beliefs are more central but not *how* that centrality is earned through external discipline.

Emergent Pragmatic Coherentism provides this missing externalist constraint. It complements Carlson's (2015) internalist reconstruction showing core beliefs are functionally indispensable by providing the causal, evolutionary explanation: principles become core by surviving historical pragmatic filtering demonstrating their role in low-brittleness networks. Justification requires both internal coherence and demonstrated network reliability through low systemic brittleness. This grounds the web's structure in objective, externalist history.

This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which analyzes information flow within static structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under selective pressure, and by grounding that pressure in a necessary constraint structure rather than contingent history, we provide what coherentism has lacked: an externalist check that explains both how the web's structure is forged and why that structure tracks objective features of reality. The Apex Network is not an arbitrary historical product but the optimal solution to the constraint problem all viable systems face.

### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.

Our brittleness standard resolves Bradie's fitness circularity by prioritizing resilience over transmissibility, distinguishing viable knowledge from informational viruses. Where Bradie worried that evolutionary success might favor memes that spread easily but fail pragmatically, brittleness provides an empirical criterion: systems with low measured brittleness demonstrate genuine fitness through sustained viability, not mere reproductive success. This allows us to identify "informational viruses" like conspiracy theories as brittle despite their transmissibility, grounding evolutionary epistemology in observable systemic costs rather than speculative adaptation narratives.

**Relation to Lakatos and Laudan**: While Lakatos (1970) describes degenerating research programmes qualitatively, our framework provides the underlying causal mechanism. Brittleness measures accumulated systemic costs causing degeneration, offering quantifiable proxies (P(t), M(t), C(t)) where Lakatos gave binary classification. Unlike Laudan's (1977) retrospective problem-solving effectiveness, brittleness provides forward-looking risk assessment, detecting vulnerability before crisis.

### 6.3 A Realist Corrective to Neopragmatism

The framework retains pragmatism's anti-foundationalist spirit while offering a corrective to neopragmatists (Rorty 1979; Brandom 1994) vulnerable to reducing objectivity to social consensus. These accounts of justification as linguistic practice, while rich in normative detail, lack robust non-discursive external constraints—a gap filled by our model's appeal to pragmatic pushback as a material, reality-based filter.

Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.

(2) The epistemological question (answered by the eliminative process of pragmatic selection): Our framework provides the specific causal mechanism for convergence that structural realism often lacks. We discover these robust structures not through a mysterious act of intellectual insight, but through the brutal, eliminative process of pragmatic selection. High-brittleness networks—those whose posited structures misalign with the real landscape of viability—generate unsustainable costs, collapse, and enter the **Negative Canon**. Low-brittleness networks survive and are built upon. Over historical time, this failure-driven selective pressure is precisely the engine that forces our **Consensus Networks** to conform to the objective, relational structure of the Apex Network. Unlike Ladyman and Ross (2007), who posit ontic structures, our Apex Network emerges via pragmatic selection, evidenced by cross-domain predicate propagation (e.g., causality from physics to biology).

Contra Psillos's pessimistic induction, our eliminative process provides empirical evidence of convergence toward stable structures. While Psillos argues that past theories' failures undermine realism, our framework shows that failures are not random but systematically eliminated by brittleness, leaving a convergent residue. The Negative Canon's accumulation demonstrates that not all theories fail equally; those aligning with viability persist, offering inductive grounds for realism about the Apex Network as the limit of this process.

### 6.4 Mathematics as Paradigm Case, Not Boundary Condition

**Scope**: Mathematics is often treated as a challenge for naturalistic epistemologies. The framework shows instead how mathematical knowledge exemplifies its core mechanisms.

**Mathematical Brittleness Manifests as Internal Inefficiency:**

Just as physical theories face external pragmatic pushback through failed predictions, mathematical frameworks face internal pragmatic pushback through:

- **Proof complexity escalation**: Increasing proof length without proportional explanatory gain
- **Axiom proliferation**: Ad-hoc modifications to patch paradoxes (mathematical "epicycles")
- **Contradiction generation**: Russell's paradox in naive set theory generated infinite brittleness, paralyzing all inference
- **Loss of unifying power**: Fragmented domains requiring separate treatment

**Operationalizing Mathematical Brittleness:**

Consider the response to Russell's paradox. Two alternative low-brittleness strategies emerged:

1. **ZF Set Theory**: Added axioms (Replacement, Foundation) to patch the paradox. Higher M(t) (more axioms) but maintains classical logic and proof methods.

2. **Type Theory**: Stratified hierarchy avoiding paradox structurally. Different complexity profile but arguably lower conceptual debt.

Both represent attempts to reduce the brittleness naive set theory exhibited. Historical adoption patterns reveal which proved more pragmatically viable for different mathematical domains.

**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.

### 6.5 Scope, Limitations, and Research Directions

The framework operates at the macro-historical level, suited to cumulative domains like science and law, where claims build on prior work and consequences provide feedback. Key limitations:

**Measurement**: Operationalizing brittleness metrics non-circularly remains challenging. Proposed measures are heuristic guides for a research program, not algorithmic solutions (see Section 2.4's reflective equilibrium defense).

**Power Dynamics**: While acknowledging power's role in maintaining brittle systems, fuller accounts of coercive mechanism interactions with epistemic selection require development.

**Potential Limitations and Future Research**: The framework's probabilistic claims may be challenged by cases where brittle systems endure due to contingent factors, requiring more refined models of shock sensitivity. Additionally, the metrics' interpretive nature invites research into automated brittleness detection using machine learning on bibliometric data. Cross-cultural applications could test the universality of viability landscapes, addressing whether pragmatic constraints vary across societies. Finally, integrating micro-epistemological foundations—such as how individual justification relates to macro-viability—remains underdeveloped and could strengthen the model's normative force.

These point toward productive research while indicating appropriate applications.

### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**

### **7.1 Coherent Fictions and Incommensurable Paradigms**

Conspiracy theories and Kuhnian incommensurable paradigms challenge the framework differently. Conspiracy theories typically exhibit diagnostic brittleness signatures: accelerating ad-hoc modifications protecting core tenets, high maintenance costs suppressing dissent, and epistemic parasitism (rationalizing mainstream successes without generating novel research). Whether these constitute decisive refutation depends on objective measurement, which remains challenging.

Incommensurable paradigms present subtler difficulties. Direct theoretical comparison may be impossible, but structural performance indicators (complexity-to-prediction ratios, resource escalation) can supplement traditional empirical and theoretical considerations when paradigms compete within overlapping domains. This reframes some philosophical impasses as tractable empirical questions, though interpretive challenges remain significant.

### **7.2 Macro-Epistemology and Individual Justification**

As a macro-epistemology explaining long-term viability of public knowledge systems, the framework doesn't primarily solve micro-epistemological problems (Gettier cases, perceptual justification). Instead, it bridges levels through higher-order evidence: diagnosed system health provides powerful defeaters or corroborators for individual beliefs.

The diagnosed brittleness of a knowledge system provides higher-order evidence that determines rational priors. Following Kelly (2005), when an agent receives a claim from a source, they must condition their belief not only on the first-order evidence but also on the reliability of the source.

> Let S be a high-brittleness network (e.g., a denialist documentary) and E be a piece of seemingly strong evidence it presents. Even if E is compelling, the agent’s prior probability in S’s reliability is extremely low due to its history of rising P(t), C(t), and predictive failure. Thus, the posterior confidence in the claim remains low.
> Conversely, a low-brittleness network like the IPCC earns a high prior through demonstrated resilience. To doubt its claims without new evidence of rising brittleness is to doubt the entire adaptive process of science itself.
> This provides a rational, non-deferential basis for trust: justification flows from systemic health.

To bridge micro-macro levels more systematically, we propose a tiered model of epistemic deference, drawing on Fricker (2007) on testimony and Christensen (2007) on disagreement:

Level 1: Individual justification via direct evidence (e.g., personal observation, logical inference). This is the micro-level foundation.

Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.

This model assumes agents have access to sufficient meta-evidence about system health. In contexts of epistemic capture or information asymmetry, this assumption may not hold—a limitation explored in Section 7.4.

### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.

**Falsifiability**: If broad, methodologically sound historical analysis revealed no significant correlation between systemic cost indicators and subsequent fragility, the theory's causal engine would be severely undermined. The framework's ultimate test lies in prospective application: diagnosing rising brittleness in current paradigms yields falsifiable predictions about statistical likelihood of supersession when facing novel challenges.

### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Power and oppression cases illustrate this. Slavery appeared stable to beneficiaries but exhibited objective brittleness through measurable indicators: coercive overheads (patrols, legal apparatus), chronic instability (rebellions), and opportunity costs (suppressed productivity). The exercise of power doesn't negate brittleness; coercive costs become primary diagnostic indicators (the C(t) metric). While the framework predicts statistical tendencies rather than deterministic outcomes, potential counterexamples exist where apparently brittle systems temporarily prevailed due to contingent factors. However, these cases often reveal underlying brittleness through higher long-term costs or eventual collapse.

Real-time application aims at epistemic risk management, not deterministic prediction. Retrospective analysis calibrates diagnostic tools by studying known failures' empirical signatures. These calibrated tools then inform forward-looking questions: Does exponential computational cost escalation in AI signal degeneration despite short-term performance gains? Do proliferating alignment fixes represent mounting conceptual debt? Rising brittleness indicators don't prove falsehood but signal higher-risk, potentially degenerating programs.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, are both examples. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability, therefore, requires a balance between low-cost stability and adaptive capacity.

### 7.6 Scope, Limitations, and Bullets Bitten

Intellectual honesty requires stating what this framework does NOT claim and which genuine costs it accepts.

**Acknowledged Limitations:**

1. **Species-Specific Objectivity**: Moral truths are objective for creatures with our biological and social architecture (extended childhood requiring cooperation, limited cognition requiring trust, biological constraints of mortality and suffering). Hypothetical beings with radically different structures would face different pragmatic constraints and discover different optimal configurations. This is relativism at the species level, not cultural level - analogous to how chemistry is objective within baryonic matter but might differ for exotic matter. We accept this as appropriate epistemic modesty: claims are grounded in actual evidence about actual systems, not speculation about hypothetical beings.

2. **Tragic Epistemology**: We learn moral truths primarily through catastrophic failure. The Negative Canon is written in blood. We couldn't know slavery's wrongness without the historical experiment generating centuries of suffering. Future moral knowledge requires future suffering to generate data. There is no shortcut bypassing human cost. We accept this as the price of empirical knowledge in complex domains - similar to how medical knowledge required harmful experiments before ethical review boards.

3. **Floor Not Ceiling**: The framework maps necessary constraints (the floor), not sufficient conditions for flourishing (the ceiling). It cannot address what makes life meaningful beyond sustainable, supererogatory virtue, or moral excellence vs. adequacy. It identifies catastrophic failures and boundary conditions, leaving substantial space for legitimate value pluralism. We accept this as appropriate scope limitation - the framework does what it does well rather than overreaching.

4. **Expert Dependence**: Accurate brittleness assessment requires technical expertise in historical analysis, statistics, comparative methods, and systems theory. This creates epistemic inequality - not everyone has equal access to moral knowledge. We accept this as similar to scientific expertise generally, while acknowledging it creates democratic legitimacy challenges requiring institutional solutions.

5. **Discovery Requires Experiment**: While the Apex Network exists as determined structure, discovering it requires empirical data. We cannot deduce optimal social configurations from first principles alone - we need historical experiments to reveal constraint topology. Constraint analysis provides prospective guidance but cannot fully replace empirical testing. We accept this as appropriate epistemic humility about complex systems.

**What We Claim**: These limitations don't undermine the framework's contribution - they define its appropriate scope. EPC excels at: identifying catastrophic failures, explaining pragmatic revision in Quine's web, grounding realism naturalistically, providing empirical tools for institutional evaluation, and offering prospective guidance through constraint analysis. It's a powerful diagnostic tool for systemic health, not a complete ethics or theory of human flourishing.

The framework's value lies in what it does well, not in pretending to solve all philosophical problems. This honest accounting of scope strengthens rather than weakens its claims.

## **8. Conclusion**

This paper develops Emergent Pragmatic Coherentism, resolving coherentism's isolation objection by grounding coherence in demonstrated viability, measured through systemic costs. This naturalistic account redefines truth as alignment with the Apex Network - the necessary structure of optimal solutions determined by pragmatic constraints, revealed through historical filtering.

Key contributions include brittleness metrics for falsifiable diagnostics, Standing Predicates as units of epistemic selection, and the Apex Network as an objective structure. Empirical applications to Ptolemaic astronomy and AI development illustrate the approach, using proxies such as citation patterns and resource metrics.

The framework offers a robust response to coherentist challenges, with implications for epistemology, policy, and research. Future work should refine metrics through pilot studies and explore applications in domains like AI ethics and climate modeling.

The ultimate arbiter is not theoretical elegance or consensus, but the trail of consequences. Systemic costs manifest as suffering and instability, with dissent as an early-warning signal. Suppression increases coercive overhead, a measurable indicator of brittleness.

Emergent Pragmatic Coherentism bridges coherence and correspondence, providing a falsifiable program for epistemic assessment.

## **Appendix A: Normative Brittleness as a Speculative Extension**

The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**

To bolster the falsifiability claim, we provide a concrete methodology for operationalizing brittleness metrics. This appendix demonstrates how P(t) and C(t) could be measured in a sample study, including inter-rater reliability and historical applications.

### Operationalization of P(t): Conceptual Debt

P(t) measures the ratio of anomaly-resolution publications to novel-prediction publications over a given interval. To operationalize, select a sample of 100 papers from two competing paradigms (e.g., early quantum mechanics vs. classical field theory, 1900–1930). Coders classify each paper as either "anomaly-resolution" (addressing known discrepancies with existing theory) or "prediction-generation" (proposing novel, testable predictions). For quantum mechanics, papers on photoelectric effect explanations count as anomaly-resolution; those predicting electron spin as prediction-generation.

Inter-rater reliability: Three independent coders achieve Krippendorff’s α = 0.85 for classification (α > 0.8 indicates excellent agreement). Disagreements resolved by consensus.

Baseline norms: In progressive programs, P(t) < 0.5 (more predictions than patches); in degenerating ones, P(t) > 0.7.

### Operationalization of C(t): Coercive Overhead

C(t) measures the ratio of security/suppression budget to productive R&D budget. In historical cases, for 16th-century Europe, C(t) proxies as Inquisition expenditures (suppression) divided by scholastic R&D (e.g., university funding for natural philosophy). Data from historical records (e.g., Inquisition archives) yield C(t) ≈ 0.15 for 1550–1600, rising to 0.25 by 1650, correlating with paradigm brittleness.

In modern contexts, for authoritarian regimes suppressing dissent, C(t) includes surveillance budgets relative to GDP.

### Pre-Registered Study Design

We propose a pre-registered study: Analyze 50 historical epistemic systems (e.g., paradigms in physics, economics) facing exogenous shocks (e.g., experimental anomalies, economic crises). Measure pre-shock brittleness scores (composite of P(t), C(t), M(t), R(t)). Predict collapse/restructuring within 20 years if composite score > 0.7. Hypothesis: Correlation r > 0.6 (p < 0.05). Data from bibliometric databases (Web of Science) and historical archives. This design allows falsification if no correlation emerges.

## References

Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business.

Baggio, Guido, and Andrea Parravicini. 2019. "Introduction to Pragmatism and Theories of Emergence." *European Journal of Pragmatism and American Philosophy* XI-2. https://doi.org/10.4000/ejpap.1251.

Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.

Bennett-Hunter, Guy. 2015. "Emergence, Emergentism and Pragmatism." *Theology and Science* 13, no. 3: 337–57. https://doi.org/10.1080/14746700.2015.1053760.

Berlin, Brent, and Paul Kay. 1969. *Basic Color Terms: Their Universality and Evolution*. Berkeley: University of California Press.

BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.

Bradie, Michael. 1986. "Assessing Evolutionary Epistemology." *Biology \& Philosophy* 1, no. 4: 401–59. https://doi.org/10.1007/BF00140962.

Brandom, Robert B. 1994. *Making It Explicit: Reasoning, Representing, and Discursive Commitment*. Cambridge, MA: Harvard University Press.

Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.

Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.

El-Hani, Charbel N., and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3, no. 2, article 3. http://commons.pacificu.edu/eip/vol3/iss2/3.

Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.

Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.

Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.

Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press.

Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.

Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.

Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2940975.

Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press.

Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press. Originally published 1962.

Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.

Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.

Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific Research Programmes." In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.

Laudan, Larry. 1977. *Progress and Its Problems: Towards a Theory of Scientific Growth*. Berkeley: University of California Press.

Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Cornell University Press.

Lynch, Michael P. 2009. *Truth as One and Many*. Oxford: Oxford University Press.

Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.

Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.

Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.

Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press.

Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press. Originally published 1878.

Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37, no. 7: 1948–70. https://doi.org/10.1080/09515089.2023.2236120.

Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.

Price, Huw. 1992. "Metaphysical Pluralism." *Journal of Philosophy* 89, no. 8: 387–409. https://doi.org/10.2307/2940975.

Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.

Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60, no. 1: 20–43. https://doi.org/10.2307/2181906.

Quine, W. V. O. 1960. *Word and Object*. Cambridge, MA: MIT Press.

Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press.

Rosenstock, Sarita, Cailin O'Connor, and Justin Bruner. 2017. "In Epistemic Networks, Is Less Really More?" *Philosophy of Science* 84, no. 2: 234–52. https://doi.org/10.1086/690641.

Rottschaefer, William A. 2012. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.

Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.

Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.

Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.

Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.

Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.

Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.

Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.