= The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth
<the-architecture-of-failure-how-systemic-brittleness-drives-convergent-coherence-to-forge-objective-truth>
== Abstract
<abstract>
Coherentist theories of justification face the isolation objection: a
perfectly coherent belief system could be detached from reality. While
Quinean naturalism grounds knowledge in holistic webs constrained by
empirical experience and pragmatic revision, the framework lacks
formalized diagnostics for assessing the structural health of knowledge
systems before failure occurs.

This paper develops Emergent Pragmatic Coherentism (EPC), building on
Quine's architecture to address this gap. This naturalistic externalist
framework grounds coherence in the long-term viability of public
knowledge systems. EPC introduces systemic brittleness as a prospective
diagnostic tool. It assesses epistemic health by tracking the observable
costs generated when a system's claims are applied. Selective pressure
from these costs drives disparate knowledge systems to converge on an
emergent, objective structure we term the Apex Network. The Apex is not
a final theory in hand. It is the constraint-determined structure shared
by maximally viable solutions, revealed through historical failure
filtering.

In this framework, justification requires both internal coherence and
the demonstrated resilience of the certifying network. Truth is analyzed
in three levels: from contextual coherence within any system, to
warranted assertion within a low-brittleness consensus network, to
objective truth as alignment with the Apex Network. EPC synthesizes
insights from Quinean holism, network epistemology, evolutionary
dynamics, and systems theory while adding the externalist diagnostic of
brittleness that its predecessors lacked.

By offering prospective guidance through constraint analysis and
grounding a falsifiable research program in historical methods, EPC
advances a form of Systemic Externalism. In this view, justification
depends on the proven reliability of public knowledge architectures.
Applications to historical paradigm shifts and contemporary epistemic
challenges illustrate the framework's diagnostic and explanatory power.

== 1. Introduction: Diagnosing Epistemic Health
<introduction-diagnosing-epistemic-health>
Why did germ theory replace miasma theory? A standard explanation cites
superior evidence. We agree, but add a systems-level claim about why
that evidence mattered historically. Miasma theory incurred mounting
operational costs, notably misdirected public health efforts in London,
and demanded constant ad hoc modifications throughout the mid-19th
century (Snow 1855). Its brittleness is evident in high patch velocity
$P \( t \)$. Germ theory, by contrast, reduced these costs while
unifying diverse phenomena.

This raises the fundamental question: how do we distinguish genuine
knowledge from coherent delusions? A Ptolemaic astronomer's system was
internally coherent, its practitioners trained in sophisticated
techniques, yet it was systematically misaligned with reality. What
external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief
system might be coherent yet detached from reality (BonJour 1985). While
internalist coherentist responses (Olsson 2005; Kvanvig 2012; Krag 2015)
struggle to provide external constraints, Quinean naturalism addresses
this by anchoring knowledge in empirical experience and pragmatic
revision. We propose Emergent Pragmatic Coherentism, building on Quine's
architecture (Quine 1960) and Kitcher's evolutionary model (1993) to
formalize the diagnostic assessment of knowledge systems through
systemic brittleness.

=== 1.1 Lineage and Contribution
<lineage-and-contribution>
Emergent Pragmatic Coherentism builds upon a tradition beginning with
Quine's naturalized epistemology (1969), continuing through Davidson's
(1986) coherence theory, Kitcher's (1993) evolutionary model of
scientific progress, and Longino's (1990) social account of objectivity.
Each sought to dissolve the dualism between justification and empirical
process. EPC accepts and extends this inheritance by describing the
convergent implications of Quine's architecture and introducing systemic
brittleness as a formalized diagnostic for epistemic health. Knowledge
is not a mirror of nature nor a purely linguistic web of belief, but a
living system maintained through adaptive feedback loops.

While Quine anchored epistemology in the psychology of individual agents
revising holistic webs in response to recalcitrant experience, EPC
extends this naturalistic foundation to analyze the macro-dynamics of
public knowledge systems. Belief-formation and revision, grounded in
Quine's account, aggregate into systems-level patterns that can be
assessed through brittleness metrics. Where Davidson focused on internal
coherence, EPC operationalizes Thagard's (1989) connectionist ECHO,
modeling coherence as activation harmony in constraint networks and
extending this with pragmatic inhibitory weights derived from
brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which
model the effects of network topology on belief propagation, EPC's
framework tracks how knowledge systems respond to real-world pragmatic
pressures through measurable costs.

The Quinean inheritance is architectural, not merely methodological. Our
framework requires knowledge structured as holistic webs subject to
pragmatic revision, coordinating into public systems converging toward
constraint-determined objectivity. This architecture is non-negotiable:
systemic brittleness requires interconnected networks; pragmatic
pushback requires revision mechanisms; convergence requires overlapping
webs aligning toward an objective standard. Quine's account provides
this in thoroughly naturalized form. Alternative foundations preserving
these features remain compatible with our systems-level analysis, though
the specific dispositional semantics remains optional.

Rescher's (1973) abstract criteria for systematicity are given concrete
diagnostic form through our brittleness indicators, while Kitcher's
(1993) credit-driven evolution gains a failure-driven engine via the
Negative Canon. Davidson's internal coherence becomes #emph[structural
homeostasis] maintained through pragmatic constraint. Kitcher's and
Longino's insights into social calibration are operationalized: the
intersubjective circulation of critique becomes a mechanism for reducing
systemic fragility.

EPC thus transforms coherence from a metaphor of fit into a measurable
function of cost and constraint. Its realism is not the correspondence
of propositions to an independent world, but the emergent stability of
constraint satisfaction across iterated cycles of error and repair.
Building on Quinean naturalism, EPC grounds justification in the
demonstrated viability of knowledge systems, providing formalized
diagnostic tools to assess the structural health that Quine's framework
presupposed but did not quantify.

Our response to the isolation objection is distinctive: coherence rests
not on historical accident but on emergent necessary structure. Reality
imposes pragmatic constraints: physical laws, biological limits, logical
requirements, and coordination necessities. These constraints create a
landscape that necessarily generates optimal configurations. These
structures emerge from the constraint landscape itself, existing whether
discovered or not, just as the lowest-energy state of a molecule emerges
from quantum mechanics whether calculated or not. Objective truth is
alignment with these emergent, constraint-determined structures.

We ground coherence in demonstrated viability of entire knowledge
systems, measured through their capacity to minimize systemic costs.
Drawing from resilience theory (Holling 1973), which models how systems
absorb disturbance without regime change, we explain how individuals'
holistic revisions to personal webs of belief in response to
recalcitrant experience drive bottom-up formation of viable public
knowledge systems. When aggregated at the systems level, this Quinean
process of pragmatic revision generates measurable patterns we diagnose
through brittleness metrics.

This transforms the isolation objection: a coherent system detached from
reality isn't truly possible because constraints force convergence
toward viable configurations. A perfectly coherent flat-earth cosmology
generates catastrophic navigational costs. A coherent phlogiston
chemistry generates accelerating conceptual debt. These aren't merely
false but structurally unstable, misaligned with constraint topology.

The process resembles constructing a navigation chart by systematically
mapping shipwrecks. Successful systems navigate safe channels revealed
by failures, triangulating toward viable peaks. The Apex Network is the
structure remaining when all unstable configurations are eliminated.
Crucially, this historical filtering is a discovery process, not a
creation mechanism. The territory is revealed by the map of failures,
not created by it.

This paper models inquiry as evolutionary cultivation of viable public
knowledge systems. It is a macro-epistemology for cumulative domains
like science and law, proposing Lamarckian-style directed adaptation
through learning rather than purely Darwinian selection.

Viability differs from mere endurance. A brutal empire persisting
through coercion exhibits high brittleness; its longevity measures
energy wasted suppressing instability. The distinction is crucial for
our framework: viability is a system's capacity to solve problems with
sustainably low systemic costs, empirically measurable through ratios of
coercive to productive resources.

#strong[Scope clarification:] This paper's primary focus is
epistemological---it addresses justification of beliefs and the
structure of knowledge systems. However, brittleness analysis applies to
any system that makes implicit claims about how the world works:
political institutions, social norms, economic arrangements. These are
not mere analogies. A brutal state makes implicit empirical claims
(e.g., "order requires centralized coercion," "dissent causes
instability") that can be assessed via the same brittleness metrics we
apply to scientific theories.

The framework treats social institutions as testable systems: they
impose structures on human interaction, those structures generate costs
when misaligned with human social reality ($C \( t \)$ from coercion,
$P \( t \)$ from proliferating rules, $M \( t \)$ from institutional
complexity), and those costs provide objective evidence of misalignment.
This is not sociology replacing epistemology---it is epistemic analysis
applied to a broader class of systems that embed empirical claims.

When we invoke social institutions in this paper, we are demonstrating
the framework's generality, not changing the subject. The logic is
identical: systems that accumulate brittleness reveal structural
misalignment; low-brittleness systems earn higher warrant.

The framework incorporates power, path dependence, and contingency as
key variables. Power exercised to maintain brittle systems becomes a
primary non-viability indicator through high coercive costs. Claims are
probabilistic: brittleness increases vulnerability to contingent shocks
without guaranteeing collapse. This failure-driven process grounds
fallible realism. Knowledge systems converge on emergent structures
determined by mind-independent constraints, yielding a falsifiable
research program.

The framework targets cumulative knowledge systems where
inter-generational claims and pragmatic feedback enable evaluation. It
provides macro-level foundations for individual higher-order evidence
(Section 7), not solutions to Gettier cases or basic perception.

Ultimately, this paper develops a framework for epistemic risk
management. This framing is crucial for understanding what brittleness
analysis claims and what it does not claim.

Brittleness indicators do not prove a system's core claims are false. A
rising trend in $P \( t \)$, $C \( t \)$, or $M \( t \)$ does not
constitute logical refutation. What it signals is that the system is
becoming a #strong[higher-risk, degenerating research program];, where
continued adherence or investment becomes increasingly irrational
relative to lower-brittleness alternatives.

This parallels financial risk assessment. A portfolio manager does not
claim that a high-volatility, illiquid asset will necessarily fail. The
claim is probabilistic: this asset profile carries elevated risk of
catastrophic loss when perturbations occur. Similarly, epistemic risk
management identifies systems where structural fragility elevates the
probability of collapse, major revision, or replacement when faced with
novel challenges.

The framework thus provides prospective guidance---the diagnostic tools
to assess structural health before hidden fragilities lead to
crisis---while preserving fallibilism. Rising brittleness is a warning
signal, not a proof of falsity.

=== 1.2 Key Terminology
<key-terminology>
This paper develops several core concepts to execute its argument.
Systemic brittleness refers to the accumulated costs that signal a
knowledge system's vulnerability to failure. We argue that through a
process of pragmatic selection, validated claims can become Standing
Predicates: reusable, action-guiding conceptual tools. This evolutionary
process drives knowledge systems toward the Apex Network, which we
define as the emergent, objective structure of maximally viable
solutions determined by mind-independent constraints.

=== 1.3 The Position in Summary
<the-position-in-summary>
To prevent misreading, we state the framework's commitments explicitly:

#strong[Realism about constraints.] The pragmatic pressures that filter
knowledge systems are real and mind-independent. The costs are
objective. The constraint landscape exists whether we map it or not.
When a brittle system collapses under its own accumulated costs, this
reflects genuine misalignment with non-negotiable features of reality,
not mere social consensus or instrumental convenience.

#strong[Deflationism about truth.] "True" is not a substantive property
naming correspondence, coherence, or utility. It is a device of
endorsement. To call P true is to assert P, to license its use in
inference, to treat it as available for building upon. Truth-talk
functions to mark which claims have earned standing through pragmatic
filtering, not to name a metaphysical relation between propositions and
facts.

#strong[Pragmatism about selection.] We do not "check" beliefs against
reality through some privileged access. Reality selects among belief
systems by imposing costs on misalignment. The structure that survives
is discovered through this filtering, not deduced from first principles
or verified through direct comparison. Our knowledge of the Apex Network
is always indirect, inferred from the systematic pattern of what fails.

#strong[Coherentism about justification.] Beliefs are justified by their
role in a web: their inferential connections, their integration with
other commitments, their contribution to the network's overall
coherence. But the web itself must earn authority externally. Coherence
is necessary but not sufficient. A belief can cohere perfectly with a
phlogiston network or a flat-earth cosmology; this provides no
justification unless that network has demonstrated viability through
sustained low brittleness.

These commitments are compatible. Realism about what selects does not
require correspondence theory about what "true" means. Deflationism
about truth does not require antirealism about constraints. Coherentism
about justification structure does not require isolation from external
pressure. The framework integrates these positions by recognizing that
they answer different questions: what exists (realism about
constraints), what "true" means (deflationism), how we discover what
works (pragmatic selection), and what makes individual beliefs justified
(coherentist structure with externalist grounding).

== 2. The Core Concepts: Units of Epistemic Selection
<the-core-concepts-units-of-epistemic-selection>
Understanding how knowledge systems evolve and thrive while others
collapse requires assessing their structural health. A naturalistic
theory needs functional tools for this analysis, moving beyond internal
consistency to gauge resilience against real-world pressures. Following
complex systems theory (Meadows 2008), this section traces how private
belief becomes a public, functional component of knowledge systems.

=== 2.1 From Individual Dispositions to Functional Propositions: Architectural Requirements and One Naturalistic Foundation
<from-individual-dispositions-to-functional-propositions-architectural-requirements-and-one-naturalistic-foundation>
Understanding how knowledge systems evolve requires clarifying their
architectural prerequisites. The framework's core claims (systemic
brittleness, pragmatic pushback, convergent evolution toward the Apex
Network) presuppose a specific knowledge structure, not a collection of
atomic beliefs or deductions from axioms. Three features prove
essential, with a fourth emerging from their interaction.

First, holism: knowledge forms interconnected webs where adjustments
ripple through the system. Brittleness accumulates systemically because
modifications create cascading costs. An isolated false belief is simply
false; a false belief embedded in a holistic web generates conceptual
debt as the system patches around it.

Second, pragmatic revision: external feedback causally modifies
knowledge structures. Without revision mechanisms, pragmatic pushback
becomes inert. Systems could acknowledge costs without adjusting,
rendering our entire framework toothless.

Third, multiple agents under shared constraints: independent agents
navigate the same reality, facing identical pragmatic constraints. This
is not about social coordination but parallel exploration of a common
constraint landscape. Like multiple explorers independently mapping the
same terrain, agents will converge on similar structures not through
communication but through bumping against the same obstacles. The
overlap we observe in human knowledge systems is evidence of this
convergent discovery, not its cause.

Fourth, constraint-determined objectivity (emergent from the first
three): knowledge systems converge toward an objective structure (the
Apex Network) determined by mind-independent pragmatic constraints. This
structure exists whether discovered or not, providing the standard for
truth, but it is revealed through elimination of failures rather than
known a priori. When multiple holistic systems undergo pragmatic
revision in response to shared constraints, the convergent patterns that
emerge reveal the objective structure of viable solutions. This is
foundational realism without traditional foundationalism: there IS an
objective foundation, but it must be discovered through extensive
parallel inquiry rather than stipulated by reason. Our fallibilism
concerns epistemic access (we never achieve certainty our map matches
the territory), not ontological status (the territory has objective
structure).

These architectural features are non-negotiable. What follows sketches
one naturalistic foundation that provides this architecture in
integrated form. Following Quine's call to naturalize epistemology
(Quine 1969), we ground knowledge in dispositions to assent: publicly
observable behavioral patterns. From a materialist perspective, even
consciousness and self-awareness are constituted by higher-order
dispositions---the recursive neural patterns that generate what we
experience as awareness emerge from, rather than transcend, the material
substrate of dispositional structures. Alternative accounts (coherentist
epistemology, inferentialist semantics, neural network theories) remain
compatible provided they preserve holism, pragmatic revision, and
operation under shared constraints. We develop the Quinean version
because it offers thoroughgoing naturalism with all required components.
However, the systems-level analysis beginning in Section 2.2 does not
depend on Quine's specific metaphysics of mental content, only on the
architectural features just outlined.

The convergence from individual disposition to shared knowledge
structures proceeds not through deliberate coordination but through
parallel discovery---multiple agents independently developing similar
solutions when faced with identical constraints. Social communication
may accelerate this process but is not structurally necessary for the
emergence of coherent, convergent knowledge systems.

==== 2.1.1 The Quinean Foundation: Disposition to Assent
<the-quinean-foundation-disposition-to-assent>
We begin with Quine's core insight: a belief is not an inner mental
representation but a disposition to assent---a stable pattern of
behavior (Quine 1960). To believe "it is raining" is to be disposed to
say "yes" when asked, to take an umbrella, and so on. This provides a
fully naturalistic starting point, free from abstract propositions.

==== 2.1.2 The Functional Bridge: Belief as Monitored Disposition
<the-functional-bridge-belief-as-monitored-disposition>
Here, we add a crucial functional layer to Quine's account. While a
disposition is a third-person behavioral fact, humans possess a natural
capacity for self-monitoring. From a materialist perspective, this
self-monitoring capacity is itself constituted by higher-order
dispositions to assent---the brain's neural architecture generates
dispositions all the way down. When we speak of "awareness" of our
dispositional states, we are not positing a separate observer but
describing how complex, layered dispositions create the phenomena we
call consciousness. This awareness is not a privileged glimpse into a
Cartesian theater but an emergent property of recursive dispositional
structures---dispositions about dispositions---analogous to
proprioception, that allows for self-report and deliberate revision.

==== 2.1.3 From Disposition to Assertion
<from-disposition-to-assertion>
Following Quine, we can treat belief as a disposition to assent. For
present purposes we need only one further step: when a speaker asserts a
sentence publicly, it becomes a claim that can be tested, challenged,
and integrated into a wider body of commitments. The unit of analysis in
what follows is therefore not a proposition in the abstract sense, but a
#strong[publicly assessable claim];---an assertion-token or
sentence-type in use---together with its inferential role in the
network.

==== 2.1.4 Parallel Discovery and the Emergence of Objectivity
<parallel-discovery-and-the-emergence-of-objectivity>
When multiple agents independently navigate the same constraint
landscape, their publicly assessable claims converge not through
coordination but through parallel discovery of viable solutions. Through
pragmatic feedback, independent agents develop shared dispositions to
assent to certain sentences in certain contexts because doing so has
proven viable when tested against the same reality. "Water boils at
100°C" is not a discovered Platonic truth, but a sentence that multiple
independent inquirers have become strongly disposed to assent to because
this disposition enables immense predictive success when facing the same
thermodynamic constraints.

This directly addresses Quine's indeterminacy thesis. While semantic
reference may remain metaphysically indeterminate, multiple agents can
achieve functional convergence. The shared disposition to assent to the
sentence "Water is H₂O" is precise enough to ground the science of
chemistry not because chemists coordinated their beliefs, but because
the constraint landscape of chemical reality forced convergence on this
particular dispositional pattern. The objectivity of the Apex Network,
therefore, is not the objectivity of a Platonic realm, but the emergent
objectivity of constraint-determined optimal configurations that
multiple agents independently discover.

#strong[Standing Predicates as Evolved Tools.] Publicly assessable
claims that dramatically reduce network brittleness undergo profound
status change. Their functional core is promoted into the network's
processing architecture, creating a Standing Predicate: a reusable
conceptual tool that functions as the "gene" of cultural evolution. When
a doctor applies the Standing Predicate `...is an infectious disease` to
a novel illness, it automatically mobilizes a cascade of validated,
cost-reducing strategies: isolate the patient, trace transmission
vectors, search for a pathogen, sterilize equipment. Its standing is
earned historically, caching generations of pragmatic success into a
single, efficient tool. Unlike static claims, Standing Predicates are
dynamic tools that unpack proven interventions, diagnostics, and
inferences.

By grounding epistemic norms in the demonstrated viability of convergent
dispositional patterns, the framework addresses normativity: epistemic
force emerges from pragmatic consequences of misalignment with
constraint-determined structures. Following Quine's engineering model,
epistemic norms function as hypothetical imperatives---if your goal is
sustainable knowledge production, minimize systemic brittleness in these
patterns.

==== 2.1.5 Why This Architecture Matters: Non-Negotiable Features
<why-this-architecture-matters-non-negotiable-features>
These architectural requirements are structural prerequisites, not
arbitrary choices. Holism matters because brittleness accumulates
through cascading costs across interconnected networks. Atomistic
beliefs cannot exhibit systemic brittleness; only in holistic webs do
adjustments create ripple effects, generating the conceptual debt (P(t))
and complexity inflation (M(t)) our diagnostics track.

Pragmatic revision matters because external costs must causally modify
knowledge structures. Without revision mechanisms, pragmatic pushback
becomes inert---systems could acknowledge costs without adjusting. The
Quinean architecture ensures costs drive actual restructuring.

Multiple agents navigating shared constraints matters because the Apex
Network emerges through parallel discovery, not social coordination.
When independent agents face the same constraint landscape and possess
pragmatic revision mechanisms, their belief webs will converge on
similar structures---not because they coordinate, but because reality's
constraints carve out the same viable pathways. Like multiple species
independently evolving eyes in response to light, multiple agents
independently develop similar knowledge structures in response to shared
pragmatic pressures. The overlapping patterns we observe aren't products
of coordination but inevitable convergences forced by the constraint
landscape itself.

Constraint-determined objectivity matters for realism. The framework's
response to the isolation objection requires that reality impose an
objective structure. The Apex Network must exist as an objective feature
of the constraint landscape, discovered through elimination rather than
created by consensus. This distinguishes viable knowledge from coherent
fiction.

These features work together as a package: holism without revision
yields stasis; revision without shared constraints yields divergent,
incomparable systems; shared constraints without revision prevents
discovery of viable structures; convergence without objective
constraints would be mere coincidence, not evidence of underlying
reality. The coherent subset of networks emerges naturally when these
conditions are met---social coordination may accelerate convergence but
is not structurally necessary. Alternative foundations preserving these
features remain compatible with our analysis.

=== 2.2 The Units of Analysis: Predicates, Networks, and Replicators
<the-units-of-analysis-predicates-networks-and-replicators>
Having established the architectural requirements (holistic,
pragmatically-revised, networks of multiple agents converging under
shared constraints toward constraint-determined objectivity) and
sketched one naturalistic foundation (Quinean dispositions), we now
shift to the systems level where the framework's distinctive
contributions emerge. Our deflationary move redirects attention from
individual agent psychology to public, functional structures. The units
of analysis that follow (Standing Predicates, Shared Networks, and their
evolutionary dynamics) depend on the required architecture but remain
neutral on metaphysics of belief. The convergence of independent agents'
belief webs into overlapping knowledge structures occurs through
parallel discovery---behavioral patterns shaped by sustained pragmatic
feedback from the same constraint landscape.

Standing Predicate: The primary unit of cultural-epistemic selection:
validated, reusable, action-guiding conceptual tools within propositions
(e.g., "…is an infectious disease"). Functioning as "genes" of cultural
evolution, Standing Predicates are compressed conceptual technology.
When applied, they unpack suites of validated knowledge: causal models,
diagnostic heuristics, licensed interventions. Functioning as
high-centrality nodes in Thagard-style networks (2000), Standing
Predicates maintain persistent activation through historical
vindication, with propagation weighted by pragmatic utility rather than
pure explanatory coherence.

In information-theoretic terms, a Standing Predicate functions as a
Markov Blanket (Pearl 1988; Friston 2013): a statistical boundary that
compresses environmental complexity into a stable, causal variable,
achieving computational closure. When a doctor applies the predicate
"…is an infectious disease," they draw a boundary allowing them to
operate on coarse-grained variables (viral load, transmission vectors)
without computing the infinite micro-complexity of molecular
trajectories. This compression minimizes prediction error---the surprise
the system experiences when its model misaligns with reality. Standing
Predicates are boundaries that have proven thermodynamically efficient
at carving reality at viable joints.

#strong[Clarifying "standing" and the replicator analogy:] The term
"Standing Predicate" denotes earned status, not metaphysical foundation.
A predicate achieves "standing" through sustained validation across
contexts: it stands ready for application without requiring
practitioners to re-argue its warrant each time. Once "infectious
disease" achieves Standing Predicate status, medical practitioners need
not relitigate germ theory when diagnosing a novel pathogen; the
predicate's historical track record grants it operational standing.

The biological replicator metaphor ("genes of cultural evolution") is
structural analogy, not literal mechanism. Standing Predicates function
analogously to biological replicators: they are informational templates
that persist and spread because they reliably solve recurrent problems.
Both encode compressed solutions to environmental challenges; both
replicate through copying with variation; both face selection through
performance. But this is analogy about functional architecture, not
claim about evolutionary psychology or memetics. We are doing pragmatic
epistemology---tracking which conceptual tools survive filtering by
constraint-determined viability---not making mechanistic claims about
cultural transmission dynamics.

Shared Network: Emergent public architecture of coherent propositions
and predicates shared across individual belief webs for collective
problem-solving. Networks nest hierarchically (germ theory within
medicine within science). Their emergence is a structural necessity, not
negotiation: failure-driven revisions converge on viable principles,
forming transmissible public knowledge. As resource-constrained systems,
networks implicitly evaluate each proposition through a brittleness
filter: will integrating this claim reduce long-term systemic costs, or
will it generate cascading adjustments and conceptual debt? This
economic logic drives convergence toward low-brittleness configurations.
We use Consensus Network to denote a Shared Network that has achieved
broad acceptance and a demonstrated low-brittleness track record.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and
cultural evolution (Mesoudi 2011), networks' informational structure
(Standing Predicates) acts as replicator (copied code) while independent
agents navigating shared reality serve as interactors (physical vessels
for testing). This explains knowledge persistence beyond particular
communities (e.g., rediscovered Roman law): the informational structure
can be preserved and retested by different agents facing the same
constraints. Independently formed networks converging on similar
structures reveal an objective constraint landscape underwriting
successful inquiry, anticipating the Apex Network (Section 4).

==== Conceptual Architecture
<conceptual-architecture>
The framework's core dynamics can be visualized as:

```
Belief Systems → Standing Predicates → Feedback Constraints → Brittleness Metrics → Apex Network
     ↓                ↓                        ↓                    ↓              ↓
Private States   Public Tools             Pragmatic Pushback   Diagnostic Tools  Objective Structure
```

This flow illustrates how individual cognition becomes public knowledge
through constraint-driven selection.

=== 2.2.1 From Local Error to Systemic Failure
<from-local-error-to-systemic-failure>
A crucial distinction grounds our framework: the difference between
local error and systemic fragility.

#strong[Local error] is ubiquitous in all knowledge systems. A single
failed prediction, an isolated false belief, an anomalous result---these
are the normal texture of inquiry. Ptolemaic astronomy made accurate
predictions for specific celestial events. Phlogiston theory correctly
identified many combustion phenomena. No knowledge system is immune to
local errors, and identifying them is what drives normal science.

#strong[Systemic fragility] emerges not from individual errors but from
the pattern of response to error over time. When a system encounters
resistance from reality, how does it adapt? Does it revise core
principles, or does it add defensive patches? Do corrections generate
broader unification, or do they fracture coherence into special cases?
Does maintaining the system require escalating resources, or does it
grow more efficient?

The framework focuses on this second-order pattern: the trajectory of
costs incurred as systems interact with pragmatic constraints. A system
exhibiting low brittleness absorbs errors through systematic refinement.
A system exhibiting high brittleness responds to errors by accumulating
conceptual debt, suppressing dissent, and requiring increasing resources
merely to maintain baseline function.

This is why brittleness predicts collapse in ways that local
falsification cannot. Practitioners can always accommodate individual
anomalies---Kuhn showed this decisively. What they cannot indefinitely
sustain is the escalating cost of accommodation itself.

=== 2.3 Pragmatic Pushback and Systemic Costs
<pragmatic-pushback-and-systemic-costs>
#strong[Defining Systemic Brittleness.] An epistemic system is
systemically brittle to the extent that maintaining its coherence and
usability under empirical or practical pressure requires escalating ad
hoc modifications, increasing insulation from feedback, or
disproportionate resource expenditure.

This definition makes explicit several key features: -
#strong[System-level property:] Brittleness is not a property of
individual propositions but of entire knowledge architectures. -
#strong[Diachronic pattern:] The key indicator is escalation over
time---rising costs, not static levels. - #strong[Comparative measure:]
Assessed relative to what maintaining coherence without escalating costs
would require.

Crucially, brittleness is distinct from several concepts it might be
confused with: - #strong[Not falsity:] A brittle system can contain true
propositions; the issue is the cost of maintaining them. - #strong[Not
irrationality:] Practitioners of brittle systems may be perfectly
rational given their evidence; brittleness is structural, not
psychological. - #strong[Not lack of coherence:] Brittle systems can be
internally coherent; the problem is external misalignment revealed
through costs. - #strong[Not unpopularity:] A system can be widely
accepted yet brittle, or marginal yet viable. Brittleness tracks
constraint-driven costs, not social opinion.

Shared networks are active systems under constant pressure from what we
term pragmatic pushback: Quine's "recalcitrant experience" observed at
the systems level. Where Quine described how anomalous sensory
stimulations force adjustments in an individual's dispositions to
assent, we track how these individual revisions aggregate into
observable patterns of systemic cost when knowledge systems are applied
at scale. Pragmatic pushback manifests as concrete, non-negotiable
consequences: a bridge collapses, a treatment fails, a society
fragments. These are the same empirical constraints Quine identified,
now visible through their accumulated impact on public knowledge
architectures.

Pragmatic pushback manifests as information leakage: when a network's
conceptual boundaries (its Markov Blankets) misalign with the
territory's actual causal structure, reality "leaks through" in the form
of prediction errors, failed interventions, and cascading anomalies. A
phlogiston theorist doesn't simply hold a false belief; their entire
causal framework generates continuous surprise as each experiment
reveals discrepancies their model cannot anticipate. Systemic
brittleness is the aggregate measure of this information leakage---the
thermodynamic cost of maintaining misaligned boundaries against
environmental feedback. This generates two cost types.

First-Order Costs are direct, material consequences: failed predictions,
wasted resources, environmental degradation, systemic instability (e.g.,
excess mortality). These are objective dysfunction signals. Systemic
Costs are secondary, internal costs a network incurs to manage,
suppress, or explain away first-order costs. These non-productive
expenditures reveal true fragility; for a formal mathematical model of
systemic brittleness and its dynamic evolution, see Appendix A.

Systemic brittleness, as used here, is a systems-theoretic measure of
structural vulnerability, not a moral or political judgment. It applies
uniformly across empirical domains (physics, medicine), abstract domains
(mathematics, logic), and social domains (institutions, norms). The
measure tracks failure sensitivity: how readily a system generates
cascading costs when its principles encounter resistance. This
diagnostic framework is evaluatively neutral regarding what kinds of
systems should exist; it identifies which configurations are
structurally sustainable given their constraint environment. A highly
coercive political system exhibits brittleness not because coercion is
morally wrong but because maintaining such systems against pragmatic
resistance (demographic stress, coordination failures, resource
depletion) generates measurable, escalating costs that signal structural
unsustainability.

Conceptual Debt Accumulation: Compounding fragility from flawed, complex
patches protecting core principles.

Coercive Overheads: Measurable resources allocated to enforcing
compliance and managing dissent. While Quine's framework focuses on how
anomalous experience forces belief revision in individual agents, it
does not address how power can actively suppress such experience before
it forces revision. Coercive overheads provide this missing diagnostic:
resources maintaining brittle systems against pressures become direct,
measurable non-viability indicators. Critically, coercion functions as
information blindness. By suppressing dissent---the primary data stream
signaling that a model is misaligned with reality---the system severs
its own sensor loops. This is not merely an energetic cost but an
epistemic one: the system goes blind to the gradient of the constraint
landscape, guaranteeing eventual collapse regardless of available
resources. A tyranny with infinite coercive capacity still cannot escape
the thermodynamic consequences of operating on systematically false
models.

Pragmatic pushback is not limited to material failures. In abstract
domains like theoretical physics or mathematics, where direct empirical
tests are deferred or unavailable, pushback manifests through Systemic
Cost accumulation: secondary costs a network incurs to manage, suppress,
or explain away dysfunction. Research programs requiring accelerating ad
hoc modifications to maintain consistency, or losing unifying power,
experience powerful pragmatic pushback.

These epistemic inefficiencies are real costs rendering networks brittle
and unproductive, even without direct experimental falsification. The
diagnostic lens thus applies to all inquiry forms, measuring viability
through external material consequences or internal systemic dysfunction.

To give the abstract concept of brittleness more concrete philosophical
content, we can identify several distinct types of systemic cost. The
following indicators serve as conceptual lenses for diagnosing the
health of a knowledge system, linking the abstract theory to observable
patterns of dysfunction. These are analytical categories for historical
and philosophical analysis, not metrics for a quantitative science.

#figure(
  align(center)[#table(
    columns: (33.33%, 33.33%, 33.33%),
    align: (left,left,left,),
    table.header([Conceptual Indicator], [Dimension of
      Brittleness], [Illustrative Manifestation],),
    table.hline(),
    [P(t)], [Conceptual Debt Accumulation], [Increasing share of
    publications devoted to anomaly-resolution vs.~novel predictions],
    [C(t)], [Coercive Overhead], [Increasing share of resources devoted
    to security/suppression vs.~productive R&D],
    [M(t)], [Model Complexity], [Rate of parameter/complexity growth
    vs.~marginal performance gains],
    [R(t)], [Resilience Reserve], [Breadth of independent, cross-domain
    confirmations of core principles],
  )]
  , kind: table
  )

#strong[Methodological Caution]

The indicators above are diagnostic proxies, not a single commensurable
scale. Brittleness is diagnosed by convergent signs: accelerating
auxiliary structure AND narrowing exportability AND rising tuning
overhead, not by any one scalar ratio that could mechanically "cancel
out" competing virtues.

We do not propose mechanical formulas of the form "P(t) \> 0.7 =
brittle." Such precision is spurious. Instead, these metrics make
visible resource allocation patterns that standard evidential accounts
systematically miss. Think of them as analogous to Minimum Description
Length (Rissanen 1978) or algorithmic complexity: we assess
incompressible exception-handling relative to core explanatory
structure. When the description length of auxiliary machinery (patches,
special cases, ad hoc adjustments) begins to exceed the description
length of core principles, brittleness is rising.

This is not ratio arithmetic where one novel prediction cancels one
auxiliary hypothesis. It is pattern recognition: identifying the
systemic signature of a knowledge system fighting reality's constraints
rather than aligning with them. The diagnostics are heuristic indicators
tied to a principled complexity measure, not mechanical scoring rules.

#strong[Operationalizing P(t): Distinguishing Patches from Development.]
A persistent challenge in applying these metrics is distinguishing
defensive patches from legitimate theoretical refinement. Not every
modification signals brittleness. The framework provides structured
criteria for this distinction, though applying them requires
interpretive judgment informed by domain expertise.

A modification counts as a patch, contributing to P(t), when it exhibits
these characteristics: it is primarily anomaly-driven rather than
prediction-driven, responding to known discrepancies rather than
generating novel testable predictions; it is ad-hoc rather than
systematic, addressing a specific problematic case without deriving from
or contributing to broader theoretical principles; it reduces
integration by adding special-case rules that fracture the theory's
unity rather than enhancing it; and it shows an accelerating pattern,
where the rate of such modifications increases over time as each patch
generates new anomalies requiring further patches.

Legitimate theoretical development, by contrast, is generative rather
than defensive, opening new research programs and predicting previously
unconsidered phenomena; it is systematically motivated, deriving from
core principles and extending their application rather than
circumventing their failures; it increases integration by unifying
previously disparate phenomena under common principles; and it shows
stable or decreasing modification rates, as refinements resolve multiple
anomalies simultaneously.

Concrete diagnostic questions help distinguish these patterns. Does the
modification resolve only the triggering anomaly, or does it address a
class of related problems? Can the modification be derived from existing
core principles, or does it require invoking entirely new mechanisms?
Does it generate testable predictions beyond the anomaly that motivated
it? Would removing it collapse a narrow application or destabilize the
entire theoretical structure? A modification answering "only the
triggering anomaly," "entirely new mechanisms," "no new predictions,"
and "narrow application" exhibits the signature of a patch. This remains
a hermeneutic exercise requiring domain knowledge, but these criteria
provide structured guidance for assessing whether a research program is
accumulating conceptual debt or genuinely progressing.

The following examples use these indicators as conceptual lenses for
retrospective philosophical analysis of historical knowledge systems,
illustrating how the abstract notion of brittleness could manifest as
observable patterns. These are not worked empirical studies but
conceptual illustrations that make the framework's diagnostic approach
concrete.

Conceptual Illustration 1: Phlogiston Chemistry

Consider the transition from phlogiston theory to oxygen-based chemistry
in the late 18th century. Phlogiston theory initially provided elegant
unification: combustion, respiration, calcination all explained as
phlogiston release. However, as quantitative measurements accumulated
(Lavoisier's careful mass tracking), the theory required increasingly
elaborate auxiliary machinery. This case demonstrates how brittleness
metrics can be retrospectively applied to historical knowledge systems,
using documented patterns to illustrate the framework's diagnostic
approach.

#strong[Brittleness Metrics Applied Retrospectively:]

#strong[M(t) - Model Complexity:] The theory began requiring "negative
weight" for phlogiston to explain mass gain during combustion.
Parameters and special cases proliferated while explanatory scope
narrowed. Each anomaly demanded novel auxiliary assumptions rather than
flowing from core principles.

#strong[P(t) - Conceptual Debt:] By the 1780s, chemistry publications
were predominantly defensive: explaining anomalies, reconciling
inconsistencies, rather than extending the framework to new phenomena.
The research program shifted from discovery to damage control.

#strong[C(t) - Coercive Overhead:] Phlogiston defenders required
increasing institutional resources to maintain the framework's authority
despite mounting difficulties. Lavoisier faced sustained resistance from
established chemists wielding institutional power.

#strong[R(t) - Resilience Reserve:] The framework's core principles
lacked exportability. Attempts to extend phlogiston reasoning to new
reaction types or thermal phenomena generated further anomalies rather
than unified explanations.

By contrast, Lavoisier's oxygen-based chemistry achieved dramatically
lower brittleness across all metrics: - #strong[Lower M(t):] Unified
quantitative framework (stoichiometry) with fewer auxiliary principles -
#strong[Lower P(t):] Immediate novel predictions and extensions to new
reaction types - #strong[Lower C(t):] Rapid adoption once empirical
weight became accessible to verification - #strong[Higher R(t):]
Framework exported successfully to thermodynamics, biochemistry,
industrial chemistry without fundamental revision

The point is not that one anomaly refuted phlogiston, but that
sustaining it required rising maintenance costs with diminishing
explanatory returns. This cost accumulation is what brittleness makes
visible. The transition was driven not by crucial experiments alone but
by the unsustainable operational burden of maintaining the phlogiston
framework.

Conceptual Illustration 2: Contemporary AI Development. Current deep
learning paradigms could be analyzed for early signs of rising
brittleness. For instance, one might assess Model Complexity (M(t)) by
examining the trend of dramatically escalating parameter counts and
computational resources required for marginal performance gains.
Likewise, one could interpret the proliferation of alignment and safety
research not just as progress, but also as a potential indicator of
rising Patch Velocity (P(t)): a growing need for post-hoc patches to
manage emergent, anomalous behaviors. These trends, viewed through the
EPC lens, serve as potential warning signs that invite cautious
comparison to past degenerating research programs.

Having established brittleness as our diagnostic concept, we now address
the methodological challenge: how to measure these costs objectively
without circularity.

== 3. The Methodology of Brittleness Assessment
<the-methodology-of-brittleness-assessment>
=== 3.1 The Challenge of Objectivity: Achieving Pragmatic Rigor
<the-challenge-of-objectivity-achieving-pragmatic-rigor>
Operationalizing brittleness faces a fundamental circularity: measuring
systemic costs objectively requires neutral standards for "waste" or
"dysfunction," yet establishing such standards appears to require the
very epistemic framework our theory aims to provide.

While this appears circular, it is the standard method of reflective
equilibrium, operationalized here with external checks. We break the
circle by anchoring our analysis in three ways: grounding measurements
in physical constraints, using comparative analysis, and requiring
convergent evidence.

Brittleness assessment remains partially hermeneutic. The framework
provides structured tools rather than mechanical algorithms, offering
"structured fallibilism" rather than neutral assessment. This
methodology provides pragmatic objectivity sufficient for comparative
assessment and institutional evaluation.

We frame the approach as epistemic risk management. A rising trend in a
system's brittleness indicators does not prove its core claims are
false. Instead, it signals the system is becoming a higher-risk,
degenerating research program, making continued investment increasingly
irrational. Just as financial risk management uses multiple converging
indicators to assess portfolio health, epistemic risk management uses
brittleness metrics to assess knowledge systems before hidden fragility
leads to catastrophic failure.

=== 3.2 The Solution: A Tiered Diagnostic Framework
<the-solution-a-tiered-diagnostic-framework>
To clarify how objective cost assessment is possible without appealing
to contested values, we organize brittleness indicators into a tiered
diagnostic framework, moving from foundational and least contestable to
domain-specific.

#strong[Tier 1: Foundational Bio-Social Costs:] The most fundamental
level: direct, material consequences of network misalignment with
conditions for its own persistence. These are objective bio-demographic
facts, measurable through historical and bioarchaeological data: -
Excess mortality and morbidity rates (relative to contemporaneous peers
with similar constraints) - Widespread malnutrition and resource
depletion - Demographic collapse or unsustainable fertility patterns -
Chronic physical suffering and injury rates

Systems generating higher death or disease rates than viable
alternatives under comparable constraints incur measurable,
non-negotiable first-order costs. These metrics are grounded in
biological facts about human survival and reproduction, not contested
normative frameworks.

#strong[Tier 2: Systemic Costs of Internal Friction:] The second tier
measures non-productive resources systems expend on internal control
rather than productive adaptation. These are energetic and informational
prices networks pay to manage dissent and dysfunction generated by Tier
1 costs, often directly quantifiable (see Section 2.3 for detailed
treatment): - #strong[Coercion Ratio (C(t)):] Share of state resources
allocated to internal security and suppression versus public health,
infrastructure, and R&D. - #strong[Information Suppression Costs:]
Resources dedicated to censorship or documented suppression of minority
viewpoints, and resulting innovation lags compared to more open rival
systems.

#strong[Tier 3: Domain-Specific Epistemic Costs:] The third tier
addresses abstract domains like science and mathematics, where costs
manifest as inefficiency: - #strong[Conceptual Debt Accumulation
(P(t)):] Rate of auxiliary hypotheses to protect core theory (literature
analysis). - #strong[Model Complexity Inflation (M(t)):] Parameter
growth without predictive gains (parameter-to-prediction ratios). -
#strong[Proof Complexity Escalation:] Increasing proof length without
explanatory gain (mathematics).

While interpreting these costs is normative for agents within a system,
their existence and magnitude are empirical questions. The framework's
core causal claim is falsifiable and descriptive: networks with high or
rising brittleness across these tiers carry statistically higher
probability of systemic failure or major revision when faced with
external shocks. This operationalizes Kitcher's (1993) 'significant
problems' pragmatically: Tier 1 bio-costs define significance without
credit cynicism, while C(t) measures coercive monopolies masking
failures.

Robustly measuring these costs requires disciplined methodology. The
triangulation method provides practical protocol for achieving pragmatic
objectivity.

==== 3.2.1 Cost-Shifting as Diagnostic Signal
<cost-shifting-as-diagnostic-signal>
This framework reveals cost-shifting: systems may excel in one tier
(epistemic efficiency) while incurring catastrophic costs in another
(bio-social harm). Such trade-offs signal hidden brittleness, as
deferred costs accumulate vulnerability. Diagnosis identifies
unsustainable patterns across tiers, not a single score.

==== 3.2.2 Model Complexity as Compression Failure
<model-complexity-as-compression-failure>
Before examining how to measure these costs objectively, we should
clarify why Model Complexity $M \( t \)$ functions as a brittleness
indicator at the conceptual level, not merely as an operational metric.
The deeper insight is that viable theories function as compression: they
reduce vast empirical data into compact, generative principles that
enable prediction beyond the original observations.

Consider Newton's laws of motion: three concise equations that compress
countless observations of falling apples, planetary orbits, and
projectile trajectories into a unified framework. This compression is
not merely elegant; it is epistemically powerful because only a
genuinely compressed theory can extrapolate reliably to novel cases. A
theory that has captured the underlying causal structure can generate
predictions for situations never before encountered. This is the
hallmark of deep compression.

Contrast this with a system approaching what we might call the
incompressibility trap. Late geocentric astronomy required proliferating
epicycles, deferents, and equants, each calibrated to specific
observational data. As the system's complexity grew toward the
complexity of the phenomena it described, it ceased to function as a
compact theory and became closer to a sophisticated lookup table: a
catalog of past observations with reduced generative power. When a
theory requires ever more auxiliary machinery for marginal gains, it can
reproduce what it has seen but cannot reliably anticipate what it has
not.

This is why rising $M \( t \)$ signals brittleness. It indicates that a
system is losing compression efficiency, requiring more and more
internal complexity to maintain performance. This is not a subjective
aesthetic preference for simplicity. It reflects a fundamental epistemic
constraint: theories that approach the complexity of their subject
matter lose the capacity to guide action in novel circumstances. They
become brittle because they cannot compress reality's patterns, only
mirror them.

This conceptual grounding also clarifies the philosophical status of
Occam's Razor. Parsimony is not an arbitrary methodological preference
but a necessary condition for viable knowledge systems. A theory
cluttered with parameters and special cases is not merely inelegant; it
is structurally incapable of the predictive compression that allows
knowledge systems to extend beyond their training data. The principle of
parsimony, understood through this lens, is a constraint imposed by the
very nature of viable inquiry.

=== 3.3 The Triangulation Method
<the-triangulation-method>
No single indicator is immune to interpretive bias. Therefore, robust
diagnosis of brittleness requires triangulation across independent
baselines. This protocol provides a concrete method for achieving
pragmatic objectivity.

#strong[Baseline 1: Comparative-Historical Analysis:] We compare system
metrics against contemporaneous peers with similar technological,
resource, and environmental constraints. For example, 17th-century
France exhibited higher excess mortality from famine than England, not
because of worse climate, but because of a more brittle
political-economic system hindering food distribution. The baseline is
what was demonstrably achievable at the time.

#strong[Baseline 2: Diachronic Trajectory Analysis:] We measure
direction and rate of change within a single system over time. A society
where life expectancy is falling, or a research program where the share
of work devoted to ad-hoc patches vs.~novel predictions is increasing,
is exhibiting rising brittleness regardless of its performance relative
to others.

#strong[Baseline 3: Biological Viability Thresholds:] Some thresholds
are determined by non-negotiable biological facts. A society with Total
Fertility Rate sustainably below 2.1 is, by definition, demographically
unviable without immigration. A system generating chronic malnutrition
in over 40% of its population is pushing against fundamental biological
limits.

Diagnosis requires convergent baselines: e.g., rising mortality
(diachronic), peer underperformance (comparative), and biological
thresholds. This parallels climate science's multi-evidence convergence,
achieving pragmatic objectivity for comparative evaluations.

#strong[Avoiding Circularity:] Defining "first-order outcomes" requires
anchoring in physical, measurable consequences that are
theory-independent where possible. For social systems, Tier 1 metrics
(mortality, morbidity, resource depletion) can be measured without
presupposing the values being tested. For knowledge systems, predictive
success and unification can be tracked through citation patterns and
cross-domain applications. The key is triangulation: multiple
independent metrics converging provides confidence that diagnoses
reflect underlying brittleness rather than observer bias.

==== 3.3.1 Why Viability Is Not Just "Evidence" Renamed
<why-viability-is-not-just-evidence-renamed>
An immediate objection: if viability is measured by successful
predictions, explanatory unification, and effective interventions, how
does this differ from standard accounts of evidential superiority? Is
systemic brittleness just a new vocabulary for an old idea?

The distinction is this: standard evidential accounts focus on the
propositional level, assessing which claims are supported by which
observations. Viability analysis operates at the systems level, tracking
the costs a knowledge network generates when its principles are
collectively applied over time. These costs include but exceed
predictive failure.

#strong[Maintenance costs:] Resources spent patching, hedging,
explaining away anomalies, even when predictions technically succeed. A
theory can accurately predict outcomes while requiring ever-increasing
ad hoc modifications to do so. The Ptolemaic system maintained
predictive adequacy for centuries, but the brittleness lay not in failed
predictions but in the proliferating machinery required to generate
those predictions.

#strong[Coercive costs:] Resources spent enforcing compliance when a
system generates resistance from its own practitioners or subjects. When
a paradigm suppresses dissent through institutional power rather than
persuasion, when it requires loyalty oaths or purges rather than open
inquiry, these costs signal brittleness invisible to purely evidential
analysis.

#strong[Opportunity costs:] Paths not taken, problems not addressed,
because the system's structure forecloses them. A research program can
have strong evidential support for what it investigates while
systematically avoiding domains where it would perform poorly. The
inability to even formulate certain questions is a cost standard
evidential accounting cannot capture.

A system can therefore have strong local evidential support while
accumulating unsustainable systemic costs. Ptolemaic astronomy made
adequate predictions for centuries. Its brittleness lay not in
predictive failure but in the proliferating machinery required to
maintain those predictions. Evidence, in the standard sense, supported
it. Viability analysis reveals the hidden costs that standard evidential
accounting misses.

The relationship is therefore not identity but inclusion. Evidence
(predictive success) is one input to viability. But viability tracks the
total cost of maintaining a system as a going concern, including costs
that never appear in confirmation theory because they are not about the
truth of individual propositions but about the sustainability of the
entire knowledge-producing enterprise.

=== 3.4 The Conceptual Distinction Between Productive and Coercive Costs
<the-conceptual-distinction-between-productive-and-coercive-costs>
A persistent objection is that classifying spending as "productive"
versus "coercive" requires the very normative commitments the framework
aims to ground, leading to circularity. The framework resolves this by
distinguishing them not by their intrinsic nature but by their
observable function within the system over time. The distinction is
conceptual and diagnostic, not based on a priori definitions.

#strong[Coercive overheads] are identifiable as expenditures that
function to suppress the symptoms of a system's underlying brittleness.
Their signature is a pattern of diminishing returns: escalating
investment is required merely to maintain baseline stability, and this
spending correlates with stagnant or worsening first-order outcomes
(such as public health or innovation). Such costs do not solve problems
but manage the dissent and friction generated by them.

#strong[Productive investments];, in contrast, are expenditures that
demonstrably reduce first-order costs and exhibit constant or increasing
returns. They address root causes of brittleness and tend to generate
positive spillovers, enhancing the system's overall viability.

The classification, therefore, emerges from analyzing the dynamic
relationship between resource allocation and systemic health, not from a
static definition of "coercion." We ask: does this expenditure pattern
reduce the system's total costs, or does it function as a secondary cost
required to mask the failures of the primary system? This conceptual
distinction allows for a non-circular, trajectory-based diagnosis of
systemic health.

== 4. The Emergent Structure of Objectivity
<the-emergent-structure-of-objectivity>
With diagnostic methodology in hand (Section 3), we can now turn to the
question these tools enable us to answer: what structure emerges when we
systematically eliminate high-brittleness systems? The preceding
sections established the methods for identifying failure. This section
addresses the framework's core theoretical contribution: building the
theory of objectivity that systematic failure-analysis makes possible.
We show how the logic of viability selects for knowledge system
evolution and drives convergence toward an emergent, objective
structure: the Apex Network.

=== 4.1 A Negative Methodology: Charting What Fails
<a-negative-methodology-charting-what-fails>
Our account of objectivity is not the pursuit of a distant star but the
painstaking construction of a reef chart from the empirical data of
shipwrecks. It begins not with visions of final truth, but with our most
secure knowledge: the clear, non-negotiable data of large-scale systemic
failure. Following Popperian insight (Popper 1959), our most secure
knowledge is often of what is demonstrably unworkable. While single
failed experiments can be debated, entire knowledge system collapse
(descent into crippling inefficiency, intellectual stagnation,
institutional decay) provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based
catalogue of invalidated principles distinguishing:

#strong[Epistemic Brittleness:] Causal failures (scholastic physics,
phlogiston) generating ad-hoc patches and predictive collapse.

#strong[Normative Brittleness:] Social failures (slavery,
totalitarianism) requiring rising coercive overheads to suppress
dissent.

Charting failures reverse-engineers viability constraints, providing
external discipline against relativism. This eliminative process is how
the chart is made, mapping hazards retrospectively to reveal the safe
channels between them. Progress accrues through better hazard maps.

=== 4.2 The Apex Network: Ontological and Epistemic Status
<the-apex-network-ontological-and-epistemic-status>
As the Negative Canon catalogs failures, pragmatic selection reveals the
contours of the Apex Network. The key distinction is two-level.
Epistemologically, our claim is coherentist-externalist: beliefs are
warranted within networks whose overall performance remains resilient
under pragmatic pressure. Metaphysically, our claim is structural
realist: the constraints producing that pressure are mind-independent.

This distinction matters for the reviewer's central worry. Saying that
brittle systems are misaligned with causal structure does not collapse
justification into one-by-one correspondence testing. It states the
external condition under which coherent networks survive. Coherence
still does the internal justificatory work. Constraint pressure explains
why some coherent networks remain stable and others do not.

We therefore define the Apex Network as the maximally shared
load-bearing structure across viable networks, not as a single complete
theory currently available to us. Formally, it is the intersection of
the structural elements that persist across low-brittleness systems
under shared constraints. In that sense, if there is any non-trivial
convergence at all, there is an Apex structure, even if it is partial
and historically expanding.

This framing also sharpens the multiple-peaks objection. Multiple local
optima can exist. We grant that directly. But the objection defeats our
view only if viable peaks are entirely disjoint. That stronger claim is
unlikely in tightly constrained domains because any system that remains
viable must solve common load-bearing problems. Distinct peaks can
differ at the frontier while sharing a substantial core.

We also grant a genuine limitation: in weakly constrained domains, the
shared core may be thin. This is not a defect in the framework. It is a
diagnosis of underdetermination. The framework should not promise thick
convergence where constraint structure does not support it.

The account remains non-circular because it gives independent criteria
for distinguishing durable convergence from local lock-in. Local lock-in
tends to show path dependence without independent rediscovery,
escalating maintenance or coercive costs, and poor extensibility to new
domains. Durable convergence tends to show independent rediscovery,
stable low brittleness across institutional changes, and cross-domain
transfer without proliferating defensive patches.

This structure lets us answer the isolation objection in the right
scope. Isolation is not denied as a synchronic possibility for any
single coherent web. It is denied as a stable diachronic outcome for
public networks operating under shared constraints. Over time, systems
that do not track those constraints accumulate costs and lose
shareability.

The conclusion is modest but strong. The Apex Network is objective
because it is constraint-determined, not because consensus creates it.
It is fallibly known because access occurs through historical
elimination, not direct inspection. That is why realism at the
metaphysical level and coherentism at the epistemic level are
complementary in this framework rather than rivals.

==== 4.2.1 Formal Characterization
<formal-characterization>
Drawing on network theory (Newman 2010), we can formally characterize
the Apex Network as:

A = ∩{W\_k | V(W\_k) = 1}

Where A = Apex Network, W\_k = possible world-systems (configurations of
predicates), V(W\_k) = viability function (determined by brittleness
metrics), and ∩ = intersection (common structure across all viable
systems).

The intersection of all maximally viable configurations reveals their
shared structure. This shared structure survives all possible variations
in historical path: the emergent, constraint-determined necessity
arising from how reality is organized.

A coherent system detached from the Apex Network isn't merely false but
structurally unstable. It will generate rising brittleness until it
either adapts toward the Apex Network or collapses. Coherence alone is
insufficient because reality's constraints force convergence.

==== 4.2.2 Cross-Domain Convergence and Pluralism
<cross-domain-convergence-and-pluralism>
Cross-domain predicate propagation drives emergence: when Standing
Predicates prove exceptionally effective at reducing brittleness in one
domain, pressure mounts for adoption in adjacent domains. Germ theory's
success in medicine pressured similar causal approaches in public health
and sanitation. This successful propagation forges load-bearing,
cross-domain connections constituting the Apex Network's emergent
structure.

This process can be conceptualized as a fitness landscape of inquiry.
Peaks represent low-brittleness, viable configurations (e.g., Germ
Theory), while valleys and chasms represent high-brittleness failures
catalogued in the Negative Canon (e.g., Ptolemaic System). Inquiry is a
process of navigating this landscape away from known hazards and toward
resilient plateaus.

==== 4.2.3 Discovery Without Blueprint
<discovery-without-blueprint>
Two claims can both be true without tension. First, the Apex Network is
not a transcendent blueprint, a Platonic object, or a teleological
endpoint built into history. Second, given a fixed constraint structure,
viable systems will share determinate structural features regardless of
which community discovers them first. The first claim blocks
metaphysical inflation. The second claim blocks relativism.

So the right modal reading is conditional. Relative to a world with
constraint profile $C$, there is a determinate apex structure
$A \( C \)$. Change the constraints, and a different structure may be
selected. This is enough objectivity for epistemology, without requiring
a timeless realm of propositions.

=== 4.3 A Three-Level Framework for Truth
<a-three-level-framework-for-truth>
This emergent structure grounds a fallibilist but realist account of
truth. It clarifies a documented tension in Quine's thought between
truth as immanent to our best theory and truth as a transcendent
regulative ideal (Tauriainen 2017). Our framework shows these are not
contradictory but two necessary components of a naturalistic
epistemology. Crucially, in this model, a demonstrated track record of
low systemic brittleness functions as our primary fallible evidence for
a system's alignment with the Apex Network; it is not, itself,
constitutive of that alignment.

A belief is not justified merely because it coheres with a Consensus
Network. Coherence with a flat-earth network or a phlogiston network
provides no warrant. What matters is coherence with a network that has
itself earned epistemic authority through sustained success under
external, non-discursive constraints.

This is the externalist move that distinguishes Systemic Externalism
from classical coherentism. Coherence remains necessary: isolated
beliefs lack inferential integration. But coherence is not sufficient.
The network must have demonstrated resilience through low brittleness,
successful interventions, minimal coercive overhead, and adaptability to
novel problems. Justification is earned through historical performance,
not stipulated by internal agreement.

A belief can therefore be justified yet false, if it coheres with a
network that has, at that time, the best available track record. And a
belief can be unjustified yet true, if it coheres only with a
high-brittleness network or none at all. This preserves the standard
distinction between justification (a property of beliefs relative to
networks) and truth (a property indexed to the Apex Network's objective
structure).

- #strong[Level 3: Contextual Coherence.] The baseline status for any
  claim. A proposition is coherent within a specific Shared Network,
  regardless of that network's long-term viability. This level explains
  the internal rationality of failed or fictional systems, but the
  framework's externalist check, the assessment of systemic brittleness,
  prevents this from being mistaken for higher warrant.
- #strong[Level 2: Warranted Assertion.] The highest practical epistemic
  status available to finite inquirers. A belief is warranted for
  assertion as true when it is certified by a Consensus Network with a
  demonstrated low-brittleness track record. This preserves the standard
  analytic distinction: beliefs and assertions are justified, truths are
  not. To doubt a claim at this level, without new evidence of rising
  brittleness, is to suspend trust in the most resilient inquiry
  structures currently available.
- #strong[Level 1: Objective Truth.] The regulative ideal of the
  process. A proposition is objectively true if its principles are part
  of the real, emergent Apex Network: the objective structure of viable
  solutions. In information-theoretic terms, this represents optimal
  computational closure, where a system's enacted boundaries (its Markov
  Blankets) achieve maximum alignment with the causal constraints of the
  environment, minimizing information leakage to the theoretical
  minimum. While this structure is never fully mapped, it functions as
  the formal standard that makes our comparative judgments of "more" or
  "less" brittle meaningful.

This layered framework avoids a simplistic "Whig history" by recognizing
that Level 2 warrant is historically situated. Newtonian mechanics
earned that status by being a maximally low-brittleness system for its
problem-space for over two centuries. Its replacement by relativity does
not retroactively invalidate the earlier warrant. It shows the
evolutionary process at work, where an expanding problem-space revealed
constraints requiring a more viable successor.

=== 4.3.1 The Hard Core: Functional Entrenchment and Pragmatic Indispensability
<the-hard-core-functional-entrenchment-and-pragmatic-indispensability>
The three-level framework reveals how propositions do not merely
"correspond" to truth as an external standard but become constitutive of
truth itself through functional transformation and entrenchment.

Our framework provides robust, naturalistic content to
truth-attributions: to assert that $P$ is true at Level 2 is to treat
$P$ as warranted by a low-brittleness Consensus Network; to say $P$ is
objectively true (Level 1) is to say $P$ aligns with the emergent,
constraint-determined Apex Network. Truth is what survives systematic
pragmatic filtering. The predicate "is true" tracks functional role
within viable knowledge systems, not correspondence to a Platonic realm.

#strong[From Validated Data to Constitutive Core: The Progression]

A proposition's journey to becoming truth itself follows a systematic
progression through functional transformation:

+ #strong[Initial Hypothesis (Being-Tested):] The proposition begins as
  a tentative claim within some Shared Network, subject to coherence
  constraints and empirical testing. It is data to be evaluated.

+ #strong[Validated Data (Locally Proven):] Through repeated application
  without generating significant brittleness, the proposition earns
  trust. Its predictions are confirmed; its applications succeed. It
  transitions from hypothesis to validated data, something the network
  can build upon.

+ #strong[Standing Predicate (Tool-That-Tests):] The proposition's
  functional core, its reusable predicate, is promoted to Standing
  Predicate status. It becomes conceptual technology: a tool for
  evaluating new phenomena rather than something being evaluated. "…is
  an infectious disease" becomes a diagnostic standard, not a claim
  under test.

+ #strong[Convergent Core Entry (Functionally Unrevisable):] As rival
  formulations are relegated to the Negative Canon after generating
  catastrophic costs, the proposition migrates to the Convergent Core.
  Here it achieves Level 2 status: Warranted Assertion.

+ #strong[Hard Core (Constitutive of Inquiry Itself):] In the most
  extreme cases, a proposition becomes so deeply entrenched that it
  functions as a constitutive condition for inquiry within its domain.
  This is Quine's hard core, the principles so fundamental that their
  removal would collapse the entire edifice.

#strong[Quine's Hard Core and Functional Entrenchment]

Quine famously argued that no claim is immune to revision in principle,
yet some claims are practically unrevisable because revising them would
require dismantling too much of our knowledge structure. Our framework
explains this tension through the concept of functional entrenchment
driven by bounded rationality (March 1978).

A proposition migrates to the hard core not through metaphysical
necessity but through pragmatic indispensability. The costs of revision
become effectively infinite:

- #strong[Logic and Basic Mathematics:] Revising logic requires using
  logic to evaluate the revision (infinite regress). Revising basic
  arithmetic requires abandoning the conceptual tools needed to track
  resources, measure consequences, or conduct any systematic inquiry.
  These exhibit maximal brittleness-if-removed.

- #strong[Thermodynamics:] The laws of thermodynamics undergird all
  engineering, chemistry, and energy policy. Revising them would
  invalidate centuries of validated applications and require
  reconstructing vast swaths of applied knowledge. The brittleness cost
  is astronomical.

- #strong[Germ Theory:] After decades of successful interventions,
  public health infrastructure, medical training, and pharmaceutical
  development all presuppose germ theory's core claims. Revision would
  collapse these systems, generating catastrophic first-order costs.

#strong[The Paradox Resolved: Fallibilism Without Relativism]

How can we be fallibilists who acknowledge all claims are revisable in
principle, while simultaneously treating hard core propositions as
effectively unrevisable in practice? The resolution: "revisable in
principle" means if we encountered sufficient pragmatic pushback, we
would revise even hard core claims. For hard core propositions, the
threshold is extraordinarily high but not infinitely high. This makes
the framework naturalistic rather than foundationalist. Hard core status
is functional achievement, not metaphysical bedrock.

Truth is not discovered in a Platonic realm but achieved through
historical filtering. Propositions become true by surviving systematic
application without generating brittleness, migrating from peripheral
hypotheses to core infrastructure, becoming functionally indispensable
to ongoing inquiry, and aligning with the emergent,
constraint-determined Apex Network.

This resolves the classical tension between Quine's holism (all claims
are revisable) and the practical unrevisability of core principles: both
describe different aspects of the same evolutionary process through
which propositions earn their status by proving their viability under
relentless pragmatic pressure.

=== 4.3.2 Why Quine's Architecture Matters: Systems-Level Implications
<why-quines-architecture-matters-systems-level-implications>
#strong[Describing Quine's Architecture: Systems-Level Implications]

Quine's "Web of Belief" (Quine 1951, 1960) provides the essential
architecture for our framework: holistic structure, pragmatic revision
in response to recalcitrant experience, and overlapping coordination
across multiple agents. The Quinean architecture is not merely
compatible with our framework but essential to it---systemic
brittleness, pragmatic pushback, and convergence all require this
structure. Our contribution is to describe the observable patterns that
emerge when this Quinean process operates at scale across public
knowledge systems, and to provide formalized diagnostics for assessing
the structural health these processes generate.

First, Quine's recalcitrant experience, aggregated across populations
and time, generates observable patterns of systemic cost. These
accumulated costs---what we track as brittleness---provide the
externalist filter grounding knowledge in mind-independent reality. This
relentless, non-discursive feedback prevents knowledge systems from
floating free of constraints.

Second, Quine's pragmatic revision mechanism, operating through bounded
rationality (March 1978), explains entrenchment: propositions migrate to
the core because they have demonstrated immense value in lowering
systemic brittleness. This functions as systemic caching---proven
principles are preserved to avoid re-derivation costs. Conservation of
Energy became entrenched after proving indispensable across domains, its
revision now prohibitively expensive.

These patterns, already present in Quine's framework, become diagnosable
through brittleness metrics. The framework provides tools to assess how
resilient cores form through systematic, externally-validated selection
(Carlson 2015). In doing so, it resolves a documented tension in Quine's
thought between truth as immanent to our best theory and truth as a
transcendent regulative ideal (Tauriainen 2017). Our three-level
framework shows these are not contradictory but two necessary components
of a naturalistic epistemology. Core principles achieve Level 2 warrant
through systematic pragmatic filtering, while the Apex Network (Level 1)
functions as the regulative structure toward which theories converge
through constraint-driven selection.

#strong[The Individual Revision Mechanism: Conscious Awareness as
Feedback Interface]

While the above describes systems-level patterns, a complete account
requires explaining the micro-level mechanism: how do individual agents
actually revise their personal webs of belief? Quine established that
recalcitrant experience forces adjustment, but left the phenomenology
and strategy selection implicit. Our disposition-based account makes
this process explicit.

As established in Section 2.1, our framework identifies belief with the
conscious awareness of one's dispositions to assent. This awareness
functions as the natural feedback mechanism that enables deliberate
revision. When an agent holds a disposition generating pragmatic costs,
the revision cycle proceeds through several stages. First, dispositional
conflict emerges: the agent's disposition produces failed predictions,
wasted effort, coordination failures, or other measurable costs. Second,
conscious recognition occurs as the agent becomes aware that holding
this disposition correlates with these costs, either through direct
experience or social signaling. Third, this awareness creates
motivational pressure (the discomfort of cognitive dissonance,
frustration at repeated failure, or pragmatic motivation to reduce
costs). Fourth, the agent consciously explores alternative dispositions
compatible with their core web commitments, testing adjustments mentally
or through limited trials. Fifth, through repeated practice and positive
reinforcement, a new disposition stabilizes, replacing the costly
pattern. Finally, the agent's conscious model of their own belief system
updates to reflect this revision, stored in memory for future reference.

This cycle operates at the individual level but drives macro-level
convergence when aggregated across populations. Multiple agents
independently experiencing costs from similar dispositions will
independently revise toward lower-cost alternatives. When these
revisions are communicated through assertion and coordinated through
social exchange, patterns of convergence emerge, not through central
planning or mysterious coordination but through distributed pragmatic
optimization. Each agent, responding to locally experienced costs, makes
adjustments that happen to align with others' adjustments because they
are all responding to the same underlying constraint structure.

The role of memory deserves emphasis. Actual belief systems require
multiple forms of memory operating simultaneously. Dispositional memory
maintains stable patterns of assent over time, preventing constant
drift. Revision memory stores awareness of past adjustments and their
outcomes, enabling learning from history. Cost memory accumulates
experience of which dispositional patterns generate brittleness,
functioning as a pragmatic evaluation metric. And coordination memory
preserves learned patterns of successful social alignment, facilitating
efficient future cooperation.

Conscious awareness of dispositions includes awareness of their history:
we remember not just what we believe but how beliefs have changed and
what costs prompted those changes. This historical awareness enables
belief systems to function as evolving, learning systems. An agent who
remembers that adopting disposition D reduced costs compared to previous
disposition D' is better positioned to make future revisions, creating a
ratcheting effect where successful adjustments are preserved while
failures are discarded.

Why does this matter for convergence on the Apex Network? The Apex is
not a pre-existing target that agents consciously aim for. It is the
emergent structure that must arise when millions of agents, each
consciously aware of their own dispositions and the costs they generate,
independently revise toward lower-brittleness patterns. Convergence is
explained not by mysterious coordination or shared access to truth but
by the simple fact that reality's constraint structure punishes certain
dispositional patterns and rewards others, and conscious agents can
detect and respond to this feedback. The conscious awareness component
enables systematic belief revision rather than random drift, explaining
deliberate adjustment, learning from experience, and the directedness of
convergence toward viability.

Quine's web, understood as this active cognitive system capable of
deliberate self-modification, provides the foundation for our
systems-level analysis. Pragmatic pushback is not an abstract force but
is experienced by individuals as the costs of holding unviable
dispositions, motivating the revision process that drives knowledge
toward the Apex Network.

#strong[The Phenomenology of Revision: Emotions as System Diagnostics]

The revision cycle described above has a phenomenological dimension that
makes epistemic processes consciously accessible. Emotions function as
the lived experience of epistemic states: anxiety manifests as high
prediction error---the discomfort when the world persistently fails to
match expectations; frustration signals accumulating costs from
misaligned dispositions; joy or "flow" marks computational closure---the
satisfaction when actions perfectly predict outcomes. From this
perspective, emotions are not noise to be filtered but data about the
structural integrity of our internal models. They are the
phenomenological dashboard of the epistemic engine, making the costs of
misalignment consciously accessible and thereby motivating revision.

#strong[Agency as the Variation Engine]

While the network provides the selection mechanism through pragmatic
filtering, the individual agent is the source of variation. 'Free will,'
in this framework, is the capacity to generate novel functional
propositions---heresies that challenge current consensus. The 'genius'
or 'reformer' is the agent willing to propose a new, potentially
lower-brittleness predicate and bear the high social cost of being an
early adopter before parallel discovery forces convergence. This
recovers individual agency within a systems framework: the constraint
landscape determines what survives, but individuals determine what gets
tested. Innovation is not centrally planned but emerges from distributed
experimentation, with reality serving as the ultimate arbiter of
viability.

This individual revision cycle is the micro-engine of macro-level
convergence. The process is not centrally coordinated, nor does it
require that agents aim for a shared truth. Rather, convergence emerges
as a result of distributed pragmatic optimization. Millions of agents,
each independently experiencing and seeking to reduce the costs
generated by their own unviable dispositions, make local adjustments.
Because they are all interacting with the same underlying constraint
structure of reality, their individual solutions are independently
pushed toward the same basin of attraction. The shared constraint
landscape is the coordinating force. Unviable patterns are punished with
costs for everyone who adopts them, while viable patterns are rewarded
with lower costs. Social communication accelerates this process by
allowing agents to learn from the costly experiments of others, but the
fundamental driver of convergence is the uniform selective pressure
exerted by a shared, mind-independent reality.

This three-level truth framework describes the justificatory status of
claims at a given moment. Over historical time, pragmatic filtering
produces a discernible two-zone structure in our evolving knowledge
systems.

=== 4.3.3 On the Nature of Truth in This Framework
<on-the-nature-of-truth-in-this-framework>
This framework adopts a deflationary stance toward truth. To call a
proposition "true" is not to ascribe a substantive property, whether
correspondence, coherence, or otherwise. It is to endorse it, to license
its use in inference, to treat it as something to build upon. "P is
true" does no more than assert P with emphasis.

What is substantive in this framework is the constraint landscape and
the costs it generates. These are real and mind-independent. A brittle
system that collapses under pragmatic pressure does so because it
misaligns with genuine features of reality, not because we collectively
decided to stop believing in it. But "truth" names our endorsement of
what survives pragmatic filtering, not a relation between propositions
and facts that we check.

This dissolves an apparent tension. When we speak of viability under
constraints or describe the Apex Network as the constraint-determined
structure our systems are filtered toward, we are not smuggling in
correspondence theory. The constraint landscape is real. The costs are
real. But the word "true" does not name a correspondence relation to
these realities. It names the status we grant to what survives them.

Consider an analogy: biological fitness is real. The environmental
pressures that select for certain adaptations over others are
mind-independent facts about the world. But to call an organism "fit" is
not to describe a correspondence relation between the organism and some
Platonic fitness form. It is to say the organism survived selection.
Similarly, to call a proposition "true" is to say it survived pragmatic
filtering. The filtering mechanism is real and objective; the predicate
"true" marks what passed through.

Realism, in this framework, applies to constraints. Deflationism applies
to truth. These are compatible: we can be realists about what selects
without being correspondence theorists about what "true" means.

This also clarifies the status of the Apex Network. It is not a realm of
abstract propositions awaiting discovery. It is the emergent
configuration of viable functional structures that any knowledge system
operating under shared constraints must converge toward. Different
communities may articulate these structures using different
vocabularies, different formalisms, even different ontologies. What
makes them converge on the "same" network is not that they are matching
a pre-existing propositional template but that they are being shaped by
the same constraint topology. The Apex Network is real in the way an
evolutionary attractor basin is real: not as a thing but as a pattern
that constraint-driven processes inevitably produce.

This deflationary approach also explains why the framework can remain
neutral on classic debates about the nature of truth while still
providing robust epistemic guidance. We need not settle whether truth is
correspondence or coherence or pragmatic utility to diagnose that a
system accumulating high brittleness is becoming epistemically
unreliable. The costs are the signal. Truth-talk is how we mark what
made it through the filter.

=== 4.3.4 Constraint Fields and Attractor Basins
<constraint-fields-and-attractor-basins>
When we speak of "alignment with constraint structure" or "viability
under constraints," we are not invoking correspondence theory's picture
of beliefs being checked against reality. Instead, we are describing how
constraint structure functions as a #strong[field] that generates
#strong[attractor basins];.

Think of constraint structure not as a fixed template to be matched, but
as a force field. Just as gravity creates attractor basins in physical
space (valleys where objects naturally accumulate), pragmatic
constraints create attractor basins in possibility space. Knowledge
systems that impose lower operational costs (lower $P \( t \)$,
$C \( t \)$, $M \( t \)$) occupy deeper basins. Systems that impose
higher costs occupy shallower basins or unstable equilibria.

"Alignment" in this framework means: the system has settled into a
stable attractor basin under the constraint field. "Misalignment" means:
the system occupies an unstable configuration that will be displaced
when sufficient perturbation occurs.

There is no separate "checking" operation where we compare our beliefs
to reality. The constraint field is reality's causal structure, and
belief systems naturally migrate toward viable configurations through
pragmatic selection. Failed predictions, rising maintenance costs, and
institutional friction are all manifestations of the field's pressure
pushing the system out of an unstable basin.

The Apex Network is not a Platonic form to be approximated. It is the
deepest, most stable attractor basin in the constraint field---the
configuration that multiple independent systems converge toward because
it imposes minimum operational costs under the full range of
constraints.

This is deflationary: we are not ascribing a substantive "truth"
property to beliefs that align with reality. We are simply observing
that certain configurations survive pragmatic filtering while others do
not. The survivors define the attractor basin we call the Apex Network.

=== 4.4 The Evolving Structure of Knowledge: Convergent Core and Pluralist Frontier
<the-evolving-structure-of-knowledge-convergent-core-and-pluralist-frontier>
The historical process of pragmatic filtering gives our evolving
#strong[Consensus Networks] a discernible structure, which can be
understood as having two distinct epistemic zones. This distinction is
not about the nature of reality itself, but describes the justificatory
status of our claims at a given time.

==== 4.4.1 The Convergent Core
<the-convergent-core>
This represents the load-bearing foundations of our current knowledge.
It comprises domains where the relentless pressure of pragmatic
selection has eliminated all known rival formulations, leaving a single,
or functionally identical, set of low-brittleness principles. Principles
reside in this core, such as the laws of thermodynamics or the germ
theory of disease, not because they are dogmatically held or
self-evident but because all tested alternatives have been relegated to
the #strong[Negative Canon] after generating catastrophically high
systemic costs. While no claim is immune to revision in principle, the
principles in the #strong[Convergent Core] are functionally unrevisable
in practice, as doing so would require dismantling the most successful
and resilient knowledge structures we have ever built. A claim from this
core achieves the highest degree of justification we can assign,
approaching our standard for Objective Truth (Level 1). For example, the
#strong[Standing Predicate] `...is a pathogen` is now functionally
indispensable, having replaced brittle pre-scientific alternatives.

==== 4.4.2 The Pluralist Frontier: Multiple Peaks, Shared Structure
<the-pluralist-frontier-multiple-peaks-shared-structure>
A central objection to any convergence thesis is underdetermination:
multiple theories may be compatible with all available evidence. This
framework accepts underdetermination as genuine pluralism. Where
pragmatic constraints underdetermine solutions, multiple stable
configurations can survive filtering. We should expect persistent
theoretical diversity, not forced convergence. But this does not entail
relativism. If viable systems share any propositions at all, a maximally
shared subset exists by definition. The substantive question is not
whether an intersection exists, but how thick it is.

The most useful image is a mountain range rather than a single peak.
Rival theories can occupy different peaks while standing on the same
geological base. The Apex Network is that shared base and ridge
structure: the load-bearing commitments that any viable peak must
inherit. In tightly constrained domains, this shared structure should be
substantial. In weakly constrained domains, it may remain thin for long
periods. Both outcomes are compatible with the framework.

Analogy: eyes evolved independently over forty times across the animal
kingdom. Many designs work. Compound eyes, camera eyes, pinhole eyes.
These are distinct local peaks, different solutions to the problem of
vision. But every visual system must solve the same problems: focusing
light, detecting wavelengths, transmitting signals to a processing
center. The "shared structure" is not one correct design. It is what any
solution to those constraints must include. This pluralism is not
relativism because all viable peaks share necessary
constraint-determined structure and are sharply distinguished from
demonstrably failed systems in the Negative Canon.

A reviewer may worry: if multiple local peaks exist, and we are
"trapped" on a local peak rather than the global maximum, doesn't this
vindicate the isolation objection? Are we simply coherent within our
peak while other peaks are equally coherent within theirs?

No, for three reasons:

#strong[First, constraint sharing:] All viable peaks must share
load-bearing structural elements required by the constraints. The eye
evolution analogy makes this concrete: forty independent solutions to
vision exist, but all share necessary features (light-focusing
mechanism, photoreceptor array, signal transmission pathway). "Multiple
peaks" does not mean "arbitrary diversity." It means: constraint
structure permits variation within boundaries, but strictly rules out
configurations that violate necessary requirements.

#strong[Second, objective ranking:] Local peaks are not epistemically
symmetric. They are ranked by measurable brittleness. A knowledge system
trapped on a high-brittleness local peak will exhibit rising
$P \( t \)$, $C \( t \)$, and $M \( t \)$ relative to systems on
low-brittleness peaks. This cost asymmetry is objective---it manifests
as publication patterns, resource allocation, institutional
friction---and provides asymmetric warrant. We are not "equally
justified" within our respective peaks.

#strong[Third, attractor basin depth:] The "global peak" metaphor
misleads. Constraint fields do not always have a single global maximum.
They have attractor basins of varying depth. What matters is not whether
we are on the global peak, but whether we are in a deep, stable basin
versus a shallow, unstable one. Brittleness metrics diagnose basin
depth. Systems in deep basins (low brittleness) earn higher warrant than
systems in shallow basins (high brittleness), even if both are
"coherent" internally.

This yields constrained pluralism, not relativism: where constraints
permit multiple stable solutions, we accept genuine diversity. But that
diversity is bounded by shared structural requirements and ranked by
objective performance metrics.

An important qualification concerns apparent ties. If two frontier peaks
exhibit comparable brittleness over the currently tested range, the
framework does not declare one "more true" by stipulation. It assigns
provisional warrant parity at Level 2. The proper response is
experimental and methodological: design new tests at the edges where the
systems diverge, expand the problem-space, and track which framework
absorbs new pressure with lower additional cost.

The Pluralist Frontier therefore describes domains where multiple
low-brittleness configurations coexist. Different interpretations of
quantum mechanics, for instance, may occupy different peaks while
sharing the mathematical structure and predictive apparatus that any
viable quantum theory requires. Their deepest disagreement is real, but
it sits at the frontier, not in the shared predictive core. Competing
models of consciousness may differ in ontological commitments while
converging on the functional architecture that any adequate theory must
capture. Pluralism about the periphery is compatible with convergence on
the core.

#strong[Against Relativism: The Constrained Pluralism Argument]

The framework's acceptance of multiple viable configurations invites
relativism charges. Here is the response in compressed form:

+ #strong[Relativism claim (thin version):] If internal coherence were
  the only standard, rival coherent systems would be epistemically
  symmetric.

+ #strong[Constraint reply:] In practice, coherent systems face
  asymmetric pressure. Some require escalating ad hoc repair and
  insulation to remain operational, while others remain resilient and
  extend smoothly to new domains.

+ #strong[Cost asymmetry is not constituted by agreement:] The costs
  that diagnose brittleness (maintenance burden, coercive overhead,
  foreclosed opportunities) are objective resource drains imposed by
  misalignment with constraints, not determined by social consensus.

+ #strong[Negative Canon provides boundary:] The space of viable
  theories is not "anything goes" but strictly bounded by what has
  demonstrably failed. Phlogiston chemistry is not a "viable contender"
  but a relegated failure in the Negative Canon.

+ #strong[Pluralism within constraints:] Multiple peaks can exist where
  constraints underdetermine solutions (e.g., QM interpretations), but
  all must share necessary load-bearing structure.

+ #strong[Conclusion:] This yields constrained pluralism rather than
  symmetry-based relativism. Rival systems are ranked by objective,
  measurable performance under shared constraints.

This constrained pluralism must be sharply distinguished from
full-blooded relativism. An immediate worry: if justification is
relative to a Consensus Network, and multiple networks exist, does this
collapse into "different frameworks, different truths"?

No.~The framework implies that networks are not epistemically equal.
Their authority is earned differentially through historical performance.
A network with low brittleness, successful interventions across domains,
and minimal coercive overhead has earned more epistemic authority than
one requiring constant patching, coercive enforcement, and ad hoc
modification.

Disagreement between networks is therefore not symmetric. When a
low-brittleness network (modern epidemiology) conflicts with a
high-brittleness network such as miasma theory circa 1850, we do not
shrug and say "different frameworks, different truths." We say: one has
earned the right to our credence through demonstrated viability; the
other has accumulated costs that disqualify it.

This is not relativism but fallibilist externalism. Justification is
relative to a network, but networks are ranked by objective, measurable
performance under shared constraints. The ranking is itself fallible and
revisable. But at any given time, it provides non-arbitrary grounds for
preferring one system's deliverances over another's.

The frontier is not an "anything goes" zone but a highly restricted
space strictly bounded on all sides by the Negative Canon. A system
based on phlogiston is not a "viable contender" on the frontier of
chemistry but a demonstrably failed research program relegated to the
canon of what does not work. This pluralism is therefore a sign of
epistemic underdetermination: a feature of our map's current
limitations, not reality's supposed indifference.

This position resonates with pragmatist accounts of functional pluralism
(Price 1992), which treat different conceptual frameworks as tools whose
legitimacy is determined by their utility within a specific practice.
Within this frontier, the core claims of each viable competing system
can be granted Level 2 warrant. This is also the zone where
non-epistemic factors, such as institutional power or contingent path
dependencies, can play their most significant role, sometimes
artificially constraining the range of options explored or creating
temporary monopolies on what is considered justified.

=== 4.5 Illustrative Cases of Convergence and Brittleness
<illustrative-cases-of-convergence-and-brittleness>
The transition from Newtonian to relativistic physics offers a canonical
example of this framework's diagnostic application. After centuries of
viability, the Newtonian system began to accumulate significant systemic
costs in the late 19th century. These manifested as first-order
predictive failures, such as its inability to account for the perihelion
of Mercury, and as rising conceptual debt in the form of ad-hoc
modifications like the Lorentz-FitzGerald contraction hypothesis. This
accumulating brittleness created what Kuhn (1962) termed a "crisis"
state preceding paradigm shifts. The Einsteinian system proved a more
resilient solution, reducing this conceptual debt and substantially
lowering the systemic costs of inquiry in physics.

A more contemporary case can be found in the recent history of
artificial intelligence, which illustrates how a brittleness assessment
might function in real time. The periodic "AI winters" can be understood
as the collapse of high-brittleness paradigms, such as symbolic AI,
which suffered from a high rate of ad-hoc modification when faced with
novel challenges. While the subsequent deep learning paradigm proved a
low-brittleness solution for many specific tasks, it may now be showing
signs of rising systemic costs. These can be described conceptually as,
for example, potentially unsustainable escalations in computational and
energy resources for marginal performance gains, or an accelerating
research focus on auxiliary, post-hoc modifications rather than on
foundational architectural advances. This situation illustrates the
#strong[Pluralist Frontier] in action, as rival architectures might now
be seen as competing to become the next low-brittleness solution.

=== 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power
<navigating-the-landscape-fitness-traps-path-dependence-and-the-role-of-power>
An evolutionary model of knowledge must account for the complexities of
history, not just an idealized linear progress. The landscape of
viability is not smooth: knowledge systems can become entrenched in
suboptimal but locally stable states, which we term "fitness traps"
(Wright 1932). This section clarifies how the framework incorporates
factors like path dependence and institutional power not as external
exceptions but as core variables that explain these historical dynamics.

The model's claim is not deterministic prediction but probabilistic
analysis: beneath the surface-level contingency historians rightly
emphasize, underlying structural pressures create statistical tendencies
over long timescales. A system accumulating brittleness is not fated to
collapse on a specific date but becomes progressively more vulnerable to
contingent shocks. The model thus complements historical explanation by
offering tools to understand why some systems prove more resilient than
others.

A system can become locked into a high-brittleness fitness trap by
coercive institutions or other path-dependent factors. A slave economy,
for instance, is a classic example. While objectively brittle in the
long run, it creates institutional structures that make escaping the
trap prohibitively costly in the short term (Acemoglu and Robinson
2012). The framework's key insight is that the exercise of power does
not negate a system's brittleness; rather, the costs of maintaining that
power become a primary indicator of it. This power manifests in two
interrelated ways. First is its defensive role: the immense coercive
overheads required to suppress dissent and manage internal friction are
a direct measure of the energy a system must expend to resist the
structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the
epistemic landscape itself. Powerful institutions do not merely respond
to brittleness defensively; they can construct and enforce the very
parameters of a fitness trap. By controlling research funding, defining
legitimate problems, and entrenching path dependencies, institutional
power can lock a system into a high-brittleness state. This is not an
unmeasurable phenomenon; the costs of this constitutive power manifest
as diagnosable symptoms, such as suppressed innovation (a low I(t)
ratio) and the need for escalating ideological enforcement, which
registers as rising Coercive Overheads (C(t)). This pattern of epistemic
capture appears across domains: from tobacco companies suppressing
health research to colonial knowledge systems that extracted Indigenous
insights while denying reciprocal engagement, thereby masking
brittleness through institutional dominance. While this can create a
temporary monopoly on justification, the framework can still diagnose
the system's underlying brittleness. The costs of this constitutive
power often manifest as a lack of adaptability, suppressed innovation,
and a growing inability to solve novel problems that fall outside the
officially sanctioned domain. To detect such hidden brittleness, we can
augment C(t) with sub-metrics for innovation stagnation, tracking lags
in novel applications or cross-domain extensions relative to comparable
systems as proxies for suppressed adaptive capacity. Concretely,
innovation lag can be operationalized as: #strong[I(t) = (Novel
Applications per Unit Time) / (Defensive Publications per Unit Time)];.
When I(t) declines while C(t) remains high, this signals power-induced
rigidity masking underlying brittleness. For example, Lysenkoist biology
in the Soviet Union showed I(t) approaching zero (no cross-domain
applications) while defensive publications proliferated. Over historical
time, even the most entrenched systems face novel shocks, where the
hidden costs of their power-induced rigidity are typically revealed.

The severity of a fitness trap can be conceptually diagnosed. The work
of historians using cliodynamic analysis, for example, is consistent
with this view, suggesting that the ratio of a state's resources
dedicated to coercive control versus productive capacity can serve as a
powerful indicator of systemic fragility. Historical analyses have found
that polities dedicating a disproportionately high and sustained share
of their resources to internal suppression often exhibit a significantly
higher probability of fragmentation when faced with external shocks
(Turchin 2003). This provides a way to conceptually diagnose the depth
of a fitness trap: by tracking the measurable, defensive costs a system
must pay to enforce its power-induced constraints.

Finally, it is necessary to distinguish this high-brittleness fitness
trap from a different state: low-brittleness stagnation. A system can
achieve a locally stable, low-cost equilibrium that is highly resilient
to existing shocks but lacks the mechanisms for generating novel
solutions. A traditional craft perfected for a stable environment but
unable to adapt to a new material, or a scientific paradigm efficient at
solving internal puzzles but resistant to revolutionary change,
exemplifies this state. While not actively accumulating systemic costs,
such a system is vulnerable to a different kind of failure: obsolescence
in the face of a faster-adapting competitor. Diagnosing this condition
requires not only a static assessment of current brittleness but also an
analysis of the system's rate of adaptive innovation. True long-term
viability therefore requires a balance between low-cost stability and
adaptive capacity. This evolutionary perspective completes our reef
chart, not as a finished map, but as an ongoing process of hazard
detection and channel discovery.

=== 4.7 The Dynamics of Paradigm Transition
<the-dynamics-of-paradigm-transition>
The preceding analysis raises a critical question: if the Apex Network
represents a more viable configuration, why do high-brittleness systems
often persist for centuries before collapsing? Why do we observe
punctuated equilibrium in the history of knowledge systems rather than
smooth, continuous adaptation toward lower-brittleness states?

The answer lies in recognizing that knowledge systems are not merely
abstract webs of propositions but material and institutional
investments. A high-brittleness system generates ongoing costs, as
documented through our brittleness metrics. However, abandoning that
system requires a massive upfront expenditure: textbooks must be
rewritten, practitioners retrained, institutional hierarchies
reorganized, and established techniques replaced. This creates two
distinct cost streams that determine system stability.

The first is the cost of maintenance: the daily, accumulating
expenditure required to keep a brittle system functioning. This includes
the resources devoted to patching anomalies (rising $P \( t \)$), the
coercive overhead needed to suppress dissent (rising $C \( t \)$), and
the escalating complexity required to maintain performance (rising
$M \( t \)$). These costs compound over time as each patch generates new
anomalies requiring further patches.

The second is the cost of transition: the catastrophic, one-time
expenditure required to abandon the existing system and reorganize
around a rival configuration. This is not merely intellectual but
material and social. Consider the transition from Ptolemaic to
Copernican astronomy: it required not just accepting new equations but
reorganizing the entire infrastructure of astronomical practice, from
observatory equipment to pedagogical curricula to theological
frameworks. The perceived magnitude of this reorganization cost creates
resistance to paradigm shifts even when the existing system is
demonstrably brittle.

A system remains stable, despite accumulating brittleness, so long as
the perceived transition cost exceeds the burden of ongoing maintenance.
This explains the persistence of fitness traps discussed in Section 4.6:
agents within the system can recognize rising brittleness yet rationally
judge that the catastrophic cost of reorganization outweighs the
incremental pain of continued patching. The system enters a phase where
brittleness compounds but institutional structures remain locked in
place.

However, this stability is not permanent. As maintenance costs
accumulate, they eventually approach and then exceed the one-time cost
of transition. This is the tipping point: the moment when the daily
energetic expenditure required to suppress reality's pushback becomes
more burdensome than the catastrophic reorganization. At this point, the
system becomes vulnerable to paradigm shift.

This dynamic generates the punctuated equilibrium pattern observed
throughout the history of science and institutions. We see long periods
of stability during which a system accumulates brittleness through
patches and coercive enforcement, followed by relatively sudden ruptures
where the system undergoes rapid reorganization. The transition is
rarely smooth because the coordination problem is severe: abandoning an
established system requires collective action, and agents must
coordinate their shift despite the immediate costs each will bear.

The role of external shocks in this process is clarificatory rather than
causal. A crisis such as war, plague, or environmental collapse does not
create the brittleness; it reveals it by suddenly spiking the
maintenance costs. A system already near its tipping point, managing
high brittleness through costly suppression, can be pushed past the
threshold by a shock that makes the daily cost of maintenance suddenly
untenable. This is why brittle systems are vulnerable to contingent
events that more resilient systems absorb without regime change.

This framework also clarifies the role of what historians might term
"early adopters" or "intellectual pioneers." These are agents who
perceive that the accumulated burden of maintenance has already exceeded
the cost of transition before this becomes consensus. By bearing the
immediate social and material costs of heresy, they function as
nucleation points for the new paradigm, demonstrating its viability and
thereby reducing the perceived transition cost for others. Their actions
are not irrational but reflect a different assessment of when the
crossing point has been reached.

This account complements the fitness trap analysis in Section 4.6 by
explaining not just why systems become locked but what unlocks them. The
interplay between accumulating maintenance costs and the threshold of
transition costs provides a naturalistic explanation for the temporal
dynamics of paradigm shifts, grounded in the same brittleness metrics
that diagnose system health. It is not a deterministic prediction of
when shifts will occur but a framework for understanding why they occur
when they do.

#strong[Qualification:] This mechanism explains one pathway to
revolutionary science---the crisis-driven pathway Kuhn emphasized, where
accumulating anomalies and rising theoretical costs create pressure for
paradigm shift. It does not claim to explain all scientific revolutions.

Some revolutions occur not through accumulated brittleness but through
sudden expansion of possibility space. The double helix discovery
(Watson & Crick 1953) revolutionized genetics not because classical
Mendelian genetics was brittle, but because molecular biology opened
entirely new investigative domains. Classical genetics remained viable
within its scope; the revolution consisted in recognizing that genetic
mechanisms could be studied at a more fundamental level.

Our framework addresses crisis-driven revolutions (phlogiston to oxygen,
Ptolemaic to Copernican) where rising operational costs force
replacement. It does not claim that brittleness is the only mechanism
for paradigm shift. Possibility-expanding revolutions like the double
helix represent a different dynamic: not replacement due to fragility,
but augmentation through new access to causal detail.

In both cases, however, the Negative Canon plays a crucial role. Even
possibility-expanding revolutions are constrained by what has
demonstrably failed. The molecular revolution still had to respect the
inheritance patterns Mendel discovered---it explained them at a deeper
level rather than replacing them.

== 5. Applications: Mathematics as a Paradigm Case of Internal Brittleness
<applications-mathematics-as-a-paradigm-case-of-internal-brittleness>
The account thus far has focused on domains where pragmatic pushback
comes from external reality: empirical predictions fail, technologies
malfunction, societies collapse. This naturally raises a challenge: does
the framework apply only to empirically testable domains, or can it
illuminate abstract knowledge systems like mathematics and logic? This
section argues that it can. By examining mathematics, we can demonstrate
the framework's generality, showing how the logic of brittleness
operates through purely internal constraints of efficiency, consistency,
and explanatory power.

#strong[The Core Insight:] Mathematical frameworks face pragmatic
pushback through internal inefficiency rather than external
falsification.

=== 5.1 The Logic of Internal Brittleness
<the-logic-of-internal-brittleness>
While mathematical frameworks cannot face direct empirical
falsification, they experience pragmatic pushback through accumulated
internal costs that render them unworkable. These costs manifest through
our standard brittleness indicators, adapted for abstract domains:

#strong[M(t): Proof Complexity Escalation] - Increasing proof length
without proportional explanatory gain - Measured as: average proof
length for theorems of comparable scope over time - Rising M(t) signals
a degenerating research program where increasing effort yields
diminishing insight

#strong[P(t): Conceptual Debt Accumulation (proxied by Axiom
Proliferation)] - Ad-hoc modifications to patch paradoxes or anomalies -
Measured as: increasing share of new axioms devoted to resolving
contradictions vs.~generating novel theorems - High P(t) indicates
mounting conceptual debt from defensive modifications

#strong[C(t): Contradiction Suppression Costs] - Resources devoted to
preventing or managing paradoxes - Measured as: proportion of research
addressing known anomalies vs.~extending theory - High C(t) reveals
underlying fragility requiring constant maintenance

#strong[R(t): Unification Power] - Ability to integrate diverse
mathematical domains under common framework - Measured as: breadth of
cross-domain applicability - Declining R(t) signals fragmentation and
loss of coherence

The abstract costs in mathematics can be operationalized using our
diagnostic toolkit, demonstrating the framework's universality across
domains where feedback is entirely internal to the system.

=== 5.2 Case Study: Brittleness Reduction in Mathematical Foundations
<case-study-brittleness-reduction-in-mathematical-foundations>
To illustrate these metrics in action, consider examples of mathematical
progress as brittleness reduction across different domains:

#strong[Non-Euclidean Geometry:] - Euclidean geometry exhibited high
brittleness for curved space applications - Required elaborate patches
(like epicycles in astronomy) to explain planetary motion -
Non-Euclidean alternatives demonstrated lower brittleness for cosmology
and general relativity - #strong[Pattern:] Replace high-brittleness
framework with lower-brittleness alternative when problem domain expands

#strong[Calculus Foundations:] - Infinitesimals were intuitive but
theoretically brittle, generating paradoxes of the infinite -
Epsilon-delta formalism demanded higher initial complexity but delivered
lower long-term brittleness - Historical adoption pattern follows
brittleness reduction trajectory - Demonstrates how short-term
complexity increase can yield long-term stability gains

#strong[Category Theory:] - More abstract and initially more complex
than set theory - But demonstrates lower brittleness for certain domains
(algebraic topology, theoretical computer science) - Adoption follows
domain-specific viability assessment - Shows how optimal framework
varies by application domain

Nowhere is this dynamic clearer than in the response to Russell's
Paradox, which provides a paradigm case of catastrophic brittleness and
competing resolution strategies.

#strong[Naive Set Theory (pre-1901):] - M(t): Moderate (proofs
reasonably concise for most theorems) - R(t): Exceptional (successfully
unified logic, number theory, and analysis under a single framework) -
Appeared to exhibit low brittleness across all indicators - Provided an
elegant foundation for mathematics

#strong[Russell's Paradox (Russell 1903):] - Revealed infinite
brittleness: the theory could derive a direct contradiction - Considered
the set R = {x | x ∉ x}. Is R ∈ R? Both yes and no follow from the
axioms - All inference paralyzed (if both A and ¬A are derivable, the
principle of explosion allows derivation of anything) - Complete
systemic collapse: the framework became unusable for rigorous
mathematics - This wasn't a peripheral anomaly but a catastrophic
failure at the system's foundation

#strong[Response 1: ZF Set Theory (Zermelo-Fraenkel + Axiom of Choice)]
\- Added carefully chosen axioms (Replacement, Foundation, Separation)
to block the paradox - M(t): Increased (more axioms create more complex
proof requirements) - P(t): Moderate (new axioms serve multiple purposes
beyond merely patching the paradox) - C(t): Low (paradox completely
resolved, no ongoing suppression or management needed) - R(t): High
(maintained most of naive set theory's unifying power across
mathematical domains) - #strong[Diagnosis:] Successful low-brittleness
resolution through principled modification - The additional complexity
was justified by restored foundational stability

#strong[Response 2: Type Theory (Russell/Whitehead)] - Introduced
stratified hierarchy that structurally prevents problematic
self-reference - M(t): High (complicated type restrictions make many
proofs substantially longer) - P(t): Low (structural solution rather
than ad-hoc patch) - C(t): Low (paradox is structurally impossible
within the system) - R(t): Moderate (some mathematical domains resist
natural formulation within type hierarchies) - #strong[Diagnosis:]
Alternative low-brittleness solution with different trade-offs -
Sacrifices some unification power for structural guarantees against
contradiction

#strong[Response 3: Paraconsistent Logic] - Accepts contradictions as
potentially derivable but attempts to control "explosion" - M(t):
Variable (depends on specific implementation details) - P(t): Very High
(requires many special rules and restrictions to prevent inferential
collapse) - C(t): Very High (demands constant management of
contradictions and their containment) - R(t): Low (marginal adoption,
limited to specialized domains) - #strong[Diagnosis:] A
higher-brittleness resolution, as it requires sustained, complex, and
costly mechanisms to manage contradictions rather than eliminating them
\- The system exhibits sustained high maintenance costs without
corresponding payoffs

#strong[Historical Outcome:] The mathematical community converged
primarily on ZF set theory as the standard foundation, with Type Theory
adopted for specific domains where its structural guarantees prove
valuable (such as computer science and constructive mathematics).
Paraconsistent approaches remain peripheral. This convergence reflects
differential brittleness among the alternatives, not arbitrary
historical preference or mere convention. The outcome demonstrates how
pragmatic selection operates in purely abstract domains through internal
efficiency rather than external empirical testing.

=== 5.3 Power, Suppression, and the Hard Core
<power-suppression-and-the-hard-core>
Engaging with insights from feminist epistemology (Harding 1991), we can
see that even mathematics is not immune to power dynamics that generate
brittleness. When a dominant mathematical community uses institutional
power to suppress alternative approaches, this incurs measurable
Coercive Overheads (C(t)):

#strong[Mechanisms of Mathematical Suppression:] - Career punishment for
heterodox approaches to foundations or proof methods - Publication
barriers for alternative mathematical frameworks - Curriculum
monopolization by dominant approaches - Citation exclusion of rival
methodologies

#strong[Measurable Costs:] - #strong[Innovation lag:] Talented
mathematicians driven from the field when their approaches are rejected
for sociological rather than technical reasons - #strong[Fragmentation:]
Splinter communities forming alternative journals and departments -
#strong[Inefficiency:] Duplication of effort as alternative approaches
cannot build on dominant framework results - #strong[Delayed
discoveries:] Useful insights suppressed for decades (e.g., non-standard
analysis resisted despite valuable applications)

#strong[The Brittleness Signal:] When a mathematical community requires
high coercive costs to maintain orthodoxy against persistent
alternatives, this signals underlying brittleness: the dominant
framework may not be optimally viable.

#strong[Historical Example: Intuitionist vs.~Classical Mathematics] -
Intuitionists demonstrated genuine technical alternatives with different
foundational commitments - Classical community initially suppressed
through institutional power (career barriers, publication difficulties)
\- High coercive costs required to maintain dominance - Eventual
accommodation as constructive methods proved valuable in computer
science and proof theory - #strong[Diagnosis:] Initial suppression
revealed brittleness in classical community's claim to unique optimality

#strong[Why Logic Occupies the Core]

Logic isn't metaphysically privileged; it's functionally indispensable.

#strong[The Entrenchment Argument:] 1. Revising logic requires using
logic to assess the revision 2. This creates infinite regress or
circularity 3. Therefore logic exhibits infinite brittleness if removed
\4. Systems under bounded rationality (March 1978) must treat such
maximal-cost revisions as core

#strong[This is pragmatic necessity, not a priori truth:] - Logic could
theoretically be revised if we encountered genuine pragmatic pressure
sufficient to justify the cost - Some quantum logics represent such
domain-specific revisions - But the cost threshold is exceptionally
high: logic underpins all systematic reasoning - Most "apparent" logic
violations turn out to be scope restrictions rather than genuine
revisions of core principles

=== 5.4 The General Principle: Mathematics as Pure Pragmatic Selection
<the-general-principle-mathematics-as-pure-pragmatic-selection>
Mathematics demonstrates the framework applies beyond empirical domains.
All domains face pragmatic selection, though the feedback mechanism
varies: external prediction failure for physics, social collapse for
politics, internal inefficiency for mathematics. The underlying
principle is analogous: brittle systems accumulate costs that drive
replacement by more viable alternatives. The costs differ by domain, but
the selection logic remains.

Mathematics is not a special case requiring different epistemology; it's
a pure case showing how pragmatic selection operates when feedback is
entirely internal to the system. The convergence on ZF set theory, the
accommodation of intuitionist insights, and the adoption of non-standard
analysis where it proves useful all demonstrate the same evolutionary
dynamics at work in physical science, but operating through internal
efficiency rather than external empirical testing. This universality
strengthens the framework's claim that objective knowledge arises from
pragmatic filtering across all domains of inquiry.

Thus, mathematics, far from being a counterexample to a naturalistic
epistemology, serves as its purest illustration, demonstrating that the
logic of brittleness reduction operates universally, guided by the
selective pressures of internal coherence and efficiency.

Having demonstrated how brittleness diagnostics apply even to abstract
domains like mathematics, we now situate EPC within broader
epistemological debates.

== 6. Situating the Framework in Contemporary Debates
<situating-the-framework-in-contemporary-debates>
This paper has developed what can be termed #strong[Systemic
Externalism];: a form of externalist epistemology that locates
justification not in individual cognitive processes but in the
demonstrated reliability of entire knowledge systems. This section
clarifies the framework's position within contemporary epistemology by
examining its relationship to four major research programs: coherentist
epistemology, social epistemology, evolutionary epistemology, and
neopragmatism.

=== 6.1 A Grounded Coherentism and a Naturalized Structural Realism
<a-grounded-coherentism-and-a-naturalized-structural-realism>
While internalist coherentists like Carlson (2015) have successfully
shown that the web must have a functionally indispensable core, they
lack resources to explain why that core is forged by external
discipline. Systemic Externalism provides this missing causal engine,
grounding Carlson's internal structure in an externalist history of
pragmatic selection.

Justification requires coherence plus network authority earned through
external performance. Coherence is necessary: a belief lacking
integration with a broader inferential system lacks the connections
required for application and evaluation. But coherence is not
sufficient. A network's authority must be earned, not stipulated. It is
earned through demonstrated low brittleness, successful interventions
across domains, minimal coercive overhead, and sustained adaptability to
novel problems.

This dual requirement distinguishes our position sharply from both
classical coherentism and purely process-based externalisms. Against
classical coherentism: a perfectly coherent phlogiston network or
flat-earth cosmology provides no justification because the network
itself has failed the externalist test. Against process reliabilism: we
do not assess individual cognitive mechanisms but the historical track
record of entire public knowledge systems.

Unlike Zollman's (2007, 2013) static network models and Rosenstock et
al.~(2017), EPC examines evolving networks under pushback, extending
ECHO's harmony principle with external brittleness filters Thagard's
internalism lacks. The coherence structure tells us which beliefs hang
together. The external performance record tells us whether that coherent
structure deserves our credence.

==== 6.1.1 Relationship to Structural Realism
<relationship-to-structural-realism>
Structural realism comes in two forms. Epistemic Structural Realism
(Worrall 1989) holds that we can only know the structure of the world,
not the intrinsic nature of entities. Ontic Structural Realism (Ladyman
and Ross 2007) holds that structure is all there is: relations are
ontologically prior to relata.

Our framework is neither, because we are answering a different question.

Structural realism asks: What can we know? (ESR) or What exists? (OSR).
These are epistemological and metaphysical questions respectively.

We ask: What makes beliefs justified? This is a question about warrant,
not about the furniture of the world or the limits of knowledge.

Our answer: beliefs are justified by their role in a web that has earned
authority through pragmatic success. That the successful webs tend to
share structure is an empirical observation about what survives
filtering, not a thesis about the intrinsic nature of reality.

We are therefore compatible with structural realism but not identical to
it. A structural realist might adopt our account of justification. An
advocate of our framework might accept or reject structural realism as a
metaphysical thesis. The questions are orthogonal.

That said, our framework does provide what structural realism has long
lacked: a naturalistic engine for convergence on objective structures.
Worrall posited structural continuity across theory change but offered
no mechanism explaining why such continuity should occur. We explain it:
brittle theories fail systematically, low-brittleness ones survive. The
historical record shows systematic elimination of high-brittleness
systems. The convergence toward low-brittleness structures, documented
in the Negative Canon, provides positive inductive grounds for realism
about the objective viability landscape our theories progressively map.

Similarly, while Ontic Structural Realism posits that the world is
fundamentally structural, our framework explains how scientific
practices are forced to converge on these objective structures through
pragmatic filtering. The Apex Network is the complete set of viable
relational structures, an emergent fact about our world's constraint
topology, discovered through pragmatic selection rather than stipulated
by metaphysics.

The relationship is therefore one of mutual support. Structural realism
(especially OSR) provides a plausible metaphysics for what the Apex
Network might be. Our framework provides the missing evolutionary
mechanism explaining how inquiry converges on it. But one can accept our
justification account without committing to structural realist
metaphysics, and one can be a structural realist without adopting our
specific theory of warrant.

==== 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
<distinguishing-systemic-externalism-from-other-externalisms>
Systemic Externalism contrasts with Process Reliabilism (Goldman 1979)
and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates
justification in the reliability of individual cognitive processes;
Systemic Externalism shifts focus to the demonstrated historical
viability of the public knowledge system that certifies the claim.
Virtue Epistemology grounds justification in individual intellectual
virtues; Systemic Externalism attributes resilience and adaptability to
the collective system. Systemic Externalism thus offers macro-level
externalism, complementing these micro-level approaches.

=== 6.2 Reconciling with Quine: From Dispositions to Objective Structures
<reconciling-with-quine-from-dispositions-to-objective-structures>
Our relationship to Quine is one of architectural inheritance rather
than metaphysical commitment. We adopt his holistic,
pragmatically-revised, coordinative web structure as essential
infrastructure for our framework. The specific dispositional semantics,
indeterminacy thesis, and debates about analyticity remain orthogonal to
our core claims about systemic brittleness, pragmatic pushback, and
convergence toward the Apex Network. This section clarifies how we build
on Quine's architecture while remaining agnostic about contested
metaphysical details.

While deeply indebted to Quine, our framework must address the apparent
tension between his austere behaviorism and our seemingly realist
claims. The resolution lies not in departing from Quine, but in building
a multi-level structure upon his foundation.

First, where Quine's indeterminacy thesis seems to preclude testable
propositions, we show how conscious awareness of dispositions allows for
the articulation of specific sentences that serve as functional
propositions. This allows for public assessment and a robust
epistemology without positing the abstract entities Quine rejected.

Second, we provide a naturalistic engine for the convergence that
Quine's underdetermination thesis leaves mysterious. The Apex Network is
not a Platonic realm but an attractor basin in the space of possible
dispositional patterns, determined by real-world pragmatic constraints.
It explains why some webs of belief systematically outperform others
without abandoning naturalism.

Finally, we supplement Quine's micro-level external constraint of
sensory stimulation with a macro-level constraint: systemic cost. A
coherent system can always accommodate anomalous experiences, but it
cannot indefinitely ignore the compounding costs its ad-hoc adjustments
generate. This provides a robust, non-discursive filter that grounds the
entire web of belief against the isolation objection. Our project,
therefore, is not a rejection of Quine but an attempt to complete his
naturalistic turn by integrating it with the dynamics of systems theory
and cultural evolution. Crucially, it supplements his external
constraint of individual sensory stimulation with a novel, macro-level
external constraint: the non-discursive, systemic costs generated by the
web as a whole.

==== 6.2.1 Structural Convergence Is the Only Kind There Could Be
<structural-convergence-is-the-only-kind-there-could-be>
A clarification is necessary here for readers expecting Peircean
convergence on propositional content. Within Quinean architecture, there
are no propositions as abstract entities bearing meanings independent of
their place in the web. Beliefs are dispositions to assent to sentences.
Meaning is exhausted by inferential role. The indeterminacy of
translation thesis holds that there is no fact of the matter about what
a sentence "really means" beyond its behavioral and inferential profile.

This transforms what "convergence" can mean. Peirce imagined inquiry
converging on truth, implicitly on propositional content that matches
reality. But if Quine is right, there is no propositional content over
and above structural relations. The "content" of a belief just is its
place in the web: its connections to stimulus, its inferential liaisons,
its role in guiding action.

Structural convergence is therefore not a weaker or deflationary
substitute for "real" convergence on truth. It is the only kind of
convergence available within a Quinean framework. When we say that
viable webs converge on shared structure, we are not offering less than
Peirce promised. We are specifying what convergence amounts to once we
take Quine's deflationism about meaning seriously.

The Apex Network is not an approximation to a Truth we could in
principle state propositionally. It is the structural configuration
toward which webs must tend under constraint. Within Quinean naturalism,
there is nothing further for them to tend toward.

Consider what this means concretely. When independent scientific
communities converge on shared mathematical structures, experimental
protocols, and predictive frameworks, they are not discovering a
pre-linguistic Platonic form. They are being shaped by the same
pragmatic constraints into functionally equivalent dispositional
patterns. The convergence is real. The constraints are objective. But
what converges is structure: patterns of inference, habits of
prediction, coordinated responses to stimuli.

This also clarifies the status of theoretical disagreement. Rival
interpretations of quantum mechanics may differ in their ontological
stories (wave collapse vs.~many worlds vs.~pilot wave) while sharing
identical mathematical structure and predictive apparatus. From a
Quinean perspective, the shared structure is what matters. The divergent
interpretations are attempts to impose propositional content where only
inferential structure exists. The convergence on structure is
convergence on what is real within this framework.

This is not instrumentalism. The structure is not "just useful." It is
what knowledge is. A Standing Predicate that has migrated to the hard
core of our web is not a convenient fiction. It is a dispositional
pattern so deeply entrenched, so widely validated, so functionally
indispensable, that it has become constitutive of inquiry in its domain.
Its reality is the reality of functional structure, not the reality of
correspondence to abstract propositions.

The framework therefore integrates Quinean naturalism with realism about
structure. We are realists because the constraint landscape is
mind-independent and the convergent patterns it forces are objective.
But we are Quinean because what converges is not propositional content
but dispositional structure. This is the only realism available within
naturalism, and it is realism enough.

=== 6.3 A Realist Corrective to Neopragmatism and Social Epistemology
<a-realist-corrective-to-neopragmatism-and-social-epistemology>
The framework developed here retains pragmatism's anti-foundationalist
spirit and focus on inquiry as a social, problem-solving practice. Its
core ambition aligns with the foundational project of classical
pragmatism: to articulate a non-reductive naturalism that can explain
the emergence of genuine novelty in the world (Baggio and Parravicini
2019). By grounding epistemology in dispositions to assent shaped by
pragmatic feedback (following Quine's call to replace traditional
epistemology with empirical psychology \[Quine 1969\]), we maintain
naturalistic rigor while avoiding the foundationalist trap of positing
privileged mental contents. Our disposition-based account provides
precisely what Quine's naturalized epistemology promised but could not
fully deliver: a bridge from individual cognitive behavior to social
knowledge systems that remains fully naturalistic while accounting for
external constraints.

However, our model offers a crucial corrective to neopragmatist
approaches that are vulnerable to the charge of conflating epistemic
values with mere practical utility (Putnam 2002; Lynch 2009) or reducing
objectivity to social consensus. Thinkers like Rorty (1979) and Brandom
(1994), in their sophisticated accounts of justification as a linguistic
or social practice, lack a robust, non-discursive external constraint.
This leaves them with inadequate resources for handling cases where
entire communities, through well-managed discourse, converge on unviable
beliefs.

Our framework provides this missing external constraint through its
analysis of systemic failure. The collapse of Lysenkoist biology in the
Soviet Union, for instance, was not due to a breakdown in its internal
"game of giving and asking for reasons"; indeed, that discourse was
brutally enforced. Its failure was a matter of catastrophic first-order
costs that no amount of conversational management could prevent. This
focus on pragmatic consequence as a real, external filter allows us to
distinguish our position from other forms of "pragmatic realism."
El-Hani and Pihlström (2002), for example, resolve the emergentist
dilemma by arguing that emergent properties "gain their ontological
status from the practice-laden ontological commitments we make." While
we agree that justification is tied to practice, our model grounds this
process in a more robustly externalist manner. Pragmatic viability is
not the source of objectivity; it is the primary empirical indicator of
a system's alignment with the mind-independent, emergent structure of
the Apex Network.

This leads to a key reframing of the relationship between agreement and
truth. Genuine solidarity is not an alternative to objectivity but an
emergent property of low-brittleness systems that have successfully
adapted to pragmatic constraints. The practical project of cultivating
viable knowledge systems is therefore the most secure path to enduring
agreement. This stands in sharp contrast to any attempt to define truth
as a stable consensus within a closed system, a procedure that our
framework would diagnose as a potential coherence trap lacking the
necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core
insights of #strong[social epistemology] (Goldman 1999; Longino 2002).
Social epistemic procedures like peer review and institutionalized
criticism can reduce systemic brittleness by filtering errors and
forcing claims to survive adversarial scrutiny. However, their
effectiveness depends on surrounding practices: data transparency, code
availability, replication incentives, adversarial diversity. When these
supports fail, peer review becomes unreliable as an error or fraud
detector.

The claim is therefore not that peer review is uniformly truth-conducive
or that existing institutions reliably track viability. The claim is
that structured adversarial scrutiny is one mechanism by which
communities reduce brittleness when properly constituted. Empirical
evidence from the replication crisis (Open Science Collaboration 2015)
shows that peer review without transparency and replication incentives
can perpetuate high-brittleness systems. In fields like biomedical
research, where referees lack access to raw data and processing code,
peer review has proven particularly unreliable as a fraud-detection
mechanism (Begley & Ellis 2012). The framework's contribution is
diagnosing which institutional configurations generate rising costs
(suppression of dissent, barriers to error correction, accumulated
conceptual debt) versus which facilitate brittleness reduction.

This provides the externalist check that purely procedural models can
lack. It also offers an empirical grounding for the central insight of
standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea
that marginalized perspectives can be a privileged source of data about
a system's hidden costs. In our model, marginalized perspectives are not
privileged due to a metaphysical claim about identity, but because they
often function as the most sensitive detectors of a system's First-Order
Costs and hidden Coercive Overheads (C(t)). A system that appears stable
to its beneficiaries may be generating immense, unacknowledged costs for
those at its margins. Their dissent is not automatically correct, but it
is epistemically valuable as a diagnostic of potential brittleness that
dominant networks may be insulating themselves from recognizing.
Suppressing these perspectives is therefore not just a moral failure,
but a critical epistemic failure that allows brittleness to accumulate
undetected. This view of collective knowledge as an emergent, adaptive
process finds resonance in contemporary work on dynamic holism (Sims
2024).

==== Collective Calibration
<collective-calibration>
Empirical models of social epistemic networks (O'Connor and Weatherall
2019) suggest that objectivity is a function of communication topology.
EPC operationalizes this insight: calibration efficiency inversely
correlates with brittleness. The more diverse the error signals
integrated (Longino 1990; Anderson 1996), the more stable the Apex
Network.

==== 6.3.1 Grounding Epistemic Norms in Systemic Viability
<grounding-epistemic-norms-in-systemic-viability>
A standard objection to naturalistic epistemology is that a descriptive
account of how we reason cannot ground a prescriptive account of how we
ought to reason (Kim 1988). As noted above, pragmatist approaches face a
similar charge of conflating epistemic values with merely practical ones
like efficiency or survival (Putnam 2002; Lynch 2009). Our framework
answers this "normativity objection" by grounding its norms not in
chosen values, but in the structural conditions required for any
cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form
of engineering (Moghaddam 2013), where epistemic norms are hypothetical
imperatives directed at a practical goal. Our framework makes this goal
concrete: the cultivation of low-brittleness knowledge systems. This
conditional form is not empty or arbitrary. Inquiry communities do not
freely choose whether to care about memory, transmission, coordination,
and error correction if they are engaged in cumulative inquiry at all.
Those goals are built into the practice's structure. The authority for
this approach rests on two arguments.

Epistemic ought-claims are also scale-dependent. We can ask what a
single inquirer ought to believe, what a specific institution ought to
adopt as procedure, or what a broad public ought to treat as warranted.
In this paper, unless otherwise specified, claims about what "we" ought
to do are claims at the widest practical scale: the maximally shared
standards of cumulative public inquiry.

First, a constitutive argument: any system engaged in a cumulative,
inter-generational project, such as science, must maintain sufficient
stability to preserve and transmit knowledge. A system that
systematically undermines its own persistence cannot, by definition,
succeed at this project. The pressure to maintain a low-brittleness
design is therefore not an optional value but an inescapable structural
constraint on the practice of cumulative inquiry itself.

Second, an instrumental argument: the framework makes a falsifiable,
empirical claim that networks with a high and rising degree of measured
brittleness are statistically more likely to collapse or require radical
revision. From this descriptive claim follows a conditional
recommendation: if an agent or institution has the goal of ensuring its
long-term stability and problem-solving capacity, then it has a
powerful, evidence-based reason to adopt principles that demonstrably
lower its systemic brittleness.

This reframes the paper's normative language. When this model describes
one network as "better" or identifies "epistemic progress," these are
not subjective value judgments but technical descriptions of systemic
performance. A "better" network is one with lower measured brittleness
and thus a higher predicted resilience against failure. Viability is not
an optional norm to be adopted; it is a structural precondition for any
system that manages to become part of the historical record at all.

=== 6.4 Distinguishing Brittleness from Adjacent Frameworks
<distinguishing-brittleness-from-adjacent-frameworks>
The brittleness framework's core innovation is making visible a specific
kind of epistemic signal: the accumulated resource costs that diagnose a
knowledge system's rising vulnerability to failure. This is not
reducible to existing philosophical concepts. The following subsections
answer the five most immediate objections: that brittleness is merely a
relabeling of Lakatosian degeneration, Ockham's simplicity, predictive
failure, sociological description, or Laudan's problem-solving
effectiveness. Each has superficial similarities to brittleness
analysis; each misses the framework's distinctive contribution.

==== 6.4.0 Brittleness and Traditional Epistemic Virtues
<brittleness-and-traditional-epistemic-virtues>
Before distinguishing brittleness from specific adjacent frameworks, we
address a general worry: is systemic brittleness simply a repackaging of
familiar epistemic criteria under new terminology?

The answer is no. Brittleness identifies a dimension of epistemic
assessment that is orthogonal to---and can diverge from---all
traditional epistemic virtues:

#strong[Accuracy.] A knowledge system can be locally accurate while
accumulating high brittleness. As we detail in Section 6.4.3, a theory
can generate correct predictions while requiring escalating ad hoc
modifications to do so. The Ptolemaic system maintained predictive
adequacy for centuries, yet its brittleness manifested not in failed
predictions but in the proliferating machinery needed to generate those
predictions. Accuracy is propositional and synchronic; brittleness is
systemic and diachronic.

#strong[Internal coherence.] A system can be perfectly coherent within
its own conceptual framework yet profoundly misaligned with reality's
constraint structure. Coherence is necessary for knowledge
systems---incoherent systems cannot function---but it is not sufficient.
As we establish in Section 4.3, coherence with a flat-earth network or a
phlogiston network provides no epistemic warrant unless that network has
demonstrated viability through sustained low brittleness. Coherentism
requires external grounding; brittleness provides it.

#strong[Predictive success.] Short-term predictive success can mask
long-term structural fragility. A research program can succeed in its
initial domain while proving brittle when exported to adjacent contexts
or when sustaining that success requires escalating resources.
Predictive success measures output; brittleness measures the
sustainability of the knowledge-generating process itself.

#strong[Simplicity.] This deserves careful treatment (Section 6.4.2).
Brittleness explains why Ockham's Razor succeeds: theories that achieve
compression can generalize to novel cases, while theories approaching
the complexity of their data lose predictive power and become brittle.
But simplicity and brittleness remain orthogonal---a simple theory can
be brittle if it requires constant patching, and a complex theory can
exhibit low brittleness if it unifies vast domains without defensive
modification. Simplicity is synchronic parameter-counting; brittleness
is diachronic cost-tracking.

#strong[The key insight:] Traditional criteria assess the epistemic
status of propositions or theories at a moment in time. Brittleness
assesses the sustainability of knowledge systems as ongoing enterprises
under continued pressure. It adds the missing dimension of maintenance
cost---the resources required to sustain coherence, accuracy, and
predictive success when reality pushes back.

This is not a replacement for traditional criteria but a complement. A
maximally viable knowledge system will exhibit low brittleness AND
traditional virtues. But when they diverge---when a system is accurate
yet brittle, or coherent yet unsustainable---brittleness provides the
diagnostic that reveals hidden fragility before catastrophic failure.

==== 6.4.1 Brittleness vs.~Degenerating Research Programmes
<brittleness-vs.-degenerating-research-programmes>
Lakatos's #emph[Methodology of Scientific Research Programmes] (Lakatos
1970) provides the closest historical precedent for brittleness
analysis. Both frameworks track the proliferation of auxiliary
hypotheses around a protective belt when a research programme faces
anomalies. A degenerating programme, in Lakatos's terms, generates no
novel predictions; its theoretical modifications are purely defensive,
accommodating known anomalies rather than anticipating new phenomena.

The similarity is genuine. But the frameworks operate at different
temporal and epistemic levels.

#strong[Lakatos is retrospective classification.] We determine whether a
programme was degenerating by examining its historical trajectory:
comparing the ratio of novel predictions to defensive modifications
across distinct time periods. Historians can apply Lakatosian categories
after the fact to explain why phlogiston chemistry yielded to
oxygen-based chemistry or why Ptolemaic astronomy gave way to
heliocentrism. The methodology tells us how to score theories once we
have the full historical record.

#strong[Brittleness provides forward-looking diagnosis.] The framework
tracks current resource allocation patterns, coercive overhead, and
complexity-to-performance ratios to signal rising risk before paradigm
shift occurs. When P(t), C(t), and M(t) are all rising in a contemporary
research programme, this diagnoses fragility and predicts increased
vulnerability to displacement, even if we cannot specify what will
replace it. Brittleness makes visible structural problems practitioners
can recognize and potentially address in real time.

The epistemic difference is this: Lakatos tells historians how to
classify past theories. Brittleness tells practitioners which current
systems are becoming unreliable engines for future inquiry. One
evaluates output; the other diagnoses structural health.

This distinction matters for practitioners. A research community can
observe rising conceptual debt $P \( t \)$, escalating coercive overhead
$C \( t \)$ to maintain the programme's authority, and accelerating
model complexity $M \( t \)$ with diminishing returns. These are not
post-hoc classifications but real-time warning signals. The brittleness
framework provides actionable epistemic feedback that Lakatosian
retrospection cannot supply.

#strong[The Causal Engine Made Explicit:]

Lakatos describes the symptoms: proliferating ad-hoc hypotheses,
declining novel predictions, protective belt expansions. Our framework
identifies the underlying causal mechanism that generates these
symptoms:

+ #strong[Initial misalignment:] The research programme's core (hard
  core plus protective belt) has structural features that misalign with
  constraint structure in some domain.
+ #strong[Pragmatic pushback:] This misalignment manifests as failed
  predictions, anomalies, or limited exportability to adjacent domains.
+ #strong[Defensive response:] Rather than revise the hard core (high
  cost), practitioners add auxiliary hypotheses to protective belt
  (lower immediate cost).
+ #strong[Rising maintenance burden:] Each auxiliary patch carries
  ongoing costs: parameters to tune, special cases to track, consistency
  constraints to maintain.
+ #strong[Cost accumulation:] As patches proliferate, $M \( t \)$ rises
  (model complexity), $P \( t \)$ rises (patch velocity), $C \( t \)$
  may rise (institutional resources defending programme).
+ #strong[Diminishing returns:] New predictions become harder to
  generate; each new phenomenon requires new auxiliary structure.
+ #strong[Unsustainability:] Eventually operational costs exceed
  benefits; practitioners migrate to alternative programmes with lower
  cost structure.
+ #strong[Abandonment:] Programme enters Negative Canon as cautionary
  example of structural misalignment.

This is a causal process: misalignment leads to pragmatic pushback,
which triggers defensive patching, which generates rising costs, which
forces abandonment. Lakatos retrospectively identifies stages 3-7 after
the fact. Brittleness metrics ($P \( t \)$, $C \( t \)$, $M \( t \)$)
track stages 4-6 in real-time, providing forward-looking diagnosis.

#strong[Example:] Phlogiston chemistry followed this exact trajectory.
Initial elegance (stages 1-2) led to negative weight patch (stage 3),
which generated proliferating parameters (stage 4), manifesting as
defensive publication patterns (stage 5-6). Lavoisier's oxygen framework
provided lower-cost alternative (stage 7), resulting in phlogiston
abandonment (stage 8).

==== 6.4.2 Why Brittleness Is Not Just Simplicity
<why-brittleness-is-not-just-simplicity>
Ockham's Razor, and the broader tradition of preferring simpler
theories, appears closely related to brittleness analysis. Both penalize
proliferating structure. A theory that requires increasingly elaborate
machinery seems both less simple and more brittle.

But simplicity and brittleness are orthogonal dimensions.

#strong[Ockham's Razor is a heuristic tie-breaker] when theories are
evidentially equivalent. If two theories predict the same phenomena
equally well, prefer the simpler one. The principle operates
synchronically: it assesses theories at a single time slice, comparing
their structural economy. Simplicity is measured by parameter counts,
ontological commitments, or other static complexity metrics.

#strong[Brittleness tracks cost dynamics over time.] The framework
assesses the diachronic trajectory: is the system's maintenance burden
rising or falling? Are modifications integrating smoothly or generating
compounding debt? A theory can be simple yet brittle if it starts
elegant but accumulates patches rapidly. Conversely, a theory can be
complex yet resilient if its high parameter count reflects genuine
structural requirements rather than defensive elaboration.

Consider contrasting cases. Geocentric astronomy began simple (celestial
spheres rotating around Earth) but accumulated brittleness as epicycles
proliferated. The Standard Model in particle physics has high parameter
count (dozens of fundamental constants) yet demonstrates low
brittleness: its complexity reflects discovered structure, not defensive
patching. Novel predictions extend smoothly; anomalies do not require ad
hoc machinery; exportability to new energy scales proceeds without
fundamental revision.

The key difference: #strong[Simplicity is a synchronic property;
brittleness is a diachronic process.] We are not measuring absolute
complexity at time t. We are tracking the rate of complexity increase
relative to marginal performance gains. When the description length of
auxiliary exception-handling machinery begins to exceed the description
length of core explanatory principles, brittleness is rising regardless
of whether the system is objectively "simple."

This connects to principled complexity measures---with crucial
qualifications. Minimum Description Length (Rissanen 1978) and
Kolmogorov complexity (Gell-Mann and Lloyd 1996) provide
#emph[conceptual] grounding for what brittleness makes intuitive:
theories that cannot compress their subject matter lose generative
power. However, these measures cannot function as computational tools
for our framework. Kolmogorov complexity is uncomputable by definition,
and MDL faces practical intractability for complex knowledge systems.

The MDL connection clarifies what brittleness tracks: the rate at which
description length grows relative to new evidence incorporated. A
resilient theory compresses new data efficiently---each observation
integrates smoothly without requiring novel parameters or special-case
machinery. A brittle theory exhibits the opposite pattern: each new
piece of evidence demands ad hoc patches, causing description length to
grow faster than explanatory power. When a system requires an
ever-longer description to account for the same phenomena it once
explained concisely, this signals rising brittleness regardless of how
the system performs on its original training data.

These measures serve as regulative ideals, clarifying the pattern we
diagnose (incompressible exception-handling overwhelming core structure)
without providing mechanical algorithms. Actual diagnosis relies on
observable proxies: publication patterns, parameter proliferation,
resource allocation---fallible indicators that track the pattern these
formal measures characterize. A theory requiring novel principles for
each new domain exhibits rising incompressible complexity. This is the
signature of brittleness that simplicity heuristics alone cannot
capture.

==== 6.4.3 Systemic Costs Beyond Predictive Success
<systemic-costs-beyond-predictive-success>
Standard evidential accounts assess theories by their predictions:
successful novel predictions count for the theory; failed predictions
count against it. Brittleness analysis seems redundant. If a theory
makes bad predictions, evidential accounting already marks this. What
does brittleness add?

The framework makes visible three categories of costs that standard
evidential accounts systematically miss:

#strong[\1. Maintenance costs when predictions succeed.] Ptolemaic
astronomy maintained predictive adequacy for centuries. Predictions did
not fail; they simply required escalating astronomical labor to
generate. Each planetary position could be calculated, but the
calculations demanded compounding effort: more epicycles, more eccentric
adjustments, more specialized practitioners. The system's predictions
worked, yet the cost of operating it as a going concern was rising
unsustainably.

Standard evidential scoring sees only that predictions matched
observations. Brittleness analysis sees the resource drain: accelerating
P(t) as defensive publications proliferate, rising M(t) as model
complexity outpaces performance gains. These are real costs imposed by
misalignment with constraint structure, even when predictions
technically succeed.

#strong[\2. Coercive overhead suppressing dissent.] A system can
maintain apparent evidential success by suppressing anomalies,
marginalizing critics, or controlling what counts as evidence. When a
research programme requires increasing institutional resources to
enforce compliance (censoring dissenting publications, defunding rival
approaches, demanding loyalty oaths to core principles), this C(t)
overhead signals brittleness invisible to propositional evidential
accounts.

The costs are epistemic, not merely political. Energy spent suppressing
challenges is energy diverted from extending the system to new domains
or integrating novel phenomena. Rising C(t) predicts fragility: the
suppressed anomalies will compound, and the system's authority will
erode once coercive mechanisms fail.

#strong[\3. Opportunity costs from foreclosed paths.] A system's
structure can foreclose entire research directions, rendering certain
questions unaskable or certain phenomena invisible. These opportunity
costs are invisible to synchronic evidential accounts focused on current
predictive success.

Phlogiston chemistry, for example, could not pose the question "What is
the role of oxygen in combustion?" because oxygen was conceptualized as
dephlogisticated air---a theoretical primitive lacking independent
identity. Lavoisier's framework opened problem spaces (quantitative
stoichiometry, gas chemistry, thermodynamic integration) that
phlogiston's architecture rendered inaccessible. The cost was not failed
predictions but stunted growth.

#strong[The difference:] Predictive accuracy is propositional;
brittleness is systemic. Standard accounts ask: "Is this claim true?"
Brittleness asks: "What is the cost of operating this system as a
knowledge-generating engine?" The former assesses truth-apt outputs; the
latter assesses the reliability and sustainability of the process
generating those outputs.

This is why a theory can be evidentially adequate yet diagnosed as
brittle. It passes standard tests, but its resource profile signals
rising fragility. The framework adds a missing epistemic dimension: the
cost of sustaining coherence.

==== 6.4.4 Brittleness as Epistemic, Not Merely Sociological
<brittleness-as-epistemic-not-merely-sociological>
The objection is direct. If brittleness tracks resource allocation,
institutional friction, and community behavior, is this not sociology of
science rather than epistemology? Are we measuring social facts (how
communities allocate funding, who gets published, which theories
dominate) rather than epistemic warrant (which beliefs are justified)?

The response requires distinguishing manifestation from source.

#strong[Brittleness has epistemic grounding.] The costs tracked by P(t),
C(t), and M(t) are not constituted by social agreement. They are imposed
by misalignment between a system's structure and reality's constraints.
When a theory makes false causal claims, applications in novel domains
reveal failures. When a model's core principles misrepresent structural
relationships, extending the framework to new problems generates
anomalies requiring ad hoc patches. When institutional authorities
suppress evidence contradicting their framework, maintaining that
suppression consumes resources that could fund genuine inquiry.

These costs are real, measurable, and non-negotiable. They are not
socially constructed but discovered through pragmatic pressure. The
constraint landscape is mind-independent; brittleness diagnoses the cost
of misalignment with that landscape.

#strong[Brittleness has sociological manifestation.] The costs appear as
observable social phenomena: publication patterns shifting from novel
research to defensive patching (P(t) rising), institutional budgets
increasingly devoted to maintaining authority rather than funding
exploration (C(t) rising), research communities requiring specialized
training to navigate byzantine theoretical machinery (M(t) rising).
These are measurable social facts.

But the analogy to biological fitness clarifies the relationship.
Fitness is tracked through population dynamics (births, deaths,
reproductive success)---sociological facts about organisms. Yet fitness
ultimately reflects alignment with environmental constraints. An
organism's high fitness is not constituted by the social fact that it
has many offspring; the offspring proliferate because the organism's
traits align with survival constraints. Similarly, rising C(t) is not
merely the social fact that institutions spend resources on suppression;
the suppression is necessary because the framework's structure generates
resistance by misaligning with reality.

#strong[The dual aspect:] Brittleness has an epistemic dimension (tracks
reliability as guide to reality) and a sociological manifestation
(appears as resource allocation). The manifestation is social; the
source is epistemic misalignment. This is naturalized epistemology
(Kitcher 1993): we track social processes as indicators of epistemic
quality, recognizing that successful knowledge systems must survive
material constraints.

The framework's contribution is showing how to read sociological signals
epistemically. Not all resource expenditure signals brittleness (some
costs reflect genuine structural requirements). But specific cost
patterns---accelerating defensive publications, rising institutional
overhead to enforce conformity, proliferating complexity with
diminishing returns---reliably diagnose epistemic fragility because they
reflect the unsustainable resource drain of fighting reality's
constraints.

==== 6.4.5 Brittleness vs.~Problem-Solving Effectiveness
<brittleness-vs.-problem-solving-effectiveness>
Laudan's #emph[Progress and Its Problems] (Laudan 1977) shares the
brittleness framework's focus on what theories accomplish rather than
abstract truth-relations. Both assess theories pragmatically: not "does
this correspond to reality?" but "what does this system achieve?"

The similarity is substantial. Laudan scores theories by counting solved
empirical problems (phenomena the theory predicts or explains) minus
unsolved conceptual problems (internal contradictions, inconsistencies
with background knowledge). A theory making progress solves more
problems over time than it generates. This is outcome-focused,
historically grounded, and avoids metaphysical disputes about truth.

But four differences distinguish brittleness from problem-solving
effectiveness:

#strong[\1. Directionality.] Laudan looks backward, summing accumulated
achievements. Brittleness looks forward, diagnosing risk. A theory can
have high problem-solving score (many puzzles solved historically) while
accumulating hidden costs that signal fragility. Phlogiston chemistry
solved major problems---combustion, respiration, calcination---earning a
respectable Laudan score. But brittleness analysis tracks the rising
maintenance burden those solutions required: escalating auxiliary
structure, defensive publications, foreclosed problem spaces. Laudan
would rank phlogiston moderately successful; brittleness diagnoses it as
vulnerable.

#strong[\2. Cost visibility.] Laudan's framework can rank a theory
highly if it solves problems, even if sustaining those solutions imposes
escalating costs. Brittleness makes costs explicit. A "successful"
theory with high P(t), C(t), and M(t) is diagnosed as fragile regardless
of problem-solving score. The framework asks: at what resource cost does
this system generate its solutions? Theories imposing unsustainable
costs are predicted to fail even if their current achievements are
impressive.

#strong[\3. Negative evidence primacy.] Laudan accumulates positive
achievements. Brittleness privileges the Negative Canon---what has
demonstrably failed provides stronger constraints than what has
contingently succeeded. The space of viable theories is bounded not by
accumulated successes but by demonstrated failures. Phlogiston is not
merely a "less successful" theory in Laudan's comparative ranking but a
relegated failure establishing a boundary: viable chemistry must explain
mass gain during combustion without invoking negative-weight substances.
The Negative Canon provides absolute constraints, not relative rankings.

#strong[\4. Mechanism specificity.] Laudan describes what makes theories
better (more problems solved). Brittleness provides a causal mechanism
for why brittle theories fail: they impose unsustainable costs forcing
replacement. When a system's maintenance burden exceeds available
resources, practitioners abandon it regardless of historical
achievements. This explains paradigm shifts mechanistically: not that
better theories win by scoring more points, but that brittle theories
become operationally unviable. The collapse is structural, not
scoreboard.

Example: Phlogiston chemistry solved combustion (phlogiston release),
respiration (phlogiston transfer), and metal formation (removal of
phlogiston from calx). Impressive problem-solving. But sustaining these
solutions required incompressible auxiliary structure: dephlogisticated
air, negative weight, context-dependent properties. When Lavoisier's
oxygen framework offered a unified quantitative account with lower M(t),
immediate exportability to new reaction types (lower P(t)), and
integration with emerging thermodynamics, the transition was not that
Lavoisier solved more problems but that his framework imposed
dramatically lower operational costs.

The frameworks complement rather than compete. Laudan provides
historical accounting; brittleness provides structural diagnosis. Laudan
explains progress; brittleness predicts fragility. One measures
achievement; the other tracks sustainability.

=== 6.5 Plantinga's Challenge: Does Evolution Select for Truth or Mere Survival?
<plantingas-challenge-does-evolution-select-for-truth-or-mere-survival>
Alvin Plantinga's Evolutionary Argument Against Naturalism (EAAN) poses
a formidable challenge to any naturalistic epistemology: if our
cognitive faculties are products of natural selection, and natural
selection optimizes for reproductive success rather than true belief,
then we have no reason to trust that our faculties reliably produce true
beliefs (Plantinga 1993, 2011). Evolution could equip us with
systematically false but adaptive beliefs: useful fictions that enhance
survival without tracking reality. If naturalism is true, the very
faculties we use to conclude naturalism is true are unreliable,
rendering naturalism self-defeating.

Our framework provides a novel response by collapsing Plantinga's
proposed gap between adaptive success and truth-tracking. We argue that
in domains where systematic misrepresentation generates costs, survival
pressure and truth-tracking converge necessarily. This is not because
evolution "cares about" truth, but because reality's constraint
structure makes persistent falsehood unsustainable.

#strong[Systemic Brittleness as the Bridge]

Plantinga's argument assumes survival and truth can come apart: that
belief systems could be adaptively successful while systematically
misrepresenting reality. Our framework challenges this through systemic
brittleness. In any domain where actions have consequences constrained
by objective features of reality, false beliefs accumulate costs:

- Navigation beliefs with systematic errors generate failures, wasted
  energy, increased predation risk
- Causal beliefs that misunderstand cause-effect relationships lead to
  ineffective interventions and resource waste
- Social beliefs that misread dynamics generate coordination failures
  and coalition instability

These costs compound. A single false belief might be offset by other
advantages, but networks of mutually reinforcing falsehoods generate
accelerating brittleness through our P(t) mechanism (conceptual debt
accumulation). The phlogiston theorist does not just hold one false
belief; they must continuously patch their system with ad-hoc
modifications as each application reveals new anomalies. This
brittleness makes the system vulnerable to displacement by more viable
alternatives.

#strong[Domain Specificity: Where Truth-Tracking Matters]

Our response is not that evolution always selects for true belief.
Rather, we identify specific conditions under which survival pressure
forces truth-tracking. Domain structure determines whether falsity
accumulates brittleness:

High-cost domains (strong selection for truth-tracking): Physical
causation, basic perception, tool use and engineering, social
coordination. Misunderstanding gravity, systematically misperceiving
predators, or holding false beliefs about material properties generates
immediate, compounding costs.

Low-cost domains (weak selection): Metaphysical speculation, aesthetic
preferences, remote historical claims. Believing in Zeus versus no gods
has minimal direct costs if behavior is similar. False beliefs about
"objective beauty" do not accumulate brittleness.

Systematically misleading domains (Plantinga's worry bites hardest):
Self-enhancement biases, tribal epistemology, certain evolved
heuristics. Here overconfidence or believing "my group is superior" may
be adaptive even if false, because they coordinate action or provide
psychological benefits.

This domain-specificity is crucial. Our framework concedes Plantinga's
point for low-cost and systematically-misleading domains: these are
precisely where the isolation objection has least force. But in
high-cost domains (those that matter most for science, engineering, and
social coordination), the gap Plantinga identifies cannot persist across
long timescales.

#strong[Convergence Through Cultural Evolution]

Plantinga focuses on individual cognitive faculties shaped by biological
evolution. Our framework operates at a different level: cultural
evolution of public knowledge systems. This shift is crucial. Individual
humans may harbor adaptive falsehoods, but public knowledge systems are
subject to distinct, higher-order selective pressures: transmissibility
(false systems that work in one context often fail when transmitted to
new contexts), cumulative testing (each generation's application exposes
misalignment), and inter-system competition (when rival systems make
incompatible predictions, differential brittleness outcomes determine
which survives).

The #strong[Negative Canon] provides overwhelming evidence for this
process. Ptolemaic astronomy, phlogiston chemistry, miasma theory, and
Lysenkoism were not merely false; they accumulated measurable,
catastrophic systemic costs that forced their abandonment. The
historical record shows systematic elimination of high-brittleness
systems across cultures and eras, suggesting convergence toward a
constraint-determined structure (the Apex Network) rather than
persistent plurality of useful fictions.

#strong[Where Plantinga's Worry Remains]

We acknowledge our response does not fully dissolve Plantinga's
challenge at the individual level. An individual human's cognitive
faculties might indeed harbor systematically false but adaptive beliefs
in low-cost or systematically-misleading domains. Our claim is more
modest: in domains where falsity accumulates brittleness, cumulative
cultural evolution forces convergence toward truth-tracking systems,
even if individual psychology remains imperfect.

This leaves open several possibilities. Evolutionary debunking arguments
may retain force in genuinely low-cost domains like pure aesthetics or
speculative metaphysics. However, moral philosophy is not low-cost:
moral systems that systematically misrepresent human needs generate
catastrophic coordination failures, demographic collapse, and rising
coercive overheads. The #strong[Negative Canon] demonstrates this
empirically. Individual-level cognitive biases may persist even as
system-level knowledge improves. Fundamental uncertainty about whether
our most basic faculties (logic, perception) are reliable cannot be
eliminated; this is acknowledged in our fallibilism.

#strong[The Self-Defeat Response Reversed]

Plantinga argues naturalism is self-defeating: if naturalism is true, we
should not trust the faculties that led us to believe it. Our framework
reverses this concern: the very fact that naturalistic science has
systematically driven down systemic brittleness across centuries
(enabling unprecedented technological success, predictive power, and
unification) provides higher-order evidence that the system tracks the
Apex Network. The proof is in the low-brittleness pudding.

If our cognitive faculties were fundamentally unreliable in the way
Plantinga suggests, we would expect accelerating conceptual debt (rising
P(t)), decreasing unification (falling R(t)), and catastrophic failures
when theories are applied in novel domains. Instead, mature sciences
show the opposite: decreasing P(t), increasing R(t), and successful
cross-domain applications. This provides inductive grounds for trusting
that our faculties, at least in high-cost domains, track real constraint
structures rather than generate useful fictions.

Our response to Plantinga: In domains where systematic misrepresentation
accumulates measurable costs, the supposed gap between adaptive success
and truth-tracking collapses. Survival is not truth, but in a universe
with stable constraints, surviving knowledge systems are forced to
approximate truth because persistent falsehood generates brittleness
that pragmatic selection eliminates. This is not metaphysical necessity
but statistical regularity: an empirical claim falsifiable through the
research program outlined in Section 7.2.

Plantinga is right that evolution per se does not guarantee reliability.
But evolution plus pragmatic filtering in a constraint-rich environment
does generate truth-tracking in precisely those domains where
coherentism faces the isolation objection. Where Plantinga sees
self-defeat, we see self-correction: the systematic reduction of
brittleness over centuries is evidence that the process works, even if
no individual step is guaranteed.

=== 6.6 Computational and Systematic Precursors
<computational-and-systematic-precursors>
EPC synthesizes four computational/systematic frameworks, advancing each
through externalist brittleness diagnostics.

#strong[Thagard's ECHO (1989, 2000):] Models explanatory coherence via 7
principles (symmetry, explanation, contradiction, etc.) as constraint
satisfaction in connectionist networks. Activation spreads through
excitatory/inhibitory weights until harmony maximizes. EPC extends this:
#strong[Standing Predicates] = high-activation nodes; propagation = ECHO
dynamics. #strong[Advance:] Brittleness adds dynamic weights derived
from pragmatic costs. Accumulating ad-hoc patches (rising P(t)) would
create progressively stronger inhibitory effects on the core
propositions they protect, while coercive overheads (rising C(t)) would
suppress dissenting nodes. ECHO operates internally; EPC adds pragmatic
pushback as external discipline, solving Thagard's isolation problem.

#strong[Zollman's Epistemic Graphs (2007, 2010):] Shows topology
effects: sparse cycles preserve diversity (reliability bonus), complete
graphs risk premature lock-in (speed/reliability trade-off). EPC's
#strong[Pluralist Frontier] operationalizes transient diversity.
#strong[Advance:] While Zollman models abstract belief propagation,
EPC's framework suggests how these models could be grounded. It points
toward using the historical record of systemic shocks (such as those
catalogued in historical databases) as a source of external validity
checks, moving beyond purely abstract network dynamics.

#strong[Rescher's Systematicity (1973, 2001):] Defines truth as
praxis-tested systematicity (completeness, consistency, functional
efficacy) but lacks quantification. #strong[Advance:] SBI(t)
operationalizes: P(t) = consistency metric, R(t) = completeness breadth,
enabling falsifiable predictions (brittleness-collapse correlations).

#strong[Kitcher (1993) on Evolutionary Progress:] Models science as
credit-driven selection with division of labor across 'significant
problems.' #strong[Advance:] #strong[Negative Canon] provides failure
engine, brittleness quantifies problem significance via coercive costs
(C(t)), diagnosing degenerating programs without reducing to cynical
credit-seeking.

#strong[Sims (2024) on Dynamic Holism:] Ecological constraints drive
diachronic cognitive revision in nonneuronal organisms. EPC parallels
this at macro-cultural scale: pragmatic pushback = ecological variables.
#strong[Distinction:] Sims focuses on individual adaptation; EPC on
intergenerational knowledge systems. Both emphasize context-sensitive,
constraint-driven evolution; EPC adds synchronic diagnostics to Sims'
diachronic methodology.

These precursors provide micro-coherence (Thagard), meso-topology
(Zollman), normative criteria (Rescher), macro-dynamics (Kitcher), and
biological analogy (Sims). EPC unifies them through falsifiable
brittleness assessment grounded in historical failure data.

== 7. Final Defense and Principled Limitations
<final-defense-and-principled-limitations>
Having situated the framework within existing epistemological traditions
and clarified its distinctive contributions, we now turn to a systematic
defense of its scope and an honest acknowledgment of its limitations.
Any philosophical framework achieves clarity through what it excludes as
much as what it includes. This section addresses three critical
questions: What epistemic problems does the framework solve, and which
does it appropriately leave to other approaches? How can retrospective
brittleness diagnostics provide prospective guidance? And what
falsifiable predictions does the framework generate? These
clarifications fortify the account against misunderstanding while
delineating its proper domain of application.

As a macro-epistemology explaining the long-term viability of public
knowledge systems, this framework does not primarily solve
micro-epistemological problems like Gettier cases. Instead, it bridges
the two levels through the concept of higher-order evidence: the
diagnosed health of a public system provides a powerful defeater or
corroborator for an individual's beliefs derived from that system.

The diagnosed brittleness of a knowledge system provides higher-order
evidence that determines rational priors. Following Kelly (2005) on
disagreement, when an agent receives a claim, they must condition their
belief not only on the first-order evidence but also on the source's
reliability (Staffel 2020). Let S be a high-brittleness network, like a
denialist documentary. Its diagnosed non-viability acts as a powerful
higher-order defeater. Therefore, even if S presents seemingly
compelling first-order evidence E, a rational agent's posterior
confidence in the claim properly remains low. Conversely, a
low-brittleness network like the IPCC earns a high prior through
demonstrated resilience. To doubt its claims without new evidence of
rising brittleness is to doubt the entire adaptive project of science
itself. This provides a rational, non-deferential basis for trust:
justification flows from systemic health, grounding micro-level belief
in macro-level viability.

=== 7.1 From Hindsight to Foresight: Calibrating the Diagnostics
<from-hindsight-to-foresight-calibrating-the-diagnostics>
To address the hindsight objection (that we can only diagnose
brittleness after failure), we frame the process as a two-stage
scientific method: #strong[Stage 1: Retrospective Calibration.] We use
the clear data from the #strong[Negative Canon] (historical failures) to
calibrate our diagnostic instruments (the P(t), C(t), M(t), R(t)
indicators), identifying the empirical signatures that reliably precede
collapse. #strong[Stage 2: Prospective Diagnosis.] We apply these
calibrated instruments to contemporary, unresolved cases not for
deterministic prediction, but to assess epistemic risk and identify
which research programs are progressive versus degenerating.

=== 7.2 A Falsifiable Research Program
<a-falsifiable-research-program>
The framework grounds a concrete empirical research program with a
falsifiable core causal claim: #emph[a system exhibiting high and rising
brittleness across multiple indicators should, over historical time,
prove more vulnerable to major revision or collapse when faced with
external shocks than a system with low and stable brittleness.]
"Collapse" is operationally defined as either (1) institutional
fragmentation requiring fundamental restructuring; (2) wholesale
paradigm shift in the domain; or (3) substantial reduction in
problem-solving capacity requiring external intervention. Historical
patterns in collapsed systems, such as Roman aqueduct failures due to
accumulating brittleness in hydraulic engineering (Hodge 1992; Turchin
2003), are consistent with this expectation. The specific metrics and
dynamic equations underlying this research program are detailed in
Appendix A.

The framework also makes a second, independent prediction about
convergence geometry. Independently developed viable systems should
exhibit substantial structural overlap, and this overlap should tend to
increase as inquiry expands and edge cases are tested. If, instead,
viable systems repeatedly form an archipelago of mostly disjoint
structures, with no stable growth in shared load-bearing commitments,
the mountain-range model fails and the Apex claim must be weakened.

#strong[Methodology];: (1) Operationalize brittleness through observable
proxies (resource allocation patterns, auxiliary hypothesis rates in
literature). (2) Conduct comparative historical analysis using databases
like Seshat (a database of historical societies) to compare outcomes
across systems with different pre-existing brittleness facing similar
shocks, controlling for contingent events. (3) Track structural overlap
across independently developed viable systems in the same domain,
especially after novel boundary tests, to assess whether shared core
commitments expand or fragment. As a conceptual illustration, consider
competing COVID-19 models (2020--2022): one might analyze whether highly
complex epidemiological models with many parameters showed signs of
rising brittleness through persistent predictive failures and required
constant revision, while simpler models maintained better predictive
alignment over time (Roda et al.~2020). Such analysis would illustrate
the framework's diagnostic potential.

=== 7.3 Principled Limitations and Scope
<principled-limitations-and-scope>
Philosophical honesty requires acknowledging not just what a framework
can do, but what it cannot. These are not flaws but principled choices
about scope and method.

==== 7.3.1 Species-Specific Objectivity
<species-specific-objectivity>
#strong[The Limitation:] Moral and epistemic truths are objective for
creatures with our biological and social architecture. Hypothetical
beings with radically different structures, for example, telepathic
beings that reproduce by fission and feel no pain, would face different
constraints and discover a different Apex Network.

#strong[Why We Accept This:] This is relativism at the species level,
not cultural level. We accept it as appropriate epistemic modesty; our
claims are grounded in constraints humans actually face. This preserves
robust objectivity for humans, as all cultures share the same core
constraints.

==== 7.3.2 Learning Through Catastrophic Failure
<learning-through-catastrophic-failure>
#strong[The Limitation:] We learn moral truths primarily through
catastrophic failure. The #strong[Negative Canon] is written in blood.
Future moral knowledge will require future suffering to generate data.

#strong[Why We Accept This:] Empirical knowledge in complex domains
requires costly data. Medicine, engineering, and social systems all
required catastrophic failures before developing safety mechanisms.

#strong[Implication:] Moral learning is necessarily slow. We should be
epistemically humble about current certainties yet confident in
#strong[Negative Canon] entries.

==== 7.3.3 Floor Not Ceiling
<floor-not-ceiling>
#strong[The Limitation:] The framework maps necessary constraints (the
floor), not sufficient conditions for flourishing (the ceiling). It
cannot address what makes life meaningful beyond sustainable,
supererogatory virtue and moral excellence, aesthetic value and beauty,
or the difference between a decent life and an exemplary one.

#strong[Why We Accept This:] Appropriate scope limitation. The framework
does what it does well rather than overreaching. It identifies
catastrophic failures and boundary conditions, leaving substantial space
for legitimate value pluralism above the floor.

#strong[What This Implies:] The framework provides necessary but not
sufficient conditions. Thick theories of the good life must build on
this foundation. The #strong[Pluralist Frontier] is real: multiple
flourishing forms exist, but all must respect the floor (avoid
#strong[Negative Canon] predicates).

==== 7.3.4 Expert Dependence and Democratic Legitimacy
<expert-dependence-and-democratic-legitimacy>
#strong[The Limitation:] Accurate brittleness assessment requires
technical expertise in historical analysis, statistics, comparative
methods, and systems theory. This creates epistemic inequality.

#strong[Why We Accept This:] Similar to scientific expertise generally.
Complex systems require specialized knowledge to evaluate.

#strong[Mitigation:] Data transparency, distributed expertise,
standpoint epistemology (marginalized groups as expert witnesses to
brittleness), institutional design (independent assessment boards), and
education can reduce but not eliminate this challenge.

==== 7.3.5 Discovery Requires Empirical Testing
<discovery-requires-empirical-testing>
#strong[The Limitation:] While the Apex Network exists as a determined
structure, discovering it requires empirical data. We cannot deduce
optimal social configurations from first principles alone; we need
historical experiments to reveal constraint topology.

#strong[Why We Accept This:] Even in physics and mathematics, we need
empirical input. Pure reason can explore logical possibilities, but
determining which possibilities are actual requires observation or
experiment. For complex social systems with feedback loops and emergent
properties, this dependence is stronger.

#strong[What This Allows:] Prospective guidance through constraint
analysis. We can reason about likely optimal solutions by analyzing
constraint structure, but we need empirical validation. This is stronger
than pure retrospection but weaker than complete a priori knowledge.

==== 7.3.6 The Viable Evil Possibility
<the-viable-evil-possibility>
#strong[The Limitation:] If a deeply repugnant system achieved genuinely
low brittleness, the framework would have to acknowledge it as viable,
though not necessarily just by other standards.

#strong[Why We Accept This:] Intellectual honesty. The framework maps
pragmatic viability, not all moral dimensions.

#strong[Empirical Bet:] We predict such systems are inherently brittle.
Historical cases like the Ottoman devşirme system or Indian caste
systems exhibited high coercive overheads, innovation lags, and
fragility under external shocks (Acemoglu & Robinson 2012; Turchin
2003). True internalization without coercion is rare and
resource-intensive. If empirics proved otherwise, we would acknowledge
the framework's incompleteness rather than deny evidence.

==== 7.3.7 Clarifying Scope and Limits
<clarifying-scope-and-limits>
These limitations do not undermine the framework's contribution; they
define appropriate scope. EPC excels at:

#strong[Strong Claims:] - Identifying catastrophic systemic failures -
Explaining moral progress as empirically detectable debugging -
Grounding realism naturalistically without non-natural properties -
Providing empirical tools for institutional evaluation - Offering
prospective guidance through constraint analysis - Unifying descriptive
and normative epistemology

#strong[Modest Claims:] - Does not provide complete ethics - Does not
solve all normative disagreements - Does not eliminate need for judgment
\- Does not achieve view-from-nowhere objectivity - Does not offer
categorical imperatives independent of systemic goals

#strong[The Value Proposition:] A powerful but limited diagnostic tool
for systemic health. Use it for what it does well. Supplement with other
resources for what it cannot address. Do not expect a complete theory of
human flourishing; expect robust tools for avoiding catastrophic failure
and identifying progressive change.

==== 7.3.8 The AI Constraint: Embodiment and the Limits of Coherence
<the-ai-constraint-embodiment-and-the-limits-of-coherence>
#strong[The Limitation:] This framework offers a principled critique of
current Large Language Models (LLMs). Contemporary LLMs operate almost
exclusively at Level 3 contextual coherence. They are sophisticated
engines for maintaining internal consistency within a linguistic corpus,
but they lack a physical Markov Blanket that faces thermodynamic
consequences for error. Without the friction of reality, an unembodied
system cannot distinguish between a plausible fiction and a viable
truth. For such a system, there is no metabolic cost to being wrong, no
pragmatic pushback to force revision toward viability.

#strong[Why This Matters:] Objective truth (Level 1) and warranted
assertion (Level 2) require embodiment in our framework. A system must
have boundaries that face real costs when misaligned with environmental
constraints. This is not a contingent limitation of current technology
but a structural requirement: coherence without consequence cannot
converge on the Apex Network. Pragmatic selection presupposes the
possibility of differential survival, which requires a physical
substrate vulnerable to the costs of misalignment.

#strong[Implication:] LLMs can serve as powerful tools for exploring the
space of contextually coherent claims (Level 3), but they cannot
independently achieve the epistemic status of justified belief. Their
outputs require validation by embodied systems---humans, robots,
scientific instruments---that face pragmatic pushback. This
distinguishes our framework from views that reduce truth to coherence
alone. Information leakage only matters when there are thermodynamic
costs to maintaining misaligned boundaries, and such costs require
physical embodiment.

This honest accounting strengthens rather than weakens the framework's
philosophical contribution.

#strong[What This Theory is NOT:]

- #strong[Not a Complete Ethics:] EPC maps necessary constraints (the
  floor) but not sufficient conditions for flourishing. It identifies
  catastrophic failures but leaves space for legitimate pluralism in
  values above the viability threshold.
- #strong[Not a Foundationalist Metaphysics:] It avoids positing
  non-natural properties or Platonic forms. Objectivity emerges from
  pragmatic selection, not metaphysical bedrock.
- #strong[Not Deterministic Prediction:] Claims are probabilistic;
  brittleness increases vulnerability but does not guarantee collapse.
- #strong[Not a Defense of Power:] Power can mask brittleness
  temporarily, but coercive costs are measurable indicators of
  non-viability.
- #strong[Not Relativist:] While species-specific, it defends robust
  objectivity within human constraints via convergent evidence.
- #strong[Not Anti-Realist:] It grounds fallibilist realism in the
  emergent Apex Network, discovered through elimination.

#strong[Steelmanned Defense of the Core:] The "Drive to Endure" is not a
smuggled value but a procedural-transcendental filter. Any project of
cumulative justification presupposes persistence as a constitutive
condition. Systems failing this filter (e.g., apocalyptic cults) cannot
sustain inquiry. The "ought" emerges instrumentally: favor low-SBI norms
to minimize systemic costs like instability and suffering, providing
evidence-based strategic advice for rational agents.

== 8. Conclusion
<conclusion>
Grounding coherence in long-term viability of knowledge systems rather
than internal consistency alone provides the external constraint
coherentism requires while preserving its holistic insights. The concept
of systemic brittleness offers a naturalistic diagnostic tool for
evaluating knowledge systems, while the notion of a
constraint-determined Apex Network explains how objective knowledge can
arise from fallible human practices.

Systematically studying the record of failed systems discerns the
contours of the Apex Network: the emergent set of maximally convergent,
pragmatically indispensable principles that successful inquiry is forced
to discover.

This model is not presented as a final, complete system but as the
foundation for a progressive and falsifiable research program. Critical
future challenges remain, such as fully modeling the role of power
asymmetries in creating path-dependent fitness traps and applying the
framework to purely aesthetic or mathematical domains.

We began with the challenge of distinguishing viable knowledge from
brittle dogma. The model we have developed suggests the ultimate arbiter
is not the elegance of a theory or the consensus of its adherents but
the trail of consequences it leaves in the world. Systemic costs are not
abstract accounting measures; they are ultimately experienced by
individuals as suffering, instability, and the frustration of human
goals. From this perspective, dissent, friction, and protest function as
primary sources of epistemological data. They are the system's own
real-time signals, indicating where First-Order Costs are accumulating
and foreshadowing the rising Coercive Overheads ($C \( t \)$) that will
be required to maintain stability against those pressures.

#strong[Qualification on dissent:] Not all dissent signals brittleness.
Dissent from misunderstanding (e.g., popular opposition to evolution
based on misinterpretation) provides no epistemic information about the
discipline's internal health. The epistemically valuable dissent comes
from practitioners or informed observers who identify specific rising
costs: replication failures, suppressed anomalies, institutional
barriers to error correction. External dissent from misunderstanding is
noise; internal dissent about rising operational costs is signal.

It provides the external constraint that coherentism has long needed,
but it does so without resorting to foundationalism. It accounts for the
convergence that motivates scientific realism, but it does so within a
fallibilist and naturalistic picture of inquiry.

Ultimately, Emergent Pragmatic Coherentism shares the realist's
conviction that convergence reflects constraint, not convention. It
reframes truth as an emergent structural attractor (the Apex Network)
stabilized through an evolutionary process of eliminating failure. The
approach calls for epistemic humility, trading the ambition of a
God's-eye view for the practical wisdom of a mariner. The payoff is not
a final map of truth, but a continuously improving reef chart: a chart
built from the architecture of failure, helping us to distinguish the
channels of viable knowledge from the hazards of brittle dogma.

== Appendix A: A Mathematical Model of Epistemic Viability
<appendix-a-a-mathematical-model-of-epistemic-viability>
This appendix provides a provisional formalization of core EPC concepts
to show that the framework is, in principle, amenable to formal
expression. It is crucial to note that these models are illustrative and
speculative. #strong[The philosophical argument presented in the main
body of the paper is self-contained and does not depend on the validity
of any specific mathematical formulation presented here.] The purpose of
this appendix is to explore one possible path for future research, not
to provide empirical validation for the paper's central claims.

=== A.1 Set-Theoretic Foundation
<a.1-set-theoretic-foundation>
Let #strong[U] be the universal set of all possible atomic predicates.
An individual's #strong[Web of Belief (W)] is a subset W ⊆ U satisfying
internal coherence condition C\_internal:

```
W = {p ∈ U | C_internal(p, W)}
```

#strong[Shared Networks (S)] emerge when agents coordinate to solve
problems. They represent the intersection of viable individual webs:

```
S = ∩{W_i | V(W_i) = 1}
```

where V is a viability function (detailed below).

Public knowledge forms nested, intersecting networks (S\_germ\_theory ⊂
S\_medicine ⊂ S\_biology), with cross-domain coherence driving
convergence.

#strong[The Apex Network (A)] is the maximal coherent subset remaining
after infinite pragmatic filtering:

```
A = ∩{W_k | V(W_k) = 1} over all possible contexts and times
```

#strong[Ontological Status:] A is not pre-existing but an emergent
structural fact about U, revealed by elimination through pragmatic
selection.

#strong[Epistemic Status:] #emph[A is unknowable directly];; it is
inferred by mapping failures catalogued in the #strong[Negative Canon]
(the historical record of collapsed, high-SBI systems).

#strong[Formal ECHO Extension:] This formalizes Thagard's ECHO
extension: net\_j = Σ w\_{ij} a\_i - β·brittleness\_j, where w
represents positive weights for explanatory coherence (Principle 1) and
negative weights for contradiction (Principle 5), with β derived from
P(t), C(t) proxies. The specific weight magnitudes would require
empirical calibration. Zollman's cycle topology models Pluralist
Frontier; complete graphs risk brittleness lock-in.

=== A.2 The Systemic Brittleness Index
<a.2-the-systemic-brittleness-index>
SBI(t) is a composite index quantifying accumulated systemic costs. We
present three functional forms, each with distinct theoretical
motivation and testable predictions.

#strong[Key Components:]

#strong[P(t) - Patch Velocity:] Rate of ad-hoc hypothesis accumulation
measuring epistemic debt - Proxy: Increasing share of auxiliary
hypotheses vs.~novel predictions

#strong[C(t) - Coercion Ratio:] Resources for internal control
vs.~productive adaptation - Proxy: (Suppression spending) / (R&D
spending)

#strong[M(t) - Model Complexity:] Information-theoretic bloat measure -
Proxy: Free parameters lacking predictive power

#strong[R(t) - Resilience Reserve:] Accumulated robust principles
buffering against shocks - Proxy: Breadth of independent confirmations,
age of stable core

#strong[Functional Forms:]

#strong[Form 1: Multiplicative Model]

```
SBI(t) = (P^α · C^β · M^γ) / R^δ
```

#strong[Rationale:] Captures interaction effects where high values in
multiple dimensions compound non-linearly. A system with both high
complexity AND high patch velocity is more brittle than the sum would
suggest. (This form could be used to model compounding effects, where
brittleness dimensions interact and accelerate).

#strong[Predictions:] Brittleness accelerates when multiple indicators
rise simultaneously. Systems can tolerate high values in one dimension
if others remain low.

#strong[Testable implication:] Historical collapses should correlate
with simultaneous elevation of 2+ metrics, not single-metric spikes.

#strong[Form 2: Additive Weighted Model]

```
SBI(t) = α·P(t) + β·C(t) + γ·M(t) - δ·R(t)
```

#strong[Rationale:] Assumes independent, additive contributions. Simpler
to estimate and interpret; each component has linear effect.

#strong[Predictions:] Each dimension contributes independently. Reducing
any single metric proportionally reduces overall brittleness.

#strong[Testable implication:] Interventions targeting single metrics
should show proportional improvement.

#strong[Form 3: Threshold Cascade Model]

```
SBI(t) = Σ[w_i · max(0, X_i(t) - T_i)] + λ·Π[H(X_j - T_j)]
```

where X\_i ∈ {P, C, M}, H is Heaviside step function, T\_i are critical
thresholds

#strong[Rationale:] Systems tolerate moderate brittleness but experience
catastrophic acceleration once thresholds are crossed. Captures
phase-transition dynamics observed in complex systems.

#strong[Predictions:] Brittleness remains low until critical thresholds
crossed, then accelerates rapidly. Non-linear "tipping point" behavior.

#strong[Testable implication:] Historical data should show periods of
stability followed by rapid collapse once multiple thresholds exceeded.

#strong[Empirical Strategy:]

These forms make distinct predictions testable through historical
analysis: 1. Compile brittleness metrics for 20-30 historical knowledge
systems (ancient to modern) 2. Code collapse/persistence outcomes 3. Fit
each model to historical data 4. Compare predictive accuracy using
cross-validation 5. Use information criteria (AIC/BIC) to select
best-fitting form

The phlogiston chemistry case (Section 3) illustrates how such data can
be assembled from historical records. A full research program would
systematically extend this approach. The framework's falsifiability
depends on committing to specific functional forms and comparing
predictions to data.

=== A.3 Dynamics: Stochastic Differential Equations
<a.3-dynamics-stochastic-differential-equations>
Knowledge evolution is not deterministic. We model SBI dynamics as:

```
d(SBI) = [α·SBI - β·D(t) + γ·S(t) - δ·R(t) + θ·I(t)]dt + σ·√(SBI)·dW(t)
```

#strong[Deterministic Terms:]

- #strong[α·SBI:] Compounding debt (brittleness begets brittleness)
- #strong[β·D(t):] Systemic debugging (cost-reducing discoveries)
- #strong[\+γ·S(t):] External shocks (novel anomalies, pressures)
- #strong[δ·R(t):] Resilience buffer (accumulated robustness)
- #strong[\+θ·I(t):] Innovation risk (costs of premature adoption)

#strong[Stochastic Term:]

- #strong[σ·√(SBI)·dW(t):] Brownian motion capturing randomness in
  discovery timing; volatility increases with brittleness

#strong[Parameter Estimation:]

The parameters α, β, γ, δ, θ, σ are unknowable a priori and must be
fitted to historical data. This is not a limitation but standard
scientific practice. Proposed empirical strategy:

+ Compile time-series brittleness data for multiple historical systems
  (as illustrated with phlogiston chemistry)
+ Use maximum likelihood estimation or Bayesian methods to fit
  parameters
+ Validate on held-out historical cases
+ Test whether fitted model successfully predicts collapse timing for
  independent test cases

The phlogiston chemistry case provides a template: with systematic
bibliometric coding, we can construct d(SBI)/dt trajectories from
publication patterns. Parameter estimation would then proceed through
standard statistical methods.

#strong[Predictive Utility:]

Once parameters are empirically estimated, the formulation enables
probabilistic predictions: "System X has P% chance of crisis within Y
years given current trajectory." This transforms brittleness from
retrospective diagnosis to prospective risk assessment. The framework's
scientific credibility depends on executing this program and comparing
predictions to outcomes.

=== A.4 Conceptualizing an Empirical Inquiry
<a.4-conceptualizing-an-empirical-inquiry>
The existence of these distinct functional forms suggests a path for
future empirical inquiry. A historian or sociologist of science could:
\1. Compile qualitative brittleness indicators for a set of historical
knowledge systems. 2. Code their outcomes (e.g., persistence, revision,
collapse). 3. Assess which of the conceptual models (multiplicative,
additive, or threshold) best describes the observed historical patterns.

Such a program would not be about fitting precise numerical data but
about determining which conceptual dynamic (compounding interaction,
linear addition, or tipping points) best accounts for the historical
evolution of knowledge. The framework's value lies in its ability to
generate such guiding questions for historical and scientific
investigation.

=== A.5 The Role of Formalism in Philosophical Diagnosis
<a.5-the-role-of-formalism-in-philosophical-diagnosis>
Once a model like this were empirically calibrated, it would not
function as a predictive algorithm but as a diagnostic tool. Its utility
would be to translate complex historical dynamics into a structured
formal language, allowing for more precise comparisons between the
health of different knowledge systems. For example, it could help answer
questions like: "Is system X accumulating costs primarily through
conceptual debt (P(t)), or is its fragility masked by coercive power
(C(t))?" This transforms brittleness from a retrospective metaphor into
a conceptually structured diagnostic, which is the primary philosophical
payoff of the formal exercise.

== References
<references>
Acemoglu, Daron, and James A. Robinson. 2013. #emph[Why Nations Fail];.
New York: Crown Business. ISBN 978-0307719225.

Anderson, Elizabeth. 1995. "Knowledge, Human Interests, and Objectivity
in Feminist Epistemology" #emph[Philosophical Topics] 23(2): 27-58.
https:\/\/doi.org/10.5840/philtopics199523213.

Baggio, Guido, and Andrea Parravicini. 2019. "Introduction to Pragmatism
and Theories of Emergence." #emph[European Journal of Pragmatism and
American Philosophy] XI-2. https:\/\/doi.org/10.4000/ejpap.1611.

BonJour, Laurence. 2005. #emph[The Structure of Empirical Knowledge];.
Cambridge, MA: Harvard University Press. ISBN 978-0674843813.

Brandom, Robert B. 1994. #emph[Making it explicit];. Cambridge, MA:
Harvard University Press. ISBN 978-0674543195.

Campbell, Donald T. 1974. "The Philosophy of Karl Popper" In #emph[The
Philosophy of Karl R. Popper];, The Library of Living Philosophers XIV,
edited by Paul Arthur Schilpp, 412-463. La Salle, IL: Open Court. ISBN
0-87548-141-8.

Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief."
#emph[Journal for the History of Analytical Philosophy] 3(5): 1--27.
https:\/\/doi.org/10.15173/jhap.v3i5.28.

Davidson, Donald. 1986. "Truth and interpretation" In #emph[Truth and
Interpretation: Perspectives on the Philosophy of Donald Davidson];,
edited by Ernest LePore, 307-319. Oxford: Basil Blackwell. ISBN
0-631-14811-6.

El-Hani, Charbel Niño, and Sami Pihlström. 2002. "Emergence Theories and
Pragmatic Realism." #emph[Essays in Philosophy] 3(2): article 3.
https:\/\/doi.org/10.5840/eip2002325.

Friston, Karl J. 2013. "Life as We Know It." #emph[Journal of the Royal
Society Interface] 10 (86): 20130475.
https:\/\/doi.org/10.1098/rsif.2013.0475.

Gell-Mann, Murray, and Seth Lloyd. 1996. "Information Measures,
Effective Complexity, and Total Information." #emph[Complexity] 2(1):
44--52.
https:\/\/doi.org/10.1002/(SICI)1099-0526(199609/10)2:1\<44::AID-CPLX9\>3.0.CO;2-X.

Goldman, Alvin I. 1999. #emph[Knowledge in a Social World];. Oxford:
Oxford University Press. ISBN 978-0198238201.

Goldman, John C., and James Dolan. 1979. "What is Justified Belief?" In
#emph[Mathematics Unlimited: 1979 and Beyond];, edited by Bjorn Engquist
and Wilfried Schmid, 29-50. Berlin: Springer.
https:\/\/doi.org/10.1007/978-3-642-56478-9\_3. ISBN 978-3540669135
(hardcover); 978-3642631146 (softcover); 978-3642564789 (ebook).

Harding, Sandra. 1991. #emph[Whose science? Whose knowledge?];. Ithaca,
NY: Cornell University Press. ISBN 978-0801497469.

Hodge, A. Trevor. 2002. #emph[Roman Aqueducts and Water Supply
(Duckworth Archaeology) (Duckworth Archaeology)];. London: Duckworth.
ISBN 978-0715631713.

Holling, C. S. 1973. "Resilience and Stability of Ecological Systems."
#emph[Annual Review of Ecology and Systematics] 4: 1--23.
https:\/\/doi.org/10.1146/annurev.es.04.110173.000245.

Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In
#emph[Oxford Studies in Epistemology];, vol.~1, edited by Tamar Szabó
Gendler and John Hawthorne, 167--96. Oxford: Oxford University Press.

Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?"
#emph[Philosophical Perspectives] 2: 381--405.
https:\/\/doi.org/10.2307/2214082.

Kitcher, Philip. 1993. #emph[The advancement of science];. New York:
Oxford University Press. ISBN 978-0195046281.

Krag, Erik. 2015. "Coherentism and Belief Fixation." #emph[Logos &
Episteme] 6, no. 2: 187--199.
https:\/\/doi.org/10.5840/logos-episteme20156211. ISSN 2069-0533.

Kuhn, Thomas S. 1996. #emph[The Structure of Scientific Revolutions];.
3rd ed.~Chicago: University of Chicago Press (originally 1962). ISBN
978-0226458083.

Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent
Beliefs: A Solution." #emph[Southern Journal of Philosophy] 50(1):
21--41. https:\/\/doi.org/10.1111/j.2041-6962.2011.00090.x.

Ladyman, James, and Don Ross. 2007. #emph[Every Thing Must Go:
Metaphysics Naturalized];. Oxford: Oxford University Press.

Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific
Research Programmes." In #emph[Criticism and the Growth of Knowledge];,
edited by Imre Lakatos and Alan Musgrave, 91--196. Cambridge: Cambridge
University Press.

Laudan, Larry. 1977. #emph[Progress and Its Problems: Towards a Theory
of Scientific Growth];. Berkeley: University of California Press. ISBN
978-0520037212.

Longino, Helen E. 1990. #emph[Science as social knowledge];. Princeton,
NJ: Princeton University Press. ISBN 978-0691020518.

Longino, Helen E. 2002. #emph[The Fate of Knowledge];. Princeton, NJ:
Princeton University Press. ISBN 978-0691088761.

Lugones, María. 2003. #emph[Pilgrimages/Peregrinajes];. Lanham, MD:
Rowman & Littlefield. ISBN 978-0742514591.

Lynch, Michael P. 2009. #emph[Truth as One and Many];. Oxford: Clarendon
Press. ISBN 978-0199218738.

March, James G. 1978. "Bounded Rationality, Ambiguity, and the
Engineering of Choice." #emph[The Bell Journal of Economics] 9, no. 2:
587--608. https:\/\/doi.org/10.2307/3003600.

Meadows, Donella H. 2008. #emph[Thinking in Systems: A Primer];. Edited
by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
ISBN 978-1603580557.

Mesoudi, Alex. 2011. #emph[Cultural evolution];. Chicago: University of
Chicago Press. ISBN 978-0226520445.

Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V.
Quine's Engineering Model and Michael A. Bishop and J.D. Trout's
Strategic Reliabilism." Master's thesis, University of Victoria.
http:\/\/hdl.handle.net/1828/4915.

Newman, Mark. 2017. #emph[Networks: An Introduction];. Oxford: Oxford
University Press. ISBN 978-0199206650.

O'Connor, Cailin, and James Owen Weatherall. 2019. #emph[The
misinformation age];. New Haven, CT: Yale University Press. ISBN
978-0300234015.

Olsson, Erik J. 2005. #emph[Against coherence];. Oxford: Oxford
University Press. ISBN 978-0199279999.

Pearl, Judea. 1988. #emph[Probabilistic Reasoning in Intelligent
Systems];. San Mateo, CA: Morgan Kaufmann. ISBN 978-0934613736.

Plantinga, Alvin. 1993. #emph[Warrant and Proper Function];. New York:
Oxford University Press. ISBN 978-0195078640.

Popper, Karl. 2002. #emph[The Logic of Scientific Discovery (Routledge
Classics)];. London: Hutchinson (originally 1934). ISBN 978-0415278447.

Price, Huw. 1992. "Metaphysical Pluralism." #emph[Journal of Philosophy]
89(8): 387--409. https:\/\/doi.org/10.2307/2940741.

Putnam, Hilary. 2004. #emph[The Collapse of the Fact/Value Dichotomy and
Other Essays];. Cambridge, MA: Harvard University Press. ISBN
978-0674013803.

Quine, W. V. O. 1960. #emph[Word and Object];. Cambridge, MA: MIT Press.
ISBN 978-0262670012.

Quine, W. V. 1969. "3. Epistemology Naturalized" In #emph[Ontological
Relativity and Other Essays];, 69--90. New York: Columbia University
Press. https:\/\/doi.org/10.7312/quin92204-004. ISBN 9780231083577,
\9780231171991.

Quine, W. V. O. 1951. "Main Trends in Recent Philosophy: Two Dogmas of
Empiricism" #emph[Philosophical Review] 60(1): 20--43.
https:\/\/doi.org/10.2307/2181906.

Rescher, Nicholas. 1973. #emph[The Coherence Theory of Truth];. Oxford:
Clarendon Press. ISBN 978-0198244011.

Rissanen, Jorma. 1978. "Modeling by Shortest Data Description."
#emph[Automatica] 14(5): 465--471.
https:\/\/doi.org/10.1016/0005-1098(78)90005-5.

Rorty, Richard. 1989. #emph[Contingency, Irony, and Solidarity];.
Cambridge: Cambridge University Press. ISBN 978-0521367813.

Rosenstock, Sarita, Cailin O'Connor, and Justin Bruner. 2017. "In
Epistemic Networks, Is Less Really More?" #emph[Philosophy of Science]
84(2): 234--52. https:\/\/doi.org/10.1086/690717.

Russell, Bertrand. 2007. #emph[The Principles Of Mathematics V1];.
Cambridge: Cambridge University Press. ISBN 978-1430476030.

Sims, Matthew. 2023. "The Principle of Dynamic Holism: Guiding
Methodology for Investigating Cognition in Nonneuronal Organisms."
#emph[Philosophy of Science] 91(2): 430--48.
https:\/\/doi.org/10.1017/psa.2023.104.

Snow, John. 1855. #emph[On the Mode of Communication of Cholera];. 2nd
ed.~London: John Churchill.

Staffel, Julia. 2020. "Reasons Fundamentalism and Rational Uncertainty
-- Comments on Lord, The Importance of Being Rational." #emph[Philosophy
and Phenomenological Research] 100, no. 2: 463--468.
https:\/\/doi.org/10.1111/phpr.12675. ISSN 0031-8205.

Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth."
Master's thesis, University of Jyväskylä, Department of Social Sciences
and Philosophy. https:\/\/urn.fi/URN:NBN:fi:jyu-201705312584.

Thagard, Paul. 1989. "Explanatory Coherence." #emph[Behavioral and Brain
Sciences] 12(3): 435--502. https:\/\/doi.org/10.1017/S0140525X00057046.

Turchin, Peter. 2003. #emph[Historical dynamics];. Princeton, NJ:
Princeton University Press. ISBN 978-0691116693.

Watson, James D., and Francis H.C. Crick. 1953. "Molecular Structure of
Nucleic Acids: A Structure for Deoxyribose Nucleic Acid." #emph[Nature]
171(4356): 737--738. https:\/\/doi.org/10.1038/171737a0.

Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?"
#emph[Dialectica] 43(1--2): 99--124.
https:\/\/doi.org/10.1111/j.1746-8361.1989.tb00933.x.

Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding
and Selection in Evolution." #emph[Proceedings of the Sixth
International Congress of Genetics] 1: 356--66.

Zagzebski, Linda Trinkaus. 1996. #emph[Virtues of the Mind: An Inquiry
into the Nature of Virtue and the Ethical Foundations of Knowledge];.
Cambridge: Cambridge University Press.

Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in
Epistemic Communities." #emph[Philosophy Compass] 8(1): 15--27.
https:\/\/doi.org/10.1111/j.1747-9991.2012.00534.x.
