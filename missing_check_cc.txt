
================================================================================
Citation Extraction Run - 2025-12-17 10:44:46
Files scanned: Computational-Closure-and-the-Architecture-of-Mind.md
Total citations: 49
================================================================================


################################################################################
FILE: Computational-Closure-and-the-Architecture-of-Mind.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 17
Citation: (Friston 2010)

Context:
This paper is a work of synthesis, not empirical discovery. It presents no new experimental data in neuroscience, no new theorems in thermodynamics or information theory. Rather, it aligns existing, well-validated concepts from information geometry (computational closure), cognitive science (predictive processing), and epistemology (Quinean holism) to resolve friction between them. Our contribution is architectural alignment—showing how these frameworks illuminate each other when properly connected. We synthesize rather than discover, integrate rather than originate.

Contemporary theories of mind and knowledge largely talk past one another. Cognitive science increasingly describes the brain as a prediction engine minimizing free energy to persist in its environment (Friston 2010). This is a fundamentally thermodynamic process. Meanwhile, naturalized epistemology describes knowledge as a web of beliefs justified by its coherence and pragmatic success (Quine 1960; Sinclair 2007). It makes little reference to the underlying mechanics. This leaves an explanatory gap: how do the physical constraints on an organism's existence give rise to the normative structures of justification and objective truth? (BonJour 1985; Olsson 2005)

While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as **Markov Blankets** (statistical boundaries that separate internal from external states) that achieve **Computational Closure** (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Reference:
Friston, Karl J. 2010. "The Free-Energy Principle: A Unified Brain Theory?" *Nature Reviews Neuroscience* 11 (2): 127–138. https://doi.org/10.1038/nrn2787.
------------------------------------------------------------

Citation 2 [PARENTHETICAL]:
Line: 17
Citation: (Quine 1960)

Context:
This paper is a work of synthesis, not empirical discovery. It presents no new experimental data in neuroscience, no new theorems in thermodynamics or information theory. Rather, it aligns existing, well-validated concepts from information geometry (computational closure), cognitive science (predictive processing), and epistemology (Quinean holism) to resolve friction between them. Our contribution is architectural alignment—showing how these frameworks illuminate each other when properly connected. We synthesize rather than discover, integrate rather than originate.

Contemporary theories of mind and knowledge largely talk past one another. Cognitive science increasingly describes the brain as a prediction engine minimizing free energy to persist in its environment (Friston 2010). This is a fundamentally thermodynamic process. Meanwhile, naturalized epistemology describes knowledge as a web of beliefs justified by its coherence and pragmatic success (Quine 1960; Sinclair 2007). It makes little reference to the underlying mechanics. This leaves an explanatory gap: how do the physical constraints on an organism's existence give rise to the normative structures of justification and objective truth? (BonJour 1985; Olsson 2005)

While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as **Markov Blankets** (statistical boundaries that separate internal from external states) that achieve **Computational Closure** (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Reference:
Quine, W. V. O. 1960. *Word and Object*. Cambridge, MA: MIT Press. ISBN 978-0262670012.
------------------------------------------------------------

Citation 3 [PARENTHETICAL]:
Line: 17
Citation: (Sinclair 2007)

Context:
This paper is a work of synthesis, not empirical discovery. It presents no new experimental data in neuroscience, no new theorems in thermodynamics or information theory. Rather, it aligns existing, well-validated concepts from information geometry (computational closure), cognitive science (predictive processing), and epistemology (Quinean holism) to resolve friction between them. Our contribution is architectural alignment—showing how these frameworks illuminate each other when properly connected. We synthesize rather than discover, integrate rather than originate.

Contemporary theories of mind and knowledge largely talk past one another. Cognitive science increasingly describes the brain as a prediction engine minimizing free energy to persist in its environment (Friston 2010). This is a fundamentally thermodynamic process. Meanwhile, naturalized epistemology describes knowledge as a web of beliefs justified by its coherence and pragmatic success (Quine 1960; Sinclair 2007). It makes little reference to the underlying mechanics. This leaves an explanatory gap: how do the physical constraints on an organism's existence give rise to the normative structures of justification and objective truth? (BonJour 1985; Olsson 2005)

While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as **Markov Blankets** (statistical boundaries that separate internal from external states) that achieve **Computational Closure** (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Reference:
Sinclair, Robert. 2007. "Quine's Naturalized Epistemology and the Third Dogma of Empiricism." *Southern Journal of Philosophy* 45, no. 3: 455–472. https://doi.org/10.1111/j.2041-6962.2007.tb00060.x.
------------------------------------------------------------

Citation 4 [PARENTHETICAL]:
Line: 17
Citation: (BonJour 1985)

Context:
This paper is a work of synthesis, not empirical discovery. It presents no new experimental data in neuroscience, no new theorems in thermodynamics or information theory. Rather, it aligns existing, well-validated concepts from information geometry (computational closure), cognitive science (predictive processing), and epistemology (Quinean holism) to resolve friction between them. Our contribution is architectural alignment—showing how these frameworks illuminate each other when properly connected. We synthesize rather than discover, integrate rather than originate.

Contemporary theories of mind and knowledge largely talk past one another. Cognitive science increasingly describes the brain as a prediction engine minimizing free energy to persist in its environment (Friston 2010). This is a fundamentally thermodynamic process. Meanwhile, naturalized epistemology describes knowledge as a web of beliefs justified by its coherence and pragmatic success (Quine 1960; Sinclair 2007). It makes little reference to the underlying mechanics. This leaves an explanatory gap: how do the physical constraints on an organism's existence give rise to the normative structures of justification and objective truth? (BonJour 1985; Olsson 2005)

While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as **Markov Blankets** (statistical boundaries that separate internal from external states) that achieve **Computational Closure** (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press. ISBN 978-0674843813.
------------------------------------------------------------

Citation 5 [PARENTHETICAL]:
Line: 17
Citation: (Olsson 2005)

Context:
This paper is a work of synthesis, not empirical discovery. It presents no new experimental data in neuroscience, no new theorems in thermodynamics or information theory. Rather, it aligns existing, well-validated concepts from information geometry (computational closure), cognitive science (predictive processing), and epistemology (Quinean holism) to resolve friction between them. Our contribution is architectural alignment—showing how these frameworks illuminate each other when properly connected. We synthesize rather than discover, integrate rather than originate.

Contemporary theories of mind and knowledge largely talk past one another. Cognitive science increasingly describes the brain as a prediction engine minimizing free energy to persist in its environment (Friston 2010). This is a fundamentally thermodynamic process. Meanwhile, naturalized epistemology describes knowledge as a web of beliefs justified by its coherence and pragmatic success (Quine 1960; Sinclair 2007). It makes little reference to the underlying mechanics. This leaves an explanatory gap: how do the physical constraints on an organism's existence give rise to the normative structures of justification and objective truth? (BonJour 1985; Olsson 2005)

While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as **Markov Blankets** (statistical boundaries that separate internal from external states) that achieve **Computational Closure** (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Reference:
Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press. ISBN 978-0199279999.
------------------------------------------------------------

Citation 6 [IN-PROSE]:
Line: 21
Citation: Ladyman and Ross (2007)

Context:
While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as **Markov Blankets** (statistical boundaries that separate internal from external states) that achieve **Computational Closure** (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Methodologically, this framework adheres to what Ladyman and Ross (2007) term the *Principle of Naturalistic Closure*: we reject a priori intuition as a guide to metaphysics and instead require that epistemological claims unify the special sciences with fundamental physics without contradiction. The Standing Predicates and Markov blankets described herein are not merely psychological heuristics but are motivated by the convergence of thermodynamics and information theory—a convergence that grounds cognition in the physical constraints any bounded system must satisfy.

Our central thesis, which remains speculative, is that consciousness may function as the user interface for a specific type of information processing: structural pattern recognition. We distinguish this from the unconscious, frequency-based processing of statistical regularities. This distinction potentially explains why some knowledge can be acquired from a single instance and may provide a functional basis for the phenomenology of understanding. It also offers a prospective diagnostic for the limitations of current artificial intelligence, though this application requires further investigation.

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 7 [PARENTHETICAL]:
Line: 37
Citation: (Landauer 1961)

Context:
Living and cognitive systems differ fundamentally. They compress information, building internal models that predict future sensory states. Rather than immediately dissipating absorbed energy, they delay dissipation to perform computational work—encoding regularities, updating predictions, guiding action. This compression is literal (Shannon's sense), not metaphorical: reducing surprise by encoding regularities into reusable patterns.

A Note on Entropy and Energy: Throughout this paper, we distinguish between information-theoretic quantities (Shannon entropy as average surprise, measured in bits) and thermodynamic quantities (energy required to process or erase bits). However, these quantities are not merely analogous. Landauer's Principle establishes that information processing has irreducible physical costs: erasing one bit of information requires dissipating at least kT ln(2) joules of energy (Landauer 1961). This means high information leakage (persistent prediction error) implies tangible thermodynamic costs to any physical system, whether in metabolic expenditure to revise models or in kinetic energy wasted on maladaptive action. The link between epistemic brittleness and physical dissipation is therefore causal, not merely metaphorical. When we speak of "information costs" in cognitive systems, we refer to functional constraints that have, at bottom, thermodynamic grounding.

Existence as a persistent bounded pattern (rather than a transient fluctuation) requires information processing. What we call "things" are better understood as *Real Patterns* in the sense of Dennett (1991) and Ladyman and Ross (2007): they exist to the extent they compress information—maintaining statistical boundaries that distinguish internal from external states while achieving genuine projectibility (predictive purchase on future states).

Reference:
Landauer, Rolf. 1961. "Irreversibility and Heat Generation in the Computing Process." *IBM Journal of Research and Development* 5(3): 183–191. https://doi.org/10.1147/rd.1961.5.3.183. ISBN 978-0199276196.
------------------------------------------------------------

Citation 8 [IN-PROSE]:
Line: 39
Citation: Dennett (1991)

Context:
A Note on Entropy and Energy: Throughout this paper, we distinguish between information-theoretic quantities (Shannon entropy as average surprise, measured in bits) and thermodynamic quantities (energy required to process or erase bits). However, these quantities are not merely analogous. Landauer's Principle establishes that information processing has irreducible physical costs: erasing one bit of information requires dissipating at least kT ln(2) joules of energy (Landauer 1961). This means high information leakage (persistent prediction error) implies tangible thermodynamic costs to any physical system, whether in metabolic expenditure to revise models or in kinetic energy wasted on maladaptive action. The link between epistemic brittleness and physical dissipation is therefore causal, not merely metaphorical. When we speak of "information costs" in cognitive systems, we refer to functional constraints that have, at bottom, thermodynamic grounding.

Existence as a persistent bounded pattern (rather than a transient fluctuation) requires information processing. What we call "things" are better understood as *Real Patterns* in the sense of Dennett (1991) and Ladyman and Ross (2007): they exist to the extent they compress information—maintaining statistical boundaries that distinguish internal from external states while achieving genuine projectibility (predictive purchase on future states).

### 2.2 The Free Energy Principle

Reference:
Dennett, Daniel C. 1991. "Real Patterns." *Journal of Philosophy* 88(1): 27-51. https://doi.org/10.2307/2027085.
------------------------------------------------------------

Citation 9 [IN-PROSE]:
Line: 39
Citation: Ladyman and Ross (2007)

Context:
A Note on Entropy and Energy: Throughout this paper, we distinguish between information-theoretic quantities (Shannon entropy as average surprise, measured in bits) and thermodynamic quantities (energy required to process or erase bits). However, these quantities are not merely analogous. Landauer's Principle establishes that information processing has irreducible physical costs: erasing one bit of information requires dissipating at least kT ln(2) joules of energy (Landauer 1961). This means high information leakage (persistent prediction error) implies tangible thermodynamic costs to any physical system, whether in metabolic expenditure to revise models or in kinetic energy wasted on maladaptive action. The link between epistemic brittleness and physical dissipation is therefore causal, not merely metaphorical. When we speak of "information costs" in cognitive systems, we refer to functional constraints that have, at bottom, thermodynamic grounding.

Existence as a persistent bounded pattern (rather than a transient fluctuation) requires information processing. What we call "things" are better understood as *Real Patterns* in the sense of Dennett (1991) and Ladyman and Ross (2007): they exist to the extent they compress information—maintaining statistical boundaries that distinguish internal from external states while achieving genuine projectibility (predictive purchase on future states).

### 2.2 The Free Energy Principle

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 10 [PARENTHETICAL]:
Line: 52
Citation: (Friston 2010)

Context:
2. **Changing the world** to match predictions (active inference)
3. **Optimizing model structure** to reduce complexity while maintaining accuracy (structural learning)

This process is not a goal-directed endeavor but a physical constraint: systems that fail to minimize free energy tend to dissipate and vanish, whereas those that succeed maintain their integrity as bounded entities. In this sense, minimizing free energy is constitutive of self-organization (Friston 2010), though this constitutive status remains a theoretical proposal rather than an empirically established fact, requiring further validation across diverse physical systems.

The Free Energy Principle (FEP) builds on broader predictive processing frameworks in cognitive science, where brains are understood as hierarchical prediction machines constantly minimizing prediction error through bidirectional cortical processing (Clark 2013). This perspective reframes perception. It treats perception not as passive reception but as active inference, where the brain tests predictions against sensory input and revises models when mismatches occur. Neurobiologically, this is implemented through predictive coding: hierarchical brain organization where higher levels generate predictions about lower-level activity, and lower levels signal back prediction errors when inputs deviate from expectations (Friston and Kiebel 2009). This architecture provides a plausible mechanistic account of how variational free energy minimization could be realized in cortical circuits.

Reference:
Friston, Karl J. 2010. "The Free-Energy Principle: A Unified Brain Theory?" *Nature Reviews Neuroscience* 11 (2): 127–138. https://doi.org/10.1038/nrn2787.
------------------------------------------------------------

Citation 11 [PARENTHETICAL]:
Line: 54
Citation: (Clark 2013)

Context:
This process is not a goal-directed endeavor but a physical constraint: systems that fail to minimize free energy tend to dissipate and vanish, whereas those that succeed maintain their integrity as bounded entities. In this sense, minimizing free energy is constitutive of self-organization (Friston 2010), though this constitutive status remains a theoretical proposal rather than an empirically established fact, requiring further validation across diverse physical systems.

The Free Energy Principle (FEP) builds on broader predictive processing frameworks in cognitive science, where brains are understood as hierarchical prediction machines constantly minimizing prediction error through bidirectional cortical processing (Clark 2013). This perspective reframes perception. It treats perception not as passive reception but as active inference, where the brain tests predictions against sensory input and revises models when mismatches occur. Neurobiologically, this is implemented through predictive coding: hierarchical brain organization where higher levels generate predictions about lower-level activity, and lower levels signal back prediction errors when inputs deviate from expectations (Friston and Kiebel 2009). This architecture provides a plausible mechanistic account of how variational free energy minimization could be realized in cortical circuits.

The FEP's scope extends beyond brain function to encompass fundamental properties of all self-organizing systems. Any system that maintains its integrity against the second law of thermodynamics must possess a statistical boundary. This boundary (a Markov blanket) separates internal from external states. It must minimize the free energy associated with that boundary (Friston et al. 2017).

Reference:
Clark, Andy. 2013. "Whatever next? Predictive brains, situated agents, and the future of cognitive science." *Behavioral and Brain Sciences* 36(3): 181–204. https://doi.org/10.1017/S0140525X12000477.
------------------------------------------------------------

Citation 12 [IN-PROSE]:
Line: 58
Citation: Kirchhoff et al. (2018)

Context:
The FEP's scope extends beyond brain function to encompass fundamental properties of all self-organizing systems. Any system that maintains its integrity against the second law of thermodynamics must possess a statistical boundary. This boundary (a Markov blanket) separates internal from external states. It must minimize the free energy associated with that boundary (Friston et al. 2017).

Kirchhoff et al. (2018) demonstrate that living systems are characterized by Markov blankets enabling autonomous organization, and that "autonomous systems are hierarchically composed of Markov blankets of Markov blankets—all the way down to individual cells, all the way up to you and me." This hierarchical assembly occurs through adaptive active inference, distinguishing living systems (capable of inferring future states) from non-living systems exhibiting mere active inference without adaptive capacity.

This connects information-theoretic principles directly to the basic requirements for life itself. Autopoiesis, allostasis, and goal-directed behavior all emerge as consequences of free energy minimization in bounded systems.

Reference:
Kirchhoff, Michael, Thomas Parr, Ensor Palacios, Karl Friston, and Julian Kiverstein. 2018. "The Markov Blankets of Life: Autonomy, Active Inference and the Free Energy Principle." *Journal of the Royal Society Interface* 15(138): 20170792. https://doi.org/10.1098/rsif.2017.0792.
------------------------------------------------------------

Citation 13 [IN-PROSE]:
Line: 62
Citation: Glenn (2025)

Context:
This connects information-theoretic principles directly to the basic requirements for life itself. Autopoiesis, allostasis, and goal-directed behavior all emerge as consequences of free energy minimization in bounded systems.

Connection to Epistemic Brittleness: Systemic brittleness is accumulated free energy. When a knowledge system's predictions consistently fail (information leakage), it must either patch the model with ad-hoc additions, suppress disconfirming evidence through coercion, or accept falsification and revise. The brittleness metrics developed in Glenn (2025) (patch velocity, coercive overhead, model complexity, resilience reserve) measure these information-theoretic costs directly.

Reality functions as the constraint landscape that shapes belief revision. Each failed prediction generates an error signal—a mismatch between expected and actual outcomes that drives the system to update its compression. In Quine's terms, beliefs form a web where central propositions (those encoding high structural compression, like logic and mathematics) resist revision because changing them would require massive reorganization of the entire system. The brittleness metrics track exactly these revision costs: when reality's constraints conflict with a system's compressions, the accumulated prediction errors manifest as increasing patch velocity, rising model complexity, and declining resilience. The system faces a choice—revise the compression to align with constraints, or expend energy suppressing the error signals through coercion.

Reference:
Glenn, Patrick. 2025. "The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth." PhilPapers. https://philpapers.org/rec/GLETAO.
------------------------------------------------------------

Citation 14 [IN-PROSE]:
Line: 68
Citation: Dittrich and Kinne (2024)

Context:
### 2.3 Dispositions as Compression Algorithms

Recent formal work provides rigorous information-theoretic foundations for this compression imperative. Dittrich and Kinne (2024) demonstrate that any system persisting under uncertainty must compress information to minimize what they term "epistemic entropy." This is the Information-Theoretic Imperative (ITI): bounded existence requires efficient compression as a constitutive condition, not merely a useful strategy. Their Compression Efficiency Principle (CEP) further shows that efficient compression mechanically selects for generative, causal models over superficial correlations. Non-causal compressions "accumulate exceptions" as novel cases fail to fit the pattern, eventually becoming unsustainable. The drive to compress is thus a physical necessity for bounded systems. This mechanism explains why systems are forced to seek computational closure: only compressions achieving genuine causal structure can sustain efficiency without catastrophic exception accumulation.

This convergence of independent formalizations, including classical information theory (Shannon 1948), thermodynamic cost analysis (Landauer 1961), and evolutionary epistemology (Campbell 1974; Bradie 1986), suggests that these principles capture genuine structural features of how bounded systems interact with constrained environments, not artifacts of any particular theoretical framework.

Reference:
Dittrich, Christian, and Jennifer Flygare Kinne. "The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence." Preprint, submitted October 30, 2024. arXiv:2510.25883 [cs.AI]. https://doi.org/10.48550/arXiv.2510.25883.
------------------------------------------------------------

Citation 15 [PARENTHETICAL]:
Line: 70
Citation: (Shannon 1948)

Context:
Recent formal work provides rigorous information-theoretic foundations for this compression imperative. Dittrich and Kinne (2024) demonstrate that any system persisting under uncertainty must compress information to minimize what they term "epistemic entropy." This is the Information-Theoretic Imperative (ITI): bounded existence requires efficient compression as a constitutive condition, not merely a useful strategy. Their Compression Efficiency Principle (CEP) further shows that efficient compression mechanically selects for generative, causal models over superficial correlations. Non-causal compressions "accumulate exceptions" as novel cases fail to fit the pattern, eventually becoming unsustainable. The drive to compress is thus a physical necessity for bounded systems. This mechanism explains why systems are forced to seek computational closure: only compressions achieving genuine causal structure can sustain efficiency without catastrophic exception accumulation.

This convergence of independent formalizations, including classical information theory (Shannon 1948), thermodynamic cost analysis (Landauer 1961), and evolutionary epistemology (Campbell 1974; Bradie 1986), suggests that these principles capture genuine structural features of how bounded systems interact with constrained environments, not artifacts of any particular theoretical framework.

Returning to the Quinean foundation: a disposition to assent functions analogously to a compressed encoding of regularities.

Reference:
Shannon, Claude E. 1948. "A Mathematical Theory of Communication." *Bell System Technical Journal* 27(3): 379–423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x.
------------------------------------------------------------

Citation 16 [PARENTHETICAL]:
Line: 70
Citation: (Landauer 1961)

Context:
Recent formal work provides rigorous information-theoretic foundations for this compression imperative. Dittrich and Kinne (2024) demonstrate that any system persisting under uncertainty must compress information to minimize what they term "epistemic entropy." This is the Information-Theoretic Imperative (ITI): bounded existence requires efficient compression as a constitutive condition, not merely a useful strategy. Their Compression Efficiency Principle (CEP) further shows that efficient compression mechanically selects for generative, causal models over superficial correlations. Non-causal compressions "accumulate exceptions" as novel cases fail to fit the pattern, eventually becoming unsustainable. The drive to compress is thus a physical necessity for bounded systems. This mechanism explains why systems are forced to seek computational closure: only compressions achieving genuine causal structure can sustain efficiency without catastrophic exception accumulation.

This convergence of independent formalizations, including classical information theory (Shannon 1948), thermodynamic cost analysis (Landauer 1961), and evolutionary epistemology (Campbell 1974; Bradie 1986), suggests that these principles capture genuine structural features of how bounded systems interact with constrained environments, not artifacts of any particular theoretical framework.

Returning to the Quinean foundation: a disposition to assent functions analogously to a compressed encoding of regularities.

Reference:
Landauer, Rolf. 1961. "Irreversibility and Heat Generation in the Computing Process." *IBM Journal of Research and Development* 5(3): 183–191. https://doi.org/10.1147/rd.1961.5.3.183. ISBN 978-0199276196.
------------------------------------------------------------

Citation 17 [PARENTHETICAL]:
Line: 70
Citation: (Campbell 1974)

Context:
Recent formal work provides rigorous information-theoretic foundations for this compression imperative. Dittrich and Kinne (2024) demonstrate that any system persisting under uncertainty must compress information to minimize what they term "epistemic entropy." This is the Information-Theoretic Imperative (ITI): bounded existence requires efficient compression as a constitutive condition, not merely a useful strategy. Their Compression Efficiency Principle (CEP) further shows that efficient compression mechanically selects for generative, causal models over superficial correlations. Non-causal compressions "accumulate exceptions" as novel cases fail to fit the pattern, eventually becoming unsustainable. The drive to compress is thus a physical necessity for bounded systems. This mechanism explains why systems are forced to seek computational closure: only compressions achieving genuine causal structure can sustain efficiency without catastrophic exception accumulation.

This convergence of independent formalizations, including classical information theory (Shannon 1948), thermodynamic cost analysis (Landauer 1961), and evolutionary epistemology (Campbell 1974; Bradie 1986), suggests that these principles capture genuine structural features of how bounded systems interact with constrained environments, not artifacts of any particular theoretical framework.

Returning to the Quinean foundation: a disposition to assent functions analogously to a compressed encoding of regularities.

Reference:
Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.
------------------------------------------------------------

Citation 18 [IN-PROSE]:
Line: 122
Citation: Ayvazov (2025)

Context:
- "F=ma" (structural—mathematical necessity once the concepts are understood)
- "Swans are white" (statistical—inductively generalized from frequency, famously failed)

Recent work in phase epistemology provides formal treatment of this distinction. Ayvazov (2025) distinguishes between classical probability and what he terms "improbabilistic coherence." Classical probability involves frequency-based likelihood, while improbabilistic coherence refers to structural integrity that exists independent of repetition. He defines this as "the generative condition for epistemic emergence." While Ayvazov proposes a speculative quantum-mechanical formalism for this distinction, we employ it here purely as an epistemic category without committing to his physical interpretation.

Implications for Information Compression:
- Statistical compressions require large ensembles to stabilize (high sample complexity)

Reference:
Ayvazov, Mahammad. 2025. "Toward a Phase Epistemology: Coherence, Response and the Vector of Mutual Uncertainty." *SSRN Electronic Journal*. https://doi.org/10.2139/ssrn.5250197.
------------------------------------------------------------

Citation 19 [PARENTHETICAL]:
Line: 132
Citation: (Aslin and Newport 2012)

Context:
Structural patterns are not subjectively imposed but constrained by reality. You cannot validly infer that "ice produces heat" from a single encounter because thermodynamics forbids this relationship. The structural constraints are objective, even if recognizable from limited data.

Empirical work in cognitive science supports this distinction. Statistical learning operates implicitly through mere exposure to input patterns, extracting regularities from repeated encounters (Aslin and Newport 2012). However, the same mechanisms that enable learning from frequency distributions also support generalization to novel instances when structural relationships are detected—suggesting a unified learning system capable of both statistical pattern matching and structural inference.

This distinction becomes essential for understanding how notions (proto-Standing Predicates) can form before extensive empirical testing, and why some singular experiences carry immediate epistemic weight while others require statistical accumulation.

Reference:
Aslin, Richard N., and Elissa L. Newport. 2012. "Statistical learning: From acquiring specific items to forming general rules." *Current Directions in Psychological Science* 21(3): 170–176. https://doi.org/10.1177/0963721412436806.
------------------------------------------------------------

Citation 20 [IN-PROSE]:
Line: 165
Citation: Mangalam (2025)

Context:
Our Position: This paper primarily makes claims at levels 1 and 2. Whether the strong mechanistic claims (level 3) are literally true remains an empirical question for neuroscience and cognitive science. Our philosophical insights about Standing Predicates, compression, and truth don't depend on resolving this empirical question. Even if the brain's actual mechanisms differ significantly from Free Energy minimization, the functional analysis of what makes a predicate successful (low brittleness, high compression, computational closure) retains its philosophical force.

Acknowledging Fundamental Critiques: Recent work has challenged the FEP's scientific status directly. Mangalam (2025) argues that the FEP operates as an unfalsifiable pseudo-theory. Its high level of abstraction and mathematical formalism make it difficult to test empirically. It may not generate novel predictions that simpler theories cannot explain.

The Strong Pivot: This critique deserves a direct response, and we can offer one that strengthens rather than weakens our philosophical framework. We accept the characterization but reject the implication that it lacks explanatory power. Even if Mangalam is correct that the FEP describes necessary consequences of being a bounded system rather than contingent empirical laws, this makes it precisely the right foundation for our philosophical project.

Reference:
Mangalam, Madhur. 2025. "The Emperor's New Pseudo-Theory: How the Free Energy Principle Ransacked Neuroscience." Preprint. DOI: 10.31234/osf.io/azkgc. https://osf.io/azkgc.
------------------------------------------------------------

Citation 21 [PARENTHETICAL]:
Line: 193
Citation: (Glenn 2025)

Context:
**Connecting Functional Constraints to Normative Claims:** Readers may wonder how later sections derive substantive claims about objective truth (Section 6) and ethics (Section 8) from what we've framed as "conceptual scaffolding." The answer: these claims rely on level 2 (functional) constraints, not level 3 (mechanistic) implementation. The argument is not "brains compute Shannon entropy, therefore truth exists," but rather "whatever systems successfully compress reality must satisfy certain functional constraints (computational closure, minimal information leakage, strong lumpability), and these constraints determine which predicates persist." The Apex Network is "objective" not because it exists as a Platonic form or neural structure, but because the functional requirements for successful compression are determined by reality's constraint structure, not by our beliefs about them. Similarly, the claim that coercion generates brittleness doesn't require literal free energy calculations—it requires only that systems refusing to model agents' autonomous responses must bear higher coordination costs. The normative force comes from functional necessity, not mechanistic implementation.

**Operationalizing Brittleness:** The concept of systemic brittleness plays a central role in this framework as the diagnostic tool for assessing knowledge system health. While this paper maintains focus on the philosophical foundations and conceptual architecture, detailed operationalization of brittleness diagnostics has been developed elsewhere (Glenn 2025). That work develops specific conceptual lenses for diagnosing brittleness across different domains: patch velocity P(t) measuring the ratio of anomaly-resolution to novel-prediction work; coercion ratio C(t) measuring security overhead versus productive investment; model complexity M(t) tracking parameter growth against marginal performance gains; and resilience reserve R(t) assessing cross-domain confirmatory breadth. These indicators serve as analytical categories for historical and philosophical analysis rather than quantitative metrics, providing structured tools for comparative assessment of epistemic system viability. The framework thus offers both philosophical coherence and practical diagnostic capacity without requiring the mechanistic claims of level 3.

Preserving the Insights: When we say "dispositions are compression algorithms" or "Standing Predicates are Markov blankets," read these as capturing functional roles rather than ontological identities. A disposition functions like a compression algorithm—it reduces redundancy, encodes regularities, enables prediction. Whether it literally performs Huffman encoding or some neurally-implemented equivalent doesn't affect the philosophical point about what makes it successful or brittle.

Reference:
Glenn, Patrick. 2025. "The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth." PhilPapers. https://philpapers.org/rec/GLETAO.
------------------------------------------------------------

Citation 22 [IN-PROSE]:
Line: 232
Citation: Ladyman and Ross (2007)

Context:
### 3.2 Rainforest Realism: Blankets Delineate Real Patterns

Markov blankets are enacted, not discovered. They emerge when certain configurations of matter successfully maintain statistical boundaries against entropic dissolution. This perspective aligns with what Ladyman and Ross (2007) call *Rainforest Realism*: the ontology of the world is not a sparse desert of fundamental particles but a rich layering of Real Patterns at every scale where information can be compressed. A Markov blanket does not invent a useful fiction; it isolates a Real Pattern that carries genuine information load—what survives the compression process because it enables prediction.

Examples Across Scales:

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 23 [IN-PROSE]:
Line: 292
Citation: Kirchhoff et al. (2018)

Context:
When all three conditions hold, emergence has succeeded: a new causal level exists.

This hierarchical emergence arises through self-assembly. Kirchhoff et al. (2018) demonstrate that collectives of Markov blankets can self-assemble into global systems that themselves possess Markov blankets, creating nested boundaries from cells to organisms to social systems. Simulations demonstrate that hierarchical self-organization emerges naturally when microscopic elements have prior beliefs that they participate in macroscopic Markov blankets (Palacios et al. 2020). This suggests nested blanket hierarchies are not imposed from outside but arise spontaneously when components minimize free energy under appropriate constraints—providing a mechanistic account of how computational closure forms across levels.

Formalizing Causal Autonomy: Rosas et al. (2024) provide a rigorous framework for determining when emergence genuinely succeeds by comparing two optimal predictors. The ε-machine (epsilon-machine) uses only macro-level variables to predict future macro-states. The υ-machine (upsilon-machine) has full access to micro-level details and uses them to predict the same macro-states.

Reference:
Kirchhoff, Michael, Thomas Parr, Ensor Palacios, Karl Friston, and Julian Kiverstein. 2018. "The Markov Blankets of Life: Autonomy, Active Inference and the Free Energy Principle." *Journal of the Royal Society Interface* 15(138): 20170792. https://doi.org/10.1098/rsif.2017.0792.
------------------------------------------------------------

Citation 24 [IN-PROSE]:
Line: 294
Citation: Rosas et al. (2024)

Context:
This hierarchical emergence arises through self-assembly. Kirchhoff et al. (2018) demonstrate that collectives of Markov blankets can self-assemble into global systems that themselves possess Markov blankets, creating nested boundaries from cells to organisms to social systems. Simulations demonstrate that hierarchical self-organization emerges naturally when microscopic elements have prior beliefs that they participate in macroscopic Markov blankets (Palacios et al. 2020). This suggests nested blanket hierarchies are not imposed from outside but arise spontaneously when components minimize free energy under appropriate constraints—providing a mechanistic account of how computational closure forms across levels.

Formalizing Causal Autonomy: Rosas et al. (2024) provide a rigorous framework for determining when emergence genuinely succeeds by comparing two optimal predictors. The ε-machine (epsilon-machine) uses only macro-level variables to predict future macro-states. The υ-machine (upsilon-machine) has full access to micro-level details and uses them to predict the same macro-states.

When these two predictors perform equally well—when ε-machine accuracy equals υ-machine accuracy—something remarkable has happened: the macro-level has achieved causal decoupling from the micro-level. The macro-variables contain all the information needed to predict their own future behavior. Substrate details have become causally irrelevant (though they remain constitutively necessary). This is computational closure: the macro-level is "running code" rather than merely describing patterns in the substrate. This equivalence provides the rigorous criterion for a "Real Pattern" in Dennett's sense: a compression that loses nothing of predictive value. When adding micro-level details yields zero additional information about the macro-future, the macro-level description has captured genuine structure in reality rather than imposing convenient fiction.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 25 [IN-PROSE]:
Line: 335
Citation: Rosas et al. (2024)

Context:
When both players predict equally well, the board position has achieved computational closure. The macro-level (piece arrangement) contains all the information needed to predict future macro-states (subsequent positions). Crucially, once the current board position is known, the history of how the game arrived there is irrelevant for predicting legal moves. The game has "detached" from its substrate (the players' brains, their training, their moods) and runs purely on positional logic. This exemplifies causal shielding—the macro-state (board position) screens off historical details, achieving genuine computational closure.

Rosas et al. (2024) formalize exactly this criterion: when your ε-machine (macro-only predictor) performs as well as the υ-machine (micro-informed predictor), you have discovered a level of organization that has achieved causal decoupling. The macro-level is "running code" rather than merely describing patterns in the substrate. This is the formal signature of successful emergence.

Why This Matters:
- Dispositions are cognitive ε-machines—they compress experience into causal states (notions, beliefs)

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 26 [IN-PROSE]:
Line: 344
Citation: Rosas et al. (2024)

Context:
The Search Process: Organisms, communities, and species are ε-machine explorers, trying different compressions. The ones that achieve genuine computational closure while minimizing brittleness survive. This is not random but hill-climbing on the landscape of viable blanket configurations.

Quine's "web of belief" metaphor finds precise formalization in the ε-machine lattice structure described by Rosas et al. (2024). Each belief is a node in this lattice—a compression of experience with specific causal relationships to other beliefs. Central beliefs (logic, mathematics, basic thermodynamics) occupy positions of high structural compression: they encode fundamental constraints that many other beliefs depend upon. Revising them requires massive reorganization, changing prediction patterns throughout the entire system. Peripheral beliefs (today's weather forecast, a stranger's name) are shallow compressions with minimal dependencies. This explains Quine's observation that we revise peripheral beliefs readily when evidence conflicts, but resist changing core commitments. The resistance is not psychological stubbornness but information-theoretic necessity—central nodes bear higher revision costs. The web metaphor thus captures genuine computational structure: beliefs form a network where some compressions are load-bearing and others decorative.

### 3.5 Pragmatic Ontology: Same Information, Different Blankets

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 27 [IN-PROSE]:
Line: 365
Citation: Rosas et al. (2024)

Context:
Key Insight: Epistemic Equifinality. In Systems Theory, equifinality describes how different structural configurations can achieve the same steady state. The Hot Dog Paradox illustrates epistemic equifinality: different Markov Blankets (definitions) can achieve comparable levels of computational closure depending on the system's goal (taxation vs. cuisine vs. engineering). Each community draws the boundary where it reduces brittleness for their purposes. The information hasn't changed—the coarse-graining has. This is not arbitrary: each blanket faces pragmatic testing. However, it is pluralistic: multiple viable configurations exist.

Rosas et al. (2024) formalize this insight: valid coarse-grainings form a mathematical lattice structure, where multiple macro-level compressions can achieve strong lumpability for the same underlying system. The existence of this lattice explains epistemic equifinality—different purposes may select different points in the space of viable compressions, all of which genuinely achieve computational closure.

This lattice structure also provides the formal justification for Rainforest Realism. Because different physical substrates can realize computationally equivalent ε-machines, reality genuinely supports multiple valid ontologies enacted at different scales. However, this is not relativism. The constraint is strong lumpability: you can draw boundaries in various ways, but only those configurations that yield self-contained, Markovian macro-dynamics qualify as genuine compressions. Most arbitrary groupings fail this test. The "rainforest" of real patterns consists precisely of those coarse-grainings that achieve computational closure. A "hot dog" is a valid object only if the hot dog variable predicts its own future behavior better than the noise floor of tracking bread molecules separately.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 28 [IN-PROSE]:
Line: 436
Citation: Glenn (2025)

Context:
### 3.6.5 From Cognitive Mechanisms to Social Truth

A potential confusion must be addressed. Glenn (2025) distinguishes Standing Predicates by their relationship to networks (Consensus Network versus Apex Network). This paper distinguishes patterns by their cognitive processing requirements (statistical versus structural). Are these competing typologies?

No. They describe different dimensions of the same epistemic landscape. The two-way distinction (statistical versus structural) describes cognitive mechanisms: how the brain processes raw data to form beliefs. Network alignment describes validation states: how we determine whether those beliefs count as justified knowledge within a social context.

Reference:
Glenn, Patrick. 2025. "The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth." PhilPapers. https://philpapers.org/rec/GLETAO.
------------------------------------------------------------

Citation 29 [PARENTHETICAL]:
Line: 474
Citation: (Chalmers 1996)

Context:
Having established how Markov blankets enable computational closure, we now turn to consciousness. The transition from mere information processing to subjective awareness remains the central puzzle of philosophy of mind. By applying our framework of structural vs. statistical pattern recognition, we can identify specific functional correlates of consciousness without claiming to fully solve the Hard Problem.

Scope of Claims: We must distinguish the Hard Problem from the Structural Problem. The Hard Problem (Chalmers 1996) asks why qualitative experience exists at all—why there is "something it is like" to see red, why phenomenology has the specific character it does. The Structural Problem asks why specific functional processes correlate with reportable awareness—why we can introspect on some cognitive states but not others, why understanding feels different from mere familiarity.

The following two sections address consciousness from complementary angles. Section 4A develops the theoretical framework, identifying functional correlates without claiming to solve the Hard Problem. Section 4B explores phenomenological applications and extensions of this framework. We identify structural pattern recognition and meta-blanket formation as functional correlates of consciousness without claiming to explain why these processes have phenomenological character. Phenomenology functions as the user interface for structural pattern recognition—the subjective signature of detecting mutual constraints between components. Whether this interface requires non-physical ontology, or whether the functional account exhausts consciousness, remains a separate metaphysical question beyond this framework's scope.

Reference:
Chalmers, David J. 2006. "Strong and Weak Emergence." In *The Re-Emergence of Emergence: The Emergentist Hypothesis from Science to Religion*, edited by Philip Clayton and Paul Davies, 244–54. Oxford: Oxford University Press. ISBN 9780199287147. https://doi.org/10.1093/acprof:oso/9780199287147.003.0011.
------------------------------------------------------------

Citation 30 [IN-PROSE]:
Line: 531
Citation: Laukkonen et al. (2025)

Context:
3. **Aha! moments:** Sudden recognition of structural relationships (Section 3.1 table below)
4. **Automation through practice:** Once structural understanding achieved, execution becomes statistical refinement (unconscious)

Connection to Active Inference Theories of Consciousness: Recent work grounds consciousness directly in Active Inference mechanisms. Laukkonen et al. (2025) propose that consciousness arises from a recursive self-evidencing loop in hierarchical systems. This requires three functional conditions: simulation of an epistemic field (world model), Bayesian binding (inferential competition to enter the world model), and epistemic depth (recurrent sharing of beliefs across levels).

Their framework aligns with our structural pattern recognition hypothesis. Consciousness may engage specifically when patterns exhibit sufficient internal constraint structure to support these recursive operations. Statistical regularities can be processed automatically without activating the full recursive loop. Structural coherence, however—precisely because components mutually constrain each other—demands the kind of inferential integration their theory describes.

Reference:
Laukkonen, Ruben, et al. 2025. "A Beautiful Loop: An Active Inference Theory of Consciousness." *Neuroscience & Biobehavioral Reviews* 176: 106296. https://doi.org/10.1016/j.neubiorev.2025.106296.
------------------------------------------------------------

Citation 31 [IN-PROSE]:
Line: 537
Citation: Marcus (2001)

Context:
The connection deepens when we consider Laukkonen et al.'s emphasis on counterfactual simulation and epistemic depth. Statistical learning creates flat predictions about the immediate sensory stream—what happens next given current input patterns. Structural learning, by contrast, enables detachment from the present to model "what if" scenarios: simulating alternative configurations, exploring constraint violations, testing structural relationships across hypothetical states. This capacity for counterfactual reasoning is precisely what Laukkonen identifies as creating "epistemic depth"—the recursive loop of simulating possible epistemic fields to select action policies. Our distinction between statistical (unconscious) and structural (conscious) processing thus maps onto their account: consciousness arises when the system engages in the counterfactual simulation that structural patterns both demand and enable.

Resonance with Classic Cognitive Science: This statistical/structural distinction also connects to foundational debates in cognitive science. Marcus (2001) famously argued that pure connectionist systems (neural networks relying on statistical pattern matching) struggle with the systematic, rule-based reasoning characteristic of human thought. While connectionist models excel at extracting correlations from data, they face difficulties with the compositional and algebraic operations that enable flexible generalization. Our framework provides an information-theoretic lens on this debate: what Marcus identified as the need for symbolic/algebraic operations corresponds to structural pattern recognition, detecting necessary relationships between mutually constraining components. Consciousness, on this account, may be the subjective signature of the system deploying these structural (algebraic) compressions, which are computationally distinct from the purely statistical regularities handled by unconscious subsystems. This suggests that the longstanding connectionist/symbolic divide reflects a genuine functional distinction in how patterns can be compressed, with consciousness tracking the structural mode.

Implications for Artificial Intelligence: This framework offers a diagnostic for current AI architectures. Large Language Models operate primarily as ε-machines for statistical regularities—they excel at frequency-based pattern matching but frequently hallucinate because they lack architecture for structural constraint detection. They approximate structure through massive statistical sampling rather than detecting necessary relationships between mutually constraining components.

Reference:
Marcus, Gary F. 2001. *The Algebraic Mind: Integrating Connectionism and Cognitive Science*. Cambridge, MA: MIT Press. ISBN 978-0262632683.
------------------------------------------------------------

Citation 32 [IN-PROSE]:
Line: 561
Citation: Parr and Friston (2025)

Context:
Only humans (as far as we know) achieve third-order regularly. This is meta-blanket formation: constructing a Markov blanket around your own Markov blankets, allowing self-modification.

Inner Screens and Imaginative Experience: Parr and Friston (2025) model this meta-level capacity through "inner screens"—internal boundaries with Markov blanket structure that function as classical information channels. These inner screens enable imaginative experience (planning, episodic memory, counterfactual reasoning) by allowing internally-generated content to employ the same spatial and conceptual reference frames used in ordinary perception. This provides a mechanistic account of how meta-awareness operates: the system constructs internal Markov blankets to model its own modeling processes, enabling the kind of third-order reflection that distinguishes human cognition.

The formation of meta-blankets is not merely metaphorical but physiologically grounded. Parr and Friston identify inner screens as functional Markov blankets within the cortical hierarchy that segregate imaginative planning from sensory perception. When you imagine biting into a lemon, you generate predictions about sourness without confusing them with actual taste input—the inner screen maintains the statistical boundary between internally-generated simulation and externally-driven sensation. This internal screening mechanism allows the system to manipulate its own dispositions as objects of thought: you can model your "distrust disposition" as a pattern to be examined, questioned, and potentially revised, rather than simply experiencing distrust as transparent reality. The meta-blanket thus enables the recursive operation where the system's predictive machinery becomes its own target of prediction, supporting the third-order reflection that characterizes metacognitive awareness.

Reference:
Parr, Thomas, and Karl J. Friston. 2025. "How Do Inner Screens Enable Imaginative Experience? Applying the Free-Energy Principle to Attention." *Neuroscience of Consciousness* 2025(1): niaf009. https://doi.org/10.1093/nc/niaf009.
------------------------------------------------------------

Citation 33 [IN-PROSE]:
Line: 565
Citation: Rosas et al. (2024)

Context:
The formation of meta-blankets is not merely metaphorical but physiologically grounded. Parr and Friston identify inner screens as functional Markov blankets within the cortical hierarchy that segregate imaginative planning from sensory perception. When you imagine biting into a lemon, you generate predictions about sourness without confusing them with actual taste input—the inner screen maintains the statistical boundary between internally-generated simulation and externally-driven sensation. This internal screening mechanism allows the system to manipulate its own dispositions as objects of thought: you can model your "distrust disposition" as a pattern to be examined, questioned, and potentially revised, rather than simply experiencing distrust as transparent reality. The meta-blanket thus enables the recursive operation where the system's predictive machinery becomes its own target of prediction, supporting the third-order reflection that characterizes metacognitive awareness.

The Self as a User Interface: Following Rosas et al. (2024), the "Self" is not a ghost in the machine but a control variable in the system's own high-level model—a user interface for the brain's self-regulation. It represents the brain's own lossy compression of its massive, distributed neural activity.

Just as a computer operating system represents billions of transistor states as a single "folder" icon, the brain compresses its complex somatic and cognitive states into a single variable: "I". This variable functions as a control parameter, allowing the system to predict and regulate its own future states without tracking every underlying neural process.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 34 [PARENTHETICAL]:
Line: 585
Citation: (Solms 2019)

Context:
We propose that consciousness relates to detecting and representing structural coherence rather than merely tracking statistical correlations. This isn't a solution to the hard problem, but it identifies a functional distinction that may map onto the phenomenological boundary between conscious and unconscious processing.

Recent work applies the Free Energy Principle directly to the hard problem, identifying affect (the feeling dimension of consciousness) as the subjective signature of free energy minimization—where decreases and increases in expected uncertainty are experienced as pleasure and unpleasure (Solms 2019). This suggests consciousness may track information-theoretic processes in a way that gives them phenomenological character, though why minimizing prediction error should feel like anything remains unexplained.

Alternative Formalization (Integrated Information Theory): A different approach starts from phenomenological axioms rather than functional principles. Oizumi et al. (2014) formalize Integrated Information Theory (IIT) by beginning with intrinsic properties of experience (it is structured, integrated, definite) and deriving physical postulates about the mechanisms that could instantiate these properties. IIT identifies consciousness with the maximally irreducible conceptual structure generated by systems with high integrated information (Φ).

Reference:
Solms, Mark. 2019. "The Hard Problem of Consciousness and the Free Energy Principle." *Frontiers in Psychology* 9: 2714. https://doi.org/10.3389/fpsyg.2018.02714.
------------------------------------------------------------

Citation 35 [IN-PROSE]:
Line: 587
Citation: Oizumi et al. (2014)

Context:
Recent work applies the Free Energy Principle directly to the hard problem, identifying affect (the feeling dimension of consciousness) as the subjective signature of free energy minimization—where decreases and increases in expected uncertainty are experienced as pleasure and unpleasure (Solms 2019). This suggests consciousness may track information-theoretic processes in a way that gives them phenomenological character, though why minimizing prediction error should feel like anything remains unexplained.

Alternative Formalization (Integrated Information Theory): A different approach starts from phenomenological axioms rather than functional principles. Oizumi et al. (2014) formalize Integrated Information Theory (IIT) by beginning with intrinsic properties of experience (it is structured, integrated, definite) and deriving physical postulates about the mechanisms that could instantiate these properties. IIT identifies consciousness with the maximally irreducible conceptual structure generated by systems with high integrated information (Φ).

This framework can be contrasted productively with IIT as representing complementary methodological approaches. IIT adopts an intrinsic-nature-first methodology: it starts from phenomenological axioms about what consciousness is like (experience has definite boundaries, internal differentiation, integration) and works backward to identify the physical systems that could instantiate these properties. Our framework, by contrast, adopts a function-first methodology: it starts from the functional problem any bounded system faces (compressing reality to minimize prediction error) and works forward to identify the processes that would require conscious engagement (structural pattern recognition, meta-blanket formation).

Reference:
Oizumi, Masafumi, et al. 2014. "From the Phenomenology to the Mechanisms of Consciousness: Integrated Information Theory 3.0." *PLOS Computational Biology* 10(5): e1003588. https://doi.org/10.1371/journal.pcbi.1003588.
------------------------------------------------------------

Citation 36 [PARENTHETICAL]:
Line: 770
Citation: (Ladyman and Ross 2007)

Context:
Connection to Apex Network: Mathematics is part of the Apex Network—the maximally compressed representation of structural constraints that any sufficiently thorough compression must discover.

Philosophical Positioning: This view differs from Mathematical Platonism in key respects. Platonism holds that mathematical objects exist independently in an abstract realm, which our minds somehow access. We claim instead that mathematical structures are optimal compression protocols for describing constraint relationships—they are discovered rather than invented, but what is being discovered is the structure of the constraint space itself, not objects in a separate ontological realm. This aligns more closely with ontic structural realism (Ladyman and Ross 2007), which holds that structure rather than objects is ontologically fundamental. The "necessity" of mathematics derives not from inhabiting a timeless Platonic heaven but from the fact that certain compression strategies are uniquely optimal given the axioms and constraints. Independent discovery of π across cultures evidences objective structure, but what makes π necessary is the relationship it compresses (circumference to diameter in Euclidean space), not its existence as an abstract entity. This framework preserves mathematical objectivity without requiring a separate realm of mathematical objects.

Mathematical Fallibility: Even mathematics has its Negative Canon. Naive Set Theory, which allowed sets to contain themselves, generated Russell's Paradox—a contradiction revealing that the blanket had leaked. The mathematical community didn't simply choose to revise set theory; they were forced to by the internal incoherence the system produced. Failed mathematical definitions, like failed scientific theories, represent compressions that could not achieve computational closure. They generated contradictions (information leakage at the logical level) and were selected against, demonstrating that mathematical knowledge, like empirical knowledge, is subject to the same selection pressures we describe throughout this framework.

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 37 [IN-PROSE]:
Line: 804
Citation: Rosas et al. (2024)

Context:
Not Just Useful Fiction: When computational closure succeeds, the emergent level is as real as the base level—it has autonomous causal dynamics.

Causal Decoupling as Emergence Criterion: Rosas et al. (2024) formalize this as causal decoupling: when the macro-level ε-machine and the micro-informed υ-machine achieve equivalence, the macro-level becomes causally autonomous. This provides a precise criterion for when emergence genuinely succeeds versus when we merely have convenient but reducible description.

A clarification on "causation": following Russell (1913) and Ladyman and Ross (2007), we acknowledge that fundamental physics describes functional dependencies rather than discrete causal "bangs." The "causation" that emerges at macro-levels is therefore best understood as *information-theoretic projectibility*—a macro-state is "causal" not because it exerts a metaphysical force, but because it maximizes mutual information with future states, effectively screening off micro-history. This protects the framework from the objection that we are smuggling metaphysically loaded causation into domains where physics reveals only functional structure.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 38 [IN-PROSE]:
Line: 806
Citation: Russell (1913)

Context:
Causal Decoupling as Emergence Criterion: Rosas et al. (2024) formalize this as causal decoupling: when the macro-level ε-machine and the micro-informed υ-machine achieve equivalence, the macro-level becomes causally autonomous. This provides a precise criterion for when emergence genuinely succeeds versus when we merely have convenient but reducible description.

A clarification on "causation": following Russell (1913) and Ladyman and Ross (2007), we acknowledge that fundamental physics describes functional dependencies rather than discrete causal "bangs." The "causation" that emerges at macro-levels is therefore best understood as *information-theoretic projectibility*—a macro-state is "causal" not because it exerts a metaphysical force, but because it maximizes mutual information with future states, effectively screening off micro-history. This protects the framework from the objection that we are smuggling metaphysically loaded causation into domains where physics reveals only functional structure.

The key insight: if adding substrate information to your macro-level predictor doesn't improve its accuracy, then the macro-level has achieved causal independence. The emergent pattern is not merely a useful fiction—it genuinely causes its own future states. High-level concepts like "temperature," "infectious disease," or "recession" are causally real precisely because they achieve this decoupling. The macro-level runs its own causal dynamics, making substrate details causally irrelevant (though they remain constitutively necessary).

Reference:
Russell, Bertrand. 1913. "On the Notion of Cause." *Proceedings of the Aristotelian Society* 13(1): 1-26. https://doi.org/10.1093/aristotelian/13.1.1.
------------------------------------------------------------

Citation 39 [IN-PROSE]:
Line: 806
Citation: Ladyman and Ross (2007)

Context:
Causal Decoupling as Emergence Criterion: Rosas et al. (2024) formalize this as causal decoupling: when the macro-level ε-machine and the micro-informed υ-machine achieve equivalence, the macro-level becomes causally autonomous. This provides a precise criterion for when emergence genuinely succeeds versus when we merely have convenient but reducible description.

A clarification on "causation": following Russell (1913) and Ladyman and Ross (2007), we acknowledge that fundamental physics describes functional dependencies rather than discrete causal "bangs." The "causation" that emerges at macro-levels is therefore best understood as *information-theoretic projectibility*—a macro-state is "causal" not because it exerts a metaphysical force, but because it maximizes mutual information with future states, effectively screening off micro-history. This protects the framework from the objection that we are smuggling metaphysically loaded causation into domains where physics reveals only functional structure.

The key insight: if adding substrate information to your macro-level predictor doesn't improve its accuracy, then the macro-level has achieved causal independence. The emergent pattern is not merely a useful fiction—it genuinely causes its own future states. High-level concepts like "temperature," "infectious disease," or "recession" are causally real precisely because they achieve this decoupling. The macro-level runs its own causal dynamics, making substrate details causally irrelevant (though they remain constitutively necessary).

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 40 [PARENTHETICAL]:
Line: 812
Citation: (Varley and Hoel 2022)

Context:
This framework distinguishes strong from weak emergence rigorously. **Weak emergence** occurs when macro-patterns are predictable from micro-details but the compression works only in limited contexts (weak lumpability). **Strong emergence** occurs when macro-dynamics persist across different substrate realizations (strong lumpability). Only strongly emergent properties qualify as objectively real features of nature rather than observer-relative conveniences.

This connects to a broader information-theoretic understanding of emergence: coarse-graining can convert information from one type to another, where macroscales reduce total information but may increase certain dependency structures (Varley and Hoel 2022). Emergence succeeds when this information conversion creates synergistic relationships at the macro-level that don't exist at the micro-level—when the whole genuinely exhibits causal properties absent from mere aggregation of parts.

Temperature Example Revisited:
- Base: 10²³ molecules with positions, momenta

Reference:
Varley, Thomas F., and Erik Hoel. 2022. "Emergence as the conversion of information: a unifying theory." *Philosophical Transactions of the Royal Society A* 380(2227): 20210150. https://doi.org/10.1098/rsta.2021.0150.
------------------------------------------------------------

Citation 41 [PARENTHETICAL]:
Line: 854
Citation: (Dittrich and Kinne 2024)

Context:
### 6.3 Standing Predicates as Successful Closures

The Compression Efficiency Principle (Dittrich and Kinne 2024) provides the micro-level mechanism underlying this brittleness. CEP identifies "exception accumulation" as the characteristic failure mode of non-generative models: systems relying on superficial correlations rather than causal structure exhibit linear growth in exceptions as novel cases fail to fit the pattern. This exception growth is precisely what our brittleness metrics track at the system level. High patch velocity P(t) operationalizes the rate of exception accumulation. Rising model complexity M(t) reflects the computational burden of managing these exceptions. Declining resilience R(t) signals that the compression is failing across contexts. The systemic brittleness quantified by these metrics directly corresponds to what CEP identifies as characteristic of correlational models that cannot achieve sustainable efficiency because they lack genuine causal structure.

We propose that Standing Predicates may function as linguistically-encoded successful computational closures, though this claim requires further empirical investigation.

Reference:
Dittrich, Christian, and Jennifer Flygare Kinne. "The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence." Preprint, submitted October 30, 2024. arXiv:2510.25883 [cs.AI]. https://doi.org/10.48550/arXiv.2510.25883.
------------------------------------------------------------

Citation 42 [IN-PROSE]:
Line: 880
Citation: Rosas et al. (2024)

Context:
The Apex Network is the complete set of Standing Predicate configurations that achieve minimum systemic brittleness—the intersection of all maximally viable compression structures. In information-theoretic terms, it represents the "ultimate" ε-machine—using the term to denote the limit of maximal compression rather than a final, static state. It functions as the thermodynamic attractor where information leakage is theoretically minimized.

Rosas et al. (2024) demonstrate that all valid coarse-grainings of a system form a mathematical **lattice**—a hierarchical structure of nested compression levels, where each node represents a different way to group micro-states into macro-states. Not all coarse-grainings are equally robust: some achieve only weak lumpability (working only for specific initial conditions), while others achieve strong lumpability (preserving macro-dynamics regardless of substrate details). The Apex Network corresponds to the optimal path through this lattice—the set of strongly lumpable coarse-grainings that maximize causal autonomy while minimizing computational complexity. Reality allows many valid maps (the full lattice), but the Apex Network represents those compressions that achieve genuine substrate independence. This is why objective truth is not correspondence to a single privileged description but the achievement of strong lumpability: the predicate holds regardless of the underlying micro-state distribution, making it a genuine feature of reality rather than an artifact of our perspective.

Terminological Note: While the mathematical structure is technically a lattice (a partially ordered set with strict hierarchical derivation), we use "Apex Network" to emphasize the socially shared and collectively discovered nature of these optimal compressions. The term "network" highlights that this structure emerges through distributed exploration by communities of knowers, rather than individual deduction. Both terms capture important aspects: "lattice" emphasizes the rigorous mathematical relationships between valid compressions, while "network" emphasizes the epistemic process by which we discover them.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 43 [IN-PROSE]:
Line: 896
Citation: Friston et al. (2025)

Context:
The constraints exist first; the optimal structure they determine is a necessary implication. Historical filtering is how we discover this structure, not how we create it.

Collective Intelligence and Emergent Knowledge: Recent work on multi-agent active inference provides a concrete mechanism for understanding how the Apex Network emerges through distributed exploration rather than centralized design. Friston et al. (2025) demonstrate that collectives can be treated as emergent agents with their own Markov blankets, possessing synergistic information that no individual member encodes. In their model of flocking birds, the collective "knows" the predator's location and optimal escape direction through the coordination of individual responses, even though no single bird detects or represents this information. The flock exhibits genuine collective knowledge that emerges from distributed processing.

This constraint-determined structure aligns with what Dittrich and Kinne (2024) call "reality-filtered data." Information produced by surviving agents already embeds causal structure because only causally accurate compressions enable the agents generating that data to persist. The Apex Network can thus be understood as the attractor state in the space of possible compressions where information-theoretic efficiency is maximized. It represents the set of Standing Predicates that constitute sustainable, generative models (in CEP's terms), discovered through distributed exploration of reality's constraint structure. This convergence mechanism explains how cultural and historical processes can approach objective truth without requiring direct access to a Platonic realm: agents exploring reality under survival pressure necessarily generate data reflecting causal constraints, and compressions of that data necessarily track those constraints.

Reference:
Friston, Karl J., Thomas Parr, Conor Heins, Axel Constant, Daniel Friedman, Takuya Isomura, Chris Fields, Tim Verbelen, Maxwell Ramstead, John Clippinger, and Christopher Frith. 2025. "What the Flock Knows That the Birds Do Not: Exploring the Emergence of Joint Agency in Multi-Agent Active Inference." *arXiv* preprint arXiv:2511.10835. https://arxiv.org/abs/2511.10835.
------------------------------------------------------------

Citation 44 [IN-PROSE]:
Line: 898
Citation: Dittrich and Kinne (2024)

Context:
Collective Intelligence and Emergent Knowledge: Recent work on multi-agent active inference provides a concrete mechanism for understanding how the Apex Network emerges through distributed exploration rather than centralized design. Friston et al. (2025) demonstrate that collectives can be treated as emergent agents with their own Markov blankets, possessing synergistic information that no individual member encodes. In their model of flocking birds, the collective "knows" the predator's location and optimal escape direction through the coordination of individual responses, even though no single bird detects or represents this information. The flock exhibits genuine collective knowledge that emerges from distributed processing.

This constraint-determined structure aligns with what Dittrich and Kinne (2024) call "reality-filtered data." Information produced by surviving agents already embeds causal structure because only causally accurate compressions enable the agents generating that data to persist. The Apex Network can thus be understood as the attractor state in the space of possible compressions where information-theoretic efficiency is maximized. It represents the set of Standing Predicates that constitute sustainable, generative models (in CEP's terms), discovered through distributed exploration of reality's constraint structure. This convergence mechanism explains how cultural and historical processes can approach objective truth without requiring direct access to a Platonic realm: agents exploring reality under survival pressure necessarily generate data reflecting causal constraints, and compressions of that data necessarily track those constraints.

This formalizes how the Apex Network operates. Just as a flock possesses information about environmental gradients that individual birds do not, a culture's network of Standing Predicates can encode compressions that no individual fully grasps. The Apex Network represents the collective computational closure achieved when communities of knowers, each exploring different regions of the compression landscape, generate a distributed set of predicates whose interactions reveal constraint structures invisible to isolated inquiry. It is neither a pre-existing blueprint awaiting discovery nor an arbitrary social construction, but the emergent pattern of collective intelligence navigating reality's constraints. The "network" metaphor captures this essential feature: optimal compressions crystallize through the interaction of many explorers facing common thermodynamic pressures, not through individual revelation or social agreement alone.

Reference:
Dittrich, Christian, and Jennifer Flygare Kinne. "The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence." Preprint, submitted October 30, 2024. arXiv:2510.25883 [cs.AI]. https://doi.org/10.48550/arXiv.2510.25883.
------------------------------------------------------------

Citation 45 [PARENTHETICAL]:
Line: 964
Citation: (Campbell 1974)

Context:
Why Different Cultures Converge on Similar Truths:

Not because of:
- Shared biology alone (though this constrains) (Campbell 1974)
- Social agreement (though this accelerates) (Longino 1990)
- Divine revelation
- Platonic access

Reference:
Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.
------------------------------------------------------------

Citation 46 [PARENTHETICAL]:
Line: 965
Citation: (Longino 1990)

Context:
Not because of:
- Shared biology alone (though this constrains) (Campbell 1974)
- Social agreement (though this accelerates) (Longino 1990)
- Divine revelation
- Platonic access

Reference:
Longino, Helen E. 1990. *Science as Social Knowledge: Values and Objectivity in Scientific Inquiry*. Princeton, NJ: Princeton University Press. ISBN 978-0691020518.
------------------------------------------------------------

Citation 47 [PARENTHETICAL]:
Line: 1059
Citation: (James 1890)

Context:
- Qualia: Potentially compression gradients rendered in subjective space, though why these have qualitative character remains unexplained
- Self-awareness: Meta-blanket formation (blanket monitoring its own blankets)
- Unity of consciousness: Integrated information across blanket hierarchies
- **Institutional Minds Without Phenomenology:** While social institutions possess Markov blankets and exhibit information processing (they minimize surprise, maintain boundaries, adapt to environments), this framework does not imply they possess phenomenal consciousness. The difference lies in the timescale of integration. Human phenomenology relies on the immediate temporal binding of diverse information streams into a unified present moment (James 1890, 609). Institutional information processing occurs over days or months—far too slow for the integrated temporal binding characteristic of subjective experience. They are functional agents without phenomenology.

The Explanatory Gap Narrows But Doesn't Close: We've identified a functional distinction (statistical vs. structural pattern recognition) that appears to track the phenomenological boundary. This narrows the explanatory target: not "why does any information processing feel like something?" but "why does detecting mutual constraints feel like something?" That's progress, but the hard problem—why any functional process has subjective character—remains open.

Reference:
James, William. 1890. *The Principles of Psychology*. 2 vols. New York: Henry Holt and Company.
------------------------------------------------------------

Citation 48 [PARENTHETICAL]:
Line: 1094
Citation: (see Glenn 2025)

Context:
Methodological Note (The Is/Ought Boundary):

We must acknowledge a crucial limit. This framework describes how certain social configurations generate higher or lower thermodynamic costs. However, it does not—and cannot—directly derive moral obligations from these descriptive facts. The move from "X generates high brittleness" to "therefore, X is morally wrong" crosses Hume's is/ought gap. (For a detailed treatment of this procedural model, see Glenn 2025).

What we can legitimately claim: Certain moral intuitions may have information-theoretic grounding. The recognition that denying others' agency generates catastrophic social costs helps explain why such behaviors are unsustainable. It also helps explain why moral progress often tracks toward lower-brittleness configurations.

Reference: NOT FOUND for 'see Glenn 2025'
------------------------------------------------------------

Citation 49 [IN-PROSE]:
Line: 1114
Citation: Rosas et al. (2024)

Context:
Information-Theoretic Analysis of Agency Denial:

Rosas et al. (2024) demonstrate that causally closed systems can be efficiently controlled through macro-level interventions—engaging with their computational closure rather than manipulating their substrate. This insight provides a mechanistic account of moral interaction: when we engage with another agent's reasons, beliefs, and goals (their ε-machine), we interact efficiently with the causally autonomous level. We work through their computational closure, allowing their internal dynamics to determine outcomes. When we bypass their agency to force their physical body or manipulate their circumstances (intervening on the substrate), we breach their causal closure and must manage all the micro-level resistance their autonomous system generates.

This explains why persuasion is thermodynamically cheaper than coercion: persuasion operates at the ε-machine level (engaging autonomous macro-dynamics), while coercion operates at the substrate level (fighting against those dynamics). Substrate interference is thermodynamically expensive in a specific way: by discarding the efficient causal structure of the agent's computational closure, the coercer must manage the full complexity of the micro-states directly. Persuasion leverages the agent's own compression architecture; coercion fights against it. The computational closure Rosas et al. identify is precisely what moral recognition respects and evil violations ignore.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

