
# **Convergent Coherence: How Failure Forges the Emergent Structure of Objective Truth**

## **Abstract**

Coherentist theories of justification remain vulnerable to the isolation objection: the possibility that a perfectly coherent belief system could be entirely detached from reality. This paper develops Emergent Pragmatic Coherentism (EPC), an externalist framework designed to resolve this challenge by grounding coherence in long-term pragmatic viability. The framework introduces "systemic brittleness" as a diagnostic for assessing a knowledge system's health by tracking the observable costs generated when its propositions are applied. It argues that the selective pressure of these costs forces disparate knowledge systems to converge on a single, maximally coherent system disciplined by mind-independent pragmatic constraints. This failure-driven process reveals an objective structure we term the **Apex Network**: not a pre-existing truth to be discovered, but a bottom-up emergent structure of maximally shared propositions that has survived historical filtering. This approach yields a form of **Systemic Externalism**, where a claim’s justification depends on the proven resilience of the public system certifying it. The result is a naturalistic theory that redefines objective truth as alignment with this structure, explaining how Quine’s web of belief is pragmatically revised, and grounding a falsifiable research program for assessing the health of our most critical epistemic systems.

## **1. Introduction: From a Static Web to a Dynamic Process**

Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump (Snow 1855). The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the **isolation objection**. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a **macro-epistemology**, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.

However, any credible theory of knowledge evolution must account for the realities of power, path dependence, and historical contingency. This framework incorporates these factors not as exceptions, but as key variables within the model. The exercise of power to maintain a brittle system, for example, is not a refutation of the model but a primary indicator of that system's non-viability, measured through its high coercive costs. This failure-driven, adaptive process grounds a robust but **fallible realism**. It explains how our evolving knowledge systems are forced to converge on an emergent structure of workable solutions determined by mind-independent pragmatic constraints. The claim of this framework is therefore probabilistic, not deterministic: it proposes that beneath the surface-level 'noise' of contingency and power, there are underlying structural pressures at work. A system accumulating systemic brittleness, a concept analogous to the notion of fragility developed by Taleb (2012), is not fated to collapse on a specific date, but it becomes progressively more vulnerable to the very contingent shocks that historians study. This model, therefore, does not offer a deterministic theory of history, but rather proposes the foundation for a falsifiable, probabilistic research program for understanding the structural dynamics that shape the landscape upon which historical events unfold.

To prevent misunderstanding about the framework's scope and ambitions, we must be precise about what this paper does and does not attempt. This is not a foundationalist epistemology that aims to ground all knowledge in indubitable starting points, nor is it a general theory of justification applicable to all domains of inquiry. Its focus is therefore on domains where pragmatic consequences provide a clear feedback loop for systemic performance. We present this not as a limitation of the theory, but as a principled delineation of its domain of application: it is a framework for knowledge systems that are held accountable to the world.

## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 From Individual Dispositions to Functional Propositions: A Naturalistic Progression**

Following Quine's call to **naturalize epistemology** (Quine 1969), our framework is grounded not in private mental states with propositional content, but in publicly observable behavior. To bridge from individual behavior to public knowledge systems, however, requires a careful philosophical construction that avoids the metaphysical commitments Quine rightly rejected. The path from disposition to public knowledge is built in stages, with conscious awareness serving as the critical link.

#### **2.1.1 The Quinean Foundation: Disposition to Assent**

We begin with Quine's core insight: a belief is not an inner mental representation but a **disposition to assent**, a stable pattern of behavior (Quine 1960). To believe "it is raining" is to be disposed to say "yes" when asked, to take an umbrella, and so on. This provides a fully naturalistic starting point, free from abstract propositions.

#### **2.1.2 The Functional Bridge: Belief as Monitored Disposition**

Here, we add a crucial functional layer to Quine's account. While a disposition is a third-person behavioral fact, humans possess a natural capacity for self-monitoring. We can become aware of our own dispositional states. This awareness is not a privileged glimpse into a Cartesian theater but a feedback mechanism, analogous to proprioception, that allows for self-report and deliberate revision. For the purposes of this framework, we functionally identify a "belief" with this state of being aware of one's own disposition to assent. When an agent reports, "I believe it is raining," they are not claiming access to an abstract proposition; they are reporting their awareness of their own readiness to assent to the sentence, "It is raining." This move acknowledges the functional importance of first-person access for coordinating and revising behavior, but does so within a fully naturalistic picture. The belief is not a non-physical mental content but a monitored, and therefore reportable, behavioral pattern.

#### **2.1.3 From Awareness to Public Claim: The Functional Proposition**

This conscious awareness is what makes a disposition epistemically functional. It allows an agent to articulate the sentence (σ) they are disposed to assent to. This articulated sentence becomes the public, testable unit of analysis for our framework. We term this a **functional proposition**. It is not a timeless, abstract meaning, but a concrete linguistic object, a sentence-type, whose function is to enable social coordination and reduce shared pragmatic costs. It is the public, testable unit that allows a community to assess the viability of the underlying disposition it expresses.

#### **2.1.4 From Articulation to Social Stabilization**

An individual's articulation of a sentence is merely a candidate for public knowledge. It becomes a stable, functional proposition only through a process of social testing and convergence. When multiple agents articulate their functional propositions, these utterances enter a public space where they are subject to coordination pressures. Through iterated interaction, communities converge on shared dispositions to assent to certain sentences in certain contexts, precisely because doing so has proven viable for collective action.

The functional proposition "Water boils at 100°C" is not a timeless, abstract truth that individuals discover. It is the stabilized linguistic form that our scientific community has become strongly disposed to assent to because this coordinated behavioral pattern enables immense predictive success and technological coordination. A proposition, in this deflationary account, has no existence independent of the coordinated behavioral patterns it formalizes. It is a social achievement, not a metaphysical entity.

#### **2.1.5 Social Convergence and the Problem of Indeterminacy**

When multiple agents articulate their functional propositions, they can coordinate their behavior. Through pragmatic feedback, communities converge on shared dispositions to assent to certain sentences in certain contexts because doing so has proven viable. "Water boils at 100°C" is not a discovered Platonic truth, but a sentence that our scientific community has become strongly and stably disposed to assent to because this disposition enables immense predictive success.

This directly addresses Quine's **indeterminacy thesis**. While semantic reference may remain metaphysically indeterminate, a community can achieve functional determinacy. The shared disposition to assent to the sentence "Water is H₂O" is precise enough to ground the science of chemistry. The objectivity of the Apex Network, therefore, is not the objectivity of a Platonic realm, but the emergent objectivity of the optimal configurations of these functional, coordinated behavioral patterns.

### **2.2 The Units of Analysis: Predicates, Networks, and Replicators**

Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection. It is the validated, reusable, and action-guiding conceptual tool within a proposition (e.g., `...is an infectious disease`). Functioning as the generative "genes" of cultural evolution, a Standing Predicate is a highly compressed piece of conceptual technology. When applied, it unpacks a suite of previously validated knowledge, including causal models, diagnostic heuristics, and licensed interventions.
*   **Shared Network:** This is an observable consequence of Quine's holism applied to social groups. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested; a specialized network like germ theory forms a coherent subset of propositions within the broader network of modern medicine, which itself must align with the predicates of empirical science. The emergence of these networks is not a conscious negotiation but a structural necessity.

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure, its core Standing Predicates and their relations, functions as the **replicator**: the "code" that is copied and transmitted. The social group and its institutions function as the **interactor**: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This explains how knowledge can persist even when the societies that created it do not. The existence of these countless, independently formed and often nested Shared Networks, all responding to the same landscape of physical and social constraints, supports a crucial inference: they collectively reveal an objective structure that underwrites all successful inquiry. This mosaic of survived predicates, biased toward low-brittleness solutions, anticipates the Apex Network, an emergent attractor forged by historical filtering (as detailed in Section 4).

### **2.3 Pragmatic Pushback and Systemic Costs**

A Shared Network is an active system under constant pressure from **pragmatic pushback**, our model’s term for the systemic analogue of what Quine called a "recalcitrant experience" at the periphery of an individual's web of belief. It is the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback is not an argument but a material outcome; a bridge collapses, a treatment fails, a society fragments. This process generates two types of costs.

**First-Order Costs** are the direct, material consequences: failed predictions, wasted resources, environmental degradation, or systemic instability (e.g., excess mortality). These are the objective signals of dysfunction. **Systemic Costs** are the secondary, internal costs a network incurs to *manage, suppress, or explain away* its first-order costs. These non-productive expenditures reveal a network's true fragility. Key forms include:

*   **Conceptual Debt:** The compounding fragility incurred by adopting flawed, complex "patches" to protect a core principle.
*   **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing dissent. These coercive overheads are the primary mechanism by which power dynamics manifest within our model; the resources spent to maintain a brittle system against internal and external pressures become a direct, measurable indicator of its non-viability. Dissent, in this model, is a critical data stream signaling that a system is generating costs for its members.

Pragmatic pushback is not limited to material failures. In highly abstract domains, such as theoretical physics or pure mathematics, where direct empirical tests are deferred or unavailable, pushback primarily manifests through the accumulation of Systemic Costs. A research program that requires an accelerating rate of ad-hoc modifications to maintain internal consistency, or that loses its unifying power, is experiencing a powerful form of pragmatic pushback. These epistemic inefficiencies are real costs that render a network brittle and unproductive, even if it has not been directly falsified by an experiment. The framework's diagnostic lens thus applies to all forms of inquiry, measuring viability through either external, material consequences or internal, systemic dysfunction.

### **2.4 Gauging Brittleness: A Conceptual Toolkit**

A system's brittleness is a measure of its accumulated systemic costs. To make this abstract concept concrete for philosophical analysis, we can identify several conceptual indicators that point to a system's underlying health. These are not meant as metrics for a quantitative science but as analytical categories for structuring historical and philosophical investigation:

*   **Rate of Ad-Hoc Modification:** In scientific paradigms, this manifests as an increasing ratio of publications aimed at resolving anomalies versus those generating novel, surprising predictions. A research program that becomes primarily defensive is accumulating Conceptual Debt.
*   **Ratio of Coercion to Production:** In socio-political systems, this can be seen in the ratio of resources dedicated to internal security and suppression versus those invested in public goods and productive capacity. High and rising coercive costs signal that a system is struggling to manage dissent generated by its own failures.
*   **Increasing Model Complexity:** In computational systems or theoretical models, this appears as a pattern of escalating complexity (e.g., parameter counts, resource requirements) needed to achieve only marginal gains in performance.

The operationalization of brittleness faces a potential circularity problem: measuring systemic costs objectively appears to require neutral standards for "waste" or "dysfunction," which our theory aims to provide. This circularity is managed through several strategies. We anchor measurements in basic biological and physical constraints (demographic collapse, resource depletion). We employ comparative rather than absolute measures, comparing trajectories across similar systems. Finally, we require convergent evidence across multiple independent indicators before diagnosing brittleness.

This methodology offers what we call "**constrained interpretation**," a structured judgment that remains accountable to multiple streams of evidence without claiming to achieve a view from nowhere. It aims to make evaluative judgments more systematic, transparent, and accountable, not to eliminate judgment entirely.

### **2.5 Two Modalities of Systemic Brittleness**

The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism without committing to a specific meta-ethical theory.

*   **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal and logical structure of the world. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict, explain, and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute Epistemic Brittleness.
*   **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **constraints on stable, cooperative social organization**. It is found in socio-political and ethical networks whose primary function is to organize human action. This form of brittleness is diagnosed through indicators of social friction and unsustainability: rising coercive overheads to manage dissent, demographic instability, and the failure to solve collective action problems. A society predicated on slavery, for instance, exhibits profound Normative Brittleness. Its unsustainability is not merely a matter of moral judgment but is demonstrated by the immense and ever-rising coercive costs required to suppress the dissent and instability its core principles generate.

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality's constraint structure. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

## **3. The Drivers of Adaptation: The Logic of Systemic Viability**

The framework for assessing systemic health detailed in Section 2 is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems.

### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "**normativity objection**" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.

Second, an **instrumental argument**: the framework makes a falsifiable, empirical claim that *networks with a high and rising degree of measured brittleness are statistically more likely to collapse or require radical revision.* From this descriptive claim follows a conditional recommendation: *if* an agent or institution has the goal of ensuring its long-term stability and problem-solving capacity, *then* it has a powerful, evidence-based reason to adopt principles that demonstrably lower its systemic brittleness.

This reframes the paper’s normative language. When this model describes one network as "better" or identifies "epistemic progress," these are not subjective value judgments but technical descriptions of systemic performance. A "better" network is one with lower measured brittleness and thus a higher predicted resilience against failure. Viability is not an optional norm to be adopted; it is a structural precondition for any system that manages to become part of the historical record at all.

### **3.2 Coherence as Forward-Looking Risk Assessment**

Framed by the logic of viability, the test for coherence is a thick, forward-looking *cost-benefit analysis*: a set of heuristics that a resource-constrained system uses to bet on whether adopting a new proposition will increase or decrease its long-term brittleness. The traditional **epistemic virtues** are the core principles of this practical calculus:

*   **Logical Consistency:** A hedge against the infinite future costs of inferential paralysis.
*   **Explanatory Power:** A measure of a proposition’s potential return on investment, reducing future inquiry costs by paying down Conceptual Debt.
*   **Simplicity / Parsimony:** A direct measure of systemic overhead; complex propositions increase long-term maintenance costs.
*   **Evidential Support:** An assessment of integrative risk; a well-supported claim is a low-risk investment unlikely to trigger a cascade of costly future revisions.

This forward-looking model of coherence also explains how revolutionary science is possible. When a dominant **Consensus Network** begins to exhibit high and rising systemic brittleness, a state that corresponds to a Kuhnian "crisis," the cost-benefit analysis for new propositions shifts dramatically. A radical new hypothesis, while having low coherence with the existing network's specific principles, may promise a massive long-term reduction in the systemic costs that are crippling the incumbent paradigm. The network, in effect, makes a high-risk, high-reward bet. The new proposition is not accepted because it fits neatly with the old, failing parts, but because it offers a viable path to restoring low-brittleness for the system as a whole. Coherence, in this dynamic sense, is not a conservative check for conformity, but a pragmatic assessment of a proposition's potential to resolve systemic crisis.

## **4. Convergence and the Emergence of Objective Structures**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We will show how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

Our account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system, its descent into crippling inefficiency, intellectual stagnation, and institutional decay, provides a clear, non-negotiable data point.

The systematic analysis of these failures allows us to build the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been empirically invalidated by the catastrophic Systemic Costs they reliably generate. This canon charts failures of both causal and normative alignment:

*   **Failures of Causal Alignment** are characteristic of Epistemic Brittleness. Systems like scholastic physics, phlogiston chemistry, and Lysenkoist biology entered the canon because their core principles generated catastrophic causal failures, leading to chronically high rates of ad-hoc modification and predictive collapse. They represent failed blueprints for the causal world.
*   **Failures of Normative Alignment** are characteristic of Normative Brittleness. Systems like chattel slavery or totalitarianism entered the canon because their core principles violated constraints on stable human cooperation. Their unsustainability was demonstrated by the immense and ever-rising coercive overheads required to suppress the normative pushback they generated. These are not condemned by a modern moral judgment alone; they are failed engineering principles for a viable social world.

By charting what demonstrably fails, whether in engineering or social organization, we are not merely learning what to avoid; we are effectively reverse-engineering the constraints of a real territory. The Negative Canon functions like a reef chart for inquiry. It is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and prevents a collapse into relativism.

### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The term "Apex" should be understood not as a single teleological peak, but as the high-altitude plateau on the landscape of viability, representing the set of maximally resilient, low-brittleness configurations. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "**structural emergent**": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

The mechanism that forges this structure is a bottom-up emergence driven by the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question. The framework makes no a priori claims about universal convergence. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

The Apex Network's function as a standard for **Objective Truth** follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle, a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge. The "pursuit of truth," then, is the practical, fallible project of refining our current Consensus Network to better align with this real, objective structure.

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state; they simply try to solve local problems and reduce costs. The Apex Network is the name we give to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

The Apex Network's status is therefore dual, a distinction critical to our fallibilist realism. Ontologically, it is real: it is the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, however, it remains a **regulative ideal**. We can never achieve a final, God's-eye view to confirm that our Consensus Network is a perfect map of it; our knowledge of the structure is necessarily incomplete and fallible. Its existence is what grounds our realism and prevents a collapse into relativism, while our epistemic limitations are what make the project of inquiry a permanent and progressive one. It is the necessary concept that makes our comparative judgments of "more" or "less" brittle meaningful.

### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check, the assessment of systemic brittleness, prevents this from being mistaken for **Justified Truth**.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful **higher-order evidence** that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
*   **Level 1: Objective Truth.** The ultimate, regulative ideal of the process. A proposition is objectively true if its principles are part of the real, emergent Apex Network. While this structure is never fully mapped, it functions as the formal standard that makes our comparative judgments of "more" or "less" brittle meaningful. It is the structure toward which the reduction of systemic costs forces our knowledge systems to converge.

This layered framework avoids a simplistic "Whig history" by recognizing that Justified Truth is a historically-situated achievement. Newtonian mechanics earned its Level 2 status by being a maximally low-brittleness system for its problem-space for over two centuries. Its replacement by relativity does not retroactively invalidate that status; it shows the evolutionary process at work, where an expanding problem-space revealed pragmatic constraints that required a new, more viable system. This allows for sharp, non-anachronistic historical judgments: a claim can be justifiably true in its time (Level 2) yet still be objectively false (not Level 1) when judged against the Apex Network from the perspective of a more resilient successor.

### **4.4 The Evolving Structure of Knowledge: Convergent Core and Pluralist Frontier**

The historical process of pragmatic filtering gives our evolving Consensus Networks a discernible structure, which can be understood as having two distinct epistemic zones. This distinction is not about the nature of reality itself, but describes the justificatory status of our claims at a given time.

*   **The Convergent Core:** This represents the load-bearing foundations of our current knowledge. It comprises domains where the relentless pressure of pragmatic selection has eliminated all known rival formulations, leaving a single, or functionally identical, set of low-brittleness principles. Principles reside in this core, such as the laws of thermodynamics or the germ theory of disease, not because they are dogmatically held or self-evident, but because all tested alternatives have been relegated to the Negative Canon after generating catastrophically high systemic costs. While no claim is immune to revision in principle, the principles in the **Convergent Core** are functionally unrevisable in practice, as doing so would require dismantling the most successful and resilient knowledge structures we have ever built. A claim from this core achieves the highest degree of justification we can assign, approaching our standard for Objective Truth (Level 1).
*   **The Pluralist Frontier:** This describes the domains of active research where our current evidence is insufficient to decide between multiple, competing, and viable reconstructions of the landscape of viability. Here, rival systems (e.g., different interpretations of quantum mechanics or competing models of consciousness) may co-exist, each with a demonstrably low and stable degree of brittleness. It is crucial to distinguish this constrained, evidence-based pluralism from relativism. The frontier is not an "anything goes" zone; it is a highly restricted space strictly bounded on all sides by the Negative Canon. A system based on phlogiston is not a "viable contender" on the frontier of chemistry; it is a demonstrably failed research program. This pluralism is therefore a sign of **epistemic underdetermination**, a feature of our map's current limitations, not reality's supposed indifference. This position resonates with pragmatist accounts of functional pluralism (e.g., Price 1992), which treat different conceptual frameworks as tools whose legitimacy is determined by their utility within a specific practice. Within this frontier, the core claims of each viable competing system can be granted the status of Justified Truth (Level 2). This is also the zone where non-epistemic factors, such as institutional power or contingent path dependencies, can play their most significant role, sometimes artificially constraining the range of options explored or creating temporary monopolies on what is considered justified.

### **4.5 Illustrative Cases of Convergence and Brittleness**

The transition from Newtonian to relativistic physics offers a canonical example of this framework's diagnostic application. After centuries of viability, the Newtonian system began to accumulate significant systemic costs in the late 19th century. These manifested as first-order predictive failures, such as its inability to account for the perihelion of Mercury, and as rising Conceptual Debt in the form of ad-hoc modifications like the Lorentz-FitzGerald contraction hypothesis. The Einsteinian system proved to be a more resilient solution, reducing this Conceptual Debt and substantially lowering the systemic costs of inquiry in physics.

A more contemporary case can be found in the recent history of artificial intelligence, which illustrates how a brittleness assessment might function in real time. The periodic "AI winters" can be understood as the collapse of high-brittleness paradigms, such as symbolic AI, which suffered from a high rate of ad-hoc modification when faced with novel challenges. While the subsequent deep learning paradigm proved to be a low-brittleness solution for many specific tasks, it may now be showing signs of rising systemic costs. These can be described conceptually as, for example, potentially unsustainable escalations in computational and energy resources for marginal performance gains, or an accelerating research focus on auxiliary, post-hoc modifications rather than on foundational architectural advances. This situation illustrates the **Pluralist Frontier** in action, as rival architectures might now be seen as competing to become the next low-brittleness solution.

### **4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "**fitness traps**." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

The severity of a fitness trap can be conceptually diagnosed. For instance, cliodynamic analysis suggests that polities dedicating a disproportionately high and sustained share of their resources to internal suppression often exhibit a significantly higher probability of fragmentation when faced with an external shock (Turchin 2003). This provides a way to conceptually diagnose the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, are both examples. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation.

## **5. The Dynamism of the Web: How Knowledge Becomes Entrenched**

Quine’s "Web of Belief" (Quine 1951, 1960) provided a powerful static model of a knowledge system, but it has been criticized for lacking a corresponding dynamic account of its formation and change. This section provides that dynamic physiology, detailing the naturalistic process by which a successful discovery migrates from the tentative "periphery" of the web to its load-bearing "core."

### **5.1 From Tentative Hypothesis to Core Principle**

A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science, its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under **bounded rationality**; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter, pragmatic pushback, that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a **directed learning mechanism**, the entrenchment of pragmatically indispensable principles, that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **6. Situating the Framework: Systemic Externalism and Its Relations**

This paper has developed Systemic Externalism, a form of externalist epistemology that locates justification not in individual cognitive processes but in the demonstrated reliability of entire knowledge systems. This section clarifies the framework's position within contemporary epistemology by examining its relationship to four major research programs: coherentist epistemology, social epistemology, evolutionary epistemology, and neopragmatism.

### **6.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

Systemic Externalism offers a unified externalist solution to this structural problem. It provides the evolutionary backstory for the web's functional structure, arguing that a principle becomes functionally indispensable, as described by Carlson (2015), precisely because it has survived a historical filtering process based on pragmatic consequences. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

### **6.2 Evolutionary Grounding for Social Epistemic Practices**

The framework provides a naturalistic foundation for core insights in **social epistemology** while addressing a persistent problem in that field. Longino (2002) and others have shown that objectivity often emerges from well-structured epistemic communities rather than isolated individuals. Critical discourse norms, peer review, and viewpoint diversity are presented as procedural guarantees of objectivity.

However, purely procedural accounts face what might be called the "**parochialism problem**": how do we know that a perfectly managed consensus is not simply a stable, shared delusion? If objectivity is secured by following local rules of discourse, what grounds do we have for evaluating those rules themselves?

This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness; they help networks detect errors, pay down Conceptual Debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **6.3 Cultural Evolution and the Problem of Fitness**

The framework contributes to **evolutionary epistemology** (Campbell, 1974; Bradie, 1986) while avoiding standard problems facing such approaches. Traditional biological models treat beliefs as competing for psychological "survival," but this creates difficulties in defining fitness without circularity, distinguishing genuinely beneficial knowledge from well-adapted "informational viruses."

This framework addresses the circularity problem by providing a hard, non-circular standard for fitness: long-term pragmatic viability as measured by systemic brittleness. The fitness of a principle is not its transmissibility or psychological appeal, but its contribution to the resilience of the knowledge system that hosts it.

The framework also addresses evolutionary epistemology's difficulty with the **directed nature of human inquiry**. Unlike blind Darwinian selection, knowledge systems exhibit Lamarckian-style inheritance of acquired insights through the functional entrenchment of successful solutions in network cores.

It is useful to distinguish our concept of systemic brittleness from related ideas in the philosophy of science, such as Lakatos's (1970) 'degenerative research programmes' and Laudan's (1977) 'problem-solving effectiveness'. While Lakatos provides a brilliant historical description of a degenerating programme, one that relies on ad-hoc hypotheses and fails to make novel predictions, our framework aims to provide the underlying causal engine for this degeneration. Brittleness is a measure of the accumulated systemic costs that cause a programme to become degenerative. It is a diagnostic of a system's structural health, not just a historical description of its output.

### **6.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to **neopragmatist** approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"; indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential **coherence trap** lacking the necessary externalist check of real-world systemic costs.

### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent Apex Network shares deep affinities with scientific **structural realism** (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007). The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities, like "ether" or "phlogiston," but its underlying mathematical or relational structure.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.

1.  **On Ontology: From Abstract Structures to an Emergent Landscape.** Our model naturalizes the ontology of these structures. The Apex Network *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. It is an emergent structural fact about our world, a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
2.  **On Epistemology: From Insight to Selection.** Our framework provides the specific causal mechanism for convergence that structural realism often lacks. We discover these robust structures not through a mysterious act of intellectual insight, but through the brutal, eliminative process of pragmatic selection. High-brittleness networks, those whose posited structures misalign with the real landscape of viability, generate unsustainable costs, collapse, and enter the Negative Canon. Low-brittleness networks survive and are built upon. Over historical time, this failure-driven selective pressure is precisely the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

Our framework thus provides a dynamic, evolutionary, and fully naturalized engine for the structural realist's core thesis. It explains *how* and *why* our problem-solving practices are forced to converge on objective structures without appealing to metaphysical mysteries, thereby grounding structural realism in a testable, historical process.

### **6.6 Implications for Contemporary Debates**

This framework has implications for several contemporary discussions in epistemology:

**Disagreement**: Following Kelly (2005), the diagnosed brittleness of knowledge systems provides powerful **higher-order evidence** that should influence how agents respond to disagreement. Claims from low-brittleness networks warrant higher confidence than those from demonstrably brittle sources.

**Testimony**: The framework suggests that testimonial justification depends not only on speaker reliability but on the systemic reliability of the knowledge traditions speakers represent. This provides resources for evaluating competing testimonial sources in an information-rich but epistemically fragmented environment.

**Applied Epistemology**: The brittleness framework offers tools for evaluating knowledge systems in real-time, with applications to science policy, institutional design, and public discourse. It suggests criteria for identifying degenerating research programs before they reach crisis points.

## **7. Defending the Model: Addressing Key Challenges**

A philosophical model is best judged by its ability to resolve the very paradoxes that plague its predecessors. This section demonstrates the resilience of our framework by engaging with a series of classic epistemological challenges. We treat these not as external objections to be deflected, but as core test cases that reveal the explanatory power of analyzing knowledge through the lens of systemic viability.

### **7.1 The Problem of History: Endurance, Hindsight, and Real-Time Diagnosis**

A powerful challenge concerns the interpretation of history. If viability is the standard, how do we account for flawed systems that endure for centuries, and how can we apply this standard to live controversies without the distorting benefit of hindsight?

First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model in fact predicts that brittle systems can persist for long periods, but only by paying immense and measurable Systemic Costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a *confirmation* of it, as it provides a long-running experiment that allows us to observe the high price of insulating a flawed core from pragmatic pushback. Its apparent stability was not a sign of health but a direct measure of the intellectual and institutional energy it had to burn to function, making it profoundly vulnerable to a more efficient competitor.

This leads to the question of **real-time application**. The goal of this framework is not deterministic prediction but **epistemic risk management**. Its retrospective analysis of historical cases is not an end in itself; it is the necessary process of calibrating our diagnostic tools. We study known failures like Ptolemaic cosmology to learn the empirical signatures of rising brittleness. Only then can we apply these calibrated tools to live, unresolved debates. This allows us to ask precise, forward-looking questions: Is the exponential rise in computational and energy costs for large language models a sign of a degenerating research program, even as its short-term performance improves? Does the proliferation of ad-hoc 'alignment' fixes represent mounting Conceptual Debt? A rising trend in a system's brittleness indicators does not prove its core claims are false, but it provides a strong, evidence-based signal that it is becoming a higher-risk, degenerating research program.

### **7.2 The Problem of Scope: From Systemic Health to Individual Belief**

It is crucial to be precise about this model's scope. It is a macro-epistemology, designed to explain the long-term viability of public knowledge systems. It does not primarily aim to solve traditional problems in micro-epistemology, such as **Gettier cases** or the justification of an individual's perceptual beliefs. Instead, it provides a robust bridge between these levels through the concept of higher-order evidence. The diagnosed health of a public system provides a powerful defeater, or corroborator, for an individual’s beliefs derived from that system.

The conceptual nature of this bridge can be illustrated using a Bayesian lens. The diagnosed brittleness of a knowledge system functions as higher-order evidence that should inform the rational **prior probability** an agent assigns to any claim from that source. A claim from a low-brittleness network (e.g., an IPCC report) warrants a high prior, while a claim from a high-brittleness network (e.g., a denialist documentary) warrants a low one. This approach shows how a rational agent should weigh first-order evidence: even if a denialist source presents a seemingly compelling piece of evidence, the extremely low prior assigned to that network means an agent's posterior confidence in its claims should properly remain low. The macro-level diagnosis thus provides a rational basis for allocating trust, grounding individual belief in the demonstrated viability of public knowledge systems.

### **7.3 From Theory to Practice: A Falsifiable Research Program**

The framework's claims are designed to ground a concrete, empirically testable research program. Its core causal hypothesis is a **falsifiable prediction**: *a network with a high or rising degree of measured brittleness carries a statistically higher probability of collapse or major revision when faced with a comparable external shock.*

Testing this claim requires a rigorous methodology. The first step is to operationalize the indicators of brittleness through quantifiable proxies for systemic cost, such as the ratio of state budgets for internal security versus R&D or the rate of non-generative auxiliary hypotheses in scientific literature. The second step is to apply these metrics in a comparative historical analysis. A significant challenge in this research is to isolate the causal impact of intrinsic brittleness from the noise of historical contingency. To address this, the hypothesis can be tested by analyzing cohorts of systems, e.g., polities, scientific paradigms, that faced similar types of external shocks. Using large-scale databases like the Seshat Databank, researchers could compare the outcomes of systems with different pre-existing brittleness indicators when faced with a comparable shock, thereby statistically controlling for the contingent event itself.

This comparative method provides a well-established procedure for testing the theory's probabilistic claims against complex historical data. The framework is therefore rigorously falsifiable: if broad, methodologically sound historical analysis revealed no statistically significant correlation between the indicators of high systemic cost and subsequent network fragility, the theory’s core causal engine would be severely undermined. We acknowledge that such a research program faces significant challenges, including the operationalization of proxies, the control of confounding variables, and the interpretation of sparse data. The claim is not that this provides a simple algorithm for historical analysis, but that it offers a conceptually coherent and empirically grounded framework for it.

Finally, a crucial component of this program involves moving from retrospective calibration to prospective testing. While historical analysis is essential for identifying and calibrating the indicators of brittleness, the framework's ultimate test lies in its ability to make probabilistic, forward-looking claims. For instance, a diagnosis of rising brittleness in a current research paradigm, such as escalating resource costs for marginal gains, would yield the falsifiable prediction that this paradigm is statistically more likely to be superseded by a more efficient rival architecture in the face of novel challenges. This prospective application is essential for demonstrating that brittleness is a genuine diagnostic tool, not merely a post-hoc explanatory device.

## **8. Conclusion: An Evolutionary Approach to Inquiry**

This paper has developed **Emergent Pragmatic Coherentism** as a response to the isolation objection facing coherentist theories of justification. By grounding coherence in the long-term viability of knowledge systems rather than internal consistency alone, the framework provides the external constraint that coherentism requires while preserving its holistic insights. The concept of systemic brittleness offers a naturalistic diagnostic tool for evaluating epistemic systems, while the notion of an emergent Apex Network explains how objective knowledge can arise from fallible human practices.

Our framework for assessing systemic brittleness makes this process analyzable. By systematically studying the record of failed systems, we can begin to discern the contours of the Apex Network: the emergent set of maximally convergent, pragmatically indispensable principles that successful inquiry is forced to discover. The result is a form of Systemic Externalism that offers a novel approach for resolving long-standing problems in post-Quinean epistemology by grounding justification in the non-discursive filter of systemic consequences.

This model is not presented as a final, complete system, but as the foundation for a progressive and falsifiable research program. Critical future challenges remain, such as fully modeling the role of power asymmetries in creating path-dependent 'fitness traps' and applying the framework to purely aesthetic or mathematical domains. These are not flaws in the model, but precisely the kinds of tractable, empirical questions it is designed to generate.

We began with the challenge of distinguishing viable knowledge from brittle dogma in an era of epistemic crisis. The model we have developed suggests the ultimate arbiter is not the elegance of a theory or the consensus of its adherents, but the trail of consequences it leaves in the world. While this framework operates at a high level of abstraction, its primary data streams originate from the ground up. As we have argued, systemic costs are ultimately experienced by individuals as suffering, instability, and the frustration of human goals. Dissent, friction, and protest are therefore not merely political problems; they are primary sources of epistemological data about a system's rising brittleness. This framework, in this light, is best understood as a tool for a democratic public to hold its own knowledge-generating systems accountable. By making the indicators of systemic health part of a shared, evidence-based language, it helps us ask the most pragmatic question of all: "Is this way of thinking, this way of organizing ourselves, still working for us?"

## **Appendix A: A Speculative Extension on Moral Realism**

*Note: This appendix presents a speculative extension of the core framework, integrating it with recent work in meta-ethics to provide one possible metaphysical grounding for the concept of Normative Brittleness. The main argument of the paper is self-contained and does not depend on the specific claims made here.*

The concept of Normative Brittleness, as discussed in Section 2.5, can be given a more robust metaphysical foundation through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Glossary**

### **Part 1: The Core Framework & Philosophical Stance**

**1. Apex Network**
The emergent, objective structure of maximally viable solutions determined by mind-independent pragmatic constraints. It functions as the ultimate regulative ideal for objective truth (Level 1).

**2. Emergent Pragmatic Coherentism (EPC)**
The name for the theoretical framework developed in this paper. It grounds coherence in demonstrated long-term viability, resolving the isolation objection.

**3. Realist Pragmatism**
The model's philosophical identity, synthesizing pragmatism's focus on consequence with realism's commitment to objective, mind-independent structure.

**4. Systemic Externalism**
The specific epistemological stance where justification requires not only internal coherence (the local condition) but also the demonstrated, low-brittleness track record of the public system certifying the claim (the external condition).

**5. Fallible Realism**
The position that objective reality, in the form of the Apex Network, exists and is the target of inquiry, but our knowledge of it (the Consensus Network) is necessarily incomplete and subject to revision.

### **Part 2: The Units of Analysis: How Knowledge is Structured**

**1. Shared Network**
The emergent, public architecture of coherent propositions and Standing Predicates that must be shared across multiple individual webs of belief for collective problem-solving to succeed.

**2. Standing Predicate**
The validated, reusable, action-guiding conceptual tool extracted from a highly successful proposition (e.g., `...is an infectious disease`). It functions as the primary "gene" of cultural-epistemic selection.

**3. Functional Proposition**
A public, linguistic expression of an individual's monitored disposition to assent. It is the unit of collective testing and assessment in the network.

**4. Disposition to Assent**
A stable pattern of linguistic and non-linguistic behavior that constitutes belief, as originally described by Quine.

### **Part 3: The Dynamics of Change: How Knowledge Evolves**

**1. Pragmatic Pushback**
The primary selective force driving knowledge evolution. It is the sum of non-negotiable, non-discursive consequences that arise when principles are applied, compelling adaptation through material outcomes.

**2. Systemic Brittleness**
The central diagnostic concept. It is a measure of accumulated, hidden systemic costs, signaling a system’s vulnerability to cascading failures and inability to maintain viability under pressure.

**3. Systemic Costs**
Secondary, internal costs a network incurs to *manage, suppress, or explain away* its primary failures. Key forms include Conceptual Debt and Coercive Overheads.

**4. Conceptual Debt**
The compounding fragility incurred by adopting flawed, complex "patches" (ad-hoc hypotheses) to protect a core principle.

**5. Coercive Overheads**
Measurable resources allocated to enforcing compliance and managing dissent. They are a primary indicator of a system's Normative Brittleness.

**6. Negative Canon**
The empirical and historical record of failed predicates and systems that have been invalidated by their catastrophic Systemic Costs. It functions as the definitive external boundary for inquiry.

**7. Convergent Core**
The load-bearing foundations of current knowledge where pragmatic selection has eliminated all known rival formulations, rendering these principles functionally unrevisable in practice.

**8. Pluralist Frontier**
Domains of active research where evidence is insufficient to eliminate all rival systems; multiple stable configurations with demonstrably low brittleness may coexist, constrained by the Negative Canon.

### **Part 4: The Justificatory Status**

**1. Objective Truth (Level 1)**
Alignment with the real, emergent structure of the Apex Network; the ultimate regulative ideal of inquiry.

**2. Justified Truth (Level 2)**
The highest practically achievable epistemic status. A proposition is certified by a Consensus Network that has a demonstrated historical track record of low systemic brittleness.

**3. Contextual Coherence (Level 3)**
The baseline status: a proposition’s fit within a specific Shared Network, regardless of that network’s long-term viability.

**4. Higher-Order Evidence**
The diagnosed historical health (low brittleness) of a Consensus Network, which provides a rational basis for assessing the reliability of claims derived from that system.