
================================================================================
Citation Extraction Run - 2025-10-20 11:25:07
Files scanned: AGENTS.md, arch_v16.2.md, CITATION_EXTRACTOR_README.md, CLAUDE.md, final.md, gemini-email-convo.md, grok_ref.md, mathy.md, other.md, paper.md, proc_v7.md, README.md, test_filtered_refs.md, test_references.md
Total citations: 327
================================================================================


################################################################################
FILE: CITATION_EXTRACTOR_README.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 8
Citation: (Author 2020)

Context:
## Features

- **Dual Citation Detection**:
  - Parenthetical citations: `(Author 2020)`, `(Author et al. 2020, p. 45)`
  - In-prose citations: `Goldman (1979)`, `Acemoglu and Robinson (2012)`

- **Reference Matching**: Automatically looks up citations in `references.md`

Reference: NOT FOUND for 'Author 2020'
------------------------------------------------------------

Citation 2 [IN-PROSE]:
Line: 9
Citation: Goldman (1979)

Context:
- **Dual Citation Detection**:
  - Parenthetical citations: `(Author 2020)`, `(Author et al. 2020, p. 45)`
  - In-prose citations: `Goldman (1979)`, `Acemoglu and Robinson (2012)`

- **Reference Matching**: Automatically looks up citations in `references.md`

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 3 [IN-PROSE]:
Line: 9
Citation: Acemoglu and Robinson (2012)

Context:
- **Dual Citation Detection**:
  - Parenthetical citations: `(Author 2020)`, `(Author et al. 2020, p. 45)`
  - In-prose citations: `Goldman (1979)`, `Acemoglu and Robinson (2012)`

- **Reference Matching**: Automatically looks up citations in `references.md`

Reference:
Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business. ISBN 978-0307719225.
------------------------------------------------------------

Citation 4 [IN-PROSE]:
Line: 113
Citation: Author (2020)

Context:
<!-- MISSING REFERENCES (Not found in references.md) -->
<!-- Please add these to references.md: -->
<!-- Author (2020) -->
```

## Output Format

Reference: NOT FOUND for 'Author 2020'
------------------------------------------------------------

Citation 5 [IN-PROSE]:
Line: 133
Citation: Goldman (1979)

Context:
```
Citation 1 [IN-PROSE]:
Line: 142
Citation: Goldman (1979)

Context:
Following naturalized epistemology (Goldman 1979; Kitcher 1993),

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 6 [PARENTHETICAL]:
Line: 136
Citation: (Goldman 1979)

Context:
Citation: Goldman (1979)

Context:
Following naturalized epistemology (Goldman 1979; Kitcher 1993),
this framework shifts from private psychological states to public,
functional structures.

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 7 [PARENTHETICAL]:
Line: 136
Citation: (Kitcher 1993)

Context:
Citation: Goldman (1979)

Context:
Following naturalized epistemology (Goldman 1979; Kitcher 1993),
this framework shifts from private psychological states to public,
functional structures.

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 8 [PARENTHETICAL]:
Line: 150
Citation: (Author 2020)

Context:
## Citation Patterns Detected

### Parenthetical Citations
- `(Author 2020)`
- `(Author and Author2 2020)`
- `(Author et al. 2020)`
- `(Author 2020, 45)` - with page numbers

Reference: NOT FOUND for 'Author 2020'
------------------------------------------------------------

Citation 9 [PARENTHETICAL]:
Line: 151
Citation: (Author and Author2 2020)

Context:
### Parenthetical Citations
- `(Author 2020)`
- `(Author and Author2 2020)`
- `(Author et al. 2020)`
- `(Author 2020, 45)` - with page numbers
- `(Author 2020, p. 45-67)` - with page ranges

Reference: NOT FOUND for 'Author and Author2 2020'
------------------------------------------------------------

Citation 10 [PARENTHETICAL]:
Line: 153
Citation: (Author 2020)

Context:
- `(Author 2020)`
- `(Author and Author2 2020)`
- `(Author et al. 2020)`
- `(Author 2020, 45)` - with page numbers
- `(Author 2020, p. 45-67)` - with page ranges
- `(Author 2020a)` - with letter suffixes

Reference: NOT FOUND for 'Author 2020'
------------------------------------------------------------

Citation 11 [PARENTHETICAL]:
Line: 154
Citation: (Author 2020)

Context:
- `(Author and Author2 2020)`
- `(Author et al. 2020)`
- `(Author 2020, 45)` - with page numbers
- `(Author 2020, p. 45-67)` - with page ranges
- `(Author 2020a)` - with letter suffixes

### In-Prose Citations

Reference: NOT FOUND for 'Author 2020'
------------------------------------------------------------

Citation 12 [IN-PROSE]:
Line: 158
Citation: Goldman (1979)

Context:
- `(Author 2020a)` - with letter suffixes

### In-Prose Citations
- `Goldman (1979)` - single author
- `Acemoglu and Robinson (2012)` - two authors
- `Sevilla et al. (2022)` - three or more authors
- `Author & Author2 (2020)` - ampersand format

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 13 [IN-PROSE]:
Line: 159
Citation: Acemoglu and Robinson (2012)

Context:
### In-Prose Citations
- `Goldman (1979)` - single author
- `Acemoglu and Robinson (2012)` - two authors
- `Sevilla et al. (2022)` - three or more authors
- `Author & Author2 (2020)` - ampersand format

Reference:
Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business. ISBN 978-0307719225.
------------------------------------------------------------

Citation 14 [IN-PROSE]:
Line: 160
Citation: Sevilla et al. (2022)

Context:
### In-Prose Citations
- `Goldman (1979)` - single author
- `Acemoglu and Robinson (2012)` - two authors
- `Sevilla et al. (2022)` - three or more authors
- `Author & Author2 (2020)` - ampersand format

## Reference Matching

Reference:
Sevilla, Jaime, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. 2022. "Compute Trends Across Three Eras of Machine Learning." arXiv preprint arXiv:2202.05924.
------------------------------------------------------------

Citation 15 [IN-PROSE]:
Line: 161
Citation: Author & Author2 (2020)

Context:
- `Goldman (1979)` - single author
- `Acemoglu and Robinson (2012)` - two authors
- `Sevilla et al. (2022)` - three or more authors
- `Author & Author2 (2020)` - ampersand format

## Reference Matching

Reference: NOT FOUND for 'Author & Author2 2020'
------------------------------------------------------------


################################################################################
FILE: CLAUDE.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 94
Citation: (Author 2020)

Context:
- `-q, --quiet` - Suppress progress messages
  - `-h, --help` - Show usage help
- **Citation Detection**: Finds TWO types of citations:
  - **Parenthetical**: `(Author 2020)`, `(Author et al. 2020, p. 45)`
  - **In-prose**: `Goldman (1979)`, `Acemoglu and Robinson (2012)`, `Sevilla et al. (2022)`
- **Output**: Writes to specified file with structured formatting including:
  - Citation type (PARENTHETICAL or IN-PROSE)

Reference: NOT FOUND for 'Author 2020'
------------------------------------------------------------

Citation 2 [IN-PROSE]:
Line: 95
Citation: Goldman (1979)

Context:
- `-h, --help` - Show usage help
- **Citation Detection**: Finds TWO types of citations:
  - **Parenthetical**: `(Author 2020)`, `(Author et al. 2020, p. 45)`
  - **In-prose**: `Goldman (1979)`, `Acemoglu and Robinson (2012)`, `Sevilla et al. (2022)`
- **Output**: Writes to specified file with structured formatting including:
  - Citation type (PARENTHETICAL or IN-PROSE)
  - File location and line number

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 3 [IN-PROSE]:
Line: 95
Citation: Acemoglu and Robinson (2012)

Context:
- `-h, --help` - Show usage help
- **Citation Detection**: Finds TWO types of citations:
  - **Parenthetical**: `(Author 2020)`, `(Author et al. 2020, p. 45)`
  - **In-prose**: `Goldman (1979)`, `Acemoglu and Robinson (2012)`, `Sevilla et al. (2022)`
- **Output**: Writes to specified file with structured formatting including:
  - Citation type (PARENTHETICAL or IN-PROSE)
  - File location and line number

Reference:
Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business. ISBN 978-0307719225.
------------------------------------------------------------

Citation 4 [IN-PROSE]:
Line: 95
Citation: Sevilla et al. (2022)

Context:
- `-h, --help` - Show usage help
- **Citation Detection**: Finds TWO types of citations:
  - **Parenthetical**: `(Author 2020)`, `(Author et al. 2020, p. 45)`
  - **In-prose**: `Goldman (1979)`, `Acemoglu and Robinson (2012)`, `Sevilla et al. (2022)`
- **Output**: Writes to specified file with structured formatting including:
  - Citation type (PARENTHETICAL or IN-PROSE)
  - File location and line number

Reference:
Sevilla, Jaime, Lennart Heim, Anson Ho, Tamay Besiroglu, Marius Hobbhahn, and Pablo Villalobos. 2022. "Compute Trends Across Three Eras of Machine Learning." arXiv preprint arXiv:2202.05924.
------------------------------------------------------------

Citation 5 [PARENTHETICAL]:
Line: 141
Citation: (Author 2020)

Context:
3. **Document Assembly**: Removes existing references section, appends filtered references
  4. **Format Conversion**: Uses pandoc to convert to LaTeX or Typst
- **Citation Detection**: Handles complex citation patterns:
  - **Parenthetical**: `(Author 2020)`, `(Author et al. 2020, p. 45)`
  - **In-prose**: `Goldman (1979)`, `Acemoglu and Robinson (2012)`
- **Reference Processing**: Generates clean, submission-ready reference lists:
  - Filters master `references.md` to include only cited works

Reference: NOT FOUND for 'Author 2020'
------------------------------------------------------------

Citation 6 [IN-PROSE]:
Line: 142
Citation: Goldman (1979)

Context:
4. **Format Conversion**: Uses pandoc to convert to LaTeX or Typst
- **Citation Detection**: Handles complex citation patterns:
  - **Parenthetical**: `(Author 2020)`, `(Author et al. 2020, p. 45)`
  - **In-prose**: `Goldman (1979)`, `Acemoglu and Robinson (2012)`
- **Reference Processing**: Generates clean, submission-ready reference lists:
  - Filters master `references.md` to include only cited works
  - Sorts alphabetically by primary author

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 7 [IN-PROSE]:
Line: 142
Citation: Acemoglu and Robinson (2012)

Context:
4. **Format Conversion**: Uses pandoc to convert to LaTeX or Typst
- **Citation Detection**: Handles complex citation patterns:
  - **Parenthetical**: `(Author 2020)`, `(Author et al. 2020, p. 45)`
  - **In-prose**: `Goldman (1979)`, `Acemoglu and Robinson (2012)`
- **Reference Processing**: Generates clean, submission-ready reference lists:
  - Filters master `references.md` to include only cited works
  - Sorts alphabetically by primary author

Reference:
Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business. ISBN 978-0307719225.
------------------------------------------------------------


################################################################################
FILE: arch_v16.2.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 11
Citation: (Olsson 2005)

Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.

Reference:
Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press. ISBN 978-0199279999.
------------------------------------------------------------

Citation 2 [PARENTHETICAL]:
Line: 11
Citation: (Kvanvig 2012)

Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.

Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50(1): 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 3 [PARENTHETICAL]:
Line: 11
Citation: (Carlson 2015)

Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 4 [PARENTHETICAL]:
Line: 11
Citation: (Holling 1973)

Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.

Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 5 [IN-PROSE]:
Line: 11
Citation: BonJour (1985)

Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.

Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press. ISBN 978-0674843813.
------------------------------------------------------------

Citation 6 [IN-PROSE]:
Line: 15
Citation: Taleb (2012)

Context:
This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.

However, any credible theory of knowledge evolution must account for the realities of power, path dependence, and historical contingency. This framework incorporates these factors not as exceptions, but as key variables within the model. The exercise of power to maintain a brittle system, for example, is not a refutation of the model but a primary indicator of that system's non-viability, measured through its high coercive costs. This failure-driven, adaptive process grounds a robust but fallible realism. It explains how our evolving knowledge systems are forced to converge on an emergent structure of workable solutions determined by mind-independent pragmatic constraints. The claim of this framework is therefore probabilistic, not deterministic: it proposes that beneath the surface-level 'noise' of contingency and power, there are underlying structural pressures at work. A system accumulating brittleness—a system’s vulnerability to collapse due to the accumulation of hidden, internal costs, a concept analogous to the notion of fragility developed by Taleb (2012)—is not fated to collapse on a specific date, but it becomes progressively more vulnerable to the very contingent shocks that historians study. This model, therefore, does not offer a deterministic theory of history, but rather proposes the foundation for a falsifiable, probabilistic research program for understanding the structural dynamics that shape the landscape upon which historical events unfold.

To prevent misunderstanding about the framework's scope and ambitions, we must be precise about what this paper does and does not attempt. This is not a foundationalist epistemology that aims to ground all knowledge in indubitable starting points, nor is it a general theory of justification applicable to all domains of inquiry. Rather, it is a specialized framework for understanding the evolution and evaluation of cumulative knowledge systems—those engaged in ongoing, inter-generational projects where claims build upon previous work and where practical consequences provide feedback about systemic performance. The framework applies most directly to domains like empirical science, legal systems, engineering, and public policy, where pragmatic pushback is relatively direct and measurable. Its application to purely theoretical domains like mathematics or formal logic requires additional development and may ultimately prove limited. We present this focus not as a defect but as a feature—the framework's power lies precisely in its focus on knowledge systems where failure and success have observable consequences.

Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House. ISBN 978-1400067824.
------------------------------------------------------------

Citation 7 [PARENTHETICAL]:
Line: 21
Citation: (Meadows 2008)

Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing. ISBN 978-1603580557.
------------------------------------------------------------

Citation 8 [PARENTHETICAL]:
Line: 25
Citation: (Goldman 1979)

Context:
### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

`\[FIGURE 1: Conceptual Flowchart for the Deflationary Path]`
`\[Belief (Private State)] --> \[Articulation into a Proposition (Public Claim)] --> \[Coherence Test] --> \[Integration as Validated Data OR Promotion to Standing Predicate (Public Tool)]`

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 9 [PARENTHETICAL]:
Line: 25
Citation: (Kitcher 1993)

Context:
### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

`\[FIGURE 1: Conceptual Flowchart for the Deflationary Path]`
`\[Belief (Private State)] --> \[Articulation into a Proposition (Public Claim)] --> \[Coherence Test] --> \[Integration as Validated Data OR Promotion to Standing Predicate (Public Tool)]`

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 10 [IN-PROSE]:
Line: 55
Citation: Mesoudi (2011)

Context:
* **Standing Predicate:** This is the primary unit of cultural-epistemic selection. It is the validated, reusable, and action-guiding conceptual tool within a proposition (e.g., `...is an infectious disease`). Functioning as the generative "genes" of cultural evolution, a Standing Predicate is a highly compressed piece of conceptual technology. When applied, it unpacks a suite of previously validated knowledge, including causal models, diagnostic heuristics, and licensed interventions.
* **Shared Network:** This concept is not a novel theoretical entity but an observable consequence of Quine's holism applied to social groups. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested; a specialized network like germ theory forms a coherent subset of propositions within the broader network of modern medicine, which itself must align with the predicates of empirical science. The emergence of these networks is not a conscious negotiation but a structural necessity. An individual craftsperson whose canoe capsizes will holistically revise their personal web of belief about hydrodynamics; when a group must build a fleet, only the shared principles that lead to non-capsizing canoes can become part of the public, transmissible craft. The Shared Network is the public residue of countless such private, failure-driven revisions under shared pragmatic pressure.

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory, as synthesized for the social sciences by Mesoudi (2011). The network’s abstract informational structure—its core Standing Predicates and their relations—functions as the **replicator**: the "code" that is copied and transmitted. The social group and its institutions (the scientific community, legal system, or society) function as the **interactor**: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This distinction explains how knowledge can persist even when the societies that created it do not; the rediscovery of Roman legal principles during the Renaissance is a prime example of a successful replicator outliving its original interactor. The existence of these countless, independently formed and often nested Shared Networks—all responding to the same landscape of physical and social constraints—supports a crucial inference: they collectively reveal an objective structure that underwrites all successful inquiry. This mosaic of survived predicates, biased toward low-brittleness solutions, anticipates the Apex Network: not a pre-existing truth, but an emergent attractor forged by historical filtering (as detailed in Section 4).

### **2.3 Pragmatic Pushback and Systemic Costs**

Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press. ISBN 978-0226520445.
------------------------------------------------------------

Citation 11 [PARENTHETICAL]:
Line: 97
Citation: (El-Hani and Pihlström 2002)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

Reference:
El-Hani, Charbel Niño, and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3(2): article 3. https://doi.org/10.5840/eip2002325.
------------------------------------------------------------

Citation 12 [PARENTHETICAL]:
Line: 97
Citation: (Rottschaefer 2012)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

Reference:
Rottschaefer, William A. 2012. "The Moral Realism of Pragmatic Naturalism." *Analyse & Kritik* 34(1): 141–156. https://doi.org/10.1515/auk-2012-0110.
------------------------------------------------------------

Citation 13 [IN-PROSE]:
Line: 97
Citation: Bennett-Hunter (2015)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

Reference:
Bennett-Hunter, Guy. 2015. *Ineffability and Religious Experience*. London: Routledge (originally Pickering & Chatto). ISBN 978-1848934719.
------------------------------------------------------------

Citation 14 [IN-PROSE]:
Line: 97
Citation: Peter (2024)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

Reference:
Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37(7): 1948–70. https://doi.org/10.1080/09515089.2023.2236120.
------------------------------------------------------------

Citation 15 [PARENTHETICAL]:
Line: 107
Citation: (Kim 1988)

Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 16 [PARENTHETICAL]:
Line: 107
Citation: (Putnam 2002)

Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press. ISBN 978-0674013803.
------------------------------------------------------------

Citation 17 [PARENTHETICAL]:
Line: 107
Citation: (Lynch 2009)

Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

Reference:
Lynch, Michael P. 2009. *Truth as One and Many*. Oxford: Clarendon Press. ISBN 978-0199218738.
------------------------------------------------------------

Citation 18 [PARENTHETICAL]:
Line: 109
Citation: (Moghaddam 2013)

Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.

Reference: NOT FOUND for 'Moghaddam 2013'
------------------------------------------------------------

Citation 19 [PARENTHETICAL]:
Line: 145
Citation: (Peirce 1878)

Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

Reference:
Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press (originally 1878).
------------------------------------------------------------

Citation 20 [PARENTHETICAL]:
Line: 147
Citation: (Berlin and Kay 1969)

Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

The mechanism that forges this structure is a bottom-up emergence driven by the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question. The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we might expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

Reference:
Berlin, Brent, and Paul Kay. 1969. *Basic Color Terms: Their Universality and Evolution*. Berkeley: University of California Press. ISBN 978-1575861623
------------------------------------------------------------

Citation 21 [PARENTHETICAL]:
Line: 147
Citation: (Henrich 2015)

Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

The mechanism that forges this structure is a bottom-up emergence driven by the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question. The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we might expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

Reference:
Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press. ISBN 978-0691178431.
------------------------------------------------------------

Citation 22 [POSSESSIVE]:
Line: 151
Citation: Haack's (1993)

Context:
The mechanism that forges this structure is a bottom-up emergence driven by the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question. The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we might expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge. The "pursuit of truth," then, is the practical, fallible project of refining our current Consensus Network to better align with this real, objective structure.

`[FIGURE 2: Conceptual Sketch of a Fitness Landscape]`
`[A 3D landscape with "Viability" on the Y-axis. Peaks are labeled with successful systems like "Germ Theory" and "Relativity." Deep valleys and low plateaus are labeled with failed systems from the Negative Canon, such as "Ptolemaic System," "Lysenkoism," and the "Ming Dynasty Trap."]`

Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell. ISBN 978-0631196792.
------------------------------------------------------------

Citation 23 [PARENTHETICAL]:
Line: 162
Citation: (Tauriainen 2017)

Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.

Reference:
Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä, Department of Social Sciences and Philosophy. https://urn.fi/URN:NBN:fi:jyu-201705312584.
------------------------------------------------------------

Citation 24 [PARENTHETICAL]:
Line: 175
Citation: (Price 1992)

Context:
The historical process of pragmatic filtering gives our evolving Consensus Networks a discernible structure, which can be understood as having two distinct epistemic zones. This distinction is not about the nature of reality itself, but describes the justificatory status of our claims at a given time.

* **The Convergent Core:** This represents the load-bearing foundations of our current knowledge. It comprises domains where the relentless pressure of pragmatic selection has eliminated all known rival formulations, leaving a single, or functionally identical, set of low-brittleness principles. Principles reside in this core—such as the laws of thermodynamics or the germ theory of disease—not because they are dogmatically held or self-evident, but because all tested alternatives have been relegated to the Negative Canon after generating catastrophically high systemic costs. While no claim is immune to revision in principle, the principles in the Convergent Core are functionally unrevisable in practice, as doing so would require dismantling the most successful and resilient knowledge structures we have ever built. A claim from this core achieves the highest degree of justification we can assign, approaching our standard for Objective Truth (Level 1).
* **The Pluralist Frontier:** This describes the domains of active research where our current evidence is insufficient to decide between multiple, competing, and viable reconstructions of the landscape of viability. Here, rival systems (e.g., different interpretations of quantum mechanics or competing models of consciousness) may co-exist, each with a demonstrably low and stable degree of brittleness. It is crucial to distinguish this constrained, evidence-based pluralism from relativism. The frontier is not an "anything goes" zone; it is a highly restricted space strictly bounded on all sides by the Negative Canon. A system based on phlogiston is not a "viable contender" on the frontier of chemistry; it is a demonstrably failed research program. This pluralism is therefore a sign of epistemic underdetermination—a feature of our map's current limitations, not reality's supposed indifference. This position resonates with pragmatist accounts of functional pluralism (e.g., Price 1992), which treat different conceptual frameworks as tools whose legitimacy is determined by their utility within a specific practice. Within this frontier, the core claims of each viable competing system can be granted the status of Justified Truth (Level 2). This is also the zone where non-epistemic factors, such as institutional power or contingent path dependencies, can play their most significant role, sometimes artificially constraining the range of options explored or creating temporary monopolies on what is considered justified.

### **4.5 Illustrative Cases of Convergence and Brittleness**

Reference:
Price, Huw. 1992. "Metaphysical Pluralism." *Journal of Philosophy* 89(8): 387–409. https://doi.org/10.2307/2940741.
------------------------------------------------------------

Citation 25 [PARENTHETICAL]:
Line: 187
Citation: (Acemoglu and Robinson 2012)

Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

Reference:
Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business. ISBN 978-0307719225.
------------------------------------------------------------

Citation 26 [IN-PROSE]:
Line: 191
Citation: Turchin (2003)

Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, are both examples. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability, therefore, requires a balance between low-cost stability and adaptive capacity.

Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press. ISBN 978-0691116693.
------------------------------------------------------------

Citation 27 [PARENTHETICAL]:
Line: 203
Citation: (Simon 1972)

Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**

Reference: NOT FOUND for 'Simon 1972'
------------------------------------------------------------

Citation 28 [PARENTHETICAL]:
Line: 207
Citation: (Carlson 2015)

Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 29 [PARENTHETICAL]:
Line: 215
Citation: (Kvanvig 2012)

Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50(1): 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 30 [PARENTHETICAL]:
Line: 215
Citation: (Carlson 2015)

Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 31 [IN-PROSE]:
Line: 215
Citation: BonJour (1985)

Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press. ISBN 978-0674843813.
------------------------------------------------------------

Citation 32 [IN-PROSE]:
Line: 215
Citation: Olsson (2005)

Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Reference:
Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press. ISBN 978-0199279999.
------------------------------------------------------------

Citation 33 [IN-PROSE]:
Line: 217
Citation: Bennett-Hunter (2015)

Context:
Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

This paper's framework, which we term *Systemic Externalism*, offers a unified externalist solution to this structural problem. It provides the evolutionary backstory for the web's functional structure, arguing that a principle becomes functionally indispensable, as described by Carlson (2015), precisely because it has survived a historical filtering process based on pragmatic consequences. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Reference:
Bennett-Hunter, Guy. 2015. *Ineffability and Religious Experience*. London: Routledge (originally Pickering & Chatto). ISBN 978-1848934719.
------------------------------------------------------------

Citation 34 [PARENTHETICAL]:
Line: 219
Citation: (Zollman 2013)

Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

This paper's framework, which we term *Systemic Externalism*, offers a unified externalist solution to this structural problem. It provides the evolutionary backstory for the web's functional structure, arguing that a principle becomes functionally indispensable, as described by Carlson (2015), precisely because it has survived a historical filtering process based on pragmatic consequences. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

### 6.2 Evolutionary Grounding for Social Epistemic Practices

Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in Epistemic Communities." *Philosophy Compass* 8(1): 15–27. https://doi.org/10.1111/j.1747-9991.2012.00534.x.
------------------------------------------------------------

Citation 35 [IN-PROSE]:
Line: 219
Citation: Carlson (2015)

Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

This paper's framework, which we term *Systemic Externalism*, offers a unified externalist solution to this structural problem. It provides the evolutionary backstory for the web's functional structure, arguing that a principle becomes functionally indispensable, as described by Carlson (2015), precisely because it has survived a historical filtering process based on pragmatic consequences. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

### 6.2 Evolutionary Grounding for Social Epistemic Practices

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 36 [IN-PROSE]:
Line: 223
Citation: Longino (2002)

Context:
### 6.2 Evolutionary Grounding for Social Epistemic Practices

The framework provides a naturalistic foundation for core insights in social epistemology while addressing a persistent problem in that field. Longino (2002) and others have shown that objectivity often emerges from well-structured epistemic communities rather than isolated individuals. Critical discourse norms, peer review, and viewpoint diversity are presented as procedural guarantees of objectivity.

However, purely procedural accounts face what might be called the "parochialism problem": how do we know that a perfectly managed consensus is not simply a stable, shared delusion? If objectivity is secured by following local rules of discourse, what grounds do we have for evaluating those rules themselves?

Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press. ISBN 978-0691088761.
------------------------------------------------------------

Citation 37 [PARENTHETICAL]:
Line: 229
Citation: (Harding 1991)

Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness

Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press. ISBN 978-0801497469.
------------------------------------------------------------

Citation 38 [IN-PROSE]:
Line: 229
Citation: Sims (2024)

Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness

Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91(2): 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 39 [POSSESSIVE]:
Line: 241
Citation: Lakatos's (1970)

Context:
The framework also addresses evolutionary epistemology's difficulty with the directed nature of human inquiry. Unlike blind Darwinian selection, knowledge systems exhibit Lamarckian-style inheritance of acquired insights through the functional entrenchment of successful solutions in network cores.

It is useful to distinguish our concept of systemic brittleness from related ideas in the philosophy of science, such as Lakatos's (1970) 'degenerative research programmes' and Laudan's (1977) 'problem-solving effectiveness'. While Lakatos provides a brilliant historical description of a degenerating programme—one that relies on ad-hoc hypotheses and fails to make novel predictions—our framework aims to provide the underlying causal engine for this degeneration. Brittleness is a measure of the accumulated systemic costs that cause a programme to become degenerative. It is a diagnostic of a system's structural health, not just a historical description of its output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of risk and resilience. A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

Reference:
Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific Research Programmes." In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 40 [POSSESSIVE]:
Line: 241
Citation: Laudan's (1977)

Context:
The framework also addresses evolutionary epistemology's difficulty with the directed nature of human inquiry. Unlike blind Darwinian selection, knowledge systems exhibit Lamarckian-style inheritance of acquired insights through the functional entrenchment of successful solutions in network cores.

It is useful to distinguish our concept of systemic brittleness from related ideas in the philosophy of science, such as Lakatos's (1970) 'degenerative research programmes' and Laudan's (1977) 'problem-solving effectiveness'. While Lakatos provides a brilliant historical description of a degenerating programme—one that relies on ad-hoc hypotheses and fails to make novel predictions—our framework aims to provide the underlying causal engine for this degeneration. Brittleness is a measure of the accumulated systemic costs that cause a programme to become degenerative. It is a diagnostic of a system's structural health, not just a historical description of its output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of risk and resilience. A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

Reference:
Laudan, Larry. 1977. *Progress and Its Problems: Towards a Theory of Scientific Growth*. Berkeley: University of California Press. ISBN 978-0520037212.
------------------------------------------------------------

Citation 41 [PARENTHETICAL]:
Line: 247
Citation: (Baggio and Parravicini 2019)

Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Baggio, Guido, and Andrea Parravicini. 2019. "Introduction to Pragmatism and Theories of Emergence." *European Journal of Pragmatism and American Philosophy* XI-2. https://doi.org/10.4000/ejpap.1611.
------------------------------------------------------------

Citation 42 [IN-PROSE]:
Line: 247
Citation: Rorty (1979)

Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press. ISBN 978-0691020167.
------------------------------------------------------------

Citation 43 [IN-PROSE]:
Line: 247
Citation: Brandom (1994)

Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Brandom, Robert B. 1994. *Making It Explicit: Reasoning, Representing, and Discursive Commitment*. Cambridge, MA: Harvard University Press. ISBN 978-0674543195.
------------------------------------------------------------

Citation 44 [IN-PROSE]:
Line: 249
Citation: El-Hani and Pihlström (2002)

Context:
The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Reference:
El-Hani, Charbel Niño, and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3(2): article 3. https://doi.org/10.5840/eip2002325.
------------------------------------------------------------

Citation 45 [PARENTHETICAL]:
Line: 255
Citation: (Worrall 1989)

Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.

Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43(1–2): 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 46 [IN-PROSE]:
Line: 255
Citation: Ladyman and Ross (2007)

Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press. ISBN 978-0199276196.
------------------------------------------------------------

Citation 47 [IN-PROSE]:
Line: 268
Citation: Kelly (2005)

Context:
This framework has implications for several contemporary discussions in epistemology:

**Disagreement**: Following Kelly (2005), the diagnosed brittleness of knowledge systems provides powerful higher-order evidence that should influence how agents respond to disagreement. Claims from low-brittleness networks warrant higher confidence than those from demonstrably brittle sources.

**Testimony**: The framework suggests that testimonial justification depends not only on speaker reliability but on the systemic reliability of the knowledge traditions speakers represent. This provides resources for evaluating competing testimonial sources in an information-rich but epistemically fragmented environment.

Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 48 [IN-PROSE]:
Line: 296
Citation: Kuhn (1962)

Context:
### **7.1 The Problem of Internal Coherence: Fictions, Paradigms, and the Limits of Isolation**

The most potent challenge to any coherentist model is the "isolation objection"—the possibility of a perfectly self-consistent but factually detached system. This manifests in sophisticated conspiracy theories and incommensurable scientific paradigms famously articulated by Thomas Kuhn (1962). Our model addresses this by introducing an externalist standard based on pragmatic performance, though significant methodological challenges remain.

"Coherent fictions" like conspiracy theories typically exhibit structural features: accelerating ad-hoc modifications to protect core tenets, high maintenance costs through suppression of dissent, and epistemic parasitism—generating no novel research but rationalizing away mainstream successes. Whether these constitute decisive refutation depends on objective measurement, which proves difficult in practice.

Reference:
Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press (originally 1962). ISBN 978-0226458083.
------------------------------------------------------------

Citation 49 [IN-PROSE]:
Line: 318
Citation: Kelly (2005)

Context:
It is crucial to be precise about this model's scope. It is a macro-epistemology, designed to explain the long-term viability of public knowledge systems. It does not primarily aim to solve traditional problems in micro-epistemology, such as Gettier cases or the justification of an individual's perceptual beliefs. Instead, it provides a robust bridge between these levels through the concept of higher-order evidence. The diagnosed health of a public system provides a powerful defeater (or corroborator) for an individual’s beliefs derived from that system.

To formalize this intuition, we can use a Bayesian framework. The diagnosed brittleness of a knowledge system should determine the rational prior probability an agent assigns to any claim from that source. A low-brittleness network (e.g., an IPCC report) warrants a high prior; a high-brittleness network (a denialist documentary) warrants a low one. As Thomas Kelly (2005) argued regarding disagreement, the properties of the source matter. When an agent receives new first-order evidence, E, their posterior confidence is updated via Bayes' rule. This formalizes why an agent should rationally favor the IPCC's claim: even if the denialist source presents a seemingly powerful piece of evidence, the extremely low prior assigned to that network means the agent's posterior confidence in its claims will remain low. The macro-level diagnosis thus provides a rational, quantitative basis for allocating trust.

### **7.4 Defending the Model's Grounding**

Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 50 [PARENTHETICAL]:
Line: 404
Citation: (Mesoudi 2011)

Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**

Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press. ISBN 978-0226520445.
------------------------------------------------------------


################################################################################
FILE: final.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 9
Citation: (Snow 1855)

Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs, such as misdirected public health efforts in London, and demanded constant ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, by contrast, reduced these costs while unifying diverse phenomena.

This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. 2nd ed. London: John Churchill. Reprinted in *International Journal of Epidemiology* 42, no. 6 (2013): 1543–1552. https://doi.org/10.1093/ije/dyt193.
------------------------------------------------------------

Citation 2 [PARENTHETICAL]:
Line: 13
Citation: (BonJour 1985)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press. ISBN 978-0674843813.
------------------------------------------------------------

Citation 3 [PARENTHETICAL]:
Line: 13
Citation: (Olsson 2005)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press. ISBN 978-0199279999.
------------------------------------------------------------

Citation 4 [PARENTHETICAL]:
Line: 13
Citation: (Kvanvig 2012)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50(1): 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 5 [PARENTHETICAL]:
Line: 13
Citation: (Krag 2015)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
Krag, Erik. 2015. "Coherentism and Belief Fixation." *Logos & Episteme* 6, no. 2: 187–199. https://doi.org/10.5840/logos-episteme20156211. ISSN 2069-0533.
------------------------------------------------------------

Citation 6 [PARENTHETICAL]:
Line: 13
Citation: (Quine 1960)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
Quine, W. V. O. 1960. *Word and Object*. Cambridge, MA: MIT Press. ISBN 978-0262670012.
------------------------------------------------------------

Citation 7 [PARENTHETICAL]:
Line: 13
Citation: (Kitcher 1993)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 8 [POSSESSIVE]:
Line: 17
Citation: Quine's (1969)

Context:
### 1.1 Lineage and Departure

Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

Reference:
Quine, W. V. 1969. "Epistemology Naturalized." In *Ontological Relativity and Other Essays*, 69–90. New York: Columbia University Press. https://doi.org/10.7312/quin92204-004. ISBN 9780231083577, 9780231171991.
------------------------------------------------------------

Citation 9 [POSSESSIVE]:
Line: 17
Citation: Davidson's (1986)

Context:
### 1.1 Lineage and Departure

Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

Reference:
Davidson, Donald. 1986. "A Coherence Theory of Truth and Knowledge." In *Truth and Interpretation: Perspectives on the Philosophy of Donald Davidson*, edited by Ernest LePore, 307–19. Oxford: Blackwell.
------------------------------------------------------------

Citation 10 [POSSESSIVE]:
Line: 17
Citation: Kitcher's (1993)

Context:
### 1.1 Lineage and Departure

Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 11 [POSSESSIVE]:
Line: 17
Citation: Longino's (1990)

Context:
### 1.1 Lineage and Departure

Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

Reference:
Longino, Helen E. 1990. *Science as Social Knowledge: Values and Objectivity in Scientific Inquiry*. Princeton, NJ: Princeton University Press. ISBN 978-0691020518.
------------------------------------------------------------

Citation 12 [POSSESSIVE]:
Line: 19
Citation: Thagard's (1989)

Context:
Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

EPC thus transforms coherence from a metaphor of fit into a measurable function of cost and constraint. Its realism is not the correspondence of propositions to an independent world, but the emergent stability of constraint satisfaction across iterated cycles of error and repair. This is a post-Quinean naturalism that grounds justification in demonstrated viability. It provides the crucial, non-discursive external filter of systemic consequence that these otherwise powerful social, psychological, and internalist accounts ultimately require.

Reference:
Thagard, Paul. 1989. "Explanatory Coherence." *Behavioral and Brain Sciences* 12(3): 435–502. https://doi.org/10.1017/S0140525X00057046.
------------------------------------------------------------

Citation 13 [POSSESSIVE]:
Line: 19
Citation: Zollman's (2007)

Context:
Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

EPC thus transforms coherence from a metaphor of fit into a measurable function of cost and constraint. Its realism is not the correspondence of propositions to an independent world, but the emergent stability of constraint satisfaction across iterated cycles of error and repair. This is a post-Quinean naturalism that grounds justification in demonstrated viability. It provides the crucial, non-discursive external filter of systemic consequence that these otherwise powerful social, psychological, and internalist accounts ultimately require.

Reference:
Zollman, Kevin J. S. 2007. "The Communication Structure of Epistemic Communities." *Philosophy of Science* 74(5): 574–87. https://doi.org/10.1086/508684.
------------------------------------------------------------

Citation 14 [POSSESSIVE]:
Line: 19
Citation: Rescher's (1973)

Context:
Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

EPC thus transforms coherence from a metaphor of fit into a measurable function of cost and constraint. Its realism is not the correspondence of propositions to an independent world, but the emergent stability of constraint satisfaction across iterated cycles of error and repair. This is a post-Quinean naturalism that grounds justification in demonstrated viability. It provides the crucial, non-discursive external filter of systemic consequence that these otherwise powerful social, psychological, and internalist accounts ultimately require.

Reference:
Rescher, Nicholas. 1973. *The Coherence Theory of Truth*. Oxford: Clarendon Press. ISBN 978-0198244011.
------------------------------------------------------------

Citation 15 [POSSESSIVE]:
Line: 19
Citation: Kitcher's (1993)

Context:
Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

EPC thus transforms coherence from a metaphor of fit into a measurable function of cost and constraint. Its realism is not the correspondence of propositions to an independent world, but the emergent stability of constraint satisfaction across iterated cycles of error and repair. This is a post-Quinean naturalism that grounds justification in demonstrated viability. It provides the crucial, non-discursive external filter of systemic consequence that these otherwise powerful social, psychological, and internalist accounts ultimately require.

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 16 [PARENTHETICAL]:
Line: 25
Citation: (Holling 1973)

Context:
Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—create a constraint landscape that necessarily generates optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures.

We ground coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences, which we term pragmatic pushback, drive bottom-up formation of viable public knowledge systems.

This transforms the isolation objection: a coherent system detached from reality isn't truly possible because constraints force convergence toward viable configurations. A perfectly coherent flat-earth cosmology generates catastrophic navigational costs. A coherent phlogiston chemistry generates accelerating conceptual debt. These aren't merely false but structurally unstable, misaligned with constraint topology. The process resembles constructing a navigation chart by systematically mapping shipwrecks. Successful systems navigate safe channels revealed by failures, triangulating toward viable peaks. The Apex Network is the structure remaining when all unstable configurations are eliminated. Crucially, this historical filtering is a discovery process, not a creation mechanism; the territory is revealed by the map of failures, not created by it.

Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 17 [PARENTHETICAL]:
Line: 50
Citation: (Meadows 2008)

Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool

Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing. ISBN 978-1603580557.
------------------------------------------------------------

Citation 18 [PARENTHETICAL]:
Line: 54
Citation: (Goldman 1979)

Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Sinclair 2007), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 19 [PARENTHETICAL]:
Line: 54
Citation: (Kitcher 1993)

Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Sinclair 2007), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 20 [PARENTHETICAL]:
Line: 54
Citation: (Sinclair 2007)

Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Sinclair 2007), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate

Reference:
Sinclair, Robert. 2007. "Quine's Naturalized Epistemology and the Third Dogma of Empiricism." *Southern Journal of Philosophy* 45, no. 3: 455–472. https://doi.org/10.1111/j.2041-6962.2007.tb00060.x.
------------------------------------------------------------

Citation 21 [POSSESSIVE]:
Line: 54
Citation: Kim's (1988)

Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Sinclair 2007), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate

Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 22 [PARENTHETICAL]:
Line: 76
Citation: (Campbell 1974)

Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is a structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

#### Conceptual Architecture

Reference:
Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.
------------------------------------------------------------

Citation 23 [PARENTHETICAL]:
Line: 76
Citation: (Mesoudi 2011)

Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is a structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

#### Conceptual Architecture

Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press. ISBN 978-0226520445.
------------------------------------------------------------

Citation 24 [POSSESSIVE]:
Line: 104
Citation: Taleb's (2012)

Context:
These epistemic inefficiencies are real costs rendering networks brittle and unproductive, even without direct experimental falsification. The diagnostic lens thus applies to all inquiry forms, measuring viability through external material consequences or internal systemic dysfunction.

To operationalize these concepts, we introduce diagnostic indicators tracking brittleness over time. Drawing from systems theory, brittleness denotes vulnerability from hidden interdependencies and cascading failures. It parallels Taleb's (2012) fragility but diagnoses structural cause (accumulated systemic weakness) rather than symptom (vulnerability to shocks). This distinction enables predictive diagnosis rather than mere description.

In evolutionary epistemology (Campbell 1974; Popper 1972), falsification acts as a selection pressure on beliefs. EPC extends this: brittleness functions analogously to free energy (Friston 2010), quantifying deviation between a network's internal model and its constraining environment. Reducing brittleness thus constitutes epistemic adaptation. Zollman (2010) shows how transient diversity increases long-term epistemic robustness, providing theoretical justification for our pluralist frontier.

Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House. ISBN 978-1400067824.
------------------------------------------------------------

Citation 25 [PARENTHETICAL]:
Line: 106
Citation: (Campbell 1974)

Context:
To operationalize these concepts, we introduce diagnostic indicators tracking brittleness over time. Drawing from systems theory, brittleness denotes vulnerability from hidden interdependencies and cascading failures. It parallels Taleb's (2012) fragility but diagnoses structural cause (accumulated systemic weakness) rather than symptom (vulnerability to shocks). This distinction enables predictive diagnosis rather than mere description.

In evolutionary epistemology (Campbell 1974; Popper 1972), falsification acts as a selection pressure on beliefs. EPC extends this: brittleness functions analogously to free energy (Friston 2010), quantifying deviation between a network's internal model and its constraining environment. Reducing brittleness thus constitutes epistemic adaptation. Zollman (2010) shows how transient diversity increases long-term epistemic robustness, providing theoretical justification for our pluralist frontier.

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |

Reference:
Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.
------------------------------------------------------------

Citation 26 [PARENTHETICAL]:
Line: 106
Citation: (Popper 1972)

Context:
To operationalize these concepts, we introduce diagnostic indicators tracking brittleness over time. Drawing from systems theory, brittleness denotes vulnerability from hidden interdependencies and cascading failures. It parallels Taleb's (2012) fragility but diagnoses structural cause (accumulated systemic weakness) rather than symptom (vulnerability to shocks). This distinction enables predictive diagnosis rather than mere description.

In evolutionary epistemology (Campbell 1974; Popper 1972), falsification acts as a selection pressure on beliefs. EPC extends this: brittleness functions analogously to free energy (Friston 2010), quantifying deviation between a network's internal model and its constraining environment. Reducing brittleness thus constitutes epistemic adaptation. Zollman (2010) shows how transient diversity increases long-term epistemic robustness, providing theoretical justification for our pluralist frontier.

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |

Reference:
Popper, Karl R. 1972. *Objective Knowledge: An Evolutionary Approach*. Oxford: Clarendon Press. ISBN 978-0198750246.
------------------------------------------------------------

Citation 27 [PARENTHETICAL]:
Line: 106
Citation: (Friston 2010)

Context:
To operationalize these concepts, we introduce diagnostic indicators tracking brittleness over time. Drawing from systems theory, brittleness denotes vulnerability from hidden interdependencies and cascading failures. It parallels Taleb's (2012) fragility but diagnoses structural cause (accumulated systemic weakness) rather than symptom (vulnerability to shocks). This distinction enables predictive diagnosis rather than mere description.

In evolutionary epistemology (Campbell 1974; Popper 1972), falsification acts as a selection pressure on beliefs. EPC extends this: brittleness functions analogously to free energy (Friston 2010), quantifying deviation between a network's internal model and its constraining environment. Reducing brittleness thus constitutes epistemic adaptation. Zollman (2010) shows how transient diversity increases long-term epistemic robustness, providing theoretical justification for our pluralist frontier.

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |

Reference:
Friston, Karl J. 2010. "The Free-Energy Principle: A Unified Brain Theory?" *Nature Reviews Neuroscience* 11 (2): 127–138. https://doi.org/10.1038/nrn2787.
------------------------------------------------------------

Citation 28 [IN-PROSE]:
Line: 106
Citation: Zollman (2010)

Context:
To operationalize these concepts, we introduce diagnostic indicators tracking brittleness over time. Drawing from systems theory, brittleness denotes vulnerability from hidden interdependencies and cascading failures. It parallels Taleb's (2012) fragility but diagnoses structural cause (accumulated systemic weakness) rather than symptom (vulnerability to shocks). This distinction enables predictive diagnosis rather than mere description.

In evolutionary epistemology (Campbell 1974; Popper 1972), falsification acts as a selection pressure on beliefs. EPC extends this: brittleness functions analogously to free energy (Friston 2010), quantifying deviation between a network's internal model and its constraining environment. Reducing brittleness thus constitutes epistemic adaptation. Zollman (2010) shows how transient diversity increases long-term epistemic robustness, providing theoretical justification for our pluralist frontier.

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |

Reference:
Zollman, Kevin J. S. 2010. "The Epistemic Benefit of Transient Diversity." *Erkenntnis* 72: 17–35. https://doi.org/10.1007/s10670-009-9194-6. ISSN 0165-0106.
------------------------------------------------------------

Citation 29 [POSSESSIVE]:
Line: 174
Citation: Kitcher's (1993)

Context:
- **Model Complexity Inflation (M(t)):** Parameter growth without predictive gains (parameter-to-prediction ratios).
  - **Proof Complexity Escalation:** Increasing proof length without explanatory gain (mathematics).

While interpreting these costs is normative for agents within a system, their existence and magnitude are empirical questions. The framework's core causal claim is falsifiable and descriptive: networks with high or rising brittleness across these tiers carry statistically higher probability of systemic failure or major revision when faced with external shocks. This operationalizes Kitcher's (1993) 'significant problems' pragmatically: Tier 1 bio-costs define significance without credit cynicism, while C(t) measures coercive monopolies masking failures.

Robustly measuring these costs requires disciplined methodology. The triangulation method provides practical protocol for achieving pragmatic objectivity.

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 30 [PARENTHETICAL]:
Line: 236
Citation: (Popper 1959)

Context:
### 4.1 A Negative Methodology: Charting What Fails

Our account of objectivity is not the pursuit of a distant star but the painstaking construction of a reef chart from the empirical data of shipwrecks. It begins not with visions of final truth, but with our most secure knowledge: the clear, non-negotiable data of large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:

Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson (originally 1934). ISBN 978-0415278447.
------------------------------------------------------------

Citation 31 [PARENTHETICAL]:
Line: 248
Citation: (Ladyman and Ross 2007)

Context:
### 4.2 The Apex Network: Ontological and Epistemic Status

As the Negative Canon catalogs failures, pragmatic selection reveals the contours of the **Apex Network**: not a pre-existing blueprint, nor our current consensus, but the objective structure of maximally viable solutions all successful inquiry must approximate. The Apex maximizes Thagard's global constraint satisfaction under Zollman's optimal topology (sparse diversity bonus), emerging as Kitcher's adaptive peak on Rescher's systematicity landscape. This network is not a pre-existing metaphysical blueprint but a structural emergent, the asymptotic intersection of all low-brittleness models (Ladyman and Ross 2007). Its status resonates with the pragmatist ideal end of inquiry (Peirce 1878). Our Consensus Network is our fallible map of this objective structure, which is stabilized through adaptive feedback from mind-independent constraints.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not an accident but an emergent structural fact demanding naturalistic explanation.

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press. ISBN 978-0199276196.
------------------------------------------------------------

Citation 32 [PARENTHETICAL]:
Line: 248
Citation: (Peirce 1878)

Context:
### 4.2 The Apex Network: Ontological and Epistemic Status

As the Negative Canon catalogs failures, pragmatic selection reveals the contours of the **Apex Network**: not a pre-existing blueprint, nor our current consensus, but the objective structure of maximally viable solutions all successful inquiry must approximate. The Apex maximizes Thagard's global constraint satisfaction under Zollman's optimal topology (sparse diversity bonus), emerging as Kitcher's adaptive peak on Rescher's systematicity landscape. This network is not a pre-existing metaphysical blueprint but a structural emergent, the asymptotic intersection of all low-brittleness models (Ladyman and Ross 2007). Its status resonates with the pragmatist ideal end of inquiry (Peirce 1878). Our Consensus Network is our fallible map of this objective structure, which is stabilized through adaptive feedback from mind-independent constraints.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not an accident but an emergent structural fact demanding naturalistic explanation.

Reference:
Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press (originally 1878).
------------------------------------------------------------

Citation 33 [PARENTHETICAL]:
Line: 252
Citation: (Berlin and Kay 1969)

Context:
The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not an accident but an emergent structural fact demanding naturalistic explanation.

Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals, such as safe water and ripe fruit, conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.

The mechanism forging this structure is bottom-up emergence driven by cross-domain consistency needs. Local Shared Networks, developed to solve specific problems, face pressure to cohere because they operate in an interconnected world. This pressure creates tendency toward integration, though whether this yields a single maximally coherent system or stable pluralism remains empirical.

Reference:
Berlin, Brent, and Paul Kay. 1969. *Basic Color Terms: Their Universality and Evolution*. Berkeley: University of California Press. ISBN 978-1575861623
------------------------------------------------------------

Citation 34 [PARENTHETICAL]:
Line: 252
Citation: (Henrich 2015)

Context:
The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not an accident but an emergent structural fact demanding naturalistic explanation.

Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals, such as safe water and ripe fruit, conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.

The mechanism forging this structure is bottom-up emergence driven by cross-domain consistency needs. Local Shared Networks, developed to solve specific problems, face pressure to cohere because they operate in an interconnected world. This pressure creates tendency toward integration, though whether this yields a single maximally coherent system or stable pluralism remains empirical.

Reference:
Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press. ISBN 978-0691178431.
------------------------------------------------------------

Citation 35 [POSSESSIVE]:
Line: 258
Citation: Haack's (1993)

Context:
The framework makes no a priori claims about universal convergence. Domains with tight pragmatic constraints (basic engineering, medicine) show strong convergence pressures. Others (aesthetic judgment, political organization) may support multiple stable configurations. The Apex Network concept is thus a limiting case: the theoretical endpoint of convergence pressures where they operate, not a guarantee of uniform action across all inquiry domains.

The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell. ISBN 978-0631196792.
------------------------------------------------------------

Citation 36 [PARENTHETICAL]:
Line: 260
Citation: (Rescher 1996)

Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.

Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press. ISBN 978-0791428184.
------------------------------------------------------------

Citation 37 [PARENTHETICAL]:
Line: 295
Citation: (Newman 2010)

Context:
#### 4.2.1 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}

Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press. ISBN 978-0199206650.
------------------------------------------------------------

Citation 38 [PARENTHETICAL]:
Line: 348
Citation: (Tauriainen 2017)

Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check, the assessment of systemic brittleness, prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.

Reference:
Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä, Department of Social Sciences and Philosophy. https://urn.fi/URN:NBN:fi:jyu-201705312584.
------------------------------------------------------------

Citation 39 [PARENTHETICAL]:
Line: 378
Citation: (March 1978)

Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (March 1978).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:

Reference:
March, James G. 1978. "Bounded Rationality, Ambiguity, and the Engineering of Choice." *The Bell Journal of Economics* 9, no. 2: 587–608. https://doi.org/10.2307/3003600.
------------------------------------------------------------

Citation 40 [PARENTHETICAL]:
Line: 398
Citation: (Quine 1951)

Context:
**Animating Quine's Web: From Static Structure to Dynamic Process**

Quine's "Web of Belief" (Quine 1951, 1960) provided a powerful static model of confirmational holism, but it has been criticized for lacking a dynamic account of its formation and change. Our framework provides the missing mechanisms.

First, pragmatic pushback supplies the externalist filter that grounds the web in mind-independent reality, decisively solving the isolation objection that haunts purely internalist readings. This relentless, non-discursive filter of real-world consequences prevents the web from floating free of constraints.

Reference:
Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60(1): 20–43. https://doi.org/10.2307/2181906.
------------------------------------------------------------

Citation 41 [PARENTHETICAL]:
Line: 402
Citation: (March 1978)

Context:
First, pragmatic pushback supplies the externalist filter that grounds the web in mind-independent reality, decisively solving the isolation objection that haunts purely internalist readings. This relentless, non-discursive filter of real-world consequences prevents the web from floating free of constraints.

Second, the entrenchment of pragmatically indispensable principles in the system's core provides a directed learning mechanism. A proposition migrates to the core not by convention but because it has demonstrated immense value in lowering the entire network's systemic brittleness, making its revision catastrophically costly. This process, driven by bounded rationality (March 1978), functions as systemic caching: proven principles are preserved to avoid re-derivation costs.

For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now prohibitively expensive.

Reference:
March, James G. 1978. "Bounded Rationality, Ambiguity, and the Engineering of Choice." *The Bell Journal of Economics* 9, no. 2: 587–608. https://doi.org/10.2307/3003600.
------------------------------------------------------------

Citation 42 [PARENTHETICAL]:
Line: 406
Citation: (Carlson 2015)

Context:
For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now prohibitively expensive.

Together, these two mechanisms animate Quine's static web. Pragmatic pushback provides the external discipline, and the entrenchment of low-brittleness principles explains how the web's resilient core is systematically constructed over time (Carlson 2015). This transforms the web from a logical snapshot into an evolving reef chart. In doing so, it resolves a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our three-level framework shows these are not contradictory but two necessary components of a naturalistic epistemology. Core principles achieve Justified Truth (Level 2) through this process of systematic, externally-validated selection, while the Apex Network (Level 3) functions as the regulative structure toward which our theories converge.

This three-level truth framework describes the justificatory status of claims at a given moment. Over historical time, pragmatic filtering produces a discernible two-zone structure in our evolving knowledge systems.

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 43 [PARENTHETICAL]:
Line: 406
Citation: (Tauriainen 2017)

Context:
For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now prohibitively expensive.

Together, these two mechanisms animate Quine's static web. Pragmatic pushback provides the external discipline, and the entrenchment of low-brittleness principles explains how the web's resilient core is systematically constructed over time (Carlson 2015). This transforms the web from a logical snapshot into an evolving reef chart. In doing so, it resolves a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our three-level framework shows these are not contradictory but two necessary components of a naturalistic epistemology. Core principles achieve Justified Truth (Level 2) through this process of systematic, externally-validated selection, while the Apex Network (Level 3) functions as the regulative structure toward which our theories converge.

This three-level truth framework describes the justificatory status of claims at a given moment. Over historical time, pragmatic filtering produces a discernible two-zone structure in our evolving knowledge systems.

Reference:
Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä, Department of Social Sciences and Philosophy. https://urn.fi/URN:NBN:fi:jyu-201705312584.
------------------------------------------------------------

Citation 44 [PARENTHETICAL]:
Line: 415
Citation: (Price 1992)

Context:
The historical process of pragmatic filtering gives our evolving Consensus Networks a discernible structure, which can be understood as having two distinct epistemic zones. This distinction is not about the nature of reality itself, but describes the justificatory status of our claims at a given time.

* **The Convergent Core:** This represents the load-bearing foundations of our current knowledge. It comprises domains where the relentless pressure of pragmatic selection has eliminated all known rival formulations, leaving a single, or functionally identical, set of low-brittleness principles. Principles reside in this core, such as the laws of thermodynamics or the germ theory of disease, not because they are dogmatically held or self-evident but because all tested alternatives have been relegated to the Negative Canon after generating catastrophically high systemic costs. While no claim is immune to revision in principle, the principles in the Convergent Core are functionally unrevisable in practice, as doing so would require dismantling the most successful and resilient knowledge structures we have ever built. A claim from this core achieves the highest degree of justification we can assign, approaching our standard for Objective Truth (Level 1).
* **The Pluralist Frontier:** This describes the domains of active research where our current evidence is insufficient to decide between multiple, competing, and viable reconstructions of the landscape of viability. Here, rival systems (e.g., different interpretations of quantum mechanics or competing models of consciousness) may coexist, each with a demonstrably low and stable degree of brittleness. It is crucial to distinguish this constrained, evidence-based pluralism from relativism. The frontier is not an "anything goes" zone but a highly restricted space strictly bounded on all sides by the Negative Canon. A system based on phlogiston is not a "viable contender" on the frontier of chemistry but a demonstrably failed research program. This pluralism is therefore a sign of epistemic underdetermination: a feature of our map's current limitations, not reality's supposed indifference. This position resonates with pragmatist accounts of functional pluralism (Price 1992), which treat different conceptual frameworks as tools whose legitimacy is determined by their utility within a specific practice. Within this frontier, the core claims of each viable competing system can be granted the status of Justified Truth (Level 2). This is also the zone where non-epistemic factors, such as institutional power or contingent path dependencies, can play their most significant role, sometimes artificially constraining the range of options explored or creating temporary monopolies on what is considered justified.

### 4.5 Illustrative Cases of Convergence and Brittleness

Reference:
Price, Huw. 1992. "Metaphysical Pluralism." *Journal of Philosophy* 89(8): 387–409. https://doi.org/10.2307/2940741.
------------------------------------------------------------

Citation 45 [IN-PROSE]:
Line: 419
Citation: Kuhn (1962)

Context:
### 4.5 Illustrative Cases of Convergence and Brittleness

The transition from Newtonian to relativistic physics offers a canonical example of this framework's diagnostic application. After centuries of viability, the Newtonian system began to accumulate significant systemic costs in the late 19th century. These manifested as first-order predictive failures, such as its inability to account for the perihelion of Mercury, and as rising conceptual debt in the form of ad-hoc modifications like the Lorentz-FitzGerald contraction hypothesis. This accumulating brittleness created what Kuhn (1962) termed a "crisis" state preceding paradigm shifts. The Einsteinian system proved a more resilient solution, reducing this conceptual debt and substantially lowering the systemic costs of inquiry in physics.

A more contemporary case can be found in the recent history of artificial intelligence, which illustrates how a brittleness assessment might function in real time. The periodic "AI winters" can be understood as the collapse of high-brittleness paradigms, such as symbolic AI, which suffered from a high rate of ad-hoc modification when faced with novel challenges. While the subsequent deep learning paradigm proved a low-brittleness solution for many specific tasks, it may now be showing signs of rising systemic costs. These can be described conceptually as, for example, potentially unsustainable escalations in computational and energy resources for marginal performance gains, or an accelerating research focus on auxiliary, post-hoc modifications rather than on foundational architectural advances. This situation illustrates the Pluralist Frontier in action, as rival architectures might now be seen as competing to become the next low-brittleness solution.

Reference:
Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press (originally 1962). ISBN 978-0226458083.
------------------------------------------------------------

Citation 46 [PARENTHETICAL]:
Line: 425
Citation: (Wright 1932)

Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics.

The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47 [PARENTHETICAL]:
Line: 429
Citation: (Acemoglu and Robinson 2012)

Context:
The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape itself. Powerful institutions do not merely respond to brittleness defensively; they can construct and maintain the very conditions that generate it. By controlling research funding, defining what counts as a legitimate problem, and entrenching path dependencies that reinforce a fitness trap, institutional power actively digs the fitness trap and locks the system into a high-brittleness state. This pattern of epistemic capture appears across domains: from tobacco companies suppressing health research to colonial knowledge systems that extracted Indigenous insights while denying reciprocal engagement, thereby masking brittleness through institutional dominance. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. To detect such hidden brittleness, we can augment C(t) with sub-metrics for innovation stagnation, tracking lags in novel applications or cross-domain extensions relative to comparable systems as proxies for suppressed adaptive capacity. Concretely, innovation lag can be operationalized as: **I(t) = (Novel Applications per Unit Time) / (Defensive Publications per Unit Time)**. When I(t) declines while C(t) remains high, this signals power-induced rigidity masking underlying brittleness. For example, Lysenkoist biology in the Soviet Union showed I(t) approaching zero (no cross-domain applications) while defensive publications proliferated. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

Reference:
Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business. ISBN 978-0307719225.
------------------------------------------------------------

Citation 48 [IN-PROSE]:
Line: 433
Citation: Turchin (2003)

Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape itself. Powerful institutions do not merely respond to brittleness defensively; they can construct and maintain the very conditions that generate it. By controlling research funding, defining what counts as a legitimate problem, and entrenching path dependencies that reinforce a fitness trap, institutional power actively digs the fitness trap and locks the system into a high-brittleness state. This pattern of epistemic capture appears across domains: from tobacco companies suppressing health research to colonial knowledge systems that extracted Indigenous insights while denying reciprocal engagement, thereby masking brittleness through institutional dominance. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. To detect such hidden brittleness, we can augment C(t) with sub-metrics for innovation stagnation, tracking lags in novel applications or cross-domain extensions relative to comparable systems as proxies for suppressed adaptive capacity. Concretely, innovation lag can be operationalized as: **I(t) = (Novel Applications per Unit Time) / (Defensive Publications per Unit Time)**. When I(t) declines while C(t) remains high, this signals power-induced rigidity masking underlying brittleness. For example, Lysenkoist biology in the Soviet Union showed I(t) approaching zero (no cross-domain applications) while defensive publications proliferated. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, exemplifies this state. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability therefore requires a balance between low-cost stability and adaptive capacity. This evolutionary perspective completes our reef chart, not as a finished map, but as an ongoing process of hazard detection and channel discovery.

Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press. ISBN 978-0691116693.
------------------------------------------------------------

Citation 49 [PARENTHETICAL]:
Line: 499
Citation: (Russell 1903)

Context:
- Appeared to exhibit low brittleness across all indicators
- Provided an elegant foundation for mathematics

**Russell's Paradox (Russell 1903):**
- Revealed infinite brittleness: the theory could derive a direct contradiction
- Considered the set R = {x | x ∉ x}. Is R ∈ R? Both yes and no follow from the axioms
- All inference paralyzed (if both A and ¬A are derivable, the principle of explosion allows derivation of anything)

Reference:
Russell, Bertrand. 1903. *The Principles of Mathematics*. Cambridge: Cambridge University Press. ISBN 978-1430476030.
------------------------------------------------------------

Citation 50 [PARENTHETICAL]:
Line: 537
Citation: (Harding 1991)

Context:
### 5.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods

Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press. ISBN 978-0801497469.
------------------------------------------------------------

Citation 51 [PARENTHETICAL]:
Line: 568
Citation: (March 1978)

Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (March 1978) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost

Reference:
March, James G. 1978. "Bounded Rationality, Ambiguity, and the Engineering of Choice." *The Bell Journal of Economics* 9, no. 2: 587–608. https://doi.org/10.2307/3003600.
------------------------------------------------------------

Citation 52 [IN-PROSE]:
Line: 592
Citation: Carlson (2015)

Context:
### 6.1 A Grounded Coherentism and a Naturalized Structural Realism

While internalist coherentists like Carlson (2015) have successfully shown that the web must have a functionally indispensable core, they lack resources to explain why that core is forged by external discipline. Systemic Externalism provides this missing causal engine, grounding Carlson's internal structure in an externalist history of pragmatic selection. Justification requires coherence plus network reliability via low brittleness. Unlike Zollman's (2007, 2013) static network models and Rosenstock et al. (2017), EPC examines evolving networks under pushback, extending ECHO's harmony principle with external brittleness filters Thagard's internalism lacks.

#### 6.1.1 A Naturalistic Engine for Structural Realism

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 53 [IN-PROSE]:
Line: 592
Citation: Rosenstock et al. (2017)

Context:
### 6.1 A Grounded Coherentism and a Naturalized Structural Realism

While internalist coherentists like Carlson (2015) have successfully shown that the web must have a functionally indispensable core, they lack resources to explain why that core is forged by external discipline. Systemic Externalism provides this missing causal engine, grounding Carlson's internal structure in an externalist history of pragmatic selection. Justification requires coherence plus network reliability via low brittleness. Unlike Zollman's (2007, 2013) static network models and Rosenstock et al. (2017), EPC examines evolving networks under pushback, extending ECHO's harmony principle with external brittleness filters Thagard's internalism lacks.

#### 6.1.1 A Naturalistic Engine for Structural Realism

Reference:
Rosenstock, Sarita, Cailin O'Connor, and Justin Bruner. 2017. "In Epistemic Networks, Is Less Really More?" *Philosophy of Science* 84(2): 234–52. https://doi.org/10.1086/690717.
------------------------------------------------------------

Citation 54 [PARENTHETICAL]:
Line: 596
Citation: (Worrall 1989)

Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman and Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43(1–2): 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 55 [PARENTHETICAL]:
Line: 598
Citation: (Ladyman and Ross 2007)

Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman and Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press. ISBN 978-0199276196.
------------------------------------------------------------

Citation 56 [PARENTHETICAL]:
Line: 602
Citation: (Goldman 1979)

Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 57 [PARENTHETICAL]:
Line: 602
Citation: (Zagzebski 1996)

Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press. ISBN 978-0521570602. https://doi.org/10.1017/CBO9780511582233.
------------------------------------------------------------

Citation 58 [PARENTHETICAL]:
Line: 606
Citation: (Baggio and Parravicini 2019)

Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Baggio, Guido, and Andrea Parravicini. 2019. "Introduction to Pragmatism and Theories of Emergence." *European Journal of Pragmatism and American Philosophy* XI-2. https://doi.org/10.4000/ejpap.1611.
------------------------------------------------------------

Citation 59 [PARENTHETICAL]:
Line: 606
Citation: (Putnam 2002)

Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press. ISBN 978-0674013803.
------------------------------------------------------------

Citation 60 [PARENTHETICAL]:
Line: 606
Citation: (Lynch 2009)

Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Lynch, Michael P. 2009. *Truth as One and Many*. Oxford: Clarendon Press. ISBN 978-0199218738.
------------------------------------------------------------

Citation 61 [IN-PROSE]:
Line: 606
Citation: Rorty (1979)

Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press. ISBN 978-0691020167.
------------------------------------------------------------

Citation 62 [IN-PROSE]:
Line: 606
Citation: Brandom (1994)

Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Brandom, Robert B. 1994. *Making It Explicit: Reasoning, Representing, and Discursive Commitment*. Cambridge, MA: Harvard University Press. ISBN 978-0674543195.
------------------------------------------------------------

Citation 63 [IN-PROSE]:
Line: 608
Citation: El-Hani and Pihlström (2002)

Context:
The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Reference:
El-Hani, Charbel Niño, and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3(2): article 3. https://doi.org/10.5840/eip2002325.
------------------------------------------------------------

Citation 64 [PARENTHETICAL]:
Line: 612
Citation: (Goldman 1999)

Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

#### Collective Calibration

Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press. ISBN 978-0198238201.
------------------------------------------------------------

Citation 65 [PARENTHETICAL]:
Line: 612
Citation: (Longino 2002)

Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

#### Collective Calibration

Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press. ISBN 978-0691088761.
------------------------------------------------------------

Citation 66 [PARENTHETICAL]:
Line: 612
Citation: (Harding 1991)

Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

#### Collective Calibration

Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press. ISBN 978-0801497469.
------------------------------------------------------------

Citation 67 [PARENTHETICAL]:
Line: 612
Citation: (Lugones 2003)

Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

#### Collective Calibration

Reference:
Lugones, María. 2003. *Pilgrimages/Peregrinajes: Theorizing Coalition against Multiple Oppressions*. Lanham, MD: Rowman & Littlefield. ISBN 978-0742514591.
------------------------------------------------------------

Citation 68 [PARENTHETICAL]:
Line: 612
Citation: (Sims 2024)

Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

#### Collective Calibration

Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91(2): 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 69 [PARENTHETICAL]:
Line: 616
Citation: (O'Connor and Weatherall 2019)

Context:
#### Collective Calibration

Empirical models of social epistemic networks (O'Connor and Weatherall 2019) suggest that objectivity is a function of communication topology. EPC operationalizes this insight: calibration efficiency inversely correlates with brittleness. The more diverse the error signals integrated (Longino 1990; Anderson 1996), the more stable the Apex Network.

### 6.3 Distinguishing from Lakatos and Laudan

Reference:
O'Connor, Cailin, and James Owen Weatherall. 2019. *The Misinformation Age: How False Beliefs Spread*. New Haven, CT: Yale University Press. ISBN 978-0300234015.
------------------------------------------------------------

Citation 70 [PARENTHETICAL]:
Line: 616
Citation: (Longino 1990)

Context:
#### Collective Calibration

Empirical models of social epistemic networks (O'Connor and Weatherall 2019) suggest that objectivity is a function of communication topology. EPC operationalizes this insight: calibration efficiency inversely correlates with brittleness. The more diverse the error signals integrated (Longino 1990; Anderson 1996), the more stable the Apex Network.

### 6.3 Distinguishing from Lakatos and Laudan

Reference:
Longino, Helen E. 1990. *Science as Social Knowledge: Values and Objectivity in Scientific Inquiry*. Princeton, NJ: Princeton University Press. ISBN 978-0691020518.
------------------------------------------------------------

Citation 71 [PARENTHETICAL]:
Line: 616
Citation: (Anderson 1996)

Context:
#### Collective Calibration

Empirical models of social epistemic networks (O'Connor and Weatherall 2019) suggest that objectivity is a function of communication topology. EPC operationalizes this insight: calibration efficiency inversely correlates with brittleness. The more diverse the error signals integrated (Longino 1990; Anderson 1996), the more stable the Apex Network.

### 6.3 Distinguishing from Lakatos and Laudan

Reference:
Anderson, Elizabeth. 1996. "Knowledge, Human Interests, and Objectivity in Feminist Epistemology." *Philosophical Topics* 23(2): 27–58. https://doi.org/10.5840/philtopics199623214.
------------------------------------------------------------

Citation 72 [IN-PROSE]:
Line: 620
Citation: Lakatos (1970)

Context:
### 6.3 Distinguishing from Lakatos and Laudan

While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

Reference:
Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific Research Programmes." In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 73 [IN-PROSE]:
Line: 620
Citation: Laudan (1977)

Context:
### 6.3 Distinguishing from Lakatos and Laudan

While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

Reference:
Laudan, Larry. 1977. *Progress and Its Problems: Towards a Theory of Scientific Growth*. Berkeley: University of California Press. ISBN 978-0520037212.
------------------------------------------------------------

Citation 74 [PARENTHETICAL]:
Line: 622
Citation: (Pritchard 2016)

Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.


### 6.4 Plantinga's Challenge: Does Evolution Select for Truth or Mere Survival?

Reference:
Pritchard, Duncan. 2016. "Epistemic Risk." *Journal of Philosophy* 113(11): 550–571. https://doi.org/10.5840/jphil20161131137.
------------------------------------------------------------

Citation 75 [PARENTHETICAL]:
Line: 627
Citation: (Plantinga 1993)

Context:
### 6.4 Plantinga's Challenge: Does Evolution Select for Truth or Mere Survival?

Alvin Plantinga's Evolutionary Argument Against Naturalism (EAAN) poses a formidable challenge to any naturalistic epistemology: if our cognitive faculties are products of natural selection, and natural selection optimizes for reproductive success rather than true belief, then we have no reason to trust that our faculties reliably produce true beliefs (Plantinga 1993, 2011). Evolution could equip us with systematically false but adaptive beliefs—useful fictions that enhance survival without tracking reality. If naturalism is true, the very faculties we use to conclude naturalism is true are unreliable, rendering naturalism self-defeating.

Our framework provides a novel response by collapsing Plantinga's proposed gap between adaptive success and truth-tracking. We argue that in domains where systematic misrepresentation generates costs, survival pressure and truth-tracking converge necessarily. This is not because evolution "cares about" truth, but because reality's constraint structure makes persistent falsehood unsustainable.

Reference:
Plantinga, Alvin. 1993. *Warrant and Proper Function*. New York: Oxford University Press. ISBN 978-0195078640.
------------------------------------------------------------

Citation 76 [IN-PROSE]:
Line: 685
Citation: Kitcher (1993)

Context:
**Rescher's Systematicity (1973, 2001):** Defines truth as praxis-tested systematicity (completeness, consistency, functional efficacy) but lacks quantification. **Advance:** SBI(t) operationalizes—P(t) = consistency metric, R(t) = completeness breadth, enabling falsifiable predictions (brittleness-collapse correlations).

**Kitcher (1993) on Evolutionary Progress:** Models science as credit-driven selection with division of labor across 'significant problems.' **Advance:** Negative Canon provides failure engine, brittleness quantifies problem significance via coercive costs (C(t)), diagnosing degenerating programs without reducing to cynical credit-seeking.

**Sims (2024) on Dynamic Holism:** Ecological constraints drive diachronic cognitive revision in nonneuronal organisms. EPC parallels this at macro-cultural scale: pragmatic pushback = ecological variables. **Distinction:** Sims focuses on individual adaptation; EPC on intergenerational knowledge systems. Both emphasize context-sensitive, constraint-driven evolution; EPC adds synchronic diagnostics to Sims' diachronic methodology.

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 77 [IN-PROSE]:
Line: 687
Citation: Sims (2024)

Context:
**Kitcher (1993) on Evolutionary Progress:** Models science as credit-driven selection with division of labor across 'significant problems.' **Advance:** Negative Canon provides failure engine, brittleness quantifies problem significance via coercive costs (C(t)), diagnosing degenerating programs without reducing to cynical credit-seeking.

**Sims (2024) on Dynamic Holism:** Ecological constraints drive diachronic cognitive revision in nonneuronal organisms. EPC parallels this at macro-cultural scale: pragmatic pushback = ecological variables. **Distinction:** Sims focuses on individual adaptation; EPC on intergenerational knowledge systems. Both emphasize context-sensitive, constraint-driven evolution; EPC adds synchronic diagnostics to Sims' diachronic methodology.

These precursors provide micro-coherence (Thagard), meso-topology (Zollman), normative criteria (Rescher), macro-dynamics (Kitcher), and biological analogy (Sims). EPC unifies them through falsifiable brittleness assessment grounded in historical failure data.

Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91(2): 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 78 [PARENTHETICAL]:
Line: 695
Citation: (Staffel 2020)

Context:
As a macro-epistemology explaining the long-term viability of public knowledge systems, this framework does not primarily solve micro-epistemological problems like Gettier cases. Instead, it bridges the two levels through the concept of higher-order evidence: the diagnosed health of a public system provides a powerful defeater or corroborator for an individual's beliefs derived from that system.

The diagnosed brittleness of a knowledge system provides higher-order evidence that determines rational priors. Following Kelly (2005) on disagreement, when an agent receives a claim, they must condition their belief not only on the first-order evidence but also on the source's reliability (Staffel 2020). Let S be a high-brittleness network, like a denialist documentary. Its diagnosed non-viability acts as a powerful higher-order defeater. Therefore, even if S presents seemingly compelling first-order evidence E, a rational agent's posterior confidence in the claim properly remains low. Conversely, a low-brittleness network like the IPCC earns a high prior through demonstrated resilience. To doubt its claims without new evidence of rising brittleness is to doubt the entire adaptive project of science itself. This provides a rational, non-deferential basis for trust: justification flows from systemic health, grounding micro-level belief in macro-level viability.

### 7.1 From Hindsight to Foresight: Calibrating the Diagnostics

Reference:
Staffel, Julia. 2020. "Reasons Fundamentalism and Rational Uncertainty – Comments on Lord, The Importance of Being Rational." *Philosophy and Phenomenological Research* 100, no. 2: 463–468. https://doi.org/10.1111/phpr.12675. ISSN 0031-8205.
------------------------------------------------------------

Citation 79 [IN-PROSE]:
Line: 695
Citation: Kelly (2005)

Context:
As a macro-epistemology explaining the long-term viability of public knowledge systems, this framework does not primarily solve micro-epistemological problems like Gettier cases. Instead, it bridges the two levels through the concept of higher-order evidence: the diagnosed health of a public system provides a powerful defeater or corroborator for an individual's beliefs derived from that system.

The diagnosed brittleness of a knowledge system provides higher-order evidence that determines rational priors. Following Kelly (2005) on disagreement, when an agent receives a claim, they must condition their belief not only on the first-order evidence but also on the source's reliability (Staffel 2020). Let S be a high-brittleness network, like a denialist documentary. Its diagnosed non-viability acts as a powerful higher-order defeater. Therefore, even if S presents seemingly compelling first-order evidence E, a rational agent's posterior confidence in the claim properly remains low. Conversely, a low-brittleness network like the IPCC earns a high prior through demonstrated resilience. To doubt its claims without new evidence of rising brittleness is to doubt the entire adaptive project of science itself. This provides a rational, non-deferential basis for trust: justification flows from systemic health, grounding micro-level belief in macro-level viability.

### 7.1 From Hindsight to Foresight: Calibrating the Diagnostics

Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 80 [PARENTHETICAL]:
Line: 703
Citation: (Hodge 1992)

Context:
### 7.2 A Falsifiable Research Program

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.² The specific metrics and dynamic equations underlying this research program are detailed in the Mathematical Appendix.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Roda et al. 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

Reference:
Hodge, A. Trevor. 1992. *Roman Aqueducts & Water Supply*. London: Duckworth. ISBN 978-0715631713.
------------------------------------------------------------

Citation 81 [PARENTHETICAL]:
Line: 703
Citation: (Turchin 2003)

Context:
### 7.2 A Falsifiable Research Program

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.² The specific metrics and dynamic equations underlying this research program are detailed in the Mathematical Appendix.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Roda et al. 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press. ISBN 978-0691116693.
------------------------------------------------------------

Citation 82 [PARENTHETICAL]:
Line: 771
Citation: (Turchin 2003)

Context:
**Why We Accept This:** Intellectual honesty. The framework is incomplete—it maps pragmatic viability, not all moral dimensions. If such a system existed (we doubt it does—internalization itself has costs), it would fall in Pluralist Frontier, not Negative Canon.

**Empirical Bet:** We predict that such systems are not merely repugnant but are, in fact, inherently brittle. Critics might point to the apparent long-term stability of systems like the Ottoman Empire's devşirme system or historical caste systems in India as counterexamples. However, our framework predicts, and historical analysis confirms, that such systems exhibit high underlying brittleness masked by coercive power. Their apparent stability was purchased at the cost of massive **Coercive Overheads (C(t))**—such as the resources spent on enforcing purity laws, suppressing revolts, and managing internal dissent—and they consistently demonstrated innovation lags and fragility in the face of external shocks, confirming their position on the landscape of non-viability (cf. Acemoglu & Robinson 2012; Turchin 2003). True internalization without coercion is rare and resource-intensive, while oppression reliably generates hidden costs that emerge under stress.

**But:** If empirics proved otherwise, we would acknowledge the framework's incompleteness rather than deny evidence.

Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press. ISBN 978-0691116693.
------------------------------------------------------------

Citation 83 [PARENTHETICAL]:
Line: 825
Citation: (Godfrey-Smith 2003)

Context:
As a practical tool for epistemic risk management, Emergent Pragmatic Coherentism provides a structured methodology for policymakers, institutional designers, and the public to diagnose the health of our most critical knowledge-producing systems—from climate models to economic paradigms to public health infrastructure—before their hidden brittleness leads to catastrophic failure. The brittleness toolkit enables real-time assessment of structural health, allowing us to identify degenerating research programs and power-masked fragility before crises force recognition. This positions the work not just as a solution to an old philosophical puzzle, but as a practical and urgent research program for the 21st century, empowering democratic publics to hold their knowledge-generating systems accountable.

EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

Reference:
Godfrey-Smith, Peter. 2003. *Theory and Reality: An Introduction to the Philosophy of Science*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 84 [POSSESSIVE]:
Line: 825
Citation: Boyd's (1973)

Context:
As a practical tool for epistemic risk management, Emergent Pragmatic Coherentism provides a structured methodology for policymakers, institutional designers, and the public to diagnose the health of our most critical knowledge-producing systems—from climate models to economic paradigms to public health infrastructure—before their hidden brittleness leads to catastrophic failure. The brittleness toolkit enables real-time assessment of structural health, allowing us to identify degenerating research programs and power-masked fragility before crises force recognition. This positions the work not just as a solution to an old philosophical puzzle, but as a practical and urgent research program for the 21st century, empowering democratic publics to hold their knowledge-generating systems accountable.

EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

Reference:
Boyd, Richard. 1973. "Realism, Underdetermination, and a Causal Theory of Evidence." *Noûs* 7(1): 1–12. https://doi.org/10.2307/2214367.
------------------------------------------------------------

Citation 85 [POSSESSIVE]:
Line: 825
Citation: Kitcher's (1993)

Context:
As a practical tool for epistemic risk management, Emergent Pragmatic Coherentism provides a structured methodology for policymakers, institutional designers, and the public to diagnose the health of our most critical knowledge-producing systems—from climate models to economic paradigms to public health infrastructure—before their hidden brittleness leads to catastrophic failure. The brittleness toolkit enables real-time assessment of structural health, allowing us to identify degenerating research programs and power-masked fragility before crises force recognition. This positions the work not just as a solution to an old philosophical puzzle, but as a practical and urgent research program for the 21st century, empowering democratic publics to hold their knowledge-generating systems accountable.

EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 86 [POSSESSIVE]:
Line: 825
Citation: Ladyman and Ross's (2007)

Context:
As a practical tool for epistemic risk management, Emergent Pragmatic Coherentism provides a structured methodology for policymakers, institutional designers, and the public to diagnose the health of our most critical knowledge-producing systems—from climate models to economic paradigms to public health infrastructure—before their hidden brittleness leads to catastrophic failure. The brittleness toolkit enables real-time assessment of structural health, allowing us to identify degenerating research programs and power-masked fragility before crises force recognition. This positions the work not just as a solution to an old philosophical puzzle, but as a practical and urgent research program for the 21st century, empowering democratic publics to hold their knowledge-generating systems accountable.

EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press. ISBN 978-0199276196.
------------------------------------------------------------

Citation 87 [PARENTHETICAL]:
Line: 827
Citation: (Henrich 2015)

Context:
EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

The approach calls for epistemic humility, trading the ambition of a God's-eye view for the practical wisdom of a mariner. The payoff is not a final map of truth, but a continuously improving reef chart—a chart built from the architecture of failure, allowing us to more safely navigate the channels of viable knowledge.

Reference:
Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press. ISBN 978-0691178431.
------------------------------------------------------------

Citation 88 [PARENTHETICAL]:
Line: 827
Citation: (Strevens 2020)

Context:
EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

The approach calls for epistemic humility, trading the ambition of a God's-eye view for the practical wisdom of a mariner. The payoff is not a final map of truth, but a continuously improving reef chart—a chart built from the architecture of failure, allowing us to more safely navigate the channels of viable knowledge.

Reference:
Strevens, Michael. 2020. *The Knowledge Machine: How Irrationality Created Modern Science*. New York: Liveright Publishing. ISBN 1631491377, 9781631491375.
------------------------------------------------------------

Citation 89 [PARENTHETICAL]:
Line: 837
Citation: (Ingthorsson 2013)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
Ingthorsson, Rögnvaldur D. 2013. "Properties: Qualities, Powers, or Both?" *Dialectica* 67, no. 1: 55–80. https://doi.org/10.1111/1746-8361.12011.
------------------------------------------------------------

Citation 90 [PARENTHETICAL]:
Line: 837
Citation: (El-Hani and Pihlström 2002)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
El-Hani, Charbel Niño, and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3(2): article 3. https://doi.org/10.5840/eip2002325.
------------------------------------------------------------

Citation 91 [PARENTHETICAL]:
Line: 837
Citation: (Rottschaefer 2012)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
Rottschaefer, William A. 2012. "The Moral Realism of Pragmatic Naturalism." *Analyse & Kritik* 34(1): 141–156. https://doi.org/10.1515/auk-2012-0110.
------------------------------------------------------------

Citation 92 [POSSESSIVE]:
Line: 837
Citation: Baysan's (2025)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110(1): 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 93 [IN-PROSE]:
Line: 837
Citation: Bennett-Hunter (2015)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
Bennett-Hunter, Guy. 2015. *Ineffability and Religious Experience*. London: Routledge (originally Pickering & Chatto). ISBN 978-1848934719.
------------------------------------------------------------

Citation 94 [IN-PROSE]:
Line: 837
Citation: Peter (2024)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37(7): 1948–70. https://doi.org/10.1080/09515089.2023.2236120.
------------------------------------------------------------


################################################################################
FILE: gemini-email-convo.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 506
Citation: (as of 2024)

Context:
### Approaching Your 15-Minute Meeting with Brown's Philosophy Chair (David Christensen)

Congrats on landing this—it's a huge opportunity for a "regular person" with an original theory! David Christensen (current chair, as of 2024) is an epistemologist specializing in **rational disagreement, peer disagreement, and Bayesian epistemology**—perfect fit for EPC's social/collective apex and pushback dynamics. He's approachable, fair-minded, and has written on coherence (e.g., in *Putting Logic in Its Place*). Read his "Epistemology of Disagreement" paper beforehand for rapport.

**Overall Strategy**: Treat this as a **consultation, not a sales pitch**. Be humble, curious, and concise—aim for 60% listening, 40% talking. Goal: Get feedback, advice on next steps (e.g., publishing, formalizing), and maybe a referral. Don't overwhelm with details; hook with the "big idea" (apex as unknowable pi + decision engine).

Reference: NOT FOUND for 'as of 2024'
------------------------------------------------------------

Citation 2 [PARENTHETICAL]:
Line: 572
Citation: (" 2023)

Context:
| **"Higher-Order Evidence"** (Paper) | 2009 | Evidence that your reasoning is unreliable (e.g., drug impairs cognition) defeats first-order beliefs, even if first-order evidence supports them. | Defends "level-connecting" principles; ties to EPC's standing predicates (meta-evidence inhibits chains). |
| **Disagreement** (Co-edited with Lackey) | 2013 | Anthology shaping the field; Christensen defends **moderate conciliationism** (not total skepticism). | Social epistemology focus—EPC extends this to collective apex emergence. |
| **"Rational Reflection"** (Paper) | 2010 | We should proportion belief to expected reliability of our processes. | Bayesian but anti-dogmatic; pragmatic pushback in EPC echoes this. |
| **Recent Papers (e.g., "Formulating Independence," 2023)** | 2020s | Explores independence of evidence in disagreement; accuracy-dominated vs. rational deliberation. | Formal models; could integrate with EPC's network simulations for disagreement resolution. |

#### Overall Impact and Style
- **Influence**: Sparked the "disagreement industry" (debates with steadfast views like Kelly/Elga). Cited >5,000 times; bridges analytic epistemology with formal methods.

Reference: NOT FOUND for '" 2023'
------------------------------------------------------------

Citation 3 [IN-PROSE]:
Line: 649
Citation: Independence (2023)

Context:
- **Epistemic Akrasia**: Believing you should believe differently than you do. EPC Solves: Aligning with apex approximation resolves akrasia pragmatically.

From recent papers:
- **Formulating Independence (2023)**: Avoid "bootstrapping" in disagreement assessments.
- **The Ineliminability of Epistemic Rationality (2021)**: Rationality norms persist even under HOE.

#### Ways to Communicate/Relate Your Ideas

Reference: NOT FOUND for 'Independence 2023'
------------------------------------------------------------

Citation 4 [IN-PROSE]:
Line: 650
Citation: Rationality (2021)

Context:
From recent papers:
- **Formulating Independence (2023)**: Avoid "bootstrapping" in disagreement assessments.
- **The Ineliminability of Epistemic Rationality (2021)**: Rationality norms persist even under HOE.

#### Ways to Communicate/Relate Your Ideas
- **Bridge Phrases** (Drop these naturally):

Reference: NOT FOUND for 'Rationality 2021'
------------------------------------------------------------

Citation 5 [IN-PROSE]:
Line: 685
Citation: Self-Doubt and Non-Closure (2010s)

Context:
- Influenced by Brandom/Boghossian: Meaning from inferential role in "game of giving and asking for reasons."
   - **"The Reliability Challenge" (2009)**: How do we know our inferential dispositions are reliable? Defends moderate rationalism.

2. **Core Contributions: Rational Self-Doubt and Non-Closure (2010s)**
   - **"Rational Self-Doubt and the Failure of Closure" (2013, Philosophical Studies)**:
     - Extends Christensen's preface paradox: Rational agents can have self-doubt without violating closure.
     - Wide-scope requirements allow "rational akrasia" (knowing you're irrational but persisting).

Reference: NOT FOUND for 'Self-Doubt and Non-Closure 2010'
------------------------------------------------------------

Citation 6 [IN-PROSE]:
Line: 730
Citation: Methods (2008)

Context:
#### In-Depth Breakdown of Key Works
Here's a chronological expansion with core arguments and quotes:

1. **Early Collaboration: Justification of Basic Methods (2008)**
   - **"How Are Basic Belief-Forming Methods Justified?" (w/ Enoch, Philosophy and Phenomenological Research)**:
     - Addresses Enoch's "basic methods" (e.g., perception, deduction): Justified if indispensable for inquiry—pragmatic indispensability argument.
     - Quote: "We cannot engage in inquiry without relying on these methods... their justification is a kind of pragmatic vindication."

Reference: NOT FOUND for 'Methods 2008'
------------------------------------------------------------

Citation 7 [IN-PROSE]:
Line: 736
Citation: Failures (2013)

Context:
- Quote: "We cannot engage in inquiry without relying on these methods... their justification is a kind of pragmatic vindication."
   - **"The Reliability of Epistemic Intuitions" (w/ Weinberg, 2008)**: Experimental philosophy angle—intuitions vary, so not reliable for a priori justification.

2. **Rational Self-Doubt and Closure Failures (2013)**
   - **"Rational Self-Doubt and the Failure of Closure" (Philosophical Studies)**:
     - Builds on Christensen: Rational agents can doubt their reliability without suspending all beliefs (wide-scope norms allow "detaching" doubt).
     - Introduces **epistemic akrasia**: Rationally believing you're irrational in some respect.

Reference: NOT FOUND for 'Failures 2013'
------------------------------------------------------------

Citation 8 [PARENTHETICAL]:
Line: 785
Citation: (" 2011)

Context:
Here's a curated list of his signature ideas/phrases, pulled from papers (all on his Brown page):

- **"Space of Reasons"** (Sellarsian core, throughout): Beliefs justified by their position in inferential networks, not causal/sensory "space of causes."
- **"Game of Giving and Asking for Reasons"** (Brandomian, in "Inferentialism and the Normativity of Meaning," 2011): Meaning/concepts from social-inferential practices—asserting commits you to defenses.
- **"Wide-Scope Rational Requirements"** (central in "Rational Self-Doubt," 2013; "Could Beliefs Be Wide-Scope Attitudes?" 2022): Norms like "If you believe P and believe (P→Q), then [believe Q or revise]"—conditional/holistic, not detachable commands.
- **"Rational Akrasia"** (2013): Rationally believing you're irrational (e.g., knowing your inference is flawed but persisting due to wide-scope).
- **"Small Steps vs. Great Leaps in Thought"** (2019 Dennett paper): Skepticism defeated incrementally via local inferences from accepted premises—no "Moorean great leaps" (dogmatic "I know I have hands").

Reference: NOT FOUND for '" 2011'
------------------------------------------------------------

Citation 9 [PARENTHETICAL]:
Line: 786
Citation: (" 2013)

Context:
- **"Space of Reasons"** (Sellarsian core, throughout): Beliefs justified by their position in inferential networks, not causal/sensory "space of causes."
- **"Game of Giving and Asking for Reasons"** (Brandomian, in "Inferentialism and the Normativity of Meaning," 2011): Meaning/concepts from social-inferential practices—asserting commits you to defenses.
- **"Wide-Scope Rational Requirements"** (central in "Rational Self-Doubt," 2013; "Could Beliefs Be Wide-Scope Attitudes?" 2022): Norms like "If you believe P and believe (P→Q), then [believe Q or revise]"—conditional/holistic, not detachable commands.
- **"Rational Akrasia"** (2013): Rationally believing you're irrational (e.g., knowing your inference is flawed but persisting due to wide-scope).
- **"Small Steps vs. Great Leaps in Thought"** (2019 Dennett paper): Skepticism defeated incrementally via local inferences from accepted premises—no "Moorean great leaps" (dogmatic "I know I have hands").
- **"Indispensability Arguments"** (w/ Enoch, 2008; 2015): Basic methods (perception, deduction) justified pragmatically—they're indispensable for any inquiry.

Reference: NOT FOUND for '" 2013'
------------------------------------------------------------

Citation 10 [PARENTHETICAL]:
Line: 845
Citation: (" 2011)

Context:
Here's a curated list of his signature ideas/phrases, pulled from papers (all on his Brown page), with brief explanations:

- **"Space of Reasons"** (Sellarsian core, throughout): Justification in terms of inferential connections, not mere causation.
- **"Game of Giving and Asking for Reasons"** (Brandomian, in "Inferentialism and the Normativity of Meaning," 2011): Meaning/concepts from social practices of asserting and challenging beliefs.
- **"Wide-Scope Rational Requirements"** (central in "Rational Self-Doubt," 2013; "Could Beliefs Be Wide-Scope Attitudes?" 2022): Norms that are holistic and conditional (e.g., "If you believe P and believe (P→Q), then [believe Q or give up one premise]"), not narrow commands like "Believe Q!"
- **"Rational Akrasia"** (2013): A state where you rationally recognize your own irrationality in some belief but don't fully revise (like knowing you're biased but sticking anyway).
- **"Small Steps vs. Great Leaps in Thought"** (2019 Dennett paper): Defeating skepticism through gradual, local inferences ("small steps") rather than bold, non-inferential assertions ("great leaps" like "I know the external world exists because I have hands").

Reference: NOT FOUND for '" 2011'
------------------------------------------------------------

Citation 11 [PARENTHETICAL]:
Line: 846
Citation: (" 2013)

Context:
- **"Space of Reasons"** (Sellarsian core, throughout): Justification in terms of inferential connections, not mere causation.
- **"Game of Giving and Asking for Reasons"** (Brandomian, in "Inferentialism and the Normativity of Meaning," 2011): Meaning/concepts from social practices of asserting and challenging beliefs.
- **"Wide-Scope Rational Requirements"** (central in "Rational Self-Doubt," 2013; "Could Beliefs Be Wide-Scope Attitudes?" 2022): Norms that are holistic and conditional (e.g., "If you believe P and believe (P→Q), then [believe Q or give up one premise]"), not narrow commands like "Believe Q!"
- **"Rational Akrasia"** (2013): A state where you rationally recognize your own irrationality in some belief but don't fully revise (like knowing you're biased but sticking anyway).
- **"Small Steps vs. Great Leaps in Thought"** (2019 Dennett paper): Defeating skepticism through gradual, local inferences ("small steps") rather than bold, non-inferential assertions ("great leaps" like "I know the external world exists because I have hands").
- **"Indispensability Arguments"** (w/ Enoch, 2008; 2015): Pragmatic justification—if a method (like deduction) is essential for any inquiry, it's vindicated.

Reference: NOT FOUND for '" 2013'
------------------------------------------------------------

Citation 12 [IN-PROSE]:
Line: 1235
Citation: Theory (1973)

Context:
**Jargon/Context**: **Barbell strategy**; via negativa (subtract weak).  
  **EPC Tie**: Negative Canon = via negativa; Apex = antifragile plateau.

- **Holling Resilience Theory (1973)**  
  **Plain Language**: Ecosystems have adaptive cycles—brittle ones collapse under shocks.  
  **Jargon/Context**: **Panarchy**; resilience as absorption capacity.  
  **EPC Tie**: Brittleness = low resilience; tiers measure cycles.

Reference: NOT FOUND for 'Theory 1973'
------------------------------------------------------------

Citation 13 [IN-PROSE]:
Line: 1418
Citation: Reid et al. (2012)

Context:
Example 2: Slime Mold Farming Yeast
Physarum polycephalum leaves slime trails that can repel or attract depending on nutritional state.

Reid et al. (2012): Showed slime molds use trails as external memory.

Epstein et al. (2021): Showed slime molds may “farm” yeast by leaving nutritive slime, returning later for food.

Reference: NOT FOUND for 'Reid et al. 2012'
------------------------------------------------------------

Citation 14 [IN-PROSE]:
Line: 1420
Citation: Epstein et al. (2021)

Context:
Reid et al. (2012): Showed slime molds use trails as external memory.

Epstein et al. (2021): Showed slime molds may “farm” yeast by leaving nutritive slime, returning later for food.

Lesson: Only by considering ecological relationships (slime → yeast growth → slime mold feeding) do we see complex decision-making and sustainable strategies.

Reference: NOT FOUND for 'Epstein et al. 2021'
------------------------------------------------------------

Citation 15 [IN-PROSE]:
Line: 1592
Citation: Bala & Goyal (1998)

Context:
---

## 🧪 The Model
Adapted from Bala & Goyal (1998):

- **Agents** = scientists choosing between two research options:
  - **A1**: Old, well-understood method (uninformative, same payoff in all states).

Reference: NOT FOUND for 'Bala & Goyal 1998'
------------------------------------------------------------

Citation 16 [IN-PROSE]:
Line: 1679
Citation: Prize (1984)

Context:
### Nicholas Rescher: Biography and Philosophical Overview

Nicholas Rescher (1928–2024) was a prolific German-born American philosopher, polymath, and academic widely regarded as one of the most published philosophers of the 20th century, authoring over 100 books and 400 articles across diverse fields including logic, epistemology, philosophy of science, metaphysics, process philosophy, ethics, and value theory. Born on July 15, 1928, in Hagen, Westphalia, Germany, Rescher emigrated to the United States in 1938 as a Jewish refugee from Nazi persecution at the age of 10. He demonstrated extraordinary precocity, earning his PhD in philosophy from Princeton University in 1951 at age 22—the youngest ever in that department. Rescher spent much of his career at the University of Pittsburgh, where he served as University Professor of Philosophy, rising to become a Distinguished University Professor of Philosophy. He held prestigious roles, including president of the American Philosophical Association (Eastern Division), the American Catholic Philosophical Association, the C.S. Peirce Society, the American G.W. Leibniz Society, and the American Metaphysical Society. His honors included the Alexander von Humboldt Prize (1984), the Cardinal Mercier Prize (2005), and the Aquinas Medal from the American Catholic Philosophical Society (2007), along with seven honorary degrees from universities across three continents. Rescher was a visiting lecturer at institutions like Oxford, Munich, and Salamanca, and served on editorial boards such as *Process Studies*. He passed away on January 5, 2024, at age 95, leaving a legacy as a "gentle giant" of philosophy, known for his rigorous, integrative approach that bridged analytic precision with speculative depth.

Rescher's philosophy is characterized by a commitment to **systematicity**—the idea that philosophy must strive for comprehensive, coherent integration of knowledge rather than isolated analyses. He developed a "system of pragmatic idealism," blending Kantian idealism (emphasizing the mind's constitutive role in shaping knowledge) with American pragmatism (validating ideas through practical success). This framework rejects both naive realism and radical relativism, advocating for a fallibilist, pluralistic approach where diverse philosophical systems coexist under rational constraints imposed by reality. Key texts include *Methodological Pragmatism* (1977), *The Strife of Systems* (1985), *A System of Pragmatic Idealism* (three volumes, 1991–1994), and *Realistic Pragmatism* (1999). Rescher argued that systematicity is not optional but essential: philosophy, like science, progresses through holistic organization, where "rational conjecture based on systematic considerations" yields the best approximations to truth. He distinguished "pragmatism of the right" (objective, security-oriented, as in Peirce and himself) from relativistic variants (e.g., Rorty), emphasizing that knowledge evolves adaptively, constrained by objective reality yet enriched by human conceptual creativity. In recognition of this emphasis, the University of Pittsburgh established the **Nicholas Rescher Prize for Systematic Philosophy** in 2009, awarded annually to scholars advancing comprehensive philosophical systems over fragmented specialization.

Reference: NOT FOUND for 'Prize 1984'
------------------------------------------------------------

Citation 17 [IN-PROSE]:
Line: 1679
Citation: Prize (2005)

Context:
### Nicholas Rescher: Biography and Philosophical Overview

Nicholas Rescher (1928–2024) was a prolific German-born American philosopher, polymath, and academic widely regarded as one of the most published philosophers of the 20th century, authoring over 100 books and 400 articles across diverse fields including logic, epistemology, philosophy of science, metaphysics, process philosophy, ethics, and value theory. Born on July 15, 1928, in Hagen, Westphalia, Germany, Rescher emigrated to the United States in 1938 as a Jewish refugee from Nazi persecution at the age of 10. He demonstrated extraordinary precocity, earning his PhD in philosophy from Princeton University in 1951 at age 22—the youngest ever in that department. Rescher spent much of his career at the University of Pittsburgh, where he served as University Professor of Philosophy, rising to become a Distinguished University Professor of Philosophy. He held prestigious roles, including president of the American Philosophical Association (Eastern Division), the American Catholic Philosophical Association, the C.S. Peirce Society, the American G.W. Leibniz Society, and the American Metaphysical Society. His honors included the Alexander von Humboldt Prize (1984), the Cardinal Mercier Prize (2005), and the Aquinas Medal from the American Catholic Philosophical Society (2007), along with seven honorary degrees from universities across three continents. Rescher was a visiting lecturer at institutions like Oxford, Munich, and Salamanca, and served on editorial boards such as *Process Studies*. He passed away on January 5, 2024, at age 95, leaving a legacy as a "gentle giant" of philosophy, known for his rigorous, integrative approach that bridged analytic precision with speculative depth.

Rescher's philosophy is characterized by a commitment to **systematicity**—the idea that philosophy must strive for comprehensive, coherent integration of knowledge rather than isolated analyses. He developed a "system of pragmatic idealism," blending Kantian idealism (emphasizing the mind's constitutive role in shaping knowledge) with American pragmatism (validating ideas through practical success). This framework rejects both naive realism and radical relativism, advocating for a fallibilist, pluralistic approach where diverse philosophical systems coexist under rational constraints imposed by reality. Key texts include *Methodological Pragmatism* (1977), *The Strife of Systems* (1985), *A System of Pragmatic Idealism* (three volumes, 1991–1994), and *Realistic Pragmatism* (1999). Rescher argued that systematicity is not optional but essential: philosophy, like science, progresses through holistic organization, where "rational conjecture based on systematic considerations" yields the best approximations to truth. He distinguished "pragmatism of the right" (objective, security-oriented, as in Peirce and himself) from relativistic variants (e.g., Rorty), emphasizing that knowledge evolves adaptively, constrained by objective reality yet enriched by human conceptual creativity. In recognition of this emphasis, the University of Pittsburgh established the **Nicholas Rescher Prize for Systematic Philosophy** in 2009, awarded annually to scholars advancing comprehensive philosophical systems over fragmented specialization.

Reference: NOT FOUND for 'Prize 2005'
------------------------------------------------------------

Citation 18 [IN-PROSE]:
Line: 1679
Citation: Society (2007)

Context:
### Nicholas Rescher: Biography and Philosophical Overview

Nicholas Rescher (1928–2024) was a prolific German-born American philosopher, polymath, and academic widely regarded as one of the most published philosophers of the 20th century, authoring over 100 books and 400 articles across diverse fields including logic, epistemology, philosophy of science, metaphysics, process philosophy, ethics, and value theory. Born on July 15, 1928, in Hagen, Westphalia, Germany, Rescher emigrated to the United States in 1938 as a Jewish refugee from Nazi persecution at the age of 10. He demonstrated extraordinary precocity, earning his PhD in philosophy from Princeton University in 1951 at age 22—the youngest ever in that department. Rescher spent much of his career at the University of Pittsburgh, where he served as University Professor of Philosophy, rising to become a Distinguished University Professor of Philosophy. He held prestigious roles, including president of the American Philosophical Association (Eastern Division), the American Catholic Philosophical Association, the C.S. Peirce Society, the American G.W. Leibniz Society, and the American Metaphysical Society. His honors included the Alexander von Humboldt Prize (1984), the Cardinal Mercier Prize (2005), and the Aquinas Medal from the American Catholic Philosophical Society (2007), along with seven honorary degrees from universities across three continents. Rescher was a visiting lecturer at institutions like Oxford, Munich, and Salamanca, and served on editorial boards such as *Process Studies*. He passed away on January 5, 2024, at age 95, leaving a legacy as a "gentle giant" of philosophy, known for his rigorous, integrative approach that bridged analytic precision with speculative depth.

Rescher's philosophy is characterized by a commitment to **systematicity**—the idea that philosophy must strive for comprehensive, coherent integration of knowledge rather than isolated analyses. He developed a "system of pragmatic idealism," blending Kantian idealism (emphasizing the mind's constitutive role in shaping knowledge) with American pragmatism (validating ideas through practical success). This framework rejects both naive realism and radical relativism, advocating for a fallibilist, pluralistic approach where diverse philosophical systems coexist under rational constraints imposed by reality. Key texts include *Methodological Pragmatism* (1977), *The Strife of Systems* (1985), *A System of Pragmatic Idealism* (three volumes, 1991–1994), and *Realistic Pragmatism* (1999). Rescher argued that systematicity is not optional but essential: philosophy, like science, progresses through holistic organization, where "rational conjecture based on systematic considerations" yields the best approximations to truth. He distinguished "pragmatism of the right" (objective, security-oriented, as in Peirce and himself) from relativistic variants (e.g., Rorty), emphasizing that knowledge evolves adaptively, constrained by objective reality yet enriched by human conceptual creativity. In recognition of this emphasis, the University of Pittsburgh established the **Nicholas Rescher Prize for Systematic Philosophy** in 2009, awarded annually to scholars advancing comprehensive philosophical systems over fragmented specialization.

Reference: NOT FOUND for 'Society 2007'
------------------------------------------------------------

Citation 19 [IN-PROSE]:
Line: 1685
Citation: Knowledge (1977)

Context:
Rescher's systematicity extended to epistemology, where he introduced "epistemetrics"—a quantitative model treating knowledge as an optimized subset of information, governed by a "law of logarithmic returns" (knowledge grows linearly despite exponential information growth, prioritizing utility over completeness). He viewed inquiry as "erotetic propagation": answering one question spawns others, ensuring endless systematization. In process philosophy (*Process Metaphysics*, 1996), he integrated Whiteheadian ideas, portraying reality as dynamic and relational, with metaphysics providing the "most general systematization of civilized thought."

### The Systematization of Knowledge (1977)

Published in *Philosophical Context* (Volume 6, Supplement, pp. 20–42), Rescher's essay "The Systematization of Knowledge" is a foundational piece in his epistemological oeuvre, articulating the interconnected, holistic nature of knowledge organization. While the full text is not publicly available online (due to academic access restrictions), the essay's core arguments are reconstructed from Rescher's own later elaborations (e.g., in *Epistemic Merit* [1992] and *Historical Perspectives on the Systematization of Knowledge* [2011]), citations in secondary literature, and contextual discussions in his corpus. It marks an early expression of his pragmatic idealism, emphasizing knowledge not as a static accumulation but as a dynamic, imputative process where the mind actively structures reality for coherence and utility.

Reference: NOT FOUND for 'Knowledge 1977'
------------------------------------------------------------

Citation 20 [PARENTHETICAL]:
Line: 1744
Citation: (b. 1947)

Context:
### Philip Kitcher and the Philosophy of Evolution: A Detailed Summary

Philip Kitcher (b. 1947) is a leading British-American philosopher of science, renowned for his rigorous defense of Darwinian evolution against pseudoscientific challengers, his critiques of overambitious applications of evolutionary theory, and his integration of scientific realism with ethical and social considerations. As the John Dewey Professor Emeritus of Philosophy at Columbia University, Kitcher's work in the philosophy of biology—particularly on evolution—bridges analytic philosophy, history of science, and pragmatism. Influenced by mentors like Carl Hempel and Thomas Kuhn, he emphasizes empirical testability, explanatory power, and the progressive nature of science, while addressing how evolutionary ideas intersect with religion, ethics, and society. His contributions span over four decades, with key texts like *Abusing Science: The Case Against Creationism* (1982), *Vaulting Ambition: Sociobiology and the Quest for Human Nature* (1985), and *Living with Darwin: Evolution, Design, and the Future of Faith* (2007). These works collectively argue that Darwinian evolution, grounded in natural selection, provides a unifying, fecund framework for biology, but its misuse—whether in creationist apologetics or deterministic sociobiology—demands philosophical scrutiny. Kitcher's approach is pragmatic and naturalistic, advocating a realism where scientific theories approximate truth through unification and problem-solving, without dogmatic absolutes.

#### Core Philosophical Framework: Criteria for "Good Science" and Explanatory Unification
Kitcher's philosophy of evolution rests on three interlocking criteria for evaluating scientific theories, first articulated in *Abusing Science* and refined across his oeuvre: **independent testability** (auxiliary hypotheses must be empirically verifiable apart from the core claim), **unification** (a theory should apply a coherent strategy to diverse phenomena), and **fecundity** (it should generate new questions and research avenues, embracing incompleteness as a driver of progress). Darwinian evolution excels here: natural selection unifies biological diversity—from adaptation to speciation—via a single mechanistic strategy, testable through fossils, genetics, and comparative anatomy, and fecund in inspiring fields like evolutionary developmental biology (evo-devo). This contrasts with alternatives like creationism, which lack these virtues, relying on untestable supernatural posits that stifle inquiry.

Reference: NOT FOUND for 'b. 1947'
------------------------------------------------------------

Citation 21 [IN-PROSE]:
Line: 1861
Citation: Kitcher (1993)

Context:
**General Tips**:
- **Tone**: "Building on Thagard's 7 principles [1989]..." or "Unlike Zollman's abstract topologies [2007], EPC..."
- **Visuals**: Add 2-3 figures (ECHO network, Zollman topologies with brittleness annotations)—reviewers love.
- **Citations**: Full: Thagard (Philosophical Review 1989), Zollman (Philosophy of Science 2007), Rescher (multiple, esp. 1973/2001), Kitcher (1993).
- **Word Fit**: Cut logos repetition (Section 4.2.3) to add.

#### Section 1: Introduction/Lineage (Hook the Synthesis)

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------


################################################################################
FILE: grok_ref.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 80
Citation: (published 2002)

Context:
### 35. Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press.

### 36. Kornblith, Hilary. 1993. *Knowledge and Its Place in Nature*. Oxford: Clarendon Press (published 2002, but listed as 1993 in some sources; verified 2002).

### 37. Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press (originally 1962).

Reference: NOT FOUND for 'published 2002'
------------------------------------------------------------


################################################################################
FILE: mathy.md
################################################################################

Citation 1 [POSSESSIVE]:
Line: 643
Citation: Lakatos's (1970)

Context:
### The Inheritance

Imre Lakatos's (1970) distinction between "progressive" and "degenerating" research programs provides the most direct ancestor to our concept of systemic brittleness. Lakatos identified the pattern we formalize:

**Lakatos's Progressive Program:**

Reference:
Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific Research Programmes." In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 2 [IN-PROSE]:
Line: 732
Citation: James (1907)

Context:
### The Shared Foundation

EPC is pragmatist through and through. We inherit the core pragmatist insight: meaning and truth are constituted through their role in successful practice. As James (1907) wrote, truth is what "works"—but he left "works" philosophically vague.

**Dewey's Instrumentalism:**
John Dewey (1938) saw inquiry as problem-solving. Beliefs are instruments for navigating experience. Good beliefs solve problems; bad beliefs generate new ones.

Reference: NOT FOUND for 'James 1907'
------------------------------------------------------------

Citation 3 [IN-PROSE]:
Line: 735
Citation: Dewey (1938)

Context:
EPC is pragmatist through and through. We inherit the core pragmatist insight: meaning and truth are constituted through their role in successful practice. As James (1907) wrote, truth is what "works"—but he left "works" philosophically vague.

**Dewey's Instrumentalism:**
John Dewey (1938) saw inquiry as problem-solving. Beliefs are instruments for navigating experience. Good beliefs solve problems; bad beliefs generate new ones.

Our SBI framework operationalizes this:

Reference:
Dewey, John. 1938. *Logic: The Theory of Inquiry*. New York: Henry Holt and Company. ISBN 978-1406731804.
------------------------------------------------------------

Citation 4 [IN-PROSE]:
Line: 750
Citation: Peirce (1877)

Context:
We provide the constraint James needed: satisfaction must be sustainable. A belief system that "satisfies" only through massive coercion (high C(t)) or mounting hidden costs (high SBI, long L(t)) is pragmatically failing, even if individuals subjectively endorse it. The long-term test is systemic viability, not momentary satisfaction.

**Peirce's Convergence:**
Charles Sanders Peirce (1877) argued that truth is what inquiry would converge to "in the long run" under honest investigation. This is strikingly similar to our Apex Network concept.

However, Peirce offered no mechanism for *why* convergence occurs or *how* to identify it. He simply asserted that persistent inquiry tends toward truth. We provide:

Reference:
Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press (originally 1878).
------------------------------------------------------------

Citation 5 [IN-PROSE]:
Line: 768
Citation: Boyd (1990)

Context:
- **Quantitative dynamics:** The SDE modeling how systems evolve
- **Falsifiable predictions:** Rising SBI → higher collapse probability

As Boyd (1990) notes, this is the difference between Darwin's verbal theory and population genetics. The math doesn't replace the philosophy—it reveals implications invisible in prose and enables empirical testing.

**2. From Individual to Collective Epistemology**

Reference:
Boyd, Richard. 1988. "How to Be a Moral Realist." In *Essays on Moral Realism*, edited by Geoffrey Sayre-McCord, 181–228. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 6 [POSSESSIVE]:
Line: 981
Citation: Quine's (1951)

Context:
### Quine's Partial Solution

Quine's (1951) "Web of Belief" made coherence dynamic:

- Beliefs form a interconnected web
- "Recalcitrant experience" at the periphery creates pressure

Reference:
Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60(1): 20–43. https://doi.org/10.2307/2181906.
------------------------------------------------------------


################################################################################
FILE: other.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 15
Citation: (Olsson 2005)

Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.

Reference:
Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press. ISBN 978-0199279999.
------------------------------------------------------------

Citation 2 [PARENTHETICAL]:
Line: 15
Citation: (Kvanvig 2012)

Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.

Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50(1): 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 3 [PARENTHETICAL]:
Line: 15
Citation: (Carlson 2015)

Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 4 [PARENTHETICAL]:
Line: 15
Citation: (Holling 1973)

Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.

Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 5 [IN-PROSE]:
Line: 15
Citation: BonJour (1985)

Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.

Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press. ISBN 978-0674843813.
------------------------------------------------------------

Citation 6 [IN-PROSE]:
Line: 23
Citation: Taleb (2012)

Context:
This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology: a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not viable in these terms but a textbook case of high brittleness; its longevity measures the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system's capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.

However, any credible theory of knowledge evolution must account for the realities of power, path dependence, and historical contingency. This framework incorporates these factors not as exceptions but as key variables within the model. The exercise of power to maintain a brittle system is not a refutation of the model but a primary indicator of that system's non-viability, measured through its high coercive costs. This failure-driven, adaptive process grounds a robust but fallible realism. It explains how evolving knowledge systems are forced to converge on an emergent structure of workable solutions determined by mind-independent pragmatic constraints. The claim is probabilistic, not deterministic: beneath the surface noise of contingency and power, there are underlying structural pressures at work. A system accumulating brittleness—a system's vulnerability to collapse due to the accumulation of hidden, internal costs, a concept analogous to the notion of fragility developed by Taleb (2012)—is not fated to collapse on a specific date, but it becomes progressively more vulnerable to the very contingent shocks that historians study. This model does not offer a deterministic theory of history but rather proposes the foundation for a falsifiable, probabilistic research program for understanding the structural dynamics that shape the landscape upon which historical events unfold.

To prevent misunderstanding about the framework's scope and ambitions, we must be precise about what this paper does and does not attempt. This is not a foundationalist epistemology that aims to ground all knowledge in indubitable starting points, nor is it a general theory of justification applicable to all domains of inquiry. Rather, it is a specialized framework for understanding the evolution and evaluation of cumulative knowledge systems—those engaged in ongoing, inter-generational projects where claims build upon previous work and where practical consequences provide feedback about systemic performance. The framework applies most directly to domains like empirical science, legal systems, engineering, public policy, and mathematics, where pragmatic pushback—whether external through failed predictions or internal through proof complexity and conceptual debt—provides measurable feedback about systemic performance. We present this focus not as a limitation but as a demonstration of the framework's power to unify apparently disparate domains under a single selective mechanism.

Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House. ISBN 978-1400067824.
------------------------------------------------------------

Citation 7 [PARENTHETICAL]:
Line: 31
Citation: (Meadows 2008)

Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing. ISBN 978-1603580557.
------------------------------------------------------------

Citation 8 [PARENTHETICAL]:
Line: 35
Citation: (Goldman 1979)

Context:
### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move makes the analysis tractable by focusing on observable phenomena and addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

`\[FIGURE 1: Conceptual Flowchart for the Deflationary Path]`
`\[Belief (Private State)] --> \[Articulation into a Proposition (Public Claim)] --> \[Coherence Test] --> \[Integration as Validated Data OR Promotion to Standing Predicate (Public Tool)]`

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 9 [PARENTHETICAL]:
Line: 35
Citation: (Kitcher 1993)

Context:
### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move makes the analysis tractable by focusing on observable phenomena and addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

`\[FIGURE 1: Conceptual Flowchart for the Deflationary Path]`
`\[Belief (Private State)] --> \[Articulation into a Proposition (Public Claim)] --> \[Coherence Test] --> \[Integration as Validated Data OR Promotion to Standing Predicate (Public Tool)]`

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 10 [IN-PROSE]:
Line: 65
Citation: Mesoudi (2011)

Context:
* **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition (e.g., `...is an infectious disease`). Functioning as the generative "genes" of cultural evolution, a Standing Predicate is a highly compressed piece of conceptual technology. When applied, it unpacks a suite of previously validated knowledge, including causal models, diagnostic heuristics, and licensed interventions.
* **Shared Network:** This concept is not a novel theoretical entity but an observable consequence of Quine's holism applied to social groups. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested: a specialized network like germ theory forms a coherent subset of propositions within the broader network of modern medicine, which itself must align with the predicates of empirical science. The emergence of these networks is not a conscious negotiation but a structural necessity. An individual craftsperson whose canoe capsizes will holistically revise their personal web of belief about hydrodynamics; when a group must build a fleet, only the shared principles that lead to non-capsizing canoes can become part of the public, transmissible craft. The Shared Network is the public residue of countless such private, failure-driven revisions under shared pragmatic pressure.

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory, as synthesized for the social sciences by Mesoudi (2011). The network's abstract informational structure—its core Standing Predicates and their relations—functions as the **replicator**: the "code" that is copied and transmitted. The social group and its institutions (the scientific community, legal system, or society) function as the **interactor**: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This distinction explains how knowledge can persist even when the societies that created it do not; the rediscovery of Roman legal principles during the Renaissance is a prime example of a successful replicator outliving its original interactor. The existence of these countless, independently formed and often nested Shared Networks—all responding to the same landscape of physical and social constraints—supports a crucial inference: they collectively reveal an objective structure that underwrites all successful inquiry. This mosaic of survived predicates, biased toward low-brittleness solutions, anticipates the Apex Network: a constraint-determined structure of necessary optimal solutions that emerges from and is revealed by historical filtering (as detailed in Section 4).

### **2.2.1 Resolving the Circularity Objection: From Categories to Trajectories**

Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press. ISBN 978-0226520445.
------------------------------------------------------------

Citation 11 [PARENTHETICAL]:
Line: 222
Citation: (El-Hani and Pihlström 2002)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

Reference:
El-Hani, Charbel Niño, and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3(2): article 3. https://doi.org/10.5840/eip2002325.
------------------------------------------------------------

Citation 12 [PARENTHETICAL]:
Line: 222
Citation: (Rottschaefer 2012)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

Reference:
Rottschaefer, William A. 2012. "The Moral Realism of Pragmatic Naturalism." *Analyse & Kritik* 34(1): 141–156. https://doi.org/10.1515/auk-2012-0110.
------------------------------------------------------------

Citation 13 [POSSESSIVE]:
Line: 222
Citation: Baysan's (2025)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110(1): 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 14 [IN-PROSE]:
Line: 222
Citation: Bennett-Hunter (2015)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

Reference:
Bennett-Hunter, Guy. 2015. *Ineffability and Religious Experience*. London: Routledge (originally Pickering & Chatto). ISBN 978-1848934719.
------------------------------------------------------------

Citation 15 [IN-PROSE]:
Line: 222
Citation: Peter (2024)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

Reference:
Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37(7): 1948–70. https://doi.org/10.1080/09515089.2023.2236120.
------------------------------------------------------------

Citation 16 [PARENTHETICAL]:
Line: 232
Citation: (Kim 1988)

Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 17 [PARENTHETICAL]:
Line: 232
Citation: (Putnam 2002)

Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press. ISBN 978-0674013803.
------------------------------------------------------------

Citation 18 [PARENTHETICAL]:
Line: 232
Citation: (Lynch 2009)

Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

Reference:
Lynch, Michael P. 2009. *Truth as One and Many*. Oxford: Clarendon Press. ISBN 978-0199218738.
------------------------------------------------------------

Citation 19 [PARENTHETICAL]:
Line: 234
Citation: (Moghaddam 2013)

Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.

Reference: NOT FOUND for 'Moghaddam 2013'
------------------------------------------------------------

Citation 20 [POSSESSIVE]:
Line: 396
Citation: Haack's (1993)

Context:
The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge.

`[FIGURE 2: Conceptual Sketch of a Fitness Landscape]`
`[A 3D landscape with "Viability" on the Y-axis. Peaks are labeled with successful systems like "Germ Theory" and "Relativity." Deep valleys and low plateaus are labeled with failed systems from the Negative Canon, such as "Ptolemaic System," "Lysenkoism," and the "Ming Dynasty Trap."]`

Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell. ISBN 978-0631196792.
------------------------------------------------------------

Citation 21 [PARENTHETICAL]:
Line: 405
Citation: (Tauriainen 2017)

Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.

Reference:
Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä, Department of Social Sciences and Philosophy. https://urn.fi/URN:NBN:fi:jyu-201705312584.
------------------------------------------------------------

Citation 22 [PARENTHETICAL]:
Line: 443
Citation: (Simon 1972)

Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:

Reference: NOT FOUND for 'Simon 1972'
------------------------------------------------------------

Citation 23 [PARENTHETICAL]:
Line: 480
Citation: (Price 1992)

Context:
The historical process of pragmatic filtering gives our evolving Consensus Networks a discernible structure, which can be understood as having two distinct epistemic zones. This distinction is not about the nature of reality itself, but describes the justificatory status of our claims at a given time.

* **The Convergent Core:** This represents the load-bearing foundations of our current knowledge. It comprises domains where the relentless pressure of pragmatic selection has eliminated all known rival formulations, leaving a single, or functionally identical, set of low-brittleness principles. Principles reside in this core—such as the laws of thermodynamics or the germ theory of disease—not because they are dogmatically held or self-evident but because all tested alternatives have been relegated to the Negative Canon after generating catastrophically high systemic costs. While no claim is immune to revision in principle, the principles in the Convergent Core are functionally unrevisable in practice, as doing so would require dismantling the most successful and resilient knowledge structures we have ever built. A claim from this core achieves the highest degree of justification we can assign, approaching our standard for Objective Truth (Level 1).
* **The Pluralist Frontier:** This describes the domains of active research where our current evidence is insufficient to decide between multiple, competing, and viable reconstructions of the landscape of viability. Here, rival systems (e.g., different interpretations of quantum mechanics or competing models of consciousness) may coexist, each with a demonstrably low and stable degree of brittleness. It is crucial to distinguish this constrained, evidence-based pluralism from relativism. The frontier is not an "anything goes" zone but a highly restricted space strictly bounded on all sides by the Negative Canon. A system based on phlogiston is not a "viable contender" on the frontier of chemistry but a demonstrably failed research program. This pluralism is therefore a sign of epistemic underdetermination: a feature of our map's current limitations, not reality's supposed indifference. This position resonates with pragmatist accounts of functional pluralism (e.g., Price 1992), which treat different conceptual frameworks as tools whose legitimacy is determined by their utility within a specific practice. Within this frontier, the core claims of each viable competing system can be granted the status of Justified Truth (Level 2). This is also the zone where non-epistemic factors, such as institutional power or contingent path dependencies, can play their most significant role, sometimes artificially constraining the range of options explored or creating temporary monopolies on what is considered justified.

### **4.5 Illustrative Cases of Convergence and Brittleness**

Reference:
Price, Huw. 1992. "Metaphysical Pluralism." *Journal of Philosophy* 89(8): 387–409. https://doi.org/10.2307/2940741.
------------------------------------------------------------

Citation 24 [PARENTHETICAL]:
Line: 492
Citation: (Acemoglu and Robinson 2012)

Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

Reference:
Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business. ISBN 978-0307719225.
------------------------------------------------------------

Citation 25 [IN-PROSE]:
Line: 496
Citation: Turchin (2003)

Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, exemplifies this state. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability therefore requires a balance between low-cost stability and adaptive capacity.

Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press. ISBN 978-0691116693.
------------------------------------------------------------

Citation 26 [PARENTHETICAL]:
Line: 508
Citation: (Simon 1972)

Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**

Reference: NOT FOUND for 'Simon 1972'
------------------------------------------------------------

Citation 27 [PARENTHETICAL]:
Line: 512
Citation: (Carlson 2015)

Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 28 [PARENTHETICAL]:
Line: 520
Citation: (Kvanvig 2012)

Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50(1): 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 29 [PARENTHETICAL]:
Line: 520
Citation: (Carlson 2015)

Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 30 [IN-PROSE]:
Line: 520
Citation: BonJour (1985)

Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press. ISBN 978-0674843813.
------------------------------------------------------------

Citation 31 [IN-PROSE]:
Line: 520
Citation: Olsson (2005)

Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Reference:
Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press. ISBN 978-0199279999.
------------------------------------------------------------

Citation 32 [IN-PROSE]:
Line: 522
Citation: Bennett-Hunter (2015)

Context:
Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

This paper's framework, which we term *Systemic Externalism*, offers a unified externalist solution to this structural problem. It provides the evolutionary backstory for the web's functional structure, arguing that a principle becomes functionally indispensable, as described by Carlson (2015), precisely because it has survived a historical filtering process based on pragmatic consequences. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Reference:
Bennett-Hunter, Guy. 2015. *Ineffability and Religious Experience*. London: Routledge (originally Pickering & Chatto). ISBN 978-1848934719.
------------------------------------------------------------

Citation 33 [PARENTHETICAL]:
Line: 524
Citation: (Zollman 2013)

Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

This paper's framework, which we term *Systemic Externalism*, offers a unified externalist solution to this structural problem. It provides the evolutionary backstory for the web's functional structure, arguing that a principle becomes functionally indispensable, as described by Carlson (2015), precisely because it has survived a historical filtering process based on pragmatic consequences. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

### 6.2 Evolutionary Grounding for Social Epistemic Practices

Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in Epistemic Communities." *Philosophy Compass* 8(1): 15–27. https://doi.org/10.1111/j.1747-9991.2012.00534.x.
------------------------------------------------------------

Citation 34 [IN-PROSE]:
Line: 524
Citation: Carlson (2015)

Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

This paper's framework, which we term *Systemic Externalism*, offers a unified externalist solution to this structural problem. It provides the evolutionary backstory for the web's functional structure, arguing that a principle becomes functionally indispensable, as described by Carlson (2015), precisely because it has survived a historical filtering process based on pragmatic consequences. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

### 6.2 Evolutionary Grounding for Social Epistemic Practices

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 35 [IN-PROSE]:
Line: 528
Citation: Longino (2002)

Context:
### 6.2 Evolutionary Grounding for Social Epistemic Practices

The framework provides a naturalistic foundation for core insights in social epistemology while addressing a persistent problem in that field. Longino (2002) and others have shown that objectivity often emerges from well-structured epistemic communities rather than isolated individuals. Critical discourse norms, peer review, and viewpoint diversity are presented as procedural guarantees of objectivity.

However, purely procedural accounts face what might be called the "parochialism problem": how do we know that a perfectly managed consensus is not simply a stable, shared delusion? If objectivity is secured by following local rules of discourse, what grounds do we have for evaluating those rules themselves?

Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press. ISBN 978-0691088761.
------------------------------------------------------------

Citation 36 [PARENTHETICAL]:
Line: 534
Citation: (Harding 1991)

Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness

Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press. ISBN 978-0801497469.
------------------------------------------------------------

Citation 37 [IN-PROSE]:
Line: 534
Citation: Sims (2024)

Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness

Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91(2): 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 38 [POSSESSIVE]:
Line: 546
Citation: Lakatos's (1970)

Context:
The framework also addresses evolutionary epistemology's difficulty with the directed nature of human inquiry. Unlike blind Darwinian selection, knowledge systems exhibit Lamarckian-style inheritance of acquired insights through the functional entrenchment of successful solutions in network cores.

It is useful to distinguish our concept of systemic brittleness from related ideas in the philosophy of science, such as Lakatos's (1970) 'degenerative research programmes' and Laudan's (1977) 'problem-solving effectiveness'. While Lakatos provides a brilliant historical description of a degenerating programme—one that relies on ad-hoc hypotheses and fails to make novel predictions—our framework aims to provide the underlying causal engine for this degeneration. Brittleness is a measure of the accumulated systemic costs that cause a programme to become degenerative. It is a diagnostic of a system's structural health, not just a historical description of its output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of risk and resilience. A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

Reference:
Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific Research Programmes." In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 39 [POSSESSIVE]:
Line: 546
Citation: Laudan's (1977)

Context:
The framework also addresses evolutionary epistemology's difficulty with the directed nature of human inquiry. Unlike blind Darwinian selection, knowledge systems exhibit Lamarckian-style inheritance of acquired insights through the functional entrenchment of successful solutions in network cores.

It is useful to distinguish our concept of systemic brittleness from related ideas in the philosophy of science, such as Lakatos's (1970) 'degenerative research programmes' and Laudan's (1977) 'problem-solving effectiveness'. While Lakatos provides a brilliant historical description of a degenerating programme—one that relies on ad-hoc hypotheses and fails to make novel predictions—our framework aims to provide the underlying causal engine for this degeneration. Brittleness is a measure of the accumulated systemic costs that cause a programme to become degenerative. It is a diagnostic of a system's structural health, not just a historical description of its output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of risk and resilience. A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

Reference:
Laudan, Larry. 1977. *Progress and Its Problems: Towards a Theory of Scientific Growth*. Berkeley: University of California Press. ISBN 978-0520037212.
------------------------------------------------------------

Citation 40 [IN-PROSE]:
Line: 585
Citation: Paradox (1901)

Context:
- R(t): Exceptional—unified logic, number theory, analysis
- Apparent low brittleness

**Russell's Paradox (1901):**
- Revealed infinite brittleness: the theory could derive contradiction
- All inference paralyzed (if A and ¬A both derivable, anything follows)
- Complete systemic collapse

Reference: NOT FOUND for 'Paradox 1901'
------------------------------------------------------------

Citation 41 [PARENTHETICAL]:
Line: 624
Citation: (Simon 1972)

Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure

Reference: NOT FOUND for 'Simon 1972'
------------------------------------------------------------

Citation 42 [PARENTHETICAL]:
Line: 652
Citation: (Harding 1991)

Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches

Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press. ISBN 978-0801497469.
------------------------------------------------------------

Citation 43 [PARENTHETICAL]:
Line: 688
Citation: (Baggio and Parravicini 2019)

Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Baggio, Guido, and Andrea Parravicini. 2019. "Introduction to Pragmatism and Theories of Emergence." *European Journal of Pragmatism and American Philosophy* XI-2. https://doi.org/10.4000/ejpap.1611.
------------------------------------------------------------

Citation 44 [IN-PROSE]:
Line: 688
Citation: Rorty (1979)

Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press. ISBN 978-0691020167.
------------------------------------------------------------

Citation 45 [IN-PROSE]:
Line: 688
Citation: Brandom (1994)

Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Brandom, Robert B. 1994. *Making It Explicit: Reasoning, Representing, and Discursive Commitment*. Cambridge, MA: Harvard University Press. ISBN 978-0674543195.
------------------------------------------------------------

Citation 46 [IN-PROSE]:
Line: 690
Citation: El-Hani and Pihlström (2002)

Context:
The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Reference:
El-Hani, Charbel Niño, and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3(2): article 3. https://doi.org/10.5840/eip2002325.
------------------------------------------------------------

Citation 47 [PARENTHETICAL]:
Line: 696
Citation: (Worrall 1989)

Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.

Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43(1–2): 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 48 [IN-PROSE]:
Line: 696
Citation: Ladyman and Ross (2007)

Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press. ISBN 978-0199276196.
------------------------------------------------------------

Citation 49 [IN-PROSE]:
Line: 709
Citation: Kelly (2005)

Context:
This framework has implications for several contemporary discussions in epistemology:

**Disagreement**: Following Kelly (2005), the diagnosed brittleness of knowledge systems provides powerful higher-order evidence that should influence how agents respond to disagreement. Claims from low-brittleness networks warrant higher confidence than those from demonstrably brittle sources.

**Testimony**: The framework suggests that testimonial justification depends not only on speaker reliability but on the systemic reliability of the knowledge traditions speakers represent. This provides resources for evaluating competing testimonial sources in an information-rich but epistemically fragmented environment.

Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 50 [IN-PROSE]:
Line: 737
Citation: Kuhn (1962)

Context:
### **7.1 The Problem of Internal Coherence: Fictions, Paradigms, and the Limits of Isolation**

The most potent challenge to any coherentist model is the "isolation objection"—the possibility of a perfectly self-consistent but factually detached system. This manifests in sophisticated conspiracy theories and incommensurable scientific paradigms famously articulated by Thomas Kuhn (1962). Our model addresses this by introducing an externalist standard based on pragmatic performance, though significant methodological challenges remain.

"Coherent fictions" like conspiracy theories typically exhibit structural features: accelerating ad-hoc modifications to protect core tenets, high maintenance costs through suppression of dissent, and epistemic parasitism—generating no novel research but rationalizing away mainstream successes. Whether these constitute decisive refutation depends on objective measurement, which proves difficult in practice.

Reference:
Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press (originally 1962). ISBN 978-0226458083.
------------------------------------------------------------

Citation 51 [IN-PROSE]:
Line: 759
Citation: Kelly (2005)

Context:
It is crucial to be precise about this model's scope. It is a macro-epistemology, designed to explain the long-term viability of public knowledge systems. It does not primarily aim to solve traditional problems in micro-epistemology, such as Gettier cases or the justification of an individual's perceptual beliefs. Instead, it provides a robust bridge between these levels through the concept of higher-order evidence. The diagnosed health of a public system provides a powerful defeater (or corroborator) for an individual’s beliefs derived from that system.

To formalize this intuition, we can use a Bayesian framework. The diagnosed brittleness of a knowledge system should determine the rational prior probability an agent assigns to any claim from that source. A low-brittleness network (e.g., an IPCC report) warrants a high prior; a high-brittleness network (a denialist documentary) warrants a low one. As Thomas Kelly (2005) argued regarding disagreement, the properties of the source matter. When an agent receives new first-order evidence, E, their posterior confidence is updated via Bayes' rule. This formalizes why an agent should rationally favor the IPCC's claim: even if the denialist source presents a seemingly powerful piece of evidence, the extremely low prior assigned to that network means the agent's posterior confidence in its claims will remain low. The macro-level diagnosis thus provides a rational, quantitative basis for allocating trust.

### **7.4 Defending the Model's Grounding**

Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 52 [PARENTHETICAL]:
Line: 985
Citation: (Mesoudi 2011)

Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**

Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press. ISBN 978-0226520445.
------------------------------------------------------------


################################################################################
FILE: paper.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 10
Citation: (Snow 1855)

Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. 2nd ed. London: John Churchill. Reprinted in *International Journal of Epidemiology* 42, no. 6 (2013): 1543–1552. https://doi.org/10.1093/ije/dyt193.
------------------------------------------------------------

Citation 2 [PARENTHETICAL]:
Line: 12
Citation: (BonJour 1985)

Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.

Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press. ISBN 978-0674843813.
------------------------------------------------------------

Citation 3 [PARENTHETICAL]:
Line: 12
Citation: (Olsson 2005)

Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.

Reference:
Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press. ISBN 978-0199279999.
------------------------------------------------------------

Citation 4 [PARENTHETICAL]:
Line: 12
Citation: (Kvanvig 2012)

Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.

Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50(1): 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 5 [PARENTHETICAL]:
Line: 12
Citation: (Quine 1960)

Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.

Reference:
Quine, W. V. O. 1960. *Word and Object*. Cambridge, MA: MIT Press. ISBN 978-0262670012.
------------------------------------------------------------

Citation 6 [PARENTHETICAL]:
Line: 12
Citation: (Kitcher 1993)

Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 7 [PARENTHETICAL]:
Line: 35
Citation: (Meadows 2008)

Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool

Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing. ISBN 978-1603580557.
------------------------------------------------------------

Citation 8 [PARENTHETICAL]:
Line: 39
Citation: (Goldman 1979)

Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

The Deflationary Path: Belief → Proposition → Validated Data → Standing Predicate

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 9 [PARENTHETICAL]:
Line: 39
Citation: (Kitcher 1993)

Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

The Deflationary Path: Belief → Proposition → Validated Data → Standing Predicate

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 10 [PARENTHETICAL]:
Line: 59
Citation: (Mesoudi 2011)

Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.

Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press. ISBN 978-0226520445.
------------------------------------------------------------

Citation 11 [IN-PROSE]:
Line: 65
Citation: Mesoudi (2011)

Context:
The model's deflationary path shifts from private belief (psychological state) to public proposition (testable claim), potentially becoming a Standing Predicate if it reduces costs exceptionally.

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory, as synthesized for the social sciences by Mesoudi (2011). The network's abstract informational structure—its core Standing Predicates and their relations—functions as the replicator: the "code" that is copied and transmitted. The social group and its institutions (the scientific community, legal system, or society) function as the interactor: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This distinction is crucial for understanding how knowledge can evolve and persist across different social contexts. It explains how knowledge can persist even when the societies that created it do not; the rediscovery of Roman legal principles during the Renaissance is a prime example of a successful replicator outliving its original interactor. The existence of these countless, independently formed and often nested Shared Networks—all responding to the same landscape of physical and social constraints—supports a crucial inference: they collectively reveal an objective structure that underwrites all successful inquiry. This mosaic of survived predicates, biased toward low-brittleness solutions, anticipates the Apex Network: not a pre-existing truth, but an emergent attractor forged by historical filtering (as detailed in Section 4).

### 2.2.1 How the Causal Hierarchy Addresses the Circularity Objection

Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press. ISBN 978-0226520445.
------------------------------------------------------------

Citation 12 [PARENTHETICAL]:
Line: 160
Citation: (Quine 1960)

Context:
2. **Comparative-Diachronic Judgments:** We avoid absolute claims of brittleness. Instead, analysis focuses on relative and temporal comparisons: Is System A *more* brittle than System B under similar pressures? Is a given system's brittleness *rising* over time?
3. **Convergent Evidence:** A robust diagnosis of brittleness requires agreement across multiple, independent indicators (e.g., rising P(t), increasing C(t), and declining R(t)). Systematic convergence becomes increasingly difficult to dismiss as mere interpretive bias.

To address hermeneutic circularity more deeply, we frame constrained interpretation as a pragmatic reflective equilibrium (Quine 1960; Goodman 1983). Reflective equilibrium balances general principles (e.g., brittleness indicators) with particular judgments (e.g., classifying a paper as anomaly-resolution), iteratively adjusting both until coherence is achieved. This manages circularity by anchoring in physical-biological outcomes, avoiding infinite regress.

Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

Reference:
Quine, W. V. O. 1960. *Word and Object*. Cambridge, MA: MIT Press. ISBN 978-0262670012.
------------------------------------------------------------

Citation 13 [PARENTHETICAL]:
Line: 160
Citation: (Goodman 1983)

Context:
2. **Comparative-Diachronic Judgments:** We avoid absolute claims of brittleness. Instead, analysis focuses on relative and temporal comparisons: Is System A *more* brittle than System B under similar pressures? Is a given system's brittleness *rising* over time?
3. **Convergent Evidence:** A robust diagnosis of brittleness requires agreement across multiple, independent indicators (e.g., rising P(t), increasing C(t), and declining R(t)). Systematic convergence becomes increasingly difficult to dismiss as mere interpretive bias.

To address hermeneutic circularity more deeply, we frame constrained interpretation as a pragmatic reflective equilibrium (Quine 1960; Goodman 1983). Reflective equilibrium balances general principles (e.g., brittleness indicators) with particular judgments (e.g., classifying a paper as anomaly-resolution), iteratively adjusting both until coherence is achieved. This manages circularity by anchoring in physical-biological outcomes, avoiding infinite regress.

Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

Reference:
Goodman, Nelson. 1983. *Fact, Fiction, and Forecast*. 4th ed. Cambridge, MA: Harvard University Press (originally 1954). ISBN 978-0674290716.
------------------------------------------------------------

Citation 14 [PARENTHETICAL]:
Line: 164
Citation: (Gadamer 1975)

Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.

Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum (originally Seabury Press; 2nd revised ed.). ISBN 978-0826400161.
------------------------------------------------------------

Citation 15 [PARENTHETICAL]:
Line: 192
Citation: (Kim 1988)

Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Sinclair 2007). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 16 [PARENTHETICAL]:
Line: 194
Citation: (Sinclair 2007)

Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Sinclair 2007). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.

Reference:
Sinclair, Robert. 2007. "Quine's Naturalized Epistemology and the Third Dogma of Empiricism." *Southern Journal of Philosophy* 45, no. 3: 455–472. https://doi.org/10.1111/j.2041-6962.2007.tb00060.x.
------------------------------------------------------------

Citation 17 [PARENTHETICAL]:
Line: 194
Citation: (Pritchard 2016)

Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Sinclair 2007). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.

Reference:
Pritchard, Duncan. 2016. "Epistemic Risk." *Journal of Philosophy* 113(11): 550–571. https://doi.org/10.5840/jphil20161131137.
------------------------------------------------------------

Citation 18 [PARENTHETICAL]:
Line: 245
Citation: (Harding 1991)

Context:
E.g., phlogiston chemistry, miasma theory, luminiferous aether, and blank slate psychology, as detailed in our worked examples. While some elements of failed theories may return in modified forms, the Canon identifies core structural patterns that reliably generate costs, not every specific claim.

To avoid implying all failed systems fail for the same reason, we distinguish epistemic failure from epistemic suppression. Epistemic failure occurs when a system's brittleness leads to collapse due to internal costs (e.g., phlogiston chemistry's predictive failures). Epistemic suppression occurs when viable systems are eliminated by contingent, external forces like colonialism or power imbalances, not brittleness (e.g., indigenous knowledge under European expansion). This engages feminist and decolonial epistemologies (Harding 1991; Lugones 2003), which highlight how suppression masks viability, preventing convergence on the Apex Network. The Negative Canon thus includes both intrinsic failures and suppressed alternatives, strengthening its role in mapping viability without endorsing a simplistic "survival-of-the-fittest" narrative.

This canon charts failures of both causal and normative alignment:

Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press. ISBN 978-0801497469.
------------------------------------------------------------

Citation 19 [PARENTHETICAL]:
Line: 245
Citation: (Lugones 2003)

Context:
E.g., phlogiston chemistry, miasma theory, luminiferous aether, and blank slate psychology, as detailed in our worked examples. While some elements of failed theories may return in modified forms, the Canon identifies core structural patterns that reliably generate costs, not every specific claim.

To avoid implying all failed systems fail for the same reason, we distinguish epistemic failure from epistemic suppression. Epistemic failure occurs when a system's brittleness leads to collapse due to internal costs (e.g., phlogiston chemistry's predictive failures). Epistemic suppression occurs when viable systems are eliminated by contingent, external forces like colonialism or power imbalances, not brittleness (e.g., indigenous knowledge under European expansion). This engages feminist and decolonial epistemologies (Harding 1991; Lugones 2003), which highlight how suppression masks viability, preventing convergence on the Apex Network. The Negative Canon thus includes both intrinsic failures and suppressed alternatives, strengthening its role in mapping viability without endorsing a simplistic "survival-of-the-fittest" narrative.

This canon charts failures of both causal and normative alignment:

Reference:
Lugones, María. 2003. *Pilgrimages/Peregrinajes: Theorizing Coalition against Multiple Oppressions*. Lanham, MD: Rowman & Littlefield. ISBN 978-0742514591.
------------------------------------------------------------

Citation 20 [POSSESSIVE]:
Line: 266
Citation: Peirce's (1878)

Context:
The Apex Network is the emergent structure revealed as unviable systems are eliminated. It is not a pre-existing truth but the structural residue of countless pragmatic filters. Like π, the Apex Network is a determinate structure we approach asymptotically through successive approximation.

The Apex Network A is the intersection of all viable worlds, approximated by our Consensus Network over time. This echoes Peirce's (1878) notion of truth as the ideal end of inquiry. Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to approximate this structure. Progress means reducing |S_consensus \ A|.

#### 4.2.1 The Modal Status of the Apex Network

Reference:
Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press (originally 1878).
------------------------------------------------------------

Citation 21 [PARENTHETICAL]:
Line: 292
Citation: (Newman 2010)

Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:

Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press. ISBN 978-0199206650.
------------------------------------------------------------

Citation 22 [POSSESSIVE]:
Line: 308
Citation: Haack's (1993)

Context:
This process operates through the differential success of Standing Predicates across domains. When a predicate proves highly effective in reducing brittleness in one domain, it creates pressure for similar principles in related domains. For example, germ theory's success in medicine pressured similar causal approaches in public health and sanitation. This cross-domain propagation drives the emergence of the Apex Network, as the most viable conceptual tools spread across knowledge systems.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge. The "pursuit of truth," then, is the practical, fallible project of refining our current Consensus Network to better align with this real, objective structure.

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state; they simply try to solve local problems and reduce costs. The Apex Network is the name we give to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell. ISBN 978-0631196792.
------------------------------------------------------------

Citation 23 [PARENTHETICAL]:
Line: 318
Citation: (Worrall 1989)

Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43(1–2): 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 24 [PARENTHETICAL]:
Line: 318
Citation: (Rescher 1996)

Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press. ISBN 978-0791428184.
------------------------------------------------------------

Citation 25 [PARENTHETICAL]:
Line: 320
Citation: (Ladyman & Ross 2007)

Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press. ISBN 978-0199276196.
------------------------------------------------------------

Citation 26 [POSSESSIVE]:
Line: 341
Citation: Peirce's (1878)

Context:
- Epistemically, it is a regulative ideal: we can never possess a final, complete view of it, but we approach it asymptotically through the historical culling of brittle systems.
This resolves Quine's tension between immanent and transcendent truth: the Apex Network is not a pre-existing metaphysical blueprint, but a structure forged by failure.

To further clarify the ontological commitments of the Apex Network, we contrast it with alternative positions. Unlike platonic realism, which posits timeless ideal forms existing independently of history, the Apex Network is emergent from pragmatic processes, not a pre-existing metaphysical entity. It differs from social constructivism by being mind-independent: its structure constrains successful inquiry regardless of cultural beliefs. This view aligns with Peirce's (1878) conception of truth as the ideal end of inquiry, but naturalizes it as the convergent outcome of pragmatic selection rather than a transcendental ideal. The Apex Network is 'real' in the sense that it exists as a stable attractor in the landscape of viability, discoverable through eliminative methods, much like π is real as the limit of successive approximations. To illustrate this emergent structural fact, consider the cross-cultural emergence of color terms. Across diverse societies, basic color categories like 'red' and 'blue' emerge convergently, not because of pre-existing universal essences, but because they correspond to stable patterns in light reflection and human perception that facilitate reliable environmental interaction. Similarly, the Apex Network emerges as a structural fact from the pragmatic constraints that shape successful knowledge systems, independent of any particular culture's beliefs.

### **4.3 A Structured Framework for Truth and Inquiry**

Reference:
Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press (originally 1878).
------------------------------------------------------------

Citation 27 [PARENTHETICAL]:
Line: 378
Citation: (Simon 1972)

Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.

Reference: NOT FOUND for 'Simon 1972'
------------------------------------------------------------

Citation 28 [PARENTHETICAL]:
Line: 388
Citation: (Kvanvig 2012)

Context:
### 6.1 Resolving Coherentism's Isolation Problem

Contemporary coherentist theories face BonJour's (1985) isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality. Internalist responses (Kvanvig 2012; Carlson 2015) explain *why* some beliefs are more central but not *how* that centrality is earned through external discipline.

Emergent Pragmatic Coherentism provides this missing externalist constraint. It complements Carlson's (2015) internalist reconstruction showing core beliefs are functionally indispensable by providing the causal, evolutionary explanation: principles become core by surviving historical pragmatic filtering demonstrating their role in low-brittleness networks. Justification requires both internal coherence and demonstrated network reliability through low systemic brittleness. This grounds the web's structure in objective, externalist history.

Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50(1): 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 29 [PARENTHETICAL]:
Line: 388
Citation: (Carlson 2015)

Context:
### 6.1 Resolving Coherentism's Isolation Problem

Contemporary coherentist theories face BonJour's (1985) isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality. Internalist responses (Kvanvig 2012; Carlson 2015) explain *why* some beliefs are more central but not *how* that centrality is earned through external discipline.

Emergent Pragmatic Coherentism provides this missing externalist constraint. It complements Carlson's (2015) internalist reconstruction showing core beliefs are functionally indispensable by providing the causal, evolutionary explanation: principles become core by surviving historical pragmatic filtering demonstrating their role in low-brittleness networks. Justification requires both internal coherence and demonstrated network reliability through low systemic brittleness. This grounds the web's structure in objective, externalist history.

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 30 [POSSESSIVE]:
Line: 388
Citation: BonJour's (1985)

Context:
### 6.1 Resolving Coherentism's Isolation Problem

Contemporary coherentist theories face BonJour's (1985) isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality. Internalist responses (Kvanvig 2012; Carlson 2015) explain *why* some beliefs are more central but not *how* that centrality is earned through external discipline.

Emergent Pragmatic Coherentism provides this missing externalist constraint. It complements Carlson's (2015) internalist reconstruction showing core beliefs are functionally indispensable by providing the causal, evolutionary explanation: principles become core by surviving historical pragmatic filtering demonstrating their role in low-brittleness networks. Justification requires both internal coherence and demonstrated network reliability through low systemic brittleness. This grounds the web's structure in objective, externalist history.

Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press. ISBN 978-0674843813.
------------------------------------------------------------

Citation 31 [POSSESSIVE]:
Line: 390
Citation: Carlson's (2015)

Context:
Contemporary coherentist theories face BonJour's (1985) isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality. Internalist responses (Kvanvig 2012; Carlson 2015) explain *why* some beliefs are more central but not *how* that centrality is earned through external discipline.

Emergent Pragmatic Coherentism provides this missing externalist constraint. It complements Carlson's (2015) internalist reconstruction showing core beliefs are functionally indispensable by providing the causal, evolutionary explanation: principles become core by surviving historical pragmatic filtering demonstrating their role in low-brittleness networks. Justification requires both internal coherence and demonstrated network reliability through low systemic brittleness. This grounds the web's structure in objective, externalist history.

This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which analyzes information flow within static structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under selective pressure, and by grounding that pressure in a necessary constraint structure rather than contingent history, we provide what coherentism has lacked: an externalist check that explains both how the web's structure is forged and why that structure tracks objective features of reality. The Apex Network is not an arbitrary historical product but the optimal solution to the constraint problem all viable systems face.

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 32 [PARENTHETICAL]:
Line: 392
Citation: (Zollman 2013)

Context:
Emergent Pragmatic Coherentism provides this missing externalist constraint. It complements Carlson's (2015) internalist reconstruction showing core beliefs are functionally indispensable by providing the causal, evolutionary explanation: principles become core by surviving historical pragmatic filtering demonstrating their role in low-brittleness networks. Justification requires both internal coherence and demonstrated network reliability through low systemic brittleness. This grounds the web's structure in objective, externalist history.

This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which analyzes information flow within static structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under selective pressure, and by grounding that pressure in a necessary constraint structure rather than contingent history, we provide what coherentism has lacked: an externalist check that explains both how the web's structure is forged and why that structure tracks objective features of reality. The Apex Network is not an arbitrary historical product but the optimal solution to the constraint problem all viable systems face.

### 6.2 Evolutionary Epistemology and the Fitness Problem

Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in Epistemic Communities." *Philosophy Compass* 8(1): 15–27. https://doi.org/10.1111/j.1747-9991.2012.00534.x.
------------------------------------------------------------

Citation 33 [PARENTHETICAL]:
Line: 396
Citation: (Campbell 1974)

Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.

Reference:
Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.
------------------------------------------------------------

Citation 34 [PARENTHETICAL]:
Line: 396
Citation: (Zollman 2013)

Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.

Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in Epistemic Communities." *Philosophy Compass* 8(1): 15–27. https://doi.org/10.1111/j.1747-9991.2012.00534.x.
------------------------------------------------------------

Citation 35 [POSSESSIVE]:
Line: 402
Citation: Laudan's (1977)

Context:
Our brittleness standard resolves Bradie's fitness circularity by prioritizing resilience over transmissibility, distinguishing viable knowledge from informational viruses. Where Bradie worried that evolutionary success might favor memes that spread easily but fail pragmatically, brittleness provides an empirical criterion: systems with low measured brittleness demonstrate genuine fitness through sustained viability, not mere reproductive success. This allows us to identify "informational viruses" like conspiracy theories as brittle despite their transmissibility, grounding evolutionary epistemology in observable systemic costs rather than speculative adaptation narratives.

**Relation to Lakatos and Laudan**: While Lakatos (1970) describes degenerating research programmes qualitatively, our framework provides the underlying causal mechanism. Brittleness measures accumulated systemic costs causing degeneration, offering quantifiable proxies (P(t), M(t), C(t)) where Lakatos gave binary classification. Unlike Laudan's (1977) retrospective problem-solving effectiveness, brittleness provides forward-looking risk assessment, detecting vulnerability before crisis.

### 6.3 A Realist Corrective to Neopragmatism

Reference:
Laudan, Larry. 1977. *Progress and Its Problems: Towards a Theory of Scientific Growth*. Berkeley: University of California Press. ISBN 978-0520037212.
------------------------------------------------------------

Citation 36 [IN-PROSE]:
Line: 402
Citation: Lakatos (1970)

Context:
Our brittleness standard resolves Bradie's fitness circularity by prioritizing resilience over transmissibility, distinguishing viable knowledge from informational viruses. Where Bradie worried that evolutionary success might favor memes that spread easily but fail pragmatically, brittleness provides an empirical criterion: systems with low measured brittleness demonstrate genuine fitness through sustained viability, not mere reproductive success. This allows us to identify "informational viruses" like conspiracy theories as brittle despite their transmissibility, grounding evolutionary epistemology in observable systemic costs rather than speculative adaptation narratives.

**Relation to Lakatos and Laudan**: While Lakatos (1970) describes degenerating research programmes qualitatively, our framework provides the underlying causal mechanism. Brittleness measures accumulated systemic costs causing degeneration, offering quantifiable proxies (P(t), M(t), C(t)) where Lakatos gave binary classification. Unlike Laudan's (1977) retrospective problem-solving effectiveness, brittleness provides forward-looking risk assessment, detecting vulnerability before crisis.

### 6.3 A Realist Corrective to Neopragmatism

Reference:
Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific Research Programmes." In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 37 [PARENTHETICAL]:
Line: 406
Citation: (Rorty 1979)

Context:
### 6.3 A Realist Corrective to Neopragmatism

The framework retains pragmatism's anti-foundationalist spirit while offering a corrective to neopragmatists (Rorty 1979; Brandom 1994) vulnerable to reducing objectivity to social consensus. These accounts of justification as linguistic practice, while rich in normative detail, lack robust non-discursive external constraints—a gap filled by our model's appeal to pragmatic pushback as a material, reality-based filter.

Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

Reference:
Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press. ISBN 978-0691020167.
------------------------------------------------------------

Citation 38 [PARENTHETICAL]:
Line: 406
Citation: (Brandom 1994)

Context:
### 6.3 A Realist Corrective to Neopragmatism

The framework retains pragmatism's anti-foundationalist spirit while offering a corrective to neopragmatists (Rorty 1979; Brandom 1994) vulnerable to reducing objectivity to social consensus. These accounts of justification as linguistic practice, while rich in normative detail, lack robust non-discursive external constraints—a gap filled by our model's appeal to pragmatic pushback as a material, reality-based filter.

Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

Reference:
Brandom, Robert B. 1994. *Making It Explicit: Reasoning, Representing, and Discursive Commitment*. Cambridge, MA: Harvard University Press. ISBN 978-0674543195.
------------------------------------------------------------

Citation 39 [PARENTHETICAL]:
Line: 410
Citation: (Worrall 1989)

Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.

Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43(1–2): 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 40 [IN-PROSE]:
Line: 414
Citation: Ladyman and Ross (2007)

Context:
(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.

(2) The epistemological question (answered by the eliminative process of pragmatic selection): Our framework provides the specific causal mechanism for convergence that structural realism often lacks. We discover these robust structures not through a mysterious act of intellectual insight, but through the brutal, eliminative process of pragmatic selection. High-brittleness networks—those whose posited structures misalign with the real landscape of viability—generate unsustainable costs, collapse, and enter the **Negative Canon**. Low-brittleness networks survive and are built upon. Over historical time, this failure-driven selective pressure is precisely the engine that forces our **Consensus Networks** to conform to the objective, relational structure of the Apex Network. Unlike Ladyman and Ross (2007), who posit ontic structures, our Apex Network emerges via pragmatic selection, evidenced by cross-domain predicate propagation (e.g., causality from physics to biology).

Contra Psillos's pessimistic induction, our eliminative process provides empirical evidence of convergence toward stable structures. While Psillos argues that past theories' failures undermine realism, our framework shows that failures are not random but systematically eliminated by brittleness, leaving a convergent residue. The Negative Canon's accumulation demonstrates that not all theories fail equally; those aligning with viability persist, offering inductive grounds for realism about the Apex Network as the limit of this process.

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press. ISBN 978-0199276196.
------------------------------------------------------------

Citation 41 [PARENTHETICAL]:
Line: 443
Citation: (Simon 1972)

Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**

Reference: NOT FOUND for 'Simon 1972'
------------------------------------------------------------

Citation 42 [PARENTHETICAL]:
Line: 447
Citation: (Harding 1991)

Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.

Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press. ISBN 978-0801497469.
------------------------------------------------------------

Citation 43 [PARENTHETICAL]:
Line: 465
Citation: (Goldman 1979)

Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 44 [PARENTHETICAL]:
Line: 467
Citation: (Zagzebski 1996)

Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press. ISBN 978-0521570602. https://doi.org/10.1017/CBO9780511582233.
------------------------------------------------------------

Citation 45 [PARENTHETICAL]:
Line: 469
Citation: (Goldman 1999)

Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**

Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press. ISBN 978-0198238201.
------------------------------------------------------------

Citation 46 [IN-PROSE]:
Line: 485
Citation: Kelly (2005)

Context:
As a macro-epistemology explaining long-term viability of public knowledge systems, the framework doesn't primarily solve micro-epistemological problems (Gettier cases, perceptual justification). Instead, it bridges levels through higher-order evidence: diagnosed system health provides powerful defeaters or corroborators for individual beliefs.

The diagnosed brittleness of a knowledge system provides higher-order evidence that determines rational priors. Following Kelly (2005), when an agent receives a claim from a source, they must condition their belief not only on the first-order evidence but also on the reliability of the source.
> Let S be a high-brittleness network (e.g., a denialist documentary) and E be a piece of seemingly strong evidence it presents. Even if E is compelling, the agent’s prior probability in S’s reliability is extremely low due to its history of rising P(t), C(t), and predictive failure. Thus, the posterior confidence in the claim remains low.
> Conversely, a low-brittleness network like the IPCC earns a high prior through demonstrated resilience. To doubt its claims without new evidence of rising brittleness is to doubt the adaptive process of science itself.
> This provides a rational, non-deferential basis for trust: justification flows from systemic health.

Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 47 [IN-PROSE]:
Line: 490
Citation: Fricker (2007)

Context:
> Conversely, a low-brittleness network like the IPCC earns a high prior through demonstrated resilience. To doubt its claims without new evidence of rising brittleness is to doubt the adaptive process of science itself.
> This provides a rational, non-deferential basis for trust: justification flows from systemic health.

To bridge micro-macro levels more systematically, we propose a tiered model of epistemic deference, drawing on Fricker (2007) on testimony and Christensen (2007) on disagreement:

Level 1: Individual justification via direct evidence (e.g., personal observation, logical inference). This is the micro-level foundation.

Reference:
Fricker, Elizabeth. 2007. *The Epistemology of Testimony*. Oxford: Oxford University Press. ISBN 978-0199276011.
------------------------------------------------------------

Citation 48 [IN-PROSE]:
Line: 490
Citation: Christensen (2007)

Context:
> Conversely, a low-brittleness network like the IPCC earns a high prior through demonstrated resilience. To doubt its claims without new evidence of rising brittleness is to doubt the adaptive process of science itself.
> This provides a rational, non-deferential basis for trust: justification flows from systemic health.

To bridge micro-macro levels more systematically, we propose a tiered model of epistemic deference, drawing on Fricker (2007) on testimony and Christensen (2007) on disagreement:

Level 1: Individual justification via direct evidence (e.g., personal observation, logical inference). This is the micro-level foundation.

Reference:
Christensen, David. 2007. "Epistemology of Disagreement: The Good News." *Philosophical Review* 116(2): 187–217. https://doi.org/10.1215/00318108-2006-035.
------------------------------------------------------------

Citation 49 [PARENTHETICAL]:
Line: 496
Citation: (Harding 1991)

Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.

Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press. ISBN 978-0801497469.
------------------------------------------------------------

Citation 50 [PARENTHETICAL]:
Line: 504
Citation: (Turchin 2003)

Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press. ISBN 978-0691116693.
------------------------------------------------------------

Citation 51 [PARENTHETICAL]:
Line: 506
Citation: (Mallapaty 2020)

Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.

Reference: NOT FOUND for 'Mallapaty 2020'
------------------------------------------------------------

Citation 52 [PARENTHETICAL]:
Line: 514
Citation: (Wright 1932)

Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 53 [PARENTHETICAL]:
Line: 514
Citation: (Mesoudi 2011)

Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press. ISBN 978-0226520445.
------------------------------------------------------------

Citation 54 [PARENTHETICAL]:
Line: 516
Citation: (Acemoglu and Robinson 2012)

Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Reference:
Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business. ISBN 978-0307719225.
------------------------------------------------------------

Citation 55 [PARENTHETICAL]:
Line: 520
Citation: (per Harding 1991)

Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Reference: NOT FOUND for 'per Harding 1991'
------------------------------------------------------------

Citation 56 [PARENTHETICAL]:
Line: 526
Citation: (Harding 1991)

Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press. ISBN 978-0801497469.
------------------------------------------------------------

Citation 57 [IN-PROSE]:
Line: 528
Citation: Turchin (2003)

Context:
This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Power and oppression cases illustrate this. Slavery appeared stable to beneficiaries but exhibited objective brittleness through measurable indicators: coercive overheads (patrols, legal apparatus), chronic instability (rebellions), and opportunity costs (suppressed productivity). The exercise of power doesn't negate brittleness; coercive costs become primary diagnostic indicators (the C(t) metric). While the framework predicts statistical tendencies rather than deterministic outcomes, potential counterexamples exist where apparently brittle systems temporarily prevailed due to contingent factors. However, these cases often reveal underlying brittleness through higher long-term costs or eventual collapse.

Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press. ISBN 978-0691116693.
------------------------------------------------------------

Citation 58 [PARENTHETICAL]:
Line: 572
Citation: (El-Hani and Pihlström 2002)

Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**

Reference:
El-Hani, Charbel Niño, and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3(2): article 3. https://doi.org/10.5840/eip2002325.
------------------------------------------------------------

Citation 59 [PARENTHETICAL]:
Line: 572
Citation: (Rottschaefer 2012)

Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**

Reference:
Rottschaefer, William A. 2012. "The Moral Realism of Pragmatic Naturalism." *Analyse & Kritik* 34(1): 141–156. https://doi.org/10.1515/auk-2012-0110.
------------------------------------------------------------

Citation 60 [IN-PROSE]:
Line: 572
Citation: Bennett-Hunter (2015)

Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**

Reference:
Bennett-Hunter, Guy. 2015. *Ineffability and Religious Experience*. London: Routledge (originally Pickering & Chatto). ISBN 978-1848934719.
------------------------------------------------------------

Citation 61 [IN-PROSE]:
Line: 572
Citation: Peter (2024)

Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**

Reference:
Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37(7): 1948–70. https://doi.org/10.1080/09515089.2023.2236120.
------------------------------------------------------------


################################################################################
FILE: proc_v7.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 103
Citation: (Fogel and Engerman 1974)

Context:
#### Measuring C(t): The Coercion Ratio

C(t) = (resources for internal coercion) / (total economic output). For example, Virginia in 1850 allocated ~2.5% of economic output to coercion (vs. 0.8-1.2% in Northern states), rising from 1.8% (1820) to 3.1% (1860), indicating escalating maintenance costs (Fogel and Engerman 1974).

#### Measuring P(t): Patch Velocity

Reference:
Fogel, Robert William, and Stanley L. Engerman. 1974. *Time on the Cross: The Economics of American Negro Slavery*. Boston: Little, Brown and Company. ISBN 978-0393312188.
------------------------------------------------------------

Citation 2 [PARENTHETICAL]:
Line: 137
Citation: (World Bank 2012)

Context:
EPC analyzes ongoing debates like patriarchy's decline. The predicate "women's roles are private and subordinate" proves profoundly inefficient: massive economic losses from excluding half the population; informational costs from silencing female perspectives; high coercive costs enforcing rigid roles.

Transition to egalitarianism involves short-term friction costs from social conflict. However, this is an investment that pays down patriarchal debt. Feminist critique wagers that fully utilizing all human resources yields greater long-term innovation and resilience (lower SBI). This transforms value clashes into empirical questions about social design efficiency. This wager is increasingly supported by development economics, which finds strong correlations between gender equality in education and economic participation and metrics of national prosperity and stability (World Bank 2012, Duflo 2012).

### 3.4. Challenging Cases: Addressing Apparent Counterexamples

Reference:
World Bank. 2012. *World Development Report 2012: Gender Equality and Development*. Washington, DC: World Bank. ISBN 978-0821388105.
------------------------------------------------------------

Citation 3 [PARENTHETICAL]:
Line: 137
Citation: (Duflo 2012)

Context:
EPC analyzes ongoing debates like patriarchy's decline. The predicate "women's roles are private and subordinate" proves profoundly inefficient: massive economic losses from excluding half the population; informational costs from silencing female perspectives; high coercive costs enforcing rigid roles.

Transition to egalitarianism involves short-term friction costs from social conflict. However, this is an investment that pays down patriarchal debt. Feminist critique wagers that fully utilizing all human resources yields greater long-term innovation and resilience (lower SBI). This transforms value clashes into empirical questions about social design efficiency. This wager is increasingly supported by development economics, which finds strong correlations between gender equality in education and economic participation and metrics of national prosperity and stability (World Bank 2012, Duflo 2012).

### 3.4. Challenging Cases: Addressing Apparent Counterexamples

Reference:
Duflo, Esther. 2012. "Women Empowerment and Economic Development." *Journal of Economic Literature* 50(4): 1051–79. https://doi.org/10.1257/jel.50.4.1051.
------------------------------------------------------------

Citation 4 [PARENTHETICAL]:
Line: 215
Citation: (Boyd 1988)

Context:
Pragmatic Procedural Realism is the metaethical instantiation of Emergent Pragmatic Coherentism. While EPC provides the general theory of justification applicable across all domains, Pragmatic Procedural Realism specifies how that framework operates in the normative domain. The relationship is one of general theory to domain-specific application: EPC is the diagnostic methodology, Pragmatic Procedural Realism is its normative realization.

Pragmatic Procedural Realism is a naturalistic moral realism (Boyd 1988, Railton 1986). Its objectivity claims are:

- **Realist**: Objective, mind-independent truths exist about normative viability. "Slavery is wrong" refers to structural facts about predicates' incoherence with the Apex Network, the emergent structure of viable norms.
- **Procedural**: Moral truths are emergent relational facts discovered historically. Truth-makers are objective facts about networks' pragmatic resilience (low SBI).

Reference:
Boyd, Richard. 1988. "How to Be a Moral Realist." In *Essays on Moral Realism*, edited by Geoffrey Sayre-McCord, 181–228. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 5 [PARENTHETICAL]:
Line: 215
Citation: (Railton 1986)

Context:
Pragmatic Procedural Realism is the metaethical instantiation of Emergent Pragmatic Coherentism. While EPC provides the general theory of justification applicable across all domains, Pragmatic Procedural Realism specifies how that framework operates in the normative domain. The relationship is one of general theory to domain-specific application: EPC is the diagnostic methodology, Pragmatic Procedural Realism is its normative realization.

Pragmatic Procedural Realism is a naturalistic moral realism (Boyd 1988, Railton 1986). Its objectivity claims are:

- **Realist**: Objective, mind-independent truths exist about normative viability. "Slavery is wrong" refers to structural facts about predicates' incoherence with the Apex Network, the emergent structure of viable norms.
- **Procedural**: Moral truths are emergent relational facts discovered historically. Truth-makers are objective facts about networks' pragmatic resilience (low SBI).

Reference:
Railton, Peter. 1986. "Moral Realism." *The Philosophical Review* 95(2): 163–207. https://doi.org/10.2307/2185589.
------------------------------------------------------------

Citation 6 [PARENTHETICAL]:
Line: 243
Citation: (March 1978)

Context:
Biological Constraints. These are empirical facts discoverable through physiology and epidemiology without normative commitments. Humans require minimum caloric intake (approximately 1,500-2,000 calories per day). Chronic malnutrition produces immune dysfunction, elevated mortality, and demographic decline. Extended childhood dependency requires caregiver investment. Social isolation causes measurable harm. Systems violating these requirements incur objective, measurable costs—mortality, morbidity, demographic collapse—independent of anyone's values.

Cognitive Constraints. Psychology, cognitive science, and behavioral economics reveal bounded rationality (March 1978): humans cannot compute optimal solutions in real-time. Working memory is limited to roughly seven items. Coordination failures occur without institutional support. Social learning has specific capacities and limitations constraining information transmission. These constraints determine which normative architectures are implementable. Systems requiring perfect rationality or unlimited processing cannot function with human agents. These are empirical facts about cognition, not value judgments.

Coordination Constraints. Game theory and institutional economics show cooperation requires enforcement mechanisms under potential defection (Axelrod 1984). Common-pool resources require boundary rules and monitoring (Ostrom 1990). Large-scale coordination requires division of labor and information aggregation. These are mathematical facts about strategic interaction under specified conditions, derivable from formal models, not normative intuitions.

Reference:
March, James G. 1978. "Bounded Rationality, Ambiguity, and the Engineering of Choice." *The Bell Journal of Economics* 9, no. 2: 587–608. https://doi.org/10.2307/3003600.
------------------------------------------------------------

Citation 7 [PARENTHETICAL]:
Line: 245
Citation: (Axelrod 1984)

Context:
Cognitive Constraints. Psychology, cognitive science, and behavioral economics reveal bounded rationality (March 1978): humans cannot compute optimal solutions in real-time. Working memory is limited to roughly seven items. Coordination failures occur without institutional support. Social learning has specific capacities and limitations constraining information transmission. These constraints determine which normative architectures are implementable. Systems requiring perfect rationality or unlimited processing cannot function with human agents. These are empirical facts about cognition, not value judgments.

Coordination Constraints. Game theory and institutional economics show cooperation requires enforcement mechanisms under potential defection (Axelrod 1984). Common-pool resources require boundary rules and monitoring (Ostrom 1990). Large-scale coordination requires division of labor and information aggregation. These are mathematical facts about strategic interaction under specified conditions, derivable from formal models, not normative intuitions.

Physical Constraints. Physics, ecology, and thermodynamics establish that energy must be extracted to sustain organization. Entropy requires continuous work to maintain structure. Finite resources constrain population and consumption. These impose hard limits on social organization.

Reference:
Axelrod, Robert. 1984. *The Evolution of Cooperation*. New York: Basic Books. ISBN 978-0465021215.
------------------------------------------------------------

Citation 8 [PARENTHETICAL]:
Line: 245
Citation: (Ostrom 1990)

Context:
Cognitive Constraints. Psychology, cognitive science, and behavioral economics reveal bounded rationality (March 1978): humans cannot compute optimal solutions in real-time. Working memory is limited to roughly seven items. Coordination failures occur without institutional support. Social learning has specific capacities and limitations constraining information transmission. These constraints determine which normative architectures are implementable. Systems requiring perfect rationality or unlimited processing cannot function with human agents. These are empirical facts about cognition, not value judgments.

Coordination Constraints. Game theory and institutional economics show cooperation requires enforcement mechanisms under potential defection (Axelrod 1984). Common-pool resources require boundary rules and monitoring (Ostrom 1990). Large-scale coordination requires division of labor and information aggregation. These are mathematical facts about strategic interaction under specified conditions, derivable from formal models, not normative intuitions.

Physical Constraints. Physics, ecology, and thermodynamics establish that energy must be extracted to sustain organization. Entropy requires continuous work to maintain structure. Finite resources constrain population and consumption. These impose hard limits on social organization.

Reference:
Ostrom, Elinor. 1990. *Governing the Commons: The Evolution of Institutions for Collective Action*. Cambridge: Cambridge University Press. ISBN 978-0521405997.
------------------------------------------------------------

Citation 9 [PARENTHETICAL]:
Line: 305
Citation: (March 1978)

Context:
2. Migration Inward: As it demonstrates immense value in lowering systemic brittleness (reducing C(t) by increasing legitimacy and stability), its revision becomes prohibitively costly. It becomes a Standing Predicate used to vet new laws and policies.
3. Core Principle (Systemic Caching): Its indispensability becomes so profound that it is embedded in the infrastructure of viable legal systems (constitutions, legal training, judicial review). This systemic caching is a rational response to bounded rationality; the system entrenches its most successful discoveries to avoid re-deriving them for every new case.

A core moral principle is not a self-evident axiom but a piece of highly optimized social technology that has survived rigorous pragmatic stress-testing. Its justification is its proven, indispensable functional role in viable social architectures. This entrenchment reflects pragmatic indispensability driven by bounded rationality (March 1978). The costs of revision become effectively infinite. Revising basic justice principles requires abandoning the conceptual tools needed to coordinate social expectations, resolve disputes, or maintain legitimate authority. After centuries of implementation, legal systems worldwide presuppose core fairness principles. Revision would generate catastrophic first-order costs, undermining the stability and legitimacy on which functional governance depends.

### 4.6. Relationship to Kitcher's Ethical Project

Reference:
March, James G. 1978. "Bounded Rationality, Ambiguity, and the Engineering of Choice." *The Bell Journal of Economics* 9, no. 2: 587–608. https://doi.org/10.2307/3003600.
------------------------------------------------------------

Citation 10 [PARENTHETICAL]:
Line: 355
Citation: (Putnam 2002)

Context:
**Objection: Moral Progress Skepticism** - Progress claims are Western bias. **Reply:** Framework predicts pluralist periphery but universal floor. Progress diagnosed empirically via SBI reduction, not cultural superiority.

**Objection: Scientific Imperialism** - Reducing ethics to science (Putnam 2002). **Reply:** Not scientism but unified pragmatic filter. Moral claims remain normative but justified externally like scientific ones.

**Objection: Evolutionary Debunking** - Evolutionary pressures shaped moral intuitions for survival, not truth (Street 2006; Sterelny 2012). **Reply:** EPC resolves Street's dilemma by collapsing one of its horns. The dilemma assumes that truth and adaptiveness are independent aims, making their alignment a coincidence. Our framework denies this premise. For us, moral truth *is* a specific, demanding form of long-term systemic adaptiveness (i.e., viability). Evolution is not a distorting influence that the realist must explain away; it is the broader category of filtering processes within which the specific, cost-based discovery of moral truth takes place. Pragmatic viability is what moral truth supervenes on.

Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press. ISBN 978-0674013803.
------------------------------------------------------------

Citation 11 [PARENTHETICAL]:
Line: 357
Citation: (Street 2006)

Context:
**Objection: Scientific Imperialism** - Reducing ethics to science (Putnam 2002). **Reply:** Not scientism but unified pragmatic filter. Moral claims remain normative but justified externally like scientific ones.

**Objection: Evolutionary Debunking** - Evolutionary pressures shaped moral intuitions for survival, not truth (Street 2006; Sterelny 2012). **Reply:** EPC resolves Street's dilemma by collapsing one of its horns. The dilemma assumes that truth and adaptiveness are independent aims, making their alignment a coincidence. Our framework denies this premise. For us, moral truth *is* a specific, demanding form of long-term systemic adaptiveness (i.e., viability). Evolution is not a distorting influence that the realist must explain away; it is the broader category of filtering processes within which the specific, cost-based discovery of moral truth takes place. Pragmatic viability is what moral truth supervenes on.

**Objection: The Naturalistic Fallacy.** The framework seems to define 'the good' as 'the viable,' improperly deriving a value from a fact. **Reply:** This misinterprets the project. We offer a naturalistic reconstruction of the function of our normative practice. The claim is that what our successful moral discourse has actually been tracking are facts about systemic viability. 'Wrongness' is not being defined as high-brittleness; rather, high-brittleness is the underlying natural property that the term 'wrongness' has been imperfectly latching onto. This is a semantic externalist move: just as 'water' successfully referred to H₂O long before we understood molecular chemistry, 'wrong' has been successfully tracking high-brittleness principles long before we developed the diagnostic tools to measure it explicitly. This naturalizes the reference of our moral terms, explaining their functional authority without committing a fallacy.

Reference:
Street, Sharon. 2006. "A Darwinian Dilemma for Realist Theories of Value." *Philosophical Studies* 127(1): 109–66. https://doi.org/10.1007/s11098-005-1726-6.
------------------------------------------------------------

Citation 12 [PARENTHETICAL]:
Line: 357
Citation: (Sterelny 2012)

Context:
**Objection: Scientific Imperialism** - Reducing ethics to science (Putnam 2002). **Reply:** Not scientism but unified pragmatic filter. Moral claims remain normative but justified externally like scientific ones.

**Objection: Evolutionary Debunking** - Evolutionary pressures shaped moral intuitions for survival, not truth (Street 2006; Sterelny 2012). **Reply:** EPC resolves Street's dilemma by collapsing one of its horns. The dilemma assumes that truth and adaptiveness are independent aims, making their alignment a coincidence. Our framework denies this premise. For us, moral truth *is* a specific, demanding form of long-term systemic adaptiveness (i.e., viability). Evolution is not a distorting influence that the realist must explain away; it is the broader category of filtering processes within which the specific, cost-based discovery of moral truth takes place. Pragmatic viability is what moral truth supervenes on.

**Objection: The Naturalistic Fallacy.** The framework seems to define 'the good' as 'the viable,' improperly deriving a value from a fact. **Reply:** This misinterprets the project. We offer a naturalistic reconstruction of the function of our normative practice. The claim is that what our successful moral discourse has actually been tracking are facts about systemic viability. 'Wrongness' is not being defined as high-brittleness; rather, high-brittleness is the underlying natural property that the term 'wrongness' has been imperfectly latching onto. This is a semantic externalist move: just as 'water' successfully referred to H₂O long before we understood molecular chemistry, 'wrong' has been successfully tracking high-brittleness principles long before we developed the diagnostic tools to measure it explicitly. This naturalizes the reference of our moral terms, explaining their functional authority without committing a fallacy.

Reference:
Sterelny, Kim. 2012. “Morality's Dark Past.” *Analyse & Kritik* 34, no. 1: 95–116. https://doi.org/10.1515/auk-2012-0107.
------------------------------------------------------------

Citation 13 [POSSESSIVE]:
Line: 361
Citation: Kitcher's (2011)

Context:
**Objection: The Naturalistic Fallacy.** The framework seems to define 'the good' as 'the viable,' improperly deriving a value from a fact. **Reply:** This misinterprets the project. We offer a naturalistic reconstruction of the function of our normative practice. The claim is that what our successful moral discourse has actually been tracking are facts about systemic viability. 'Wrongness' is not being defined as high-brittleness; rather, high-brittleness is the underlying natural property that the term 'wrongness' has been imperfectly latching onto. This is a semantic externalist move: just as 'water' successfully referred to H₂O long before we understood molecular chemistry, 'wrong' has been successfully tracking high-brittleness principles long before we developed the diagnostic tools to measure it explicitly. This naturalizes the reference of our moral terms, explaining their functional authority without committing a fallacy.

**Objection: How does this differ from Kitcher's 'Ethical Project'?** **Reply:** Our project shares much with Kitcher's (2011) view of ethics as a social technology for solving problems of altruism. However, EPC offers two crucial advancements. First, it provides a more general diagnostic toolkit (the SBI) that applies equally to scientific and ethical 'technologies,' grounding the project in a unified theory of justification. Second, EPC's concept of the modally necessary Apex Network provides a more robustly realist foundation. Where Kitcher's progress is defined by functional enhancement relative to a historical starting point, our framework grounds progress in convergence toward an objective, mind-independent structure of viability. This offers a stronger defense against charges of historicism or relativism.

**Objection: Hindsight Rationalization.** The framework can only diagnose brittleness after failure, making it merely retrospective rather than providing prospective guidance. **Reply:** This misunderstands the calibration process. We use clear historical data (the Negative Canon) to calibrate our diagnostic instruments, identifying the empirical signatures that reliably precede collapse. These calibrated instruments then enable prospective diagnosis, not deterministic prediction, but epistemic risk assessment for contemporary systems. This parallels medical science: we learn disease patterns from past cases to diagnose present patients before symptoms become catastrophic. The framework thus operates in two stages: retrospective calibration using historical failures to identify brittleness indicators, then prospective application of these calibrated metrics to assess current systems and identify degenerating research programs before collapse.

Reference:
Kitcher, Philip. 2011. *The Ethical Project*. Cambridge, MA: Harvard University Press. ISBN 978-0674061446.
------------------------------------------------------------

Citation 14 [PARENTHETICAL]:
Line: 365
Citation: (Rawls 1971)

Context:
**Objection: Hindsight Rationalization.** The framework can only diagnose brittleness after failure, making it merely retrospective rather than providing prospective guidance. **Reply:** This misunderstands the calibration process. We use clear historical data (the Negative Canon) to calibrate our diagnostic instruments, identifying the empirical signatures that reliably precede collapse. These calibrated instruments then enable prospective diagnosis, not deterministic prediction, but epistemic risk assessment for contemporary systems. This parallels medical science: we learn disease patterns from past cases to diagnose present patients before symptoms become catastrophic. The framework thus operates in two stages: retrospective calibration using historical failures to identify brittleness indicators, then prospective application of these calibrated metrics to assess current systems and identify degenerating research programs before collapse.

**Objection: Why Historical over Idealized Procedures?** Contemporary constructivists (Rawls 1971, Korsgaard 1996) also ground normativity in procedures, but use idealized rational procedures (original position, categorical imperative procedure) rather than historical filtering. What makes PPR's historical procedure superior? Doesn't idealization avoid the contamination of actual history by power, ignorance, and bias?

**Reply:** There are three problems with idealized procedures and corresponding advantages to historical ones.

Reference:
Rawls, John. 1971. *A Theory of Justice*. Cambridge, MA: Harvard University Press. ISBN 978-0674880108.
------------------------------------------------------------

Citation 15 [PARENTHETICAL]:
Line: 365
Citation: (Korsgaard 1996)

Context:
**Objection: Hindsight Rationalization.** The framework can only diagnose brittleness after failure, making it merely retrospective rather than providing prospective guidance. **Reply:** This misunderstands the calibration process. We use clear historical data (the Negative Canon) to calibrate our diagnostic instruments, identifying the empirical signatures that reliably precede collapse. These calibrated instruments then enable prospective diagnosis, not deterministic prediction, but epistemic risk assessment for contemporary systems. This parallels medical science: we learn disease patterns from past cases to diagnose present patients before symptoms become catastrophic. The framework thus operates in two stages: retrospective calibration using historical failures to identify brittleness indicators, then prospective application of these calibrated metrics to assess current systems and identify degenerating research programs before collapse.

**Objection: Why Historical over Idealized Procedures?** Contemporary constructivists (Rawls 1971, Korsgaard 1996) also ground normativity in procedures, but use idealized rational procedures (original position, categorical imperative procedure) rather than historical filtering. What makes PPR's historical procedure superior? Doesn't idealization avoid the contamination of actual history by power, ignorance, and bias?

**Reply:** There are three problems with idealized procedures and corresponding advantages to historical ones.

Reference:
Korsgaard, Christine M. 1996. *The Sources of Normativity*. Cambridge: Cambridge University Press. ISBN 978-0521559607. https://doi.org/10.1017/CBO9780511554476.
------------------------------------------------------------

Citation 16 [PARENTHETICAL]:
Line: 385
Citation: (Acemoglu and Robinson 2012)

Context:
We accept this implication for intellectual honesty. The framework maps pragmatic viability, not all moral dimensions. If such a system existed, it would fall in the Pluralist Frontier, not the Negative Canon.

However, our empirical wager is that such systems are inherently brittle. Apparent stability in historical examples like Ottoman devşirme or Indian caste systems masked high coercive overheads, innovation lags, and fragility under shocks (Acemoglu and Robinson 2012, Turchin 2003). True, cost-free internalization is likely a sociological impossibility. Oppression generates hidden costs that manifest under stress. Even systems like *Brave New World* that suppress cognitive capacities incur massive information suppression costs (Tier 2) that cripple long-term adaptation.

Species-Specific: Apex Network for cooperative primates like humans. Empirical discipline, not relativism.

Reference:
Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business. ISBN 978-0307719225.
------------------------------------------------------------

Citation 17 [PARENTHETICAL]:
Line: 385
Citation: (Turchin 2003)

Context:
We accept this implication for intellectual honesty. The framework maps pragmatic viability, not all moral dimensions. If such a system existed, it would fall in the Pluralist Frontier, not the Negative Canon.

However, our empirical wager is that such systems are inherently brittle. Apparent stability in historical examples like Ottoman devşirme or Indian caste systems masked high coercive overheads, innovation lags, and fragility under shocks (Acemoglu and Robinson 2012, Turchin 2003). True, cost-free internalization is likely a sociological impossibility. Oppression generates hidden costs that manifest under stress. Even systems like *Brave New World* that suppress cognitive capacities incur massive information suppression costs (Tier 2) that cripple long-term adaptation.

Species-Specific: Apex Network for cooperative primates like humans. Empirical discipline, not relativism.

Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press. ISBN 978-0691116693.
------------------------------------------------------------


################################################################################
FILE: test_filtered_refs.md
################################################################################

Citation 1 [IN-PROSE]:
Line: 157
Citation: Gingerich (1993)

Context:
<!-- MISSING REFERENCES (Not found in references.md) -->
<!-- Please add these to references.md: -->

<!-- Gingerich (1993) -->
<!-- Holism (2024) -->
<!-- Progress (1993) -->

Reference:
Gingerich, Owen. 1993. *The Eye of Heaven: Ptolemy, Copernicus, Kepler*. New York: American Institute of Physics. ISBN 978-0883188637.
------------------------------------------------------------

Citation 2 [IN-PROSE]:
Line: 158
Citation: Holism (2024)

Context:
<!-- Please add these to references.md: -->

<!-- Gingerich (1993) -->
<!-- Holism (2024) -->
<!-- Progress (1993) -->

Reference: NOT FOUND for 'Holism 2024'
------------------------------------------------------------

Citation 3 [IN-PROSE]:
Line: 159
Citation: Progress (1993)

Context:
<!-- Gingerich (1993) -->
<!-- Holism (2024) -->
<!-- Progress (1993) -->

Reference: NOT FOUND for 'Progress 1993'
------------------------------------------------------------

