
================================================================================
Citation Extraction Run - 228 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference: NOT FOUND
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference: NOT FOUND
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference: NOT FOUND
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference: NOT FOUND
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference: NOT FOUND
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference: NOT FOUND
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference: NOT FOUND
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference: NOT FOUND
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference: NOT FOUND
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference: NOT FOUND
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference: NOT FOUND
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference: NOT FOUND
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference: NOT FOUND
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference: NOT FOUND
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference: NOT FOUND
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference: NOT FOUND
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference: NOT FOUND
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference: NOT FOUND
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference: NOT FOUND
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference: NOT FOUND
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference: NOT FOUND
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference: NOT FOUND
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference: NOT FOUND
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference: NOT FOUND
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference: NOT FOUND
------------------------------------------------------------

Citation 39:
File: final.md
Line: 49
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference: NOT FOUND
------------------------------------------------------------

Citation 40:
File: final.md
Line: 170
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference: NOT FOUND
------------------------------------------------------------

Citation 41:
File: final.md
Line: 182
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 42:
File: final.md
Line: 192
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference: NOT FOUND
------------------------------------------------------------

Citation 43:
File: final.md
Line: 224
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference: NOT FOUND
------------------------------------------------------------

Citation 44:
File: final.md
Line: 251
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 45:
File: final.md
Line: 281
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference: NOT FOUND
------------------------------------------------------------

Citation 46:
File: final.md
Line: 314
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 47:
File: final.md
Line: 316
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 48:
File: final.md
Line: 328
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference: NOT FOUND
------------------------------------------------------------

Citation 49:
File: final.md
Line: 330
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference: NOT FOUND
------------------------------------------------------------

Citation 50:
File: final.md
Line: 332
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference: NOT FOUND
------------------------------------------------------------

Citation 51:
File: final.md
Line: 434
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference: NOT FOUND
------------------------------------------------------------

Citation 52:
File: final.md
Line: 465
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference: NOT FOUND
------------------------------------------------------------

Citation 53:
File: final.md
Line: 496
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference: NOT FOUND
------------------------------------------------------------

Citation 54:
File: final.md
Line: 498
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND
------------------------------------------------------------

Citation 55:
File: final.md
Line: 502
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference: NOT FOUND
------------------------------------------------------------

Citation 56:
File: final.md
Line: 502
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference: NOT FOUND
------------------------------------------------------------

Citation 57:
File: final.md
Line: 506
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 58:
File: final.md
Line: 512
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference: NOT FOUND
------------------------------------------------------------

Citation 59:
File: final.md
Line: 518
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference: NOT FOUND
------------------------------------------------------------

Citation 60:
File: final.md
Line: 522
Citation: (Christensen 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference: NOT FOUND
------------------------------------------------------------

Citation 61:
File: final.md
Line: 522
Citation: (Fricker 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference: NOT FOUND
------------------------------------------------------------

Citation 62:
File: final.md
Line: 522
Citation: (Kelly 2005)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference: NOT FOUND
------------------------------------------------------------

Citation 63:
File: final.md
Line: 528
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference: NOT FOUND
------------------------------------------------------------

Citation 64:
File: final.md
Line: 649
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference: NOT FOUND
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference: NOT FOUND
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference: NOT FOUND
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference: NOT FOUND
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference: NOT FOUND
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference: NOT FOUND
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference: NOT FOUND
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference: NOT FOUND
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference: NOT FOUND
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference: NOT FOUND
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference: NOT FOUND
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference: NOT FOUND
------------------------------------------------------------

Citation 78:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference: NOT FOUND
------------------------------------------------------------

Citation 79:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 80:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference: NOT FOUND
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference: NOT FOUND
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference: NOT FOUND
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference: NOT FOUND
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference: NOT FOUND
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference: NOT FOUND
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference: NOT FOUND
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference: NOT FOUND
------------------------------------------------------------

Citation 94:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference: NOT FOUND
------------------------------------------------------------

Citation 95:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 96:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference: NOT FOUND
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference: NOT FOUND
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 100:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 101:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference: NOT FOUND
------------------------------------------------------------

Citation 102:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference: NOT FOUND
------------------------------------------------------------

Citation 103:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference: NOT FOUND
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference: NOT FOUND
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference: NOT FOUND
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference: NOT FOUND
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference: NOT FOUND
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference: NOT FOUND
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference: NOT FOUND
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference: NOT FOUND
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference: NOT FOUND
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference: NOT FOUND
------------------------------------------------------------

Citation 126:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 129:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference: NOT FOUND
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference: NOT FOUND
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference: NOT FOUND
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference: NOT FOUND
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference: NOT FOUND
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference: NOT FOUND
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference: NOT FOUND
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference: NOT FOUND
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference: NOT FOUND
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference: NOT FOUND
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference: NOT FOUND
------------------------------------------------------------

Citation 147:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 148:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 149:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 150:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference: NOT FOUND
------------------------------------------------------------

Citation 151:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 152:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 153:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference: NOT FOUND
------------------------------------------------------------

Citation 154:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 155:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference: NOT FOUND
------------------------------------------------------------

Citation 156:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference: NOT FOUND
------------------------------------------------------------

Citation 157:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference: NOT FOUND
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference: NOT FOUND
------------------------------------------------------------

Citation 159:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference: NOT FOUND
------------------------------------------------------------

Citation 160:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference: NOT FOUND
------------------------------------------------------------

Citation 161:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference: NOT FOUND
------------------------------------------------------------

Citation 162:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 163:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference: NOT FOUND
------------------------------------------------------------

Citation 164:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference: NOT FOUND
------------------------------------------------------------

Citation 165:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference: NOT FOUND
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference: NOT FOUND
------------------------------------------------------------

Citation 167:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference: NOT FOUND
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference: NOT FOUND
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference: NOT FOUND
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference: NOT FOUND
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference: NOT FOUND
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference: NOT FOUND
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference: NOT FOUND
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference: NOT FOUND
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference: NOT FOUND
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference: NOT FOUND
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference: NOT FOUND
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference: NOT FOUND
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference: NOT FOUND
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference: NOT FOUND
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference: NOT FOUND
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference: NOT FOUND
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference: NOT FOUND
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference: NOT FOUND
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference: NOT FOUND
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference: NOT FOUND
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference: NOT FOUND
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference: NOT FOUND
------------------------------------------------------------

Citation 196:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 200:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference: NOT FOUND
------------------------------------------------------------

Citation 202:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference: NOT FOUND
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 204:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference: NOT FOUND
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference: NOT FOUND
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference: NOT FOUND
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference: NOT FOUND
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference: NOT FOUND
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference: NOT FOUND
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference: NOT FOUND
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference: NOT FOUND
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference: NOT FOUND
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference: NOT FOUND
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference: NOT FOUND
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference: NOT FOUND
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference: NOT FOUND
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference: NOT FOUND
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference: NOT FOUND
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference: NOT FOUND
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference: NOT FOUND
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference: NOT FOUND
------------------------------------------------------------

Citation 228:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference: NOT FOUND
------------------------------------------------------------


================================================================================
Citation Extraction Run - 228 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference: NOT FOUND
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference: NOT FOUND
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 39:
File: final.md
Line: 49
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 40:
File: final.md
Line: 170
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.
------------------------------------------------------------

Citation 41:
File: final.md
Line: 182
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 42:
File: final.md
Line: 192
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 43:
File: final.md
Line: 224
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 44:
File: final.md
Line: 251
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 45:
File: final.md
Line: 281
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 46:
File: final.md
Line: 314
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47:
File: final.md
Line: 316
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 48:
File: final.md
Line: 328
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference: NOT FOUND
------------------------------------------------------------

Citation 49:
File: final.md
Line: 330
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 50:
File: final.md
Line: 332
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 51:
File: final.md
Line: 434
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 52:
File: final.md
Line: 465
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 53:
File: final.md
Line: 496
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 54:
File: final.md
Line: 498
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND
------------------------------------------------------------

Citation 55:
File: final.md
Line: 502
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 56:
File: final.md
Line: 502
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 57:
File: final.md
Line: 506
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 58:
File: final.md
Line: 512
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 59:
File: final.md
Line: 518
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 60:
File: final.md
Line: 522
Citation: (Christensen 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Christensen, David. 2007. "Epistemology of Disagreement: The Good News." *Philosophical Review* 116 (2): 187–217.
------------------------------------------------------------

Citation 61:
File: final.md
Line: 522
Citation: (Fricker 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Fricker, Elizabeth. 2007. *The Epistemology of Testimony*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 62:
File: final.md
Line: 522
Citation: (Kelly 2005)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 63:
File: final.md
Line: 528
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 64:
File: final.md
Line: 649
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 78:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 79:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 80:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 94:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 95:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 96:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 100:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 101:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 102:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 103:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 126:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 129:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 147:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 148:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 149:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 150:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 151:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 152:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 153:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 154:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 155:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 156:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 157:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 159:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 160:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 161:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 162:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 163:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 164:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 165:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 167:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 196:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 200:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 202:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 204:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 228:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------


================================================================================
Citation Extraction Run - 228 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference: NOT FOUND
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference: NOT FOUND
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 39:
File: final.md
Line: 49
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 40:
File: final.md
Line: 170
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.
------------------------------------------------------------

Citation 41:
File: final.md
Line: 182
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 42:
File: final.md
Line: 192
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 43:
File: final.md
Line: 224
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 44:
File: final.md
Line: 251
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 45:
File: final.md
Line: 281
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 46:
File: final.md
Line: 314
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47:
File: final.md
Line: 316
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 48:
File: final.md
Line: 328
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference: NOT FOUND
------------------------------------------------------------

Citation 49:
File: final.md
Line: 330
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 50:
File: final.md
Line: 332
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 51:
File: final.md
Line: 434
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 52:
File: final.md
Line: 465
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 53:
File: final.md
Line: 496
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 54:
File: final.md
Line: 498
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND
------------------------------------------------------------

Citation 55:
File: final.md
Line: 502
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 56:
File: final.md
Line: 502
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 57:
File: final.md
Line: 506
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 58:
File: final.md
Line: 512
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 59:
File: final.md
Line: 518
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 60:
File: final.md
Line: 522
Citation: (Christensen 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Christensen, David. 2007. "Epistemology of Disagreement: The Good News." *Philosophical Review* 116 (2): 187–217.
------------------------------------------------------------

Citation 61:
File: final.md
Line: 522
Citation: (Fricker 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Fricker, Elizabeth. 2007. *The Epistemology of Testimony*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 62:
File: final.md
Line: 522
Citation: (Kelly 2005)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 63:
File: final.md
Line: 528
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 64:
File: final.md
Line: 649
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 78:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 79:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 80:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 94:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 95:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 96:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 100:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 101:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 102:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 103:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 126:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 129:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 147:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 148:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 149:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 150:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 151:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 152:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 153:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 154:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 155:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 156:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 157:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 159:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 160:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 161:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 162:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 163:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 164:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 165:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 167:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 196:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 200:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 202:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 204:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 228:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------


================================================================================
Citation Extraction Run - 228 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference: NOT FOUND (key: '(Holling 1973)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND (key: '(Peirce 1878)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND (key: '(Acemoglu and Robinson 2012)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND (key: '(Baggio and Parravicini 2019)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference: NOT FOUND (key: '(Holling 1973)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 39:
File: final.md
Line: 49
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 40:
File: final.md
Line: 170
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.
------------------------------------------------------------

Citation 41:
File: final.md
Line: 182
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND (key: '(Peirce 1878)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 42:
File: final.md
Line: 192
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 43:
File: final.md
Line: 224
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 44:
File: final.md
Line: 251
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 45:
File: final.md
Line: 281
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 46:
File: final.md
Line: 314
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47:
File: final.md
Line: 316
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND (key: '(Acemoglu and Robinson 2012)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 48:
File: final.md
Line: 328
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference: NOT FOUND (key: '(Quine 1951)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 49:
File: final.md
Line: 330
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 50:
File: final.md
Line: 332
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 51:
File: final.md
Line: 434
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 52:
File: final.md
Line: 465
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 53:
File: final.md
Line: 496
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 54:
File: final.md
Line: 498
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND (key: '(Ladyman & Ross 2007)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 55:
File: final.md
Line: 502
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 56:
File: final.md
Line: 502
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 57:
File: final.md
Line: 506
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND (key: '(Baggio and Parravicini 2019)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 58:
File: final.md
Line: 512
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 59:
File: final.md
Line: 518
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 60:
File: final.md
Line: 522
Citation: (Christensen 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Christensen, David. 2007. "Epistemology of Disagreement: The Good News." *Philosophical Review* 116 (2): 187–217.
------------------------------------------------------------

Citation 61:
File: final.md
Line: 522
Citation: (Fricker 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Fricker, Elizabeth. 2007. *The Epistemology of Testimony*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 62:
File: final.md
Line: 522
Citation: (Kelly 2005)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 63:
File: final.md
Line: 528
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 64:
File: final.md
Line: 649
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference: NOT FOUND (key: '(Holling 1973)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 78:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 79:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND (key: '(Acemoglu and Robinson 2012)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 80:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND (key: '(Peirce 1878)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND (key: '(Acemoglu and Robinson 2012)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 94:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 95:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND (key: '(Baggio and Parravicini 2019)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 96:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 100:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND (key: '(Berlin and Kay 1969)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 101:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 102:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 103:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND (key: '(Leiter Reports 2023)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND (key: '(Wiley 2024)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND (key: '(MDPI 2025)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND (key: '(Wiley 2024)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND (key: '(Gaifman & Snir, 1982)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND (key: '(Kelly, 1996)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND (key: '(Plantinga, 1993)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 126:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND (key: '(Gaifman & Snir, 1982)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND (key: '(Kelly, 1996)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND (key: '(Plantinga, 1993)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 129:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND (key: '(Holling 1973)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND (key: '(Acemoglu and Robinson 2012)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND (key: '(Baggio and Parravicini 2019)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 147:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND (key: '(Holling 1973)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 148:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 149:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 150:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 151:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 152:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 153:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 154:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND (key: '(Acemoglu and Robinson 2012)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 155:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 156:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 157:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 159:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 160:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 161:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 162:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND (key: '(Baggio and Parravicini 2019)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 163:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 164:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 165:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 167:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND (key: '(Ladyman & Ross 2007)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND (key: '(Zollman 2013)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND (key: '(Acemoglu and Robinson 2012)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND (key: '(per Harding 1991)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 196:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND (key: '(Leiter Reports 2023)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND (key: '(Wiley 2024)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND (key: '(MDPI 2025)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND (key: '(Wiley 2024)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 200:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 202:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 204:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND (key: '(Zollman 2013)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND (key: '(Acemoglu and Robinson 2012)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND (key: '(per Harding 1991)')
Sample available keys: ['(Ayvazov 2025)', '(Ayvazov, Mahammad 2025)', 'Ayvazov 2025', 'Ayvazov, Mahammad 2025', '(Baggio 2019)']
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 228:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------


================================================================================
Citation Extraction Run - 228 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference: NOT FOUND
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference: NOT FOUND
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 39:
File: final.md
Line: 49
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 40:
File: final.md
Line: 170
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.
------------------------------------------------------------

Citation 41:
File: final.md
Line: 182
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 42:
File: final.md
Line: 192
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 43:
File: final.md
Line: 224
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 44:
File: final.md
Line: 251
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 45:
File: final.md
Line: 281
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 46:
File: final.md
Line: 314
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47:
File: final.md
Line: 316
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 48:
File: final.md
Line: 328
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference: NOT FOUND
------------------------------------------------------------

Citation 49:
File: final.md
Line: 330
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 50:
File: final.md
Line: 332
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 51:
File: final.md
Line: 434
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 52:
File: final.md
Line: 465
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 53:
File: final.md
Line: 496
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 54:
File: final.md
Line: 498
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND
------------------------------------------------------------

Citation 55:
File: final.md
Line: 502
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 56:
File: final.md
Line: 502
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 57:
File: final.md
Line: 506
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 58:
File: final.md
Line: 512
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 59:
File: final.md
Line: 518
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 60:
File: final.md
Line: 522
Citation: (Christensen 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Christensen, David. 2007. "Epistemology of Disagreement: The Good News." *Philosophical Review* 116 (2): 187–217.
------------------------------------------------------------

Citation 61:
File: final.md
Line: 522
Citation: (Fricker 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Fricker, Elizabeth. 2007. *The Epistemology of Testimony*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 62:
File: final.md
Line: 522
Citation: (Kelly 2005)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 63:
File: final.md
Line: 528
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 64:
File: final.md
Line: 649
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 78:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 79:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 80:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 94:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 95:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 96:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 100:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 101:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 102:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 103:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 126:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 129:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 147:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 148:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 149:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 150:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 151:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 152:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 153:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 154:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 155:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 156:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 157:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 159:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 160:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 161:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 162:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 163:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 164:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 165:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 167:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 196:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 200:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 202:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 204:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 228:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------


================================================================================
Citation Extraction Run - 228 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference: NOT FOUND
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference: NOT FOUND
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 39:
File: final.md
Line: 49
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 40:
File: final.md
Line: 170
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.
------------------------------------------------------------

Citation 41:
File: final.md
Line: 182
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 42:
File: final.md
Line: 192
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 43:
File: final.md
Line: 224
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 44:
File: final.md
Line: 251
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 45:
File: final.md
Line: 281
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 46:
File: final.md
Line: 314
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47:
File: final.md
Line: 316
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 48:
File: final.md
Line: 328
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference: NOT FOUND
------------------------------------------------------------

Citation 49:
File: final.md
Line: 330
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 50:
File: final.md
Line: 332
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 51:
File: final.md
Line: 434
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 52:
File: final.md
Line: 465
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 53:
File: final.md
Line: 496
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 54:
File: final.md
Line: 498
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND
------------------------------------------------------------

Citation 55:
File: final.md
Line: 502
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 56:
File: final.md
Line: 502
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 57:
File: final.md
Line: 506
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 58:
File: final.md
Line: 512
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 59:
File: final.md
Line: 518
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 60:
File: final.md
Line: 522
Citation: (Christensen 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Christensen, David. 2007. "Epistemology of Disagreement: The Good News." *Philosophical Review* 116 (2): 187–217.
------------------------------------------------------------

Citation 61:
File: final.md
Line: 522
Citation: (Fricker 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Fricker, Elizabeth. 2007. *The Epistemology of Testimony*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 62:
File: final.md
Line: 522
Citation: (Kelly 2005)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 63:
File: final.md
Line: 528
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 64:
File: final.md
Line: 649
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 78:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 79:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 80:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 94:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 95:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 96:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 100:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 101:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 102:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 103:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 126:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 129:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 147:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 148:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 149:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 150:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 151:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 152:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 153:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 154:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 155:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 156:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 157:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 159:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 160:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 161:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 162:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 163:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 164:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 165:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 167:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 196:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 200:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 202:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 204:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 228:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------


================================================================================
Citation Extraction Run - 228 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference: NOT FOUND
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference: NOT FOUND
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 39:
File: final.md
Line: 49
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 40:
File: final.md
Line: 170
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.
------------------------------------------------------------

Citation 41:
File: final.md
Line: 182
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 42:
File: final.md
Line: 192
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 43:
File: final.md
Line: 224
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 44:
File: final.md
Line: 251
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 45:
File: final.md
Line: 281
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 46:
File: final.md
Line: 314
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47:
File: final.md
Line: 316
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 48:
File: final.md
Line: 328
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference: NOT FOUND
------------------------------------------------------------

Citation 49:
File: final.md
Line: 330
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 50:
File: final.md
Line: 332
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 51:
File: final.md
Line: 434
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 52:
File: final.md
Line: 465
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 53:
File: final.md
Line: 496
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 54:
File: final.md
Line: 498
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND
------------------------------------------------------------

Citation 55:
File: final.md
Line: 502
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 56:
File: final.md
Line: 502
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 57:
File: final.md
Line: 506
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 58:
File: final.md
Line: 512
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 59:
File: final.md
Line: 518
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 60:
File: final.md
Line: 522
Citation: (Christensen 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Christensen, David. 2007. "Epistemology of Disagreement: The Good News." *Philosophical Review* 116 (2): 187–217.
------------------------------------------------------------

Citation 61:
File: final.md
Line: 522
Citation: (Fricker 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Fricker, Elizabeth. 2007. *The Epistemology of Testimony*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 62:
File: final.md
Line: 522
Citation: (Kelly 2005)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 63:
File: final.md
Line: 528
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 64:
File: final.md
Line: 649
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 78:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 79:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 80:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 94:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 95:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 96:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 100:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 101:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 102:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 103:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 126:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 129:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 147:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 148:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 149:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 150:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 151:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 152:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 153:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 154:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 155:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 156:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 157:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 159:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 160:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 161:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 162:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 163:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 164:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 165:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 167:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 196:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 200:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 202:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 204:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 228:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------


================================================================================
Citation Extraction Run - 228 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference: NOT FOUND
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference: NOT FOUND
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 39:
File: final.md
Line: 49
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 40:
File: final.md
Line: 170
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.
------------------------------------------------------------

Citation 41:
File: final.md
Line: 182
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 42:
File: final.md
Line: 192
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 43:
File: final.md
Line: 224
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 44:
File: final.md
Line: 251
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 45:
File: final.md
Line: 281
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 46:
File: final.md
Line: 314
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47:
File: final.md
Line: 316
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 48:
File: final.md
Line: 328
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference: NOT FOUND
------------------------------------------------------------

Citation 49:
File: final.md
Line: 330
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 50:
File: final.md
Line: 332
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 51:
File: final.md
Line: 434
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 52:
File: final.md
Line: 465
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 53:
File: final.md
Line: 496
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 54:
File: final.md
Line: 498
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND
------------------------------------------------------------

Citation 55:
File: final.md
Line: 502
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 56:
File: final.md
Line: 502
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 57:
File: final.md
Line: 506
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 58:
File: final.md
Line: 512
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 59:
File: final.md
Line: 518
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 60:
File: final.md
Line: 522
Citation: (Christensen 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Christensen, David. 2007. "Epistemology of Disagreement: The Good News." *Philosophical Review* 116 (2): 187–217.
------------------------------------------------------------

Citation 61:
File: final.md
Line: 522
Citation: (Fricker 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Fricker, Elizabeth. 2007. *The Epistemology of Testimony*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 62:
File: final.md
Line: 522
Citation: (Kelly 2005)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 63:
File: final.md
Line: 528
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 64:
File: final.md
Line: 649
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference: NOT FOUND
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 78:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 79:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 80:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 94:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 95:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 96:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 100:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 101:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 102:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 103:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 126:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 129:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 147:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference: NOT FOUND
------------------------------------------------------------

Citation 148:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 149:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 150:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 151:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 152:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------

Citation 153:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 154:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 155:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 156:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 157:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 159:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 160:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 161:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 162:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 163:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 164:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 165:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 167:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 196:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 200:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 202:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 204:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference: NOT FOUND
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference:
Mallapaty, Smriti. 2020b. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference:
Rottschaefer, William A. 2012b. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.
------------------------------------------------------------

Citation 228:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference:
Tauriainen, Teemu. 2017b. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.
------------------------------------------------------------


================================================================================
Citation Extraction Run - 228 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference: NOT FOUND
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference: NOT FOUND
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 39:
File: final.md
Line: 49
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 40:
File: final.md
Line: 170
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.
------------------------------------------------------------

Citation 41:
File: final.md
Line: 182
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 42:
File: final.md
Line: 192
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 43:
File: final.md
Line: 224
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 44:
File: final.md
Line: 251
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 45:
File: final.md
Line: 281
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 46:
File: final.md
Line: 314
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47:
File: final.md
Line: 316
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 48:
File: final.md
Line: 328
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference:
Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60, no. 1: 20–43. https://doi.org/10.2307/2181906.
------------------------------------------------------------

Citation 49:
File: final.md
Line: 330
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 50:
File: final.md
Line: 332
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 51:
File: final.md
Line: 434
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 52:
File: final.md
Line: 465
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 53:
File: final.md
Line: 496
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 54:
File: final.md
Line: 498
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND
------------------------------------------------------------

Citation 55:
File: final.md
Line: 502
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 56:
File: final.md
Line: 502
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 57:
File: final.md
Line: 506
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 58:
File: final.md
Line: 512
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 59:
File: final.md
Line: 518
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 60:
File: final.md
Line: 522
Citation: (Christensen 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Christensen, David. 2007. "Epistemology of Disagreement: The Good News." *Philosophical Review* 116 (2): 187–217.
------------------------------------------------------------

Citation 61:
File: final.md
Line: 522
Citation: (Fricker 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Fricker, Elizabeth. 2007. *The Epistemology of Testimony*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 62:
File: final.md
Line: 522
Citation: (Kelly 2005)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 63:
File: final.md
Line: 528
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference: NOT FOUND
------------------------------------------------------------

Citation 64:
File: final.md
Line: 649
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference: NOT FOUND
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference: NOT FOUND
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 78:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 79:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 80:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference: NOT FOUND
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 94:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 95:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 96:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 100:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 101:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 102:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 103:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference: NOT FOUND
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference: NOT FOUND
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 126:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 129:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 147:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 148:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 149:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 150:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 151:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 152:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 153:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 154:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 155:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 156:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 157:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 159:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 160:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 161:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 162:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 163:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 164:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 165:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 167:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference: NOT FOUND
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference: NOT FOUND
------------------------------------------------------------

Citation 196:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 200:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 202:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 204:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference: NOT FOUND
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference: NOT FOUND
------------------------------------------------------------

Citation 228:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference: NOT FOUND
------------------------------------------------------------


================================================================================
Citation Extraction Run - 228 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference: NOT FOUND
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference: NOT FOUND
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 39:
File: final.md
Line: 49
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 40:
File: final.md
Line: 170
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.
------------------------------------------------------------

Citation 41:
File: final.md
Line: 182
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 42:
File: final.md
Line: 192
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 43:
File: final.md
Line: 224
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 44:
File: final.md
Line: 251
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 45:
File: final.md
Line: 281
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 46:
File: final.md
Line: 314
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47:
File: final.md
Line: 316
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 48:
File: final.md
Line: 328
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference:
Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60, no. 1: 20–43. https://doi.org/10.2307/2181906.
------------------------------------------------------------

Citation 49:
File: final.md
Line: 330
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 50:
File: final.md
Line: 332
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 51:
File: final.md
Line: 434
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 52:
File: final.md
Line: 465
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 53:
File: final.md
Line: 496
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 54:
File: final.md
Line: 498
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND
------------------------------------------------------------

Citation 55:
File: final.md
Line: 502
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 56:
File: final.md
Line: 502
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 57:
File: final.md
Line: 506
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 58:
File: final.md
Line: 512
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 59:
File: final.md
Line: 518
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 60:
File: final.md
Line: 522
Citation: (Christensen 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Christensen, David. 2007. "Epistemology of Disagreement: The Good News." *Philosophical Review* 116 (2): 187–217.
------------------------------------------------------------

Citation 61:
File: final.md
Line: 522
Citation: (Fricker 2007)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Fricker, Elizabeth. 2007. *The Epistemology of Testimony*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 62:
File: final.md
Line: 522
Citation: (Kelly 2005)
Context:
## 7. Final Defense and Principled Limitations

Before defending against external objections, we clarify the relationship between this macro-epistemology and individual justification. Drawing on insights from the epistemology of disagreement (Christensen 2007) and testimony (Fricker 2007), the framework provides a robust theory of higher-order evidence. The diagnosed brittleness of a knowledge system provides a powerful defeater or corroborator for an individual's beliefs derived from that system. In a Bayesian framework (Kelly 2005), the diagnosed health of a source network determines an agent's rational prior probability. A claim from a low-brittleness network warrants a high prior; a claim from a high-brittleness network warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust.

### 7.1 A Falsifiable Research Program
Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 63:
File: final.md
Line: 528
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference: NOT FOUND
------------------------------------------------------------

Citation 64:
File: final.md
Line: 649
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference: NOT FOUND
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference: NOT FOUND
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 78:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 79:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 80:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference: NOT FOUND
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 94:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 95:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 96:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 100:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 101:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 102:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 103:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference: NOT FOUND
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference: NOT FOUND
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 126:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 129:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 147:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 148:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 149:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 150:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 151:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 152:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 153:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 154:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 155:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 156:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 157:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 159:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 160:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 161:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 162:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 163:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 164:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 165:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 167:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference: NOT FOUND
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference: NOT FOUND
------------------------------------------------------------

Citation 196:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 200:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 202:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 204:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference: NOT FOUND
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference: NOT FOUND
------------------------------------------------------------

Citation 228:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference: NOT FOUND
------------------------------------------------------------


================================================================================
Citation Extraction Run - 229 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference: NOT FOUND
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference: NOT FOUND
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 39:
File: final.md
Line: 52
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 40:
File: final.md
Line: 173
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.
------------------------------------------------------------

Citation 41:
File: final.md
Line: 185
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 42:
File: final.md
Line: 195
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 43:
File: final.md
Line: 227
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 44:
File: final.md
Line: 254
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 45:
File: final.md
Line: 284
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 46:
File: final.md
Line: 317
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47:
File: final.md
Line: 319
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 48:
File: final.md
Line: 331
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference:
Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60, no. 1: 20–43. https://doi.org/10.2307/2181906.
------------------------------------------------------------

Citation 49:
File: final.md
Line: 333
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 50:
File: final.md
Line: 335
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 51:
File: final.md
Line: 437
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 52:
File: final.md
Line: 468
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 53:
File: final.md
Line: 499
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 54:
File: final.md
Line: 501
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND
------------------------------------------------------------

Citation 55:
File: final.md
Line: 505
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 56:
File: final.md
Line: 505
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 57:
File: final.md
Line: 509
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 58:
File: final.md
Line: 515
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 59:
File: final.md
Line: 521
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 60:
File: final.md
Line: 533
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.² The specific metrics and dynamic equations underlying this research program are detailed in the Mathematical Appendix.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference: NOT FOUND
------------------------------------------------------------

Citation 61:
File: final.md
Line: 654
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference: NOT FOUND
------------------------------------------------------------

Citation 62:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 63:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.
------------------------------------------------------------

Citation 64:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference: NOT FOUND
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 78:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 79:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 80:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference: NOT FOUND
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 94:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 95:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 96:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 100:
File: grok_ref.md
Line: 59
Citation: (originally 1954)
Context:
### 18. Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
This book develops social epistemology, emphasizing veritistic value and practices that promote true beliefs. It applies epistemology to social institutions.

### 19. Goodman, Nelson. 1983. *Fact, Fiction, and Forecast*. 4th ed. Cambridge, MA: Harvard University Press (originally 1954).
This book addresses induction and confirmation, introducing the "new riddle of induction" with "grue." It explores projectibility and counterfactuals.

### 20. Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
Reference: NOT FOUND
------------------------------------------------------------

Citation 101:
File: grok_ref.md
Line: 92
Citation: (originally 1962)
Context:
### 29. Kornblith, Hilary. 1993. *Knowledge and Its Place in Nature*. Oxford: Clarendon Press (published 2002, but listed as 1993 in some sources; verified 2002).
This book naturalizes epistemology, treating knowledge as a natural kind studied empirically. It critiques traditional a priori approaches.

### 30. Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press (originally 1962).
This classic argues science progresses through paradigm shifts, not linear accumulation. It introduces incommensurability and normal science.

### 31. Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50(1): 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
Reference: NOT FOUND
------------------------------------------------------------

Citation 102:
File: grok_ref.md
Line: 146
Citation: (originally 1878)
Context:
### 47. Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press.
This book critiques coherence theories, arguing coherence does not imply truth-conduciveness. It employs probabilistic analysis.

### 48. Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press (originally 1878).
This essay introduces the pragmatic maxim, clarifying concepts through practical effects. It critiques Cartesian clarity.

### 49. Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37(7): 1948–70. https://doi.org/10.1080/09515089.2023.2236120.
Reference: NOT FOUND
------------------------------------------------------------

Citation 103:
File: grok_ref.md
Line: 152
Citation: (originally 1934)
Context:
### 49. Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37(7): 1948–70. https://doi.org/10.1080/09515089.2023.2236120.
This paper introduces moral affordances as opportunities for fitting moral actions, linking to relational demands. It critiques alternative views.

### 50. Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson (originally 1934).
This book advocates falsificationism, emphasizing testable predictions over verification. It critiques inductivism.

### 51. Price, Huw. 1992. "Metaphysical Pluralism." *Journal of Philosophy* 89(8): 387–409. https://doi.org/10.2307/2940975.
Reference: NOT FOUND
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference: NOT FOUND
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference: NOT FOUND
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 126:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 129:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 147:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 148:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 149:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 150:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 151:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 152:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 153:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 154:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 155:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 156:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 157:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 159:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 160:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 161:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 162:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 163:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 164:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 165:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 167:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference: NOT FOUND
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 196:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference: NOT FOUND
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 200:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 202:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 204:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference: NOT FOUND
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 228:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference: NOT FOUND
------------------------------------------------------------

Citation 229:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference: NOT FOUND
------------------------------------------------------------


================================================================================
Citation Extraction Run - 229 citations found
================================================================================

Citation 1:
File: arch_v16.2.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory’s focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not a viable system in these terms, but a textbook case of a high-brittleness one; its longevity is a measure of the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system’s capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 2:
File: arch_v16.2.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 3:
File: arch_v16.2.md
Line: 97
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 4:
File: arch_v16.2.md
Line: 107
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 5:
File: arch_v16.2.md
Line: 109
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 6:
File: arch_v16.2.md
Line: 145
Citation: (Peirce 1878)
Context:
### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 7:
File: arch_v16.2.md
Line: 162
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 8:
File: arch_v16.2.md
Line: 187
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 9:
File: arch_v16.2.md
Line: 203
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 10:
File: arch_v16.2.md
Line: 207
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 11:
File: arch_v16.2.md
Line: 215
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 12:
File: arch_v16.2.md
Line: 215
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 13:
File: arch_v16.2.md
Line: 229
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 14:
File: arch_v16.2.md
Line: 247
Citation: (Baggio and Parravicini 2019)
Context:
### 6.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 15:
File: arch_v16.2.md
Line: 255
Citation: (Worrall 1989)
Context:
### **6.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 16:
File: arch_v16.2.md
Line: 404
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 17:
File: assessment.md
Line: 870
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 18:
File: assessment.md
Line: 883
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 19:
File: assessment.md
Line: 928
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 20:
File: assessment.md
Line: 928
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 21:
File: assessment.md
Line: 941
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 22:
File: assessment.md
Line: 950
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 23:
File: assessment.md
Line: 1009
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference: NOT FOUND
------------------------------------------------------------

Citation 24:
File: assessment.md
Line: 1018
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 25:
File: assessment.md
Line: 1088
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 26:
File: assessment.md
Line: 1142
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 27:
File: assessment.md
Line: 1142
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 28:
File: assessment.md
Line: 1168
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference: NOT FOUND
------------------------------------------------------------

Citation 29:
File: assessment.md
Line: 1214
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 30:
File: assessment.md
Line: 1237
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 31:
File: assessment.md
Line: 1273
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 32:
File: assessment.md
Line: 1302
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 33:
File: final.md
Line: 9
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 34:
File: final.md
Line: 11
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 35:
File: final.md
Line: 11
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 36:
File: final.md
Line: 11
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? Standard answers cite superior evidence, but a deeper analysis reveals systemic viability. Although miasma theory's sanitation focus had some positive effects, its core principles were degenerating. The network demonstrated brittleness: catastrophic costs (thousands died in London from misdirected public health efforts), and accelerating ad hoc patches (why was "bad air" only deadly near specific water pumps?) (Snow 1855). Germ theory proved vastly more resilient, dramatically reducing costs through effective interventions while explaining diverse phenomena with a single conceptual tool.

This dynamic highlights coherentism's isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality (BonJour 1985). While coherentists have developed responses (Lehrer 1990; Olsson 2005; Kvanvig 2012), most rely on internalist resources failing to provide necessary external constraint. Scholars have made compelling cases for a structured, asymmetrical web of belief within Quine's framework (Carlson 2015), but the question of what external pressures forge this structure remains. This paper grounds coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences—pragmatic pushback—drive bottom-up formation of viable public knowledge systems.

Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology necessarily generating optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is discovery, not creation. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward peaks emerging necessarily from reality's organization.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 37:
File: final.md
Line: 29
Citation: (Meadows 2008)
Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 38:
File: final.md
Line: 33
Citation: (Moghaddam 2013)
Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Moghaddam 2013), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 39:
File: final.md
Line: 52
Citation: (Mesoudi 2011)
Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 40:
File: final.md
Line: 173
Citation: (Popper 1959)
Context:
### 4.1 A Negative Methodology: Charting What Fails

Constructing our reef chart begins with systematically cataloguing shipwrecks. Our account of objectivity begins not with speculative visions of final truth, but with the most unambiguous empirical evidence: large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:
Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.
------------------------------------------------------------

Citation 41:
File: final.md
Line: 185
Citation: (Peirce 1878)
Context:
### 4.2 The Apex Network: An Emergent Structure of Modal Necessity

Filtering out high-brittleness systems is not merely destructive. As unviable designs enter the Negative Canon, pragmatic selection reveals contours of an objective structure all successful inquiry is forced to approximate: the Apex Network. This is not a pre-existing metaphysical blueprint, nor reality's territory itself, nor merely our current consensus. The Apex Network is the theoretical limit-point of convergence, resonating with the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not accident but emergent structural fact demanding naturalistic explanation. Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals—safe water, ripe fruit—conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.
Reference: NOT FOUND
------------------------------------------------------------

Citation 42:
File: final.md
Line: 195
Citation: (Rescher 1996)
Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 43:
File: final.md
Line: 227
Citation: (Newman 2010)
Context:
#### 4.2.4 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 44:
File: final.md
Line: 254
Citation: (Tauriainen 2017)
Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 45:
File: final.md
Line: 284
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 46:
File: final.md
Line: 317
Citation: (Wright 1932)
Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 47:
File: final.md
Line: 319
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 48:
File: final.md
Line: 331
Citation: (Quine 1951, 1960)
Context:
### 5.1 Animating the Web of Belief

Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.
Reference:
Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60, no. 1: 20–43. https://doi.org/10.2307/2181906.
------------------------------------------------------------

Citation 49:
File: final.md
Line: 333
Citation: (Simon 1972)
Context:
Quine's static "Web of Belief" (Quine 1951, 1960) lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 50:
File: final.md
Line: 335
Citation: (Carlson 2015)
Context:
Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### 5.2 Mathematics as a Paradigm Case of Internal Brittleness
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 51:
File: final.md
Line: 437
Citation: (Harding 1991)
Context:
#### 5.2.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 52:
File: final.md
Line: 468
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 53:
File: final.md
Line: 499
Citation: (Worrall 1989)
Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 54:
File: final.md
Line: 501
Citation: (Ladyman & Ross 2007)
Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms
Reference: NOT FOUND
------------------------------------------------------------

Citation 55:
File: final.md
Line: 505
Citation: (Goldman 1979)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 56:
File: final.md
Line: 505
Citation: (Zagzebski 1996)
Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 57:
File: final.md
Line: 509
Citation: (Baggio and Parravicini 2019)
Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 58:
File: final.md
Line: 515
Citation: (Sims 2024)
Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

### 6.3 Distinguishing from Lakatos and Laudan
Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 59:
File: final.md
Line: 521
Citation: (Pritchard 2016)
Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## 7. Final Defense and Principled Limitations
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 60:
File: final.md
Line: 533
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.² The specific metrics and dynamic equations underlying this research program are detailed in the Mathematical Appendix.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### 7.2 Principled Limitations and Scope
Reference: NOT FOUND
------------------------------------------------------------

Citation 61:
File: final.md
Line: 669
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example
Reference: NOT FOUND
------------------------------------------------------------

Citation 62:
File: gemini-again.md
Line: 23
Citation: (Snow 1855)
Context:
#### **1.1 The Isolation Objection and the Quinean Web**

This paper addresses a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality. The historical replacement of miasma theory with germ theory provides a canonical illustration. While standard accounts cite superior evidence, a deeper view reveals a contest of systemic viability. Miasma theory proved brittle: it generated catastrophic costs—thousands died in London from misdirected efforts against odors (Snow 1855)—and demanded accelerating ad hoc fixes for anomalies, such as why "bad air" was only deadly near certain water pumps. Germ theory, by contrast, proved resilient and adaptive, dramatically reducing these costs while unifying diverse phenomena under a single powerful tool. This dynamic suggests that the solution to the isolation objection lies not within the internal logic of our beliefs, but in the external, pragmatic consequences of the systems they create.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 63:
File: gemini-again.md
Line: 33
Citation: (Taleb 2012)
Context:
This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle—a system's vulnerability to collapse from accumulated hidden costs, analogous to fragility (Taleb 2012); its longevity measures wasted energy suppressing instability. Brittleness is a diachronic, systemic property of a research program in action, not a timeless property of a proposition; a new theory might be false but hasn't yet accumulated systemic costs like conceptual debt or coercive overheads. Psychologically "fit" but pragmatically brittle ideas, like conspiracy theories, are informational viruses—transmissible but not viable. Such informational viruses are diagnosed by their characteristic signatures of high brittleness—particularly the massive coercive overheads (C(t)) required to maintain adherence in the face of persistent pragmatic failure. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

The framework's contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.
Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.
------------------------------------------------------------

Citation 64:
File: gemini-again.md
Line: 43
Citation: (Holling 1973)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 65:
File: gemini-again.md
Line: 43
Citation: (Meadows 2008)
Context:
### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with resilience theory in systems ecology (Holling 1973) and complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 66:
File: gemini-again.md
Line: 53
Citation: (Mesoudi 2011)
Context:
Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—are promoted to **Standing Predicates**: reusable conceptual tools for evaluating new cases. The term is chosen to connect with, yet distinguish from, predicates in formal logic. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs, turning tested data into a trusted testing tool.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure, functioning as replicators of ideas (Mesoudi 2011). The network's informational structure functions as the replicator—the code copied and transmitted—while social groups and institutions function as the interactor—the vessel expressing and testing this code.

#### **2.2 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 67:
File: gemini-again.md
Line: 168
Citation: (Kim 1988)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 68:
File: gemini-again.md
Line: 168
Citation: (Putnam 2002)
Context:
#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.
Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 69:
File: gemini-again.md
Line: 170
Citation: (Moghaddam 2013)
Context:
Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 70:
File: gemini-again.md
Line: 221
Citation: (Tauriainen 2017)
Context:
#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
Reference: NOT FOUND
------------------------------------------------------------

Citation 71:
File: gemini-again.md
Line: 251
Citation: (BonJour 1985)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 72:
File: gemini-again.md
Line: 251
Citation: (Carlson 2015)
Context:
#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 73:
File: gemini-again.md
Line: 253
Citation: (Worrall 1989)
Context:
Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 74:
File: gemini-again.md
Line: 259
Citation: (Longino 2002)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 75:
File: gemini-again.md
Line: 259
Citation: (Harding 1991)
Context:
While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice suffer from the parochialism problem: they lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 76:
File: gemini-again.md
Line: 279
Citation: (Acemoglu and Robinson 2012)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference: NOT FOUND
------------------------------------------------------------

Citation 77:
File: gemini-again.md
Line: 279
Citation: (Harding 1991)
Context:
First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Marginalized perspectives often function as "early warning systems" for rising brittleness, providing qualitative data on hidden systemic costs long before macro-level quantitative metrics become visible.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 78:
File: gemini.md
Line: 12
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

Systemic externalism holds that justification requires two conditions: internal coherence within a shared network and the network's demonstrated reliability through low brittleness.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 79:
File: gemini.md
Line: 24
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 80:
File: gemini.md
Line: 115
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness." To reiterate without redundancy, systemic brittleness—whether epistemic or normative—is distinguished from mere longevity: a system may endure via high energy expenditure but lacks viability if it cannot adapt at low cost (as qualified in Section 1).
Reference: NOT FOUND
------------------------------------------------------------

Citation 81:
File: gemini.md
Line: 127
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 82:
File: gemini.md
Line: 129
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 83:
File: gemini.md
Line: 171
Citation: (Peirce 1878)
Context:
The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. The *Apex Network* is the emergent, mind-independent structure of viable principles (ontologically real, epistemically a regulative ideal we approximate), contrasted with the *Consensus Network*, our fallible reconstruction certified by low brittleness. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like '{associating blue with positive, stable conditions} is a viable perceptual default' is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. One might object that this risks conflating epistemic accessibility with ontological reality; however, the example shows how emergent patterns can be real (exerting causal influence on behavior) while remaining fallibly approximated through inquiry.
Reference: NOT FOUND
------------------------------------------------------------

Citation 84:
File: gemini.md
Line: 192
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 85:
File: gemini.md
Line: 219
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 86:
File: gemini.md
Line: 235
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 87:
File: gemini.md
Line: 239
Citation: (Carlson 2015)
Context:
### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## **7. Situating the Framework: Systemic Externalism and Its Relations**
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 88:
File: gemini.md
Line: 247
Citation: (Kvanvig 2012)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 89:
File: gemini.md
Line: 247
Citation: (Carlson 2015)
Context:
### **7.1 Addressing the Isolation Objection in Coherentism**

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 90:
File: gemini.md
Line: 251
Citation: (Carlson 2015)
Context:
This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2019) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 91:
File: gemini.md
Line: 265
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### **7.3 Cultural Evolution and the Problem of Fitness**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 92:
File: gemini.md
Line: 289
Citation: (Baggio and Parravicini 2019)
Context:
### **7.4 A Realist Corrective to Neopragmatism**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 93:
File: gemini.md
Line: 297
Citation: (Worrall 1989)
Context:
### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 94:
File: gemini_.md
Line: 21
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 95:
File: gemini_.md
Line: 46
Citation: (Mesoudi 2011)
Context:
*   **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition. When applied, it unpacks a suite of previously validated knowledge.
*   **Shared Network:** An observable consequence of Quine's holism applied socially. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested (e.g., germ theory within medicine).

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory (Mesoudi 2011). The network’s abstract informational structure functions as the **replicator**: the "code" that is copied. The social group and its institutions function as the **interactor**: the physical "vessel" through which this code is expressed and tested.

### **2.3 Pragmatic Pushback and Systemic Costs**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 96:
File: gemini_.md
Line: 71
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine, we treat normative epistemology as a form of engineering, where norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on a constitutive argument: any system engaged in a cumulative, inter-generational project must maintain sufficient stability to preserve and transmit knowledge. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 97:
File: gemini_.md
Line: 101
Citation: (Berlin and Kay 1969)
Context:
Historical filtering is therefore the **discovery process**, not the creation mechanism. This gives the Apex Network counterfactual stability: if history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by constraints, not by historical contingency.

Ontologically, the Apex Network is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain. To clarify this naturalized status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference seems contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology, where efficiently tracking ecologically critical signals conferred a viability advantage (Berlin and Kay 1969). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule but a point of maximal, stable convergence. The Apex Network, we argue, has the same ontological status: it is not a pre-existing entity to be found, but the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

### **4.3 A Three-Level Framework for Truth**
Reference: NOT FOUND
------------------------------------------------------------

Citation 98:
File: gemini_.md
Line: 131
Citation: (Harding 1991)
Context:
### **6.1 An Evolutionary Grounding for Social Epistemology**

Our framework provides a naturalistic foundation for the core insights of social epistemology (e.g., Longino 2002). Social epistemic procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the crucial externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991): marginalized perspectives can be a privileged source of data about systemic flaws.

### **6.2 Refining Philosophy of Science: Lakatos and Laudan**
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 99:
File: gemini_.md
Line: 143
Citation: (Worrall 1989)
Context:
### **6.4 A Naturalistic Engine for Structural Realism**

Our concept of the Apex Network shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. The Apex Network *is* the complete set of viable relational structures, but it is an emergent structural fact about our world, discovered retrospectively through the historical process of culling what fails. High-brittleness networks collapse and enter the Negative Canon. Low-brittleness networks survive. This failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

### **6.5 Mathematics as a Paradigm Case of Internal Brittleness**
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 100:
File: grok_ref.md
Line: 59
Citation: (originally 1954)
Context:
### 18. Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
This book develops social epistemology, emphasizing veritistic value and practices that promote true beliefs. It applies epistemology to social institutions.

### 19. Goodman, Nelson. 1983. *Fact, Fiction, and Forecast*. 4th ed. Cambridge, MA: Harvard University Press (originally 1954).
This book addresses induction and confirmation, introducing the "new riddle of induction" with "grue." It explores projectibility and counterfactuals.

### 20. Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
Reference: NOT FOUND
------------------------------------------------------------

Citation 101:
File: grok_ref.md
Line: 92
Citation: (originally 1962)
Context:
### 29. Kornblith, Hilary. 1993. *Knowledge and Its Place in Nature*. Oxford: Clarendon Press (published 2002, but listed as 1993 in some sources; verified 2002).
This book naturalizes epistemology, treating knowledge as a natural kind studied empirically. It critiques traditional a priori approaches.

### 30. Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press (originally 1962).
This classic argues science progresses through paradigm shifts, not linear accumulation. It introduces incommensurability and normal science.

### 31. Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50(1): 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
Reference: NOT FOUND
------------------------------------------------------------

Citation 102:
File: grok_ref.md
Line: 146
Citation: (originally 1878)
Context:
### 47. Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press.
This book critiques coherence theories, arguing coherence does not imply truth-conduciveness. It employs probabilistic analysis.

### 48. Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press (originally 1878).
This essay introduces the pragmatic maxim, clarifying concepts through practical effects. It critiques Cartesian clarity.

### 49. Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37(7): 1948–70. https://doi.org/10.1080/09515089.2023.2236120.
Reference: NOT FOUND
------------------------------------------------------------

Citation 103:
File: grok_ref.md
Line: 152
Citation: (originally 1934)
Context:
### 49. Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37(7): 1948–70. https://doi.org/10.1080/09515089.2023.2236120.
This paper introduces moral affordances as opportunities for fitting moral actions, linking to relational demands. It critiques alternative views.

### 50. Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson (originally 1934).
This book advocates falsificationism, emphasizing testable predictions over verification. It critiques inductivism.

### 51. Price, Huw. 1992. "Metaphysical Pluralism." *Journal of Philosophy* 89(8): 387–409. https://doi.org/10.2307/2940975.
Reference: NOT FOUND
------------------------------------------------------------

Citation 104:
File: late_notes.md
Line: 157
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 105:
File: late_notes.md
Line: 398
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 106:
File: late_notes.md
Line: 421
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 107:
File: late_notes.md
Line: 466
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 108:
File: late_notes.md
Line: 1507
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 109:
File: late_notes.md
Line: 1546
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 110:
File: late_notes.md
Line: 1557
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 111:
File: late_notes.md
Line: 2575
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter...
> 

**Benefit:** This frames Carlson's work as laying the groundwork for yours. He establishes the *need* for a structured web; you provide the *engine* that builds it.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 112:
File: late_notes.md
Line: 2588
Citation: (Carlson 2015)
Context:
**Proposed Addition:**

> ...This framework offers a distinctively externalist solution. Rather than seeking stronger internal constraints, it grounds coherence in the external performance of knowledge systems over time. This approach complements internalist reconstructions of Quine that argue for a "systematic structure" where core beliefs are functionally indispensable (Carlson 2015). Our framework provides the causal, evolutionary explanation for this indispensability: a principle becomes part of the system's core not by a priori fiat, but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. A belief achieves full justification only when it meets a two-level condition: internal coherence within a network, and demonstrated reliability of that network itself...
> 

**Benefit:** This is the most sophisticated move. You are using Carlson's work to enrich your own argument. You are saying, "Carlson is right that the web has a systematic, foundational structure. My theory of pragmatic selection and systemic viability is the naturalistic mechanism that *explains how* that structure is built and maintained over time." This elevates your contribution from merely "adding dynamism" to providing the deep causal explanation for a known structural feature of Quine's web.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 113:
File: late_notes.md
Line: 2633
Citation: (Kim 1988)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 114:
File: late_notes.md
Line: 2633
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal.
> 

**Benefit:** This is a high-impact citation. It immediately situates your paper within a major debate in epistemology and shows that your "engineering approach" is a direct, sophisticated response to it.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 115:
File: late_notes.md
Line: 2646
Citation: (Moghaddam 2013)
Context:
**Proposed Addition:**

> By adding these two dynamics, EPC transforms Quine's web from a static logical structure into a dynamic, evolving system. It provides a testable, historical explanation for how the web’s most crucial components are forged and pressure-tested over time, thereby providing a fully naturalized account of the normativity inherent in Quine's own "engineering model" of inquiry (Moghaddam 2013).
> 

**Benefit:** This reinforces that your model isn't just *like* Quine's, but is a direct and powerful extension of his own project for defending a normative, naturalized epistemology.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 116:
File: late_notes.md
Line: 2655
Citation: (Moghaddam 2013)
Context:
**In-text citation:**

> (Moghaddam 2013)
> 

**Reference list entry:**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 117:
File: late_notes.md
Line: 2714
Citation: (Tauriainen 2017)
Context:
**Proposed Addition:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective standard our inquiry aims at (The Apex Network) and our current, best approximation of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

**Benefit:** This makes your glossary entry even sharper, showing how your conceptual distinction does real philosophical work.
Reference: NOT FOUND
------------------------------------------------------------

Citation 118:
File: late_notes.md
Line: 2723
Citation: (Tauriainen 2017)
Context:
**In-text citation:**

> (Tauriainen 2017)
> 

**Reference list entry:**
Reference: NOT FOUND
------------------------------------------------------------

Citation 119:
File: late_notes.md
Line: 2793
Citation: (Haack 1993)
Context:
**In-text citation:**

> (Haack 1993)
> 

**Reference list entry:**
Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.
------------------------------------------------------------

Citation 120:
File: late_notes.md
Line: 2847
Citation: (Kim 1988)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 121:
File: late_notes.md
Line: 2847
Citation: (Moghaddam 2013)
Context:
**Proposed Enhanced Text:**

> This focus on viability provides a robust answer to the classic "normativity objection" against naturalized epistemology—the charge that a descriptive science of knowledge cannot ground prescriptive 'oughts' (Kim 1988). Following Quine's own later work, the solution is to frame normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives grounded in a practical goal. Our framework makes this engineering model concrete: the goal is the cultivation of low-brittleness systems, and our norms are the evidence-based strategies for achieving that goal. The model's authority is therefore not grounded in a chosen value, but in a constitutive condition for the practice of cumulative, inter-generational inquiry itself... (continue with the rest of the section).
> 

### **2. Revise Section 4.3: A Three-Level Framework for Truth**
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 122:
File: late_notes.md
Line: 2873
Citation: (Tauriainen 2017)
Context:
**Proposed Enhanced Entry:**

> The Objective Standard vs. Our Best Approximation
This clarifies the crucial distinction between the objective structure of viability our inquiry aims at (The Apex Network) and our current, fallible map of it (The Consensus Network). This distinction resolves a long-standing tension in Quine scholarship between his view of truth as immanent to our current best theory and his realist commitment to truth as a transcendent goal (Tauriainen 2017).
> 

### **4. Add the Citations to Your Reference List (Chicago Style)**
Reference: NOT FOUND
------------------------------------------------------------

Citation 123:
File: late_notes.md
Line: 2919
Citation: (Baysan 2025)
Context:
**Proposed Addition:**

> To be precise about its status, the Apex Network is not a pre-existing blueprint of truth waiting to be discovered... It is best understood as the emergent, trans-historical set of propositions and principles that achieve maximal, stable convergence across shared networks. This model of epistemic emergence is structurally analogous to contemporary models of moral emergence, which hold that normative properties can arise from and depend on descriptive properties without being reducible to them (Baysan 2025). Its objectivity is therefore grounded not in a top-down correspondence...
> 

**Benefit:** This single citation powerfully reinforces your ontological claim. It shows that "emergence" is not just a loose metaphor for you, but a concept with real currency in contemporary metaphysics.
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 124:
File: late_notes.md
Line: 2942
Citation: (Baysan 2025)
Context:
**In-text citation:**

> (Baysan 2025)
> 

**Reference list entry:**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 125:
File: late_notes.md
Line: 2978
Citation: (Baysan 2025)
Context:
> While the framework for assessing brittleness is universal, its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints that a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.
> 
> - **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
> - **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the **emergent normative structure of the world**. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. While this misalignment also generates first-order costs (social decay, instability), it is most acutely diagnosed through its unique systemic costs, which can be understood as the price of fighting against **noncausal normative powers (Baysan 2025)**. For example, a society predicated on slavery exhibits profound normative brittleness. The immense **coercive overheads** required to maintain the institution are a direct measure of the energy needed to suppress the noncausal power of injustice—its inherent tendency to justify resentment and require condemnation. The system is brittle not just because it is economically inefficient, but because it is in a constant state of struggle against the normative relations it violates.
> 
> The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
>
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 126:
File: late_notes.md
Line: 3007
Citation: (Baysan 2025)
Context:
**Proposed Revision of the Core Argument in Section 6.7:**

> Our systems-level approach forms a powerful and constructive synthesis with agent-focused, naturalist forms of moral realism. More fundamentally, our Emergent Pragmatic Coherentism can be seen as the naturalistic epistemology that explains how we discover the kinds of objective moral properties posited by theories like Emergent Moral Non-Naturalism (Baysan 2025).
> 
> 
> On Baysan's view, moral properties are real, noncausal, and emergent. But how could we ever come to know about them? Our framework provides the answer: we discover them *negatively* and *historically*. A society that builds its legal and economic system on principles that violate these emergent moral facts will begin to accumulate **Normative Brittleness**. It will exhibit rising coercive overheads, social fragmentation, and systemic instability. These measurable, empirical symptoms are the epistemic signals that the society's core principles are misaligned with the objective normative landscape. **Epistemic progress in the normative domain, therefore, is the process of identifying and replacing high-brittleness normative principles with more viable, low-brittleness alternatives.**
Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 127:
File: mathy.md
Line: 927
Citation: (Gaifman & Snir, 1982)
Context:
**4. The Apex as Bayesian Convergence**

Bayesian convergence theorems (Gaifman & Snir, 1982) prove that agents with different priors converge given sufficient shared evidence. But these theorems require strong assumptions:

- Agents consider all hypotheses
- Evidence is fully shared
Reference: NOT FOUND
------------------------------------------------------------

Citation 128:
File: mathy.md
Line: 966
Citation: (Kelly, 1996)
Context:
EPC provides a complementary imprecision measure: systems with high SBI should have *wider* credence intervals because their predictions are less reliable. We can formalize: σ(SBI) = uncertainty measure.

**Formal Learning Theory (Kelly, 1996):**
Studies which truths are learnable "in the limit" given computable inquiry methods.

EPC adds: learnability isn't just about logical possibility but pragmatic viability. Even logically learnable truths may be unreachable if the learning path crosses high-SBI valleys that cause civilizational collapse before learning completes.
Reference: NOT FOUND
------------------------------------------------------------

Citation 129:
File: mathy.md
Line: 977
Citation: (Plantinga, 1993)
Context:
Coherentism (BonJour, 1985; Lehrer, 1990) holds that beliefs are justified by their coherence with other beliefs, not by foundational certainties or correspondence to reality.

The classic objection (Plantinga, 1993): a perfectly coherent system of beliefs could be entirely false—a sophisticated delusion. If coherence is purely internal, what guarantees contact with reality?

### Quine's Partial Solution
Reference: NOT FOUND
------------------------------------------------------------

Citation 130:
File: other - Copy.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 131:
File: other - Copy.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 132:
File: other - Copy.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 133:
File: other - Copy.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 134:
File: other - Copy.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 135:
File: other - Copy.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 136:
File: other - Copy.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 137:
File: other - Copy.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 138:
File: other - Copy.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 139:
File: other - Copy.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 140:
File: other - Copy.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 141:
File: other - Copy.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 142:
File: other - Copy.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 143:
File: other - Copy.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 144:
File: other - Copy.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 145:
File: other - Copy.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 146:
File: other - Copy.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 147:
File: other - Copy.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 148:
File: other.md
Line: 15
Citation: (Holling 1973)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.
Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 149:
File: other.md
Line: 31
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 150:
File: other.md
Line: 222
Citation: (Rottschaefer 2012)
Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the causal structure of the world. It is found in descriptive knowledge systems like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.
Reference: NOT FOUND
------------------------------------------------------------

Citation 151:
File: other.md
Line: 232
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 152:
File: other.md
Line: 234
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013), where epistemic norms are hypothetical imperatives directed at a practical goal. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 153:
File: other.md
Line: 405
Citation: (Tauriainen 2017)
Context:
### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
Reference: NOT FOUND
------------------------------------------------------------

Citation 154:
File: other.md
Line: 443
Citation: (Simon 1972)
Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 155:
File: other.md
Line: 492
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness but that the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.
Reference: NOT FOUND
------------------------------------------------------------

Citation 156:
File: other.md
Line: 508
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **5.2 The Payoff: An Animated Web**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 157:
File: other.md
Line: 512
Citation: (Carlson 2015)
Context:
### **5.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 158:
File: other.md
Line: 520
Citation: (Kvanvig 2012)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 159:
File: other.md
Line: 520
Citation: (Carlson 2015)
Context:
### 6.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 160:
File: other.md
Line: 534
Citation: (Harding 1991)
Context:
This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 6.3 Cultural Evolution and the Problem of Fitness
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 161:
File: other.md
Line: 624
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 162:
File: other.md
Line: 652
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 163:
File: other.md
Line: 688
Citation: (Baggio and Parravicini 2019)
Context:
### 6.5 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.
Reference: NOT FOUND
------------------------------------------------------------

Citation 164:
File: other.md
Line: 696
Citation: (Worrall 1989)
Context:
### 6.6 A Naturalistic Engine for Structural Realism

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 165:
File: other.md
Line: 985
Citation: (Mesoudi 2011)
Context:
The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 166:
File: other_revisions.md
Line: 516
Citation: (Simon 1972)
Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 167:
File: other_revisions.md
Line: 544
Citation: (Harding 1991)
Context:
#### 6.4.5 Power and Suppression in Mathematics

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 168:
File: paper.md
Line: 10
Citation: (Snow 1855)
Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).
Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.
------------------------------------------------------------

Citation 169:
File: paper.md
Line: 12
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 170:
File: paper.md
Line: 35
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 171:
File: paper.md
Line: 59
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 172:
File: paper.md
Line: 164
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 173:
File: paper.md
Line: 192
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 174:
File: paper.md
Line: 194
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 175:
File: paper.md
Line: 194
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 176:
File: paper.md
Line: 292
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 177:
File: paper.md
Line: 318
Citation: (Worrall 1989)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 178:
File: paper.md
Line: 318
Citation: (Rescher 1996)
Context:
To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.
Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.
------------------------------------------------------------

Citation 179:
File: paper.md
Line: 320
Citation: (Ladyman & Ross 2007)
Context:
This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.
Reference: NOT FOUND
------------------------------------------------------------

Citation 180:
File: paper.md
Line: 378
Citation: (Simon 1972)
Context:
A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 181:
File: paper.md
Line: 396
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.
------------------------------------------------------------

Citation 182:
File: paper.md
Line: 410
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 183:
File: paper.md
Line: 443
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 184:
File: paper.md
Line: 447
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 185:
File: paper.md
Line: 465
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 186:
File: paper.md
Line: 467
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 187:
File: paper.md
Line: 469
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 188:
File: paper.md
Line: 496
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 189:
File: paper.md
Line: 504
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 190:
File: paper.md
Line: 506
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference: NOT FOUND
------------------------------------------------------------

Citation 191:
File: paper.md
Line: 514
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 192:
File: paper.md
Line: 514
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 193:
File: paper.md
Line: 516
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 194:
File: paper.md
Line: 520
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 195:
File: paper.md
Line: 526
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 196:
File: paper.md
Line: 572
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference: NOT FOUND
------------------------------------------------------------

Citation 197:
File: reading_guide.md
Line: 147
Citation: (Leiter Reports 2023)
Context:
### Mitigation and Revision Pathways

Targeted fixes could elevate from "reject" to "revise": Bolster novelty with a matrix contrasting EPC against rivals (e.g., vs. Olsson's anti-coherentism). For clarity, pilot-reader test sections (e.g., via PhilPapers preprints). Engage recent lit via arXiv sweeps (2024-2025 on "coherentism externalism"). Resubmit cascade: Start mid-tier (*Synthese*), descend if needed. Long-term, this feedback hones EPC into a series—e.g., empirical paper on brittleness proxies. Philosophy rewards persistence: ~60% of published papers face 3+ rejections (Leiter Reports 2023).

In essence, while EPC innovates pragmatically, tightening focus and evidence would mitigate these hurdles, aligning with review norms that value dialogic precision over encyclopedic scope.
Reference: NOT FOUND
------------------------------------------------------------

Citation 198:
File: reading_guide.md
Line: 388
Citation: (Wiley 2024)
Context:
`### 7.5.1 Pilot Test: Ad-Hoc Ratios in AstronomyTo operationalize EPC, consider Ptolemaic astronomy’s collapse (150–300 CE), where ad-hoc modifications (C1) exceeded 5/year, signaling high brittleness (Kuhn 1996, 82–92). Seshat data on scientific output (Turchin 2003, 150–170) suggests Ptolemy’s system accrued C2 (institutional coercion) at ~30% of resources, unlike Copernicus’s low-B model (C1 < 2/year). Applying B = C1 + λC2 (λ = 0.5 for historical weight), Ptolemy’s B > 10, predicting failure, while Copernicus’s B < 3 aligns with viability. Modern analogs, like AI winters (1980s–2000s), show high C1 via overfitted models (arXiv trends, ~20% annual retraction spikes). This falsifiable test validates EPC’s predictive power across epistemic domains.`
    
- **Word Count**: ~400 words, expanding thin program outline (~200 words).
- **Rationale**: Empirical pilots cut "untestable" critiques by 15% (Wiley 2024); historical cases align with *Synthese*’s science-philosophy bridge. Turchin’s Seshat data adds rigor.
- **Verification**: Check Seshat via Princeton UP; arXiv for AI trends (2020–2025).

### **5. Structural and Stylistic Polish**
Reference: NOT FOUND
------------------------------------------------------------

Citation 199:
File: reading_guide.md
Line: 411
Citation: (MDPI 2025)
Context:
`![Figure 2: Deflationary Path](deflation_path.png)**Caption**: Path from belief to Standing Predicate, reducing brittleness via selection (Adapted from Mesoudi 2011, 30–35).`
    
- **Word Count**: Net ~1,000-word reduction; appendix ~200 words; captions ~100 words.
- **Rationale**: Clarity boosts acceptance odds by 20% (MDPI 2025); appendices streamline flow per *Erkenntnis* norms. Visuals enhance argument uptake.
- **Verification**: Use LaTeX for figure rendering; confirm Mesoudi via UChicago Press preview.

### **Revised Cover Letter Excerpt**
Reference: NOT FOUND
------------------------------------------------------------

Citation 200:
File: reading_guide.md
Line: 456
Citation: (Wiley 2024)
Context:
### **Why These Edits Work**

1. **Novelty Subsection**: The comparative table directly counters "incremental" critiques by positioning EPC as a distinct advance over Haack and Price, a tactic that boosts acceptance odds by ~20% when explicit contrasts are added (Wiley 2024). Precise page citations (e.g., Haack pp. 120–125) signal thorough engagement, a must for *Philosophical Studies*.
2. **Brittleness Formalization**: A heuristic equation and fitness landscape figure address clarity concerns (~25% of rejections), making brittleness tangible, per *Philosophy of Science*’s preference for quasi-formal models in epistemology. Adapting Holling’s resilience (pp. 14–17) grounds the analogy empirically.
3. **Literature Engagement**: Integrating Staffel (2019) and precise pages for Olsson/Baysan preempts "outdated" or "superficial" flags, common in 20% of reviews. Bayesian rebuttals align with recent coherentism debates, ensuring relevance.
4. **Pilot Case Study**: The Ptolemaic test case, leveraging Seshat data, transforms Sec. 7.5 from aspirational to testable, aligning with Popperian rigor in *Erkenntnis*. Historical metrics (e.g., C1 > 5/year) add falsifiability, reducing "speculative" risks by 15%.
Reference: NOT FOUND
------------------------------------------------------------

Citation 201:
File: revision.md
Line: 125
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 202:
File: revision.md
Line: 129
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
```
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 203:
File: suggestions.md
Line: 32
Citation: (Carlson 2015)
Context:
Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.
```

**Rationale:** I've added a sentence to explicitly connect the historical example to the philosophical problem being addressed. This helps readers understand the relevance of the example to the paper's thesis.
Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.
------------------------------------------------------------

Citation 204:
File: suggestions.md
Line: 98
Citation: (Moghaddam 2013)
Context:
**Edit:**
```
Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 205:
File: syn.md
Line: 15
Citation: (BonJour 1985)
Context:
Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.
Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.
------------------------------------------------------------

Citation 206:
File: syn.md
Line: 38
Citation: (Meadows 2008)
Context:
## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool
Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.
------------------------------------------------------------

Citation 207:
File: syn.md
Line: 62
Citation: (Mesoudi 2011)
Context:
Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 208:
File: syn.md
Line: 167
Citation: (Gadamer 1975)
Context:
Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.
Reference:
Gadamer, Hans-Georg. 1975. *Truth and Method*. 2nd rev. ed. Translated by Joel Weinsheimer and Donald G. Marshall. New York: Continuum. Originally published 1960.
------------------------------------------------------------

Citation 209:
File: syn.md
Line: 195
Citation: (Kim 1988)
Context:
### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.
Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 210:
File: syn.md
Line: 197
Citation: (Moghaddam 2013)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.
------------------------------------------------------------

Citation 211:
File: syn.md
Line: 197
Citation: (Pritchard 2016)
Context:
A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.
Reference:
Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 212:
File: syn.md
Line: 287
Citation: (Newman 2010)
Context:
To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:
Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 213:
File: syn.md
Line: 342
Citation: (Zollman 2013)
Context:
### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.
Reference:
Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.
------------------------------------------------------------

Citation 214:
File: syn.md
Line: 356
Citation: (Worrall 1989)
Context:
Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 215:
File: syn.md
Line: 389
Citation: (Simon 1972)
Context:
**Why Logic Occupies the Core:**

Logic isn't metaphysically privileged - it's functionally indispensable. Revising it would generate infinite brittleness: you cannot perform the cost-benefit analysis to assess a revision to logic without using logic. This maximal entrenchment follows from bounded rationality (Simon 1972), not a priori necessity.

**Addressing Power Dynamics:**
Reference:
Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.
------------------------------------------------------------

Citation 216:
File: syn.md
Line: 393
Citation: (Harding 1991)
Context:
**Addressing Power Dynamics:**

Engaging feminist epistemology (Harding 1991), institutional suppression of alternative proof methods or foundational approaches delays brittleness detection. When dominant mathematical communities use coercive tactics (career punishment, publication barriers) to enforce orthodoxy, this generates measurable systemic costs: innovation lags, talented mathematicians driven from field, fragmentation of subdisciplines. These C(t) indicators signal brittleness in mathematical practice, not just theory.

**The General Point:** Mathematics demonstrates the framework's universality. All domains - physical, social, mathematical - face pragmatic selection. The feedback mechanism varies (external prediction vs. internal coherence), but the underlying filter is the same: systems accumulating brittleness are replaced by more viable alternatives.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 217:
File: syn.md
Line: 411
Citation: (Goldman 1979)
Context:
### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.
Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.
------------------------------------------------------------

Citation 218:
File: syn.md
Line: 413
Citation: (Zagzebski 1996)
Context:
Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.
Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 219:
File: syn.md
Line: 415
Citation: (Goldman 1999)
Context:
Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**
Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 220:
File: syn.md
Line: 441
Citation: (Harding 1991)
Context:
Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 221:
File: syn.md
Line: 449
Citation: (Turchin 2003)
Context:
### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.
Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.
------------------------------------------------------------

Citation 222:
File: syn.md
Line: 451
Citation: (Mallapaty 2020)
Context:
The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.
Reference: NOT FOUND
------------------------------------------------------------

Citation 223:
File: syn.md
Line: 459
Citation: (Wright 1932)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 224:
File: syn.md
Line: 459
Citation: (Mesoudi 2011)
Context:
### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).
Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 225:
File: syn.md
Line: 461
Citation: (Acemoglu and Robinson 2012)
Context:
An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.
Reference: NOT FOUND
------------------------------------------------------------

Citation 226:
File: syn.md
Line: 465
Citation: (per Harding 1991)
Context:
The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.
Reference: NOT FOUND
------------------------------------------------------------

Citation 227:
File: syn.md
Line: 471
Citation: (Harding 1991)
Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.
Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.
------------------------------------------------------------

Citation 228:
File: syn.md
Line: 517
Citation: (Rottschaefer 2012)
Context:
The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**
Reference: NOT FOUND
------------------------------------------------------------

Citation 229:
File: synth_guide.md
Line: 87
Citation: (Tauriainen 2017)
Context:
1.  **Strengthening the Connection to Quine and Holism:**
    *   **Recommendation:** While the paper does a great job *animating* Quine's web, it can be even more explicit about how it *solves a specific tension within Quine's own work*.
    *   **Integration:** Steal the specific citation from **`arch_v16.2.md`** that mentions the "documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017)." Then, explicitly state that your Three-Level Framework of Truth (Contextual Coherence, Justified Truth, Objective Truth) is designed to resolve *this very tension*. This move frames your contribution not just as an improvement on coherentism in general, but as a solution to a core problem in post-Quinean epistemology.

2.  **Refining the Engagement with Social Epistemology and Standpoint Theory:**
    *   **Recommendation:** The current versions connect to standpoint theory by naturalizing it—marginalized perspectives detect brittleness. This is good. It can be made even stronger by framing dissent as a crucial *epistemic data stream*.
Reference: NOT FOUND
------------------------------------------------------------

