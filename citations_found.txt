
================================================================================
Citation Extraction Run - 2025-12-18 10:59:22
Files scanned: Computational-Closure-and-the-Architecture-of-Mind.md
Total citations: 49
================================================================================


################################################################################
FILE: Computational-Closure-and-the-Architecture-of-Mind.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 17
Citation: (Friston 2010)

Context:
This paper is a work of synthesis rather than empirical discovery. It presents no new experimental data in neuroscience, nor new theorems in thermodynamics or information theory. It aligns existing, well-validated concepts from information geometry (computational closure), cognitive science (predictive processing), and epistemology (Quinean holism) to resolve friction between them. Our contribution is architectural alignment: showing how these frameworks illuminate each other when properly connected.

Contemporary theories of mind and knowledge largely talk past one another. Cognitive science increasingly describes the brain as a prediction engine minimizing free energy to persist in its environment (Friston 2010). This is a fundamentally thermodynamic process. Meanwhile, naturalized epistemology describes knowledge as a web of beliefs justified by its coherence and pragmatic success (Quine 1960; Sinclair 2007). It makes little reference to the underlying mechanics. This leaves an explanatory gap: how do the physical constraints on an organism's existence give rise to the normative structures of justification and objective truth? (BonJour 1985; Olsson 2005)

While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as *Markov blankets* (statistical boundaries that separate internal from external states) that achieve *computational closure* (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Reference:
Friston, Karl J. 2010. "The Free-Energy Principle: A Unified Brain Theory?" *Nature Reviews Neuroscience* 11 (2): 127–138. https://doi.org/10.1038/nrn2787.
------------------------------------------------------------

Citation 2 [PARENTHETICAL]:
Line: 17
Citation: (Quine 1960)

Context:
This paper is a work of synthesis rather than empirical discovery. It presents no new experimental data in neuroscience, nor new theorems in thermodynamics or information theory. It aligns existing, well-validated concepts from information geometry (computational closure), cognitive science (predictive processing), and epistemology (Quinean holism) to resolve friction between them. Our contribution is architectural alignment: showing how these frameworks illuminate each other when properly connected.

Contemporary theories of mind and knowledge largely talk past one another. Cognitive science increasingly describes the brain as a prediction engine minimizing free energy to persist in its environment (Friston 2010). This is a fundamentally thermodynamic process. Meanwhile, naturalized epistemology describes knowledge as a web of beliefs justified by its coherence and pragmatic success (Quine 1960; Sinclair 2007). It makes little reference to the underlying mechanics. This leaves an explanatory gap: how do the physical constraints on an organism's existence give rise to the normative structures of justification and objective truth? (BonJour 1985; Olsson 2005)

While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as *Markov blankets* (statistical boundaries that separate internal from external states) that achieve *computational closure* (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Reference:
Quine, W. V. O. 1960. *Word and Object*. Cambridge, MA: MIT Press. ISBN 978-0262670012.
------------------------------------------------------------

Citation 3 [PARENTHETICAL]:
Line: 17
Citation: (Sinclair 2007)

Context:
This paper is a work of synthesis rather than empirical discovery. It presents no new experimental data in neuroscience, nor new theorems in thermodynamics or information theory. It aligns existing, well-validated concepts from information geometry (computational closure), cognitive science (predictive processing), and epistemology (Quinean holism) to resolve friction between them. Our contribution is architectural alignment: showing how these frameworks illuminate each other when properly connected.

Contemporary theories of mind and knowledge largely talk past one another. Cognitive science increasingly describes the brain as a prediction engine minimizing free energy to persist in its environment (Friston 2010). This is a fundamentally thermodynamic process. Meanwhile, naturalized epistemology describes knowledge as a web of beliefs justified by its coherence and pragmatic success (Quine 1960; Sinclair 2007). It makes little reference to the underlying mechanics. This leaves an explanatory gap: how do the physical constraints on an organism's existence give rise to the normative structures of justification and objective truth? (BonJour 1985; Olsson 2005)

While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as *Markov blankets* (statistical boundaries that separate internal from external states) that achieve *computational closure* (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Reference:
Sinclair, Robert. 2007. "Quine's Naturalized Epistemology and the Third Dogma of Empiricism." *Southern Journal of Philosophy* 45, no. 3: 455–472. https://doi.org/10.1111/j.2041-6962.2007.tb00060.x.
------------------------------------------------------------

Citation 4 [PARENTHETICAL]:
Line: 17
Citation: (BonJour 1985)

Context:
This paper is a work of synthesis rather than empirical discovery. It presents no new experimental data in neuroscience, nor new theorems in thermodynamics or information theory. It aligns existing, well-validated concepts from information geometry (computational closure), cognitive science (predictive processing), and epistemology (Quinean holism) to resolve friction between them. Our contribution is architectural alignment: showing how these frameworks illuminate each other when properly connected.

Contemporary theories of mind and knowledge largely talk past one another. Cognitive science increasingly describes the brain as a prediction engine minimizing free energy to persist in its environment (Friston 2010). This is a fundamentally thermodynamic process. Meanwhile, naturalized epistemology describes knowledge as a web of beliefs justified by its coherence and pragmatic success (Quine 1960; Sinclair 2007). It makes little reference to the underlying mechanics. This leaves an explanatory gap: how do the physical constraints on an organism's existence give rise to the normative structures of justification and objective truth? (BonJour 1985; Olsson 2005)

While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as *Markov blankets* (statistical boundaries that separate internal from external states) that achieve *computational closure* (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press. ISBN 978-0674843813.
------------------------------------------------------------

Citation 5 [PARENTHETICAL]:
Line: 17
Citation: (Olsson 2005)

Context:
This paper is a work of synthesis rather than empirical discovery. It presents no new experimental data in neuroscience, nor new theorems in thermodynamics or information theory. It aligns existing, well-validated concepts from information geometry (computational closure), cognitive science (predictive processing), and epistemology (Quinean holism) to resolve friction between them. Our contribution is architectural alignment: showing how these frameworks illuminate each other when properly connected.

Contemporary theories of mind and knowledge largely talk past one another. Cognitive science increasingly describes the brain as a prediction engine minimizing free energy to persist in its environment (Friston 2010). This is a fundamentally thermodynamic process. Meanwhile, naturalized epistemology describes knowledge as a web of beliefs justified by its coherence and pragmatic success (Quine 1960; Sinclair 2007). It makes little reference to the underlying mechanics. This leaves an explanatory gap: how do the physical constraints on an organism's existence give rise to the normative structures of justification and objective truth? (BonJour 1985; Olsson 2005)

While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as *Markov blankets* (statistical boundaries that separate internal from external states) that achieve *computational closure* (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Reference:
Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press. ISBN 978-0199279999.
------------------------------------------------------------

Citation 6 [IN-PROSE]:
Line: 21
Citation: Ladyman and Ross (2007)

Context:
While other attempts to bridge this gap exist (e.g., evolutionary epistemology emphasizing adaptation or social epistemology focusing on testimony), this paper tentatively proposes that the bridge may lie in information compression. By treating concepts not as abstract representations but as *Markov blankets* (statistical boundaries that separate internal from external states) that achieve *computational closure* (a hypothesized state where the macro-level is causally self-contained), we may be able to trace a continuous line from the thermodynamics of living systems to the structure of belief. We suggest that the drive to minimize prediction error could be the engine that shapes both cognitive architecture and epistemic norms, though this remains a theoretical proposal requiring empirical validation.

Methodologically, this framework adheres to what Ladyman and Ross (2007) term the *Principle of Naturalistic Closure*: we reject a priori intuition as a guide to metaphysics and instead require that epistemological claims unify the special sciences with fundamental physics without contradiction. The generative priors and Markov blankets described herein are not merely psychological heuristics but are motivated by the convergence of thermodynamics and information theory, a convergence that grounds cognition in the physical constraints any bounded system must satisfy.

Our central thesis, which remains speculative, is that consciousness may function as the user interface for a specific type of information processing: structural pattern recognition. We distinguish this from the unconscious, frequency-based processing of statistical regularities. This distinction potentially explains why some knowledge can be acquired from a single instance and may provide a functional basis for the phenomenology of understanding. It also offers a prospective diagnostic for the limitations of current artificial intelligence, though this application requires further investigation.

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 7 [IN-PROSE]:
Line: 39
Citation: Dennett (1991)

Context:
A Note on Entropy and Energy: Throughout this paper, we distinguish between information-theoretic quantities (Shannon entropy as average surprise, measured in bits) and thermodynamic quantities (energy required to process or erase bits). These are not identical but deeply connected. Landauer's Principle establishes that information processing has irreducible physical costs. While erasing a single bit costs negligible energy, maintaining a high-fidelity internal model against a shifting environment requires continuous processing of massive information streams. At the macro-scale, this 'information processing cost' becomes metabolic and economic overhead. The 'heat' generated by a failing belief system manifests as wasted bureaucratic effort, enforcement friction, and cognitive dissonance. High information leakage (persistent prediction error) thus implies tangible costs to any physical system. The link between epistemic brittleness and physical inefficiency is functional, not merely metaphorical.

Persistence as a bounded pattern requires information processing. What we call "things" are better understood as *real patterns* in the sense of Dennett (1991) and Ladyman and Ross (2007). Crucially, we must distinguish between material substrate (the underlying physical constituents) and causal structure (the constraints that shape outcomes). A real pattern exists not by virtue of what it is made of, but by what it does to a probability distribution. If a pattern reduces uncertainty and supports counterfactual interventions (if heating this boundary causes the interior to produce heat shock proteins), it constitutes an ontological unit. Real patterns compress information, maintaining statistical boundaries that distinguish internal from external states while achieving genuine projectibility (predictive purchase on future states).

### 2.2 The Free Energy Principle

Reference:
Dennett, Daniel C. 1991. "Real Patterns." *Journal of Philosophy* 88(1): 27-51. https://doi.org/10.2307/2027085.
------------------------------------------------------------

Citation 8 [IN-PROSE]:
Line: 39
Citation: Ladyman and Ross (2007)

Context:
A Note on Entropy and Energy: Throughout this paper, we distinguish between information-theoretic quantities (Shannon entropy as average surprise, measured in bits) and thermodynamic quantities (energy required to process or erase bits). These are not identical but deeply connected. Landauer's Principle establishes that information processing has irreducible physical costs. While erasing a single bit costs negligible energy, maintaining a high-fidelity internal model against a shifting environment requires continuous processing of massive information streams. At the macro-scale, this 'information processing cost' becomes metabolic and economic overhead. The 'heat' generated by a failing belief system manifests as wasted bureaucratic effort, enforcement friction, and cognitive dissonance. High information leakage (persistent prediction error) thus implies tangible costs to any physical system. The link between epistemic brittleness and physical inefficiency is functional, not merely metaphorical.

Persistence as a bounded pattern requires information processing. What we call "things" are better understood as *real patterns* in the sense of Dennett (1991) and Ladyman and Ross (2007). Crucially, we must distinguish between material substrate (the underlying physical constituents) and causal structure (the constraints that shape outcomes). A real pattern exists not by virtue of what it is made of, but by what it does to a probability distribution. If a pattern reduces uncertainty and supports counterfactual interventions (if heating this boundary causes the interior to produce heat shock proteins), it constitutes an ontological unit. Real patterns compress information, maintaining statistical boundaries that distinguish internal from external states while achieving genuine projectibility (predictive purchase on future states).

### 2.2 The Free Energy Principle

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 9 [PARENTHETICAL]:
Line: 52
Citation: (Friston 2010)

Context:
2. **Changing the world** to match predictions (active inference)
3. **Optimizing model structure** to reduce complexity while maintaining accuracy (structural learning)

This process is not a goal-directed endeavor but a physical constraint: systems that fail to minimize free energy tend to dissipate and vanish, whereas those that succeed maintain their integrity as bounded entities. In this sense, minimizing free energy is constitutive of self-organization (Friston 2010).

The Free Energy Principle (FEP) builds on broader predictive processing frameworks in cognitive science, where brains are understood as hierarchical prediction machines constantly minimizing prediction error through bidirectional cortical processing (Clark 2013). This perspective reframes perception. It treats perception not as passive reception but as active inference, where the brain tests predictions against sensory input and revises models when mismatches occur. Neurobiologically, this is implemented through predictive coding: hierarchical brain organization where higher levels generate predictions about lower-level activity, and lower levels signal back prediction errors when inputs deviate from expectations (Friston and Kiebel 2009). This architecture provides a plausible mechanistic account of how variational free energy minimization could be realized in cortical circuits.

Reference:
Friston, Karl J. 2010. "The Free-Energy Principle: A Unified Brain Theory?" *Nature Reviews Neuroscience* 11 (2): 127–138. https://doi.org/10.1038/nrn2787.
------------------------------------------------------------

Citation 10 [PARENTHETICAL]:
Line: 54
Citation: (Clark 2013)

Context:
This process is not a goal-directed endeavor but a physical constraint: systems that fail to minimize free energy tend to dissipate and vanish, whereas those that succeed maintain their integrity as bounded entities. In this sense, minimizing free energy is constitutive of self-organization (Friston 2010).

The Free Energy Principle (FEP) builds on broader predictive processing frameworks in cognitive science, where brains are understood as hierarchical prediction machines constantly minimizing prediction error through bidirectional cortical processing (Clark 2013). This perspective reframes perception. It treats perception not as passive reception but as active inference, where the brain tests predictions against sensory input and revises models when mismatches occur. Neurobiologically, this is implemented through predictive coding: hierarchical brain organization where higher levels generate predictions about lower-level activity, and lower levels signal back prediction errors when inputs deviate from expectations (Friston and Kiebel 2009). This architecture provides a plausible mechanistic account of how variational free energy minimization could be realized in cortical circuits.

The FEP's scope extends beyond brain function to encompass fundamental properties of all self-organizing systems. Any system that maintains its integrity against the second law of thermodynamics must possess a statistical boundary. This boundary (a Markov blanket) separates internal from external states. It must minimize the free energy associated with that boundary (Friston et al. 2017).

Reference:
Clark, Andy. 2013. "Whatever next? Predictive brains, situated agents, and the future of cognitive science." *Behavioral and Brain Sciences* 36(3): 181–204. https://doi.org/10.1017/S0140525X12000477.
------------------------------------------------------------

Citation 11 [IN-PROSE]:
Line: 58
Citation: Kirchhoff et al. (2018)

Context:
The FEP's scope extends beyond brain function to encompass fundamental properties of all self-organizing systems. Any system that maintains its integrity against the second law of thermodynamics must possess a statistical boundary. This boundary (a Markov blanket) separates internal from external states. It must minimize the free energy associated with that boundary (Friston et al. 2017).

Kirchhoff et al. (2018) demonstrate that living systems are characterized by Markov blankets enabling autonomous organization, and that "autonomous systems are hierarchically composed of Markov blankets of Markov blankets—all the way down to individual cells, all the way up to you and me." This hierarchical assembly occurs through adaptive active inference, distinguishing living systems (capable of inferring future states) from non-living systems exhibiting mere active inference without adaptive capacity.

This connects information-theoretic principles directly to the basic requirements for life itself. Autopoiesis, allostasis, and goal-directed behavior all emerge as consequences of free energy minimization in bounded systems.

Reference:
Kirchhoff, Michael, Thomas Parr, Ensor Palacios, Karl Friston, and Julian Kiverstein. 2018. "The Markov Blankets of Life: Autonomy, Active Inference and the Free Energy Principle." *Journal of the Royal Society Interface* 15(138): 20170792. https://doi.org/10.1098/rsif.2017.0792.
------------------------------------------------------------

Citation 12 [IN-PROSE]:
Line: 76
Citation: Dittrich and Kinne (2024)

Context:
### 2.3 Dispositions as Compression Algorithms

Recent formal work provides rigorous information-theoretic foundations for this compression imperative. Dittrich and Kinne (2024) demonstrate that any system persisting under uncertainty must compress information to minimize what they term "epistemic entropy." This is the Information-Theoretic Imperative (ITI): bounded existence requires efficient compression as a constitutive condition, not merely a useful strategy. Their Compression Efficiency Principle (CEP) further shows that efficient compression mechanically selects for generative, causal models over superficial correlations. Non-causal compressions "accumulate exceptions" as novel cases fail to fit the pattern, eventually becoming unsustainable. The drive to compress is thus a physical necessity for bounded systems. This mechanism explains why systems are forced to seek computational closure: only compressions achieving genuine causal structure can sustain efficiency without catastrophic exception accumulation.

This convergence of independent formalizations, including classical information theory (Shannon 1948), thermodynamic cost analysis (Landauer 1961), and evolutionary epistemology (Campbell 1974; Bradie 1986), suggests that these principles capture genuine structural features of how bounded systems interact with constrained environments, not artifacts of any particular theoretical framework.

Reference:
Dittrich, Christian, and Jennifer Flygare Kinne. "The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence." Preprint, submitted October 30, 2024. arXiv:2510.25883 [cs.AI]. https://doi.org/10.48550/arXiv.2510.25883.
------------------------------------------------------------

Citation 13 [PARENTHETICAL]:
Line: 78
Citation: (Shannon 1948)

Context:
Recent formal work provides rigorous information-theoretic foundations for this compression imperative. Dittrich and Kinne (2024) demonstrate that any system persisting under uncertainty must compress information to minimize what they term "epistemic entropy." This is the Information-Theoretic Imperative (ITI): bounded existence requires efficient compression as a constitutive condition, not merely a useful strategy. Their Compression Efficiency Principle (CEP) further shows that efficient compression mechanically selects for generative, causal models over superficial correlations. Non-causal compressions "accumulate exceptions" as novel cases fail to fit the pattern, eventually becoming unsustainable. The drive to compress is thus a physical necessity for bounded systems. This mechanism explains why systems are forced to seek computational closure: only compressions achieving genuine causal structure can sustain efficiency without catastrophic exception accumulation.

This convergence of independent formalizations, including classical information theory (Shannon 1948), thermodynamic cost analysis (Landauer 1961), and evolutionary epistemology (Campbell 1974; Bradie 1986), suggests that these principles capture genuine structural features of how bounded systems interact with constrained environments, not artifacts of any particular theoretical framework.

Returning to the Quinean foundation: a disposition to assent functions analogously to a compressed encoding of regularities.

Reference:
Shannon, Claude E. 1948. "A Mathematical Theory of Communication." *Bell System Technical Journal* 27(3): 379–423. https://doi.org/10.1002/j.1538-7305.1948.tb01338.x.
------------------------------------------------------------

Citation 14 [PARENTHETICAL]:
Line: 78
Citation: (Landauer 1961)

Context:
Recent formal work provides rigorous information-theoretic foundations for this compression imperative. Dittrich and Kinne (2024) demonstrate that any system persisting under uncertainty must compress information to minimize what they term "epistemic entropy." This is the Information-Theoretic Imperative (ITI): bounded existence requires efficient compression as a constitutive condition, not merely a useful strategy. Their Compression Efficiency Principle (CEP) further shows that efficient compression mechanically selects for generative, causal models over superficial correlations. Non-causal compressions "accumulate exceptions" as novel cases fail to fit the pattern, eventually becoming unsustainable. The drive to compress is thus a physical necessity for bounded systems. This mechanism explains why systems are forced to seek computational closure: only compressions achieving genuine causal structure can sustain efficiency without catastrophic exception accumulation.

This convergence of independent formalizations, including classical information theory (Shannon 1948), thermodynamic cost analysis (Landauer 1961), and evolutionary epistemology (Campbell 1974; Bradie 1986), suggests that these principles capture genuine structural features of how bounded systems interact with constrained environments, not artifacts of any particular theoretical framework.

Returning to the Quinean foundation: a disposition to assent functions analogously to a compressed encoding of regularities.

Reference:
Landauer, Rolf. 1961. "Irreversibility and Heat Generation in the Computing Process." *IBM Journal of Research and Development* 5(3): 183–191. https://doi.org/10.1147/rd.1961.5.3.183. ISBN 978-0199276196.
------------------------------------------------------------

Citation 15 [PARENTHETICAL]:
Line: 78
Citation: (Campbell 1974)

Context:
Recent formal work provides rigorous information-theoretic foundations for this compression imperative. Dittrich and Kinne (2024) demonstrate that any system persisting under uncertainty must compress information to minimize what they term "epistemic entropy." This is the Information-Theoretic Imperative (ITI): bounded existence requires efficient compression as a constitutive condition, not merely a useful strategy. Their Compression Efficiency Principle (CEP) further shows that efficient compression mechanically selects for generative, causal models over superficial correlations. Non-causal compressions "accumulate exceptions" as novel cases fail to fit the pattern, eventually becoming unsustainable. The drive to compress is thus a physical necessity for bounded systems. This mechanism explains why systems are forced to seek computational closure: only compressions achieving genuine causal structure can sustain efficiency without catastrophic exception accumulation.

This convergence of independent formalizations, including classical information theory (Shannon 1948), thermodynamic cost analysis (Landauer 1961), and evolutionary epistemology (Campbell 1974; Bradie 1986), suggests that these principles capture genuine structural features of how bounded systems interact with constrained environments, not artifacts of any particular theoretical framework.

Returning to the Quinean foundation: a disposition to assent functions analogously to a compressed encoding of regularities.

Reference:
Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.
------------------------------------------------------------

Citation 16 [IN-PROSE]:
Line: 130
Citation: Ayvazov (2025)

Context:
- "$F=ma$" (structural: mathematical necessity once the concepts are understood)
- "Swans are white" (statistical: inductively generalized from frequency, famously failed)

Recent work in phase epistemology provides formal treatment of this distinction. Ayvazov (2025) distinguishes between classical probability and what he terms "improbabilistic coherence." Classical probability involves frequency-based likelihood, while improbabilistic coherence refers to structural integrity that exists independent of repetition. He defines this as "the generative condition for epistemic emergence." While Ayvazov proposes a speculative quantum-mechanical formalism for this distinction, we employ it here purely as an epistemic category without committing to his physical interpretation.

Implications for Information Compression:
- Statistical compressions require large ensembles to stabilize (high sample complexity)

Reference:
Ayvazov, Mahammad. 2025. "Toward a Phase Epistemology: Coherence, Response and the Vector of Mutual Uncertainty." *SSRN Electronic Journal*. https://doi.org/10.2139/ssrn.5250197.
------------------------------------------------------------

Citation 17 [PARENTHETICAL]:
Line: 140
Citation: (Aslin and Newport 2012)

Context:
Structural patterns are not subjectively imposed but constrained by reality. You cannot validly infer that "heat flows spontaneously from ice to hand" from a single encounter, because thermodynamics forbids this relationship (violating the Clausius statement of the Second Law). The structural constraints are objective, even if recognizable from limited data.

Empirical work in cognitive science supports this distinction. Statistical learning operates implicitly through mere exposure to input patterns, extracting regularities from repeated encounters (Aslin and Newport 2012). However, the same mechanisms that enable learning from frequency distributions also support generalization to novel instances when structural relationships are detected, suggesting a unified learning system capable of both statistical pattern matching and structural inference.

This distinction becomes essential for understanding how notions (proto-Generative Priors) can form before extensive empirical testing, and why some singular experiences carry immediate epistemic weight while others require statistical accumulation.

Reference:
Aslin, Richard N., and Elissa L. Newport. 2012. "Statistical learning: From acquiring specific items to forming general rules." *Current Directions in Psychological Science* 21(3): 170–176. https://doi.org/10.1177/0963721412436806.
------------------------------------------------------------

Citation 18 [PARENTHETICAL]:
Line: 157
Citation: (Pearl 2000)

Context:
#### 2.5.2 The Interventional Diagnostic

How does a cognitive system distinguish a robust structural constraint from a brittle statistical regularity? We propose the mechanism lies in the capacity for interventional counterfactuals (formally, the distinction between $P(Y|X)$ and $P(Y|do(X))$ (Pearl 2000)).

Statistical patterns capture passive observations: seeing *X* predicts *Y* (conditioning on *X*). Structural patterns capture interventional truths: *causing* *X* necessitates *Y* (intervening on *X*). This latter operation, the "dictator" or "do-operator," involves "graph surgery"; it severs the incoming causal arrows to *X* to isolate its downstream effects from confounding variables.

Reference:
Pearl, Judea. 2000. *Causality: Models, Reasoning, and Inference*. Cambridge: Cambridge University Press. ISBN 978-0521773621.
------------------------------------------------------------

Citation 19 [IN-PROSE]:
Line: 201
Citation: Mangalam (2025)

Context:
Our Position: This paper primarily makes claims at levels 1 and 2. Whether the strong mechanistic claims (level 3) are literally true remains an empirical question for neuroscience and cognitive science. Our philosophical insights about generative priors, compression, and truth don't depend on resolving this empirical question. Even if the brain's actual mechanisms differ significantly from Free Energy minimization, the functional analysis of what makes a predicate successful (low brittleness, high compression, computational closure) retains its philosophical force.

Acknowledging Fundamental Critiques: Recent work has challenged the FEP's scientific status directly. Mangalam (2025) argues that the FEP operates as an unfalsifiable pseudo-theory. Its high level of abstraction and mathematical formalism make it difficult to test empirically. It may not generate novel predictions that simpler theories cannot explain.

The Strong Pivot: This critique deserves a direct response, and we can offer one that strengthens rather than weakens our philosophical framework. We accept the characterization but reject the implication that it lacks explanatory power. Even if Mangalam is correct that the FEP describes necessary consequences of being a bounded system rather than contingent empirical laws, this makes it precisely the right foundation for our philosophical project.

Reference:
Mangalam, Madhur. 2025. "The Emperor's New Pseudo-Theory: How the Free Energy Principle Ransacked Neuroscience." Preprint. DOI: 10.31234/osf.io/azkgc. https://osf.io/azkgc.
------------------------------------------------------------

Citation 20 [PARENTHETICAL]:
Line: 229
Citation: (Glenn 2025)

Context:
**Connecting Functional Constraints to Normative Claims:** Readers may wonder how later sections derive substantive claims about objective truth (Section 6) and ethics (Section 8) from what we've framed as "conceptual scaffolding." The answer: these claims rely on level 2 (functional) constraints, not level 3 (mechanistic) implementation. The argument is not "brains compute Shannon entropy, therefore truth exists," but rather "whatever systems successfully compress reality must satisfy certain functional constraints (computational closure, minimal information leakage, strong lumpability), and these constraints determine which predicates persist." The Optimal Constraint Configuration is "objective" not because it exists as a Platonic form or neural structure, but because the functional requirements for successful compression are determined by reality's constraint structure, not by our beliefs about them. Similarly, the claim that coercion generates brittleness doesn't require literal free energy calculations; it requires only that systems refusing to model agents' autonomous responses must bear higher coordination costs. The normative force comes from functional necessity, not mechanistic implementation.

**Operationalizing Brittleness:** The concept of systemic brittleness plays a central role in this framework as the diagnostic tool for assessing knowledge system health. While this paper maintains focus on the philosophical foundations and conceptual architecture, detailed operationalization of brittleness diagnostics has been developed elsewhere (Glenn 2025). That work develops specific conceptual lenses for diagnosing brittleness across different domains: patch velocity $P(t)$ measuring the ratio of anomaly-resolution to novel-prediction work; coercion ratio $C(t)$ measuring security overhead versus productive investment; model complexity $M(t)$ tracking parameter growth against marginal performance gains; and resilience reserve $R(t)$ assessing cross-domain confirmatory breadth. These indicators serve as analytical categories for historical and philosophical analysis rather than quantitative metrics, providing structured tools for comparative assessment of epistemic system viability. The framework thus offers both philosophical coherence and practical diagnostic capacity without requiring the mechanistic claims of level 3.

Preserving the Insights: When we say "dispositions are compression algorithms" or "generative priors are Markov blankets," read these as capturing functional roles rather than ontological identities. A disposition functions like a compression algorithm: it reduces redundancy, encodes regularities, enables prediction. Whether it literally performs Huffman encoding or some neurally-implemented equivalent doesn't affect the philosophical point about what makes it successful or brittle.

Reference:
Glenn, Patrick. 2025. "The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth." PhilPapers. https://philpapers.org/rec/GLETAO.
------------------------------------------------------------

Citation 21 [IN-PROSE]:
Line: 270
Citation: Ladyman and Ross (2007)

Context:
### 3.2 Rainforest Realism: Blankets Delineate Real Patterns

Markov blankets are enacted, not discovered. They emerge when certain configurations of matter successfully maintain statistical boundaries against entropic dissolution. This perspective aligns with what Ladyman and Ross (2007) call *Rainforest Realism*: the ontology of the world is not a sparse desert of fundamental particles but a rich layering of real patterns at every scale where information can be compressed. A Markov blanket does not invent a useful fiction; it isolates a real pattern that carries genuine information load (what survives the compression process because it enables prediction).

#### 3.2.1 Explicit Ontological Criterion: What Earns Existence?

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 22 [IN-PROSE]:
Line: 276
Citation: Dennett (1991)

Context:
This framework provides a clear answer to the fundamental metaphysical question: *What earns existence?* The answer is causal autonomy via computational closure.

A system exists as a distinct entity when it achieves a statistical boundary that makes its internal dynamics conditionally independent of its exterior. This is not merely about epistemic usefulness; it is about causal power. While Dennett (1991) rightly identified reliable compression as the hallmark of *real patterns*, we contend that nature itself enforces these compressions through thermodynamic constraints. *Causal Autonomy* occurs when a system’s future states are fully predictable and controllable, to a specified tolerance, using only variables defined at its own boundary.

#### 3.2.2 Engagement with Mereology: Why Nihilism is Obsolete

Reference:
Dennett, Daniel C. 1991. "Real Patterns." *Journal of Philosophy* 88(1): 27-51. https://doi.org/10.2307/2027085.
------------------------------------------------------------

Citation 23 [IN-PROSE]:
Line: 351
Citation: Kirchhoff et al. (2018)

Context:
This aligns with Dennett's definition of a real pattern as one that allows for descriptive efficiency better than a bit-map, but we add a crucial physical constraint: the universe itself must produce the boundary. The higher-level system becomes self-contained when you can predict future macro-states using only current macro-states, without tracking micro-details.

This hierarchical emergence arises through self-assembly. Kirchhoff et al. (2018) demonstrate that collectives of Markov blankets can self-assemble into global systems that themselves possess Markov blankets, creating nested boundaries from cells to organisms to social systems. Simulations demonstrate that hierarchical self-organization emerges naturally when microscopic elements have prior beliefs that they participate in macroscopic Markov blankets (Palacios et al. 2020). This suggests nested blanket hierarchies are not imposed from outside but arise spontaneously when components minimize free energy under appropriate constraints, providing a mechanistic account of how computational closure forms across levels.

Formalizing Causal Autonomy: Rosas et al. (2024) provide a rigorous framework for determining when emergence genuinely succeeds by comparing two optimal predictors. The ε-machine (epsilon-machine) uses only macro-level variables to predict future macro-states. The υ-machine (upsilon-machine) has full access to micro-level details and uses them to predict the same macro-states.

Reference:
Kirchhoff, Michael, Thomas Parr, Ensor Palacios, Karl Friston, and Julian Kiverstein. 2018. "The Markov Blankets of Life: Autonomy, Active Inference and the Free Energy Principle." *Journal of the Royal Society Interface* 15(138): 20170792. https://doi.org/10.1098/rsif.2017.0792.
------------------------------------------------------------

Citation 24 [IN-PROSE]:
Line: 353
Citation: Rosas et al. (2024)

Context:
This hierarchical emergence arises through self-assembly. Kirchhoff et al. (2018) demonstrate that collectives of Markov blankets can self-assemble into global systems that themselves possess Markov blankets, creating nested boundaries from cells to organisms to social systems. Simulations demonstrate that hierarchical self-organization emerges naturally when microscopic elements have prior beliefs that they participate in macroscopic Markov blankets (Palacios et al. 2020). This suggests nested blanket hierarchies are not imposed from outside but arise spontaneously when components minimize free energy under appropriate constraints, providing a mechanistic account of how computational closure forms across levels.

Formalizing Causal Autonomy: Rosas et al. (2024) provide a rigorous framework for determining when emergence genuinely succeeds by comparing two optimal predictors. The ε-machine (epsilon-machine) uses only macro-level variables to predict future macro-states. The υ-machine (upsilon-machine) has full access to micro-level details and uses them to predict the same macro-states.

When these two predictors perform equally well (when ε-machine accuracy equals υ-machine accuracy), something remarkable has happened: the macro-level has achieved causal decoupling from the micro-level. The macro-variables contain all the information needed to predict their own future behavior. Substrate details have become causally irrelevant (though they remain constitutively necessary). This is computational closure: the macro-level is "running code" rather than merely describing patterns in the substrate. This equivalence provides the rigorous criterion for a "Real Pattern" in Dennett's sense, transforming his "nontrivial compression" from a qualitative judgment into a quantifiable test. When adding micro-level details yields zero additional information about the macro-future, the macro-level description has captured genuine structure in reality rather than imposing convenient fiction. This equivalence admits degrees of robustness, formalized as lumpability. **Weak lumpability** holds when macro-dynamics work only for specific initial micro-state distributions: the compression succeeds in limited contexts. **Strong lumpability** holds when macro-dynamics work regardless of underlying micro-details: the compression achieves genuine substrate independence and persists across different physical realizations. Only strongly lumpable compressions qualify as objective features of reality; weakly lumpable ones are context-dependent approximations.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 25 [PARENTHETICAL]:
Line: 392
Citation: (Dennett 1987)

Context:
- **The ε-machine player** sees only the current board position: the macro-state. No history, no neural data, just the pieces and their locations.

When both players predict equally well, the board position has achieved computational closure. The macro-level (piece arrangement) contains all the information needed to predict future macro-states (subsequent positions). Crucially, once the current board position is known, the history of how the game arrived there is irrelevant for predicting legal moves. The game has "detached" from its substrate (the players' brains, their training, their moods) and runs purely on positional logic. This exemplifies causal shielding: the macro-state (board position) screens off historical details, achieving genuine computational closure. This effectively formalizes the *intentional stance* (Dennett 1987): treating the system as a macro-agent works not because it is a convenient fiction, but because the macro-dynamics have genuinely decoupled from the micro-details.

Rosas et al. (2024) formalize exactly this criterion: when your ε-machine (macro-only predictor) performs as well as the υ-machine (micro-informed predictor), you have discovered a level of organization that has achieved causal decoupling. The macro-level is "running code" rather than merely describing patterns in the substrate. This is the formal signature of successful emergence.

Reference:
Dennett, Daniel C. 1987. *The Intentional Stance*. Cambridge, MA: MIT Press. ISBN 978-0262540537.
------------------------------------------------------------

Citation 26 [IN-PROSE]:
Line: 394
Citation: Rosas et al. (2024)

Context:
When both players predict equally well, the board position has achieved computational closure. The macro-level (piece arrangement) contains all the information needed to predict future macro-states (subsequent positions). Crucially, once the current board position is known, the history of how the game arrived there is irrelevant for predicting legal moves. The game has "detached" from its substrate (the players' brains, their training, their moods) and runs purely on positional logic. This exemplifies causal shielding: the macro-state (board position) screens off historical details, achieving genuine computational closure. This effectively formalizes the *intentional stance* (Dennett 1987): treating the system as a macro-agent works not because it is a convenient fiction, but because the macro-dynamics have genuinely decoupled from the micro-details.

Rosas et al. (2024) formalize exactly this criterion: when your ε-machine (macro-only predictor) performs as well as the υ-machine (micro-informed predictor), you have discovered a level of organization that has achieved causal decoupling. The macro-level is "running code" rather than merely describing patterns in the substrate. This is the formal signature of successful emergence.

A different example illustrates continuous dynamical systems. Consider a traffic jam. A υ-machine attempting to predict when the jam clears by tracking every car's velocity and position faces an intractable computational task. An ε-machine tracking only traffic density and average flow rate achieves equivalent predictive accuracy with vastly fewer variables. When the ε-machine succeeds, the macro-variable "traffic density" has achieved computational closure. The micro-details of individual vehicles are constitutively necessary (no cars, no jam) but causally redundant; they have been screened off by the macro-level dynamics.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 27 [IN-PROSE]:
Line: 428
Citation: Rosas et al. (2024)

Context:
Key Insight: Epistemic Equifinality. In Systems Theory, equifinality describes how different structural configurations can achieve the same steady state. The Hot Dog Paradox illustrates epistemic equifinality: different Markov Blankets (definitions) can achieve comparable levels of computational closure depending on the system's goal (taxation vs. cuisine vs. engineering). Each community draws the boundary where it reduces brittleness for their purposes. The information hasn't changed; the coarse-graining has. This is not arbitrary: each blanket faces pragmatic testing. However, it is pluralistic: multiple viable configurations exist.

Rosas et al. (2024) formalize this insight: valid coarse-grainings form a mathematical lattice structure, where multiple macro-level compressions can achieve strong lumpability for the same underlying system. The existence of this lattice explains epistemic equifinality: different purposes may select different points in the space of viable compressions, all of which genuinely achieve computational closure.

This lattice structure also provides the formal justification for Rainforest Realism. Because different physical substrates can realize computationally equivalent ε-machines, reality genuinely supports multiple valid ontologies enacted at different scales. This is a **Pragmatic Realism**: the *choice* of which blanket to draw is pragmatic (driven by the system's goals), but the *validity* of that blanket is realistic (constrained by strong lumpability). A "hot dog" is a valid object only if the hot dog variable predicts its own future better than noise. The rainforest is populated by all such valid closures.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 28 [IN-PROSE]:
Line: 501
Citation: Glenn (2025)

Context:
### 3.6.5 From Cognitive Mechanisms to Social Truth

A potential confusion must be addressed. Glenn (2025) distinguishes generative priors by their relationship to networks (Consensus Network versus Apex Network). This paper distinguishes patterns by their cognitive processing requirements (statistical versus structural). Are these competing typologies?

No. They describe different dimensions of the same epistemic landscape. The two-way distinction (statistical versus structural) describes cognitive mechanisms: how the brain processes raw data to form beliefs. Network alignment describes validation states: how we determine whether those beliefs count as justified knowledge within a social context.

Reference:
Glenn, Patrick. 2025. "The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth." PhilPapers. https://philpapers.org/rec/GLETAO.
------------------------------------------------------------

Citation 29 [PARENTHETICAL]:
Line: 511
Citation: (Kahneman 2011)

Context:
Most cultural knowledge propagates through statistical learning. You hear "property rights are fundamental" or "democracy is best" thousands of times across family, education, media. Your brain detects frequency patterns, extracts correlations, builds compressions matching community norms. This achieves Consensus Network alignment automatically through repetition.

Phenomenologically, such knowledge feels natural and obvious. You "know" these truths without necessarily grasping their structural justification. This is System 1 processing (Kahneman 2011): unconscious, automatic, relying on signal redundancy. The knowledge rests on pattern matching rather than causal understanding.

The risk: if the Consensus Network encodes false compressions ("Miasma causes plague," "Phlogiston explains combustion"), statistical learning traps you in shared delusion. You align with the network by matching its patterns, regardless of whether those patterns correspond to reality's constraint structure. Statistical learning maintains network cohesion but provides no error-correction mechanism when the entire network drifts from truth.

Reference:
Kahneman, Daniel. 2011. *Thinking, Fast and Slow*. New York: Farrar, Straus and Giroux. ISBN 978-0374275631.
------------------------------------------------------------

Citation 30 [PARENTHETICAL]:
Line: 541
Citation: (Chalmers 1996)

Context:
Scope of Claims: To be precise about our explanatory target, we distinguish three problems in consciousness research:

1. **The Hard Problem** (Chalmers 1996): Why does any information processing have phenomenal character at all? Why is there "something it is like" to see red rather than mere information processing without subjective experience?

2. **The Access Problem**: Which cognitive processes become available for conscious awareness, verbal report, and deliberate reasoning? Why can we introspect on some mental states but not others?

Reference:
Chalmers, David J. 2006. "Strong and Weak Emergence." In *The Re-Emergence of Emergence: The Emergentist Hypothesis from Science to Religion*, edited by Philip Clayton and Paul Davies, 244–54. Oxford: Oxford University Press. ISBN 9780199287147. https://doi.org/10.1093/acprof:oso/9780199287147.003.0011.
------------------------------------------------------------

Citation 31 [IN-PROSE]:
Line: 606
Citation: Laukkonen et al. (2025)

Context:
3. **Aha! moments:** Sudden recognition of structural relationships (see Section 4B.1 phenomenology table)
4. **Automation through practice:** Once structural understanding achieved, execution becomes statistical refinement (unconscious)

Connection to Active Inference Theories of Consciousness: Recent work grounds consciousness directly in Active Inference mechanisms. Laukkonen et al. (2025) propose that consciousness arises from a recursive self-evidencing loop in hierarchical systems. This requires three functional conditions: simulation of an epistemic field (world model), Bayesian binding (inferential competition to enter the world model), and epistemic depth (recurrent sharing of beliefs across levels).

Their framework aligns with our structural pattern recognition hypothesis. Consciousness may engage specifically when patterns exhibit sufficient internal constraint structure to support these recursive operations. Statistical regularities can be processed automatically without activating the full recursive loop. Structural coherence, however (precisely because components mutually constrain each other), demands the kind of inferential integration their theory describes.

Reference:
Laukkonen, Ruben, et al. 2025. "A Beautiful Loop: An Active Inference Theory of Consciousness." *Neuroscience & Biobehavioral Reviews* 176: 106296. https://doi.org/10.1016/j.neubiorev.2025.106296.
------------------------------------------------------------

Citation 32 [IN-PROSE]:
Line: 612
Citation: Marcus (2001)

Context:
The connection deepens when we consider Laukkonen et al.'s emphasis on counterfactual simulation and epistemic depth. Statistical learning creates flat predictions about the immediate sensory stream—what happens next given current input patterns. Structural learning, by contrast, enables detachment from the present to model "what if" scenarios: simulating alternative configurations, exploring constraint violations, testing structural relationships across hypothetical states. This capacity for counterfactual reasoning is precisely what Laukkonen identifies as creating "epistemic depth"—the recursive loop of simulating possible epistemic fields to select action policies. Our distinction between statistical (unconscious) and structural (conscious) processing thus maps onto their account: consciousness arises when the system engages in the counterfactual simulation that structural patterns both demand and enable.

Resonance with Classic Cognitive Science: This statistical/structural distinction also connects to foundational debates in cognitive science. Marcus (2001) famously argued that pure connectionist systems (neural networks relying on statistical pattern matching) struggle with the systematic, rule-based reasoning characteristic of human thought. While connectionist models excel at extracting correlations from data, they face difficulties with the compositional and algebraic operations that enable flexible generalization. Our framework provides an information-theoretic lens on this debate: what Marcus identified as the need for symbolic/algebraic operations corresponds to structural pattern recognition, detecting necessary relationships between mutually constraining components. Consciousness, on this account, may be the subjective signature of the system deploying these structural (algebraic) compressions, which are computationally distinct from the purely statistical regularities handled by unconscious subsystems. This suggests that the longstanding connectionist/symbolic divide reflects a genuine functional distinction in how patterns can be compressed, with consciousness tracking the structural mode.

Implications for Artificial Intelligence: This framework offers a diagnostic for the "hallucination" problem in current AI. Large Language Models operate primarily as ε-machines for statistical regularities, excelling at frequency-based pattern matching. While advanced Transformers can induce structural representations, their training objective (next-token prediction) prioritizes statistical likelihood over causal necessity. Hallucinations are often successful statistical mimicries that fail structural viability. A system predicting outcomes from corpus statistics rather than a causal world-model will inevitably "glitch" when statistical associations diverge from structural constraints.

Reference:
Marcus, Gary F. 2001. *The Algebraic Mind: Integrating Connectionism and Cognitive Science*. Cambridge, MA: MIT Press. ISBN 978-0262632683.
------------------------------------------------------------

Citation 33 [IN-PROSE]:
Line: 632
Citation: Parr and Friston (2025)

Context:
Only humans (as far as we know) achieve third-order regularly. This is meta-blanket formation: constructing a Markov blanket around your own Markov blankets, allowing self-modification.

Inner Screens and Imaginative Experience: Parr and Friston (2025) model this meta-level capacity through "inner screens"—internal boundaries with Markov blanket structure that function as classical information channels. These inner screens enable imaginative experience (planning, episodic memory, counterfactual reasoning) by allowing internally-generated content to employ the same spatial and conceptual reference frames used in ordinary perception. This provides a mechanistic account of how meta-awareness operates: the system constructs internal Markov blankets to model its own modeling processes, enabling the kind of third-order reflection that distinguishes human cognition.

The formation of meta-blankets is not merely metaphorical but physiologically grounded. Parr and Friston identify inner screens as functional Markov blankets within the cortical hierarchy that segregate imaginative planning from sensory perception. When you imagine biting into a lemon, you generate predictions about sourness without confusing them with actual taste input—the inner screen maintains the statistical boundary between internally-generated simulation and externally-driven sensation. This internal screening mechanism allows the system to manipulate its own dispositions as objects of thought: you can model your "distrust disposition" as a pattern to be examined, questioned, and potentially revised, rather than simply experiencing distrust as transparent reality. The meta-blanket thus enables the recursive operation where the system's predictive machinery becomes its own target of prediction, supporting the third-order reflection that characterizes metacognitive awareness.

Reference:
Parr, Thomas, and Karl J. Friston. 2025. "How Do Inner Screens Enable Imaginative Experience? Applying the Free-Energy Principle to Attention." *Neuroscience of Consciousness* 2025(1): niaf009. https://doi.org/10.1093/nc/niaf009.
------------------------------------------------------------

Citation 34 [IN-PROSE]:
Line: 636
Citation: Rosas et al. (2024)

Context:
The formation of meta-blankets is not merely metaphorical but physiologically grounded. Parr and Friston identify inner screens as functional Markov blankets within the cortical hierarchy that segregate imaginative planning from sensory perception. When you imagine biting into a lemon, you generate predictions about sourness without confusing them with actual taste input—the inner screen maintains the statistical boundary between internally-generated simulation and externally-driven sensation. This internal screening mechanism allows the system to manipulate its own dispositions as objects of thought: you can model your "distrust disposition" as a pattern to be examined, questioned, and potentially revised, rather than simply experiencing distrust as transparent reality. The meta-blanket thus enables the recursive operation where the system's predictive machinery becomes its own target of prediction, supporting the third-order reflection that characterizes metacognitive awareness.

The Self as a User Interface: Following Rosas et al. (2024), the "Self" is not a ghost in the machine but a control variable in the system's own high-level model: a user interface for the brain's self-regulation. It represents the brain's own lossy compression of its massive, distributed neural activity.

Just as a computer operating system represents billions of transistor states as a single "folder" icon, the brain compresses its complex somatic and cognitive states into a single variable: "I". This variable functions as a control parameter, allowing the system to predict and regulate its own future states without tracking every underlying neural process.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 35 [PARENTHETICAL]:
Line: 656
Citation: (Solms 2019)

Context:
We propose that consciousness relates to detecting and representing structural coherence rather than merely tracking statistical correlations. This isn't a solution to the hard problem, but it identifies a functional distinction that may map onto the phenomenological boundary between conscious and unconscious processing.

Recent work applies the Free Energy Principle directly to the hard problem, identifying affect (the feeling dimension of consciousness) as the subjective signature of free energy minimization—where decreases and increases in expected uncertainty are experienced as pleasure and unpleasure (Solms 2019). This suggests consciousness may track information-theoretic processes in a way that gives them phenomenological character, though why minimizing prediction error should feel like anything remains unexplained.

Alternative Formalization (Integrated Information Theory): A different approach starts from phenomenological axioms rather than functional principles. Oizumi et al. (2014) formalize Integrated Information Theory (IIT) by beginning with intrinsic properties of experience (it is structured, integrated, definite) and deriving physical postulates about the mechanisms that could instantiate these properties. IIT identifies consciousness with the maximally irreducible conceptual structure generated by systems with high integrated information (Φ).

Reference:
Solms, Mark. 2019. "The Hard Problem of Consciousness and the Free Energy Principle." *Frontiers in Psychology* 9: 2714. https://doi.org/10.3389/fpsyg.2018.02714.
------------------------------------------------------------

Citation 36 [IN-PROSE]:
Line: 658
Citation: Oizumi et al. (2014)

Context:
Recent work applies the Free Energy Principle directly to the hard problem, identifying affect (the feeling dimension of consciousness) as the subjective signature of free energy minimization—where decreases and increases in expected uncertainty are experienced as pleasure and unpleasure (Solms 2019). This suggests consciousness may track information-theoretic processes in a way that gives them phenomenological character, though why minimizing prediction error should feel like anything remains unexplained.

Alternative Formalization (Integrated Information Theory): A different approach starts from phenomenological axioms rather than functional principles. Oizumi et al. (2014) formalize Integrated Information Theory (IIT) by beginning with intrinsic properties of experience (it is structured, integrated, definite) and deriving physical postulates about the mechanisms that could instantiate these properties. IIT identifies consciousness with the maximally irreducible conceptual structure generated by systems with high integrated information (Φ).

This framework can be contrasted productively with IIT as representing complementary methodological approaches. IIT adopts an intrinsic-nature-first methodology: it starts from phenomenological axioms about what consciousness is like (experience has definite boundaries, internal differentiation, integration) and works backward to identify the physical systems that could instantiate these properties. Our framework, by contrast, adopts a function-first methodology: it starts from the functional problem any bounded system faces (compressing reality to minimize prediction error) and works forward to identify the processes that would require conscious engagement (structural pattern recognition, meta-blanket formation).

Reference:
Oizumi, Masafumi, et al. 2014. "From the Phenomenology to the Mechanisms of Consciousness: Integrated Information Theory 3.0." *PLOS Computational Biology* 10(5): e1003588. https://doi.org/10.1371/journal.pcbi.1003588.
------------------------------------------------------------

Citation 37 [PARENTHETICAL]:
Line: 843
Citation: (Ladyman and Ross 2007)

Context:
Connection to Optimal Structure: Mathematics is part of the optimal constraint configuration: the maximally compressed representation of structural constraints that any sufficiently thorough compression must discover.

Philosophical Positioning: This view differs from Mathematical Platonism in key respects. Platonism holds that mathematical objects exist independently in an abstract realm, which our minds somehow access. We claim instead that mathematical structures are optimal compression protocols for describing constraint relationships; they are discovered rather than invented, but what is being discovered is the structure of the constraint space itself, not objects in a separate ontological realm. This aligns more closely with ontic structural realism (Ladyman and Ross 2007), which holds that structure rather than objects is ontologically fundamental. The "necessity" of mathematics derives not from inhabiting a timeless Platonic heaven but from the fact that certain compression strategies are uniquely optimal given the axioms and constraints. Independent discovery of π across cultures evidences objective structure, but what makes π necessary is the relationship it compresses (circumference to diameter in Euclidean space), not its existence as an abstract entity. This framework preserves mathematical objectivity without requiring a separate realm of mathematical objects.

Mathematical Fallibility: Even mathematics has its Negative Canon. Naive Set Theory, which allowed sets to contain themselves, generated Russell's Paradox, a contradiction revealing that the blanket had leaked. The mathematical community didn't simply choose to revise set theory; they were forced to by the internal incoherence the system produced. Failed mathematical definitions, like failed scientific theories, represent compressions that could not achieve computational closure. They generated contradictions (information leakage at the logical level) and were selected against, demonstrating that mathematical knowledge, like empirical knowledge, is subject to the same selection pressures we describe throughout this framework.

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 38 [IN-PROSE]:
Line: 870
Citation: Rosas et al. (2024)

Context:
The answer follows from Section 3.3: emergence succeeds when a Markov blanket configuration achieves computational closure (lumpability, Markovianness, and causal shielding). When these conditions hold, the emergent level is as causally real as the base level.

Rosas et al. (2024) formalize this as causal decoupling: when the macro-level ε-machine achieves equivalent predictive accuracy to the micro-informed υ-machine, the macro-level has achieved genuine autonomy. This is strong emergence: not merely useful description but substrate-independent causal dynamics.

Failed emergence equals information leakage. Phlogiston's attempted macro-variable ("phlogiston content") failed lumpability: predicting combustion required oxygen levels, molecular structure, and other substrate details the macro-variable couldn't capture. The blanket was porous; brittleness accumulated until abandonment.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 39 [IN-PROSE]:
Line: 882
Citation: Rosas et al. (2024)

Context:
The optimal constraint configuration is the complete set of generative prior configurations that achieve minimum systemic brittleness: the intersection of all maximally viable compression structures. In information-theoretic terms, it represents the "ultimate" ε-machine (using the term to denote the limit of maximal compression rather than a final, static state). It functions as the thermodynamic attractor where information leakage is theoretically minimized.

Rosas et al. (2024) demonstrate that all valid coarse-grainings of a system form a mathematical **lattice**: a hierarchical structure of nested compression levels, where each node represents a different way to group micro-states into macro-states. Not all coarse-grainings are equally robust: some achieve only weak lumpability (working only for specific initial conditions), while others achieve strong lumpability (preserving macro-dynamics regardless of substrate details). The optimal constraint configuration corresponds to the optimal path through this lattice: the set of strongly lumpable coarse-grainings that maximize causal autonomy while minimizing computational complexity. Reality allows many valid maps (the full lattice), but the optimal constraint configuration represents those compressions that achieve genuine substrate independence. This is why objective truth is not correspondence to a single privileged description but the achievement of strong lumpability: the predicate holds regardless of the underlying micro-state distribution, making it a genuine feature of reality rather than an artifact of our perspective.

Terminological Note: While the mathematical structure is technically a lattice (a partially ordered set with strict hierarchical derivation), we use "optimal constraint configuration" to emphasize the socially shared and collectively discovered nature of these optimal compressions. This configuration emerges through distributed exploration by communities of knowers rather than individual deduction.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 40 [IN-PROSE]:
Line: 898
Citation: Friston et al. (2025)

Context:
The constraints exist first; the optimal structure they determine is a necessary implication. Historical filtering is how we discover this structure, not how we create it.

Collective Intelligence and Emergent Knowledge: Recent work on multi-agent active inference provides a concrete mechanism for understanding how the optimal constraint configuration emerges through distributed exploration rather than centralized design. Friston et al. (2025) demonstrate that collectives can be treated as emergent agents with their own Markov blankets, possessing synergistic information that no individual member encodes. In their model of flocking birds, the collective "knows" the predator's location and optimal escape direction through the coordination of individual responses, even though no single bird detects or represents this information. The flock exhibits genuine collective knowledge that emerges from distributed processing.

This constraint-determined structure aligns with what Dittrich and Kinne (2024) call "reality-filtered data." Information produced by surviving agents already embeds causal structure because only causally accurate compressions enable the agents generating that data to persist. The optimal constraint configuration can thus be understood as the attractor state in the space of possible compressions where information-theoretic efficiency is maximized. It represents the set of generative priors that constitute sustainable, generative models (in CEP's terms), discovered through distributed exploration of reality's constraint structure. This convergence mechanism explains how cultural and historical processes can approach objective truth without requiring direct access to a Platonic realm: agents exploring reality under survival pressure necessarily generate data reflecting causal constraints, and compressions of that data necessarily track those constraints.

Reference:
Friston, Karl J., Thomas Parr, Conor Heins, Axel Constant, Daniel Friedman, Takuya Isomura, Chris Fields, Tim Verbelen, Maxwell Ramstead, John Clippinger, and Christopher Frith. 2025. "What the Flock Knows That the Birds Do Not: Exploring the Emergence of Joint Agency in Multi-Agent Active Inference." *arXiv* preprint arXiv:2511.10835. https://arxiv.org/abs/2511.10835.
------------------------------------------------------------

Citation 41 [IN-PROSE]:
Line: 900
Citation: Dittrich and Kinne (2024)

Context:
Collective Intelligence and Emergent Knowledge: Recent work on multi-agent active inference provides a concrete mechanism for understanding how the optimal constraint configuration emerges through distributed exploration rather than centralized design. Friston et al. (2025) demonstrate that collectives can be treated as emergent agents with their own Markov blankets, possessing synergistic information that no individual member encodes. In their model of flocking birds, the collective "knows" the predator's location and optimal escape direction through the coordination of individual responses, even though no single bird detects or represents this information. The flock exhibits genuine collective knowledge that emerges from distributed processing.

This constraint-determined structure aligns with what Dittrich and Kinne (2024) call "reality-filtered data." Information produced by surviving agents already embeds causal structure because only causally accurate compressions enable the agents generating that data to persist. The optimal constraint configuration can thus be understood as the attractor state in the space of possible compressions where information-theoretic efficiency is maximized. It represents the set of generative priors that constitute sustainable, generative models (in CEP's terms), discovered through distributed exploration of reality's constraint structure. This convergence mechanism explains how cultural and historical processes can approach objective truth without requiring direct access to a Platonic realm: agents exploring reality under survival pressure necessarily generate data reflecting causal constraints, and compressions of that data necessarily track those constraints.

This formalizes how the optimal constraint configuration operates. Just as a flock possesses information about environmental gradients that individual birds do not, a culture's network of generative priors can encode compressions that no individual fully grasps. The optimal constraint configuration represents the collective computational closure achieved when communities of knowers, each exploring different regions of the compression landscape, generate a distributed set of predicates whose interactions reveal constraint structures invisible to isolated inquiry. It is neither a pre-existing blueprint awaiting discovery nor an arbitrary social construction, but the emergent pattern of collective intelligence navigating reality's constraints.

Reference:
Dittrich, Christian, and Jennifer Flygare Kinne. "The Information-Theoretic Imperative: Compression and the Epistemic Foundations of Intelligence." Preprint, submitted October 30, 2024. arXiv:2510.25883 [cs.AI]. https://doi.org/10.48550/arXiv.2510.25883.
------------------------------------------------------------

Citation 42 [PARENTHETICAL]:
Line: 905
Citation: (Bennett 1988)

Context:
This formalizes how the optimal constraint configuration operates. Just as a flock possesses information about environmental gradients that individual birds do not, a culture's network of generative priors can encode compressions that no individual fully grasps. The optimal constraint configuration represents the collective computational closure achieved when communities of knowers, each exploring different regions of the compression landscape, generate a distributed set of predicates whose interactions reveal constraint structures invisible to isolated inquiry. It is neither a pre-existing blueprint awaiting discovery nor an arbitrary social construction, but the emergent pattern of collective intelligence navigating reality's constraints.

#### 7.1.1 Logical Depth as Epistemic Capital
We must distinguish between mere compressibility (Kolmogorov complexity) and the value of the compression. A random string is incompressible; a string of a million zeros is highly compressible but trivial. The optimal constraint configuration consists of compressions that exhibit high *logical depth* (Bennett 1988). Logical depth measures the computational work (time and energy) required to generate a pattern from its minimal description.

Stable concepts (acting as shared generative priors) function as batteries of stored computational work. The predicate "$F=ma$" is brief, but its Logical Depth is immense: it encodes the integrated output of centuries of astronomical observation and calculus development. When an agent adopts a high-depth predicate from the consensus network, they inherit the result of this thermodynamic expenditure without performing the computation themselves. This formalizes the function of a generative prior: it encapsulates the computational history required to derive it. In this framework, Truth is Stored Work. Specifically, it is the maximization of Logical Depth: the thermodynamic cost saved by inheriting a compression rather than deriving it from scratch. F=ma is true not just because it predicts, but because it is a battery containing centuries of computational labor. The Optimal Constraint Configuration is simply the set of all such batteries that cannot be compressed further. This explains why the optimal constraint configuration is an attractor: it represents the path of least resistance for minimizing future metabolic costs via the inheritance of past computational work.

Reference:
Bennett, Charles H. 1988. "Logical Depth and Physical Complexity." In *The Universal Turing Machine: A Half-Century Survey*, edited by Rolf Herken, 227–257. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 43 [IN-PROSE]:
Line: 959
Citation: Glenn (2025)

Context:
This formalism captures the concept but should not be mistaken for literal metaphysics. It represents the structural pattern that emerges from constraint-driven selection, not a pre-temporal mathematical object.

Terminological Note: Glenn (2025) uses "Apex Network" to refer to this same structure. The concepts are identical: both denote the objective, constraint-determined configuration of minimum systemic brittleness toward which viable knowledge systems must converge. We use "Optimal Constraint Configuration" here for its more technical, information-theoretic flavor, while "Apex Network" serves the same function with more natural phrasing. The choice is rhetorical rather than conceptual. Both terms describe the thermodynamic attractor in the phase space of possible belief systems, the intersection of all maximally viable configurations, existing whether discovered or not because it is determined by reality's constraint structure rather than by our beliefs about it.

### 7.2 Truth as Successful Computational Closure

Reference:
Glenn, Patrick. 2025. "The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth." PhilPapers. https://philpapers.org/rec/GLETAO.
------------------------------------------------------------

Citation 44 [PARENTHETICAL]:
Line: 991
Citation: (Campbell 1974)

Context:
Why Different Cultures Converge on Similar Truths:

Not because of:
- Shared biology alone (though this constrains) (Campbell 1974)
- Social agreement (though this accelerates) (Longino 1990)
- Divine revelation
- Platonic access

Reference:
Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.
------------------------------------------------------------

Citation 45 [PARENTHETICAL]:
Line: 992
Citation: (Longino 1990)

Context:
Not because of:
- Shared biology alone (though this constrains) (Campbell 1974)
- Social agreement (though this accelerates) (Longino 1990)
- Divine revelation
- Platonic access

Reference:
Longino, Helen E. 1990. *Science as Social Knowledge: Values and Objectivity in Scientific Inquiry*. Princeton, NJ: Princeton University Press. ISBN 978-0691020518.
------------------------------------------------------------

Citation 46 [PARENTHETICAL]:
Line: 1102
Citation: (James 1890)

Context:
- Qualia: Potentially compression gradients rendered in subjective space, though why these have qualitative character remains unexplained
- Self-awareness: Meta-blanket formation (blanket monitoring its own blankets)
- Unity of consciousness: Integrated information across blanket hierarchies
- *Institutional Minds Without Phenomenology*: While social institutions possess Markov blankets and exhibit information processing (they minimize surprise, maintain boundaries, adapt to environments), this framework does not imply they possess phenomenal consciousness. The difference lies in the timescale of integration. Human phenomenology relies on the immediate temporal binding of diverse information streams into a unified present moment (James 1890, 609). Institutional information processing occurs over days or months, far too slow for the integrated temporal binding characteristic of subjective experience. They are functional agents without phenomenology.

The Explanatory Gap Narrows But Doesn't Close: We've identified a functional distinction (statistical vs. structural pattern recognition) that appears to track the phenomenological boundary. This narrows the explanatory target: not "why does any information processing feel like something?" but "why does detecting mutual constraints feel like something?" That's progress, but the hard problem: why any functional process has subjective character, remains open.

Reference:
James, William. 1890. *The Principles of Psychology*. 2 vols. New York: Henry Holt and Company.
------------------------------------------------------------

Citation 47 [PARENTHETICAL]:
Line: 1137
Citation: (Glenn 2025)

Context:
Methodological Note (The Is/Ought Boundary):

We must acknowledge a crucial limit. This framework describes how certain social configurations generate higher or lower thermodynamic costs. However, it does not (and cannot) directly derive moral obligations from these descriptive facts. The move from "X generates high brittleness" to "therefore, X is morally wrong" crosses Hume's is/ought gap. (Glenn 2025).

What we can legitimately claim: Certain moral intuitions may have information-theoretic grounding. The recognition that denying others' agency generates catastrophic social costs helps explain why such behaviors are unsustainable. It also helps explain why moral progress often tracks toward lower-brittleness configurations.

Reference:
Glenn, Patrick. 2025. "The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth." PhilPapers. https://philpapers.org/rec/GLETAO.
------------------------------------------------------------

Citation 48 [IN-PROSE]:
Line: 1157
Citation: Rosas et al. (2024)

Context:
Information-Theoretic Analysis of Agency Denial:

Rosas et al. (2024) demonstrate that causally closed systems can be efficiently controlled through macro-level interventions—engaging with their computational closure rather than manipulating their substrate. This insight provides a mechanistic account of moral interaction: when we engage with another agent's reasons, beliefs, and goals (their ε-machine), we interact efficiently with the causally autonomous level. We work through their computational closure, allowing their internal dynamics to determine outcomes. When we bypass their agency to force their physical body or manipulate their circumstances (intervening on the substrate), we breach their causal closure and must manage all the micro-level resistance their autonomous system generates.

This explains why persuasion is thermodynamically cheaper than coercion: persuasion operates at the ε-machine level (engaging autonomous macro-dynamics), while coercion operates at the substrate level (fighting against those dynamics). Substrate interference is thermodynamically expensive in a specific way: by discarding the efficient causal structure of the agent's computational closure, the coercer must manage the full complexity of the micro-states directly. Persuasion leverages the agent's own compression architecture; coercion fights against it. The computational closure Rosas et al. identify is precisely what moral recognition respects and evil violations ignore.

Reference:
Rosas, Fernando E., et al. 2024. "Disentangling High-Order Mechanisms and High-Order Behaviours in Complex Systems." *Nature Physics* 20: 1095–1104. https://doi.org/10.1038/s41567-024-02477-4.
------------------------------------------------------------

Citation 49 [IN-PROSE]:
Line: 1231
Citation: Glenn (2025)

Context:
Consciousness, agency, and truth remain real within this framework: not eliminated or reduced away, but understood as emerging from information processing under constraint. The framework is naturalistic without being eliminativist.

Glenn (2025) develops these concepts in application to macro-epistemology and historical knowledge systems. This paper provides the theoretical grounding in information theory, computational closure, and constraint-driven selection.

Ultimately, this framework suggests a unification of metaphysics and information theory. Reality is not a flat soup of particles where only the smallest things are real. It is a lattice of computational machines, nested within one another. Each machine (a cell, a mind, a society) maintains its own closure and runs its own causal code. Science is not the reduction of these machines to their parts, but the discovery of which machine correctly predicts the phenomenon at hand.

Reference:
Glenn, Patrick. 2025. "The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth." PhilPapers. https://philpapers.org/rec/GLETAO.
------------------------------------------------------------

