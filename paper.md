
# **The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth**

## **Abstract**

Coherentist theories of justification are vulnerable to the isolation objection: the possibility that a perfectly coherent belief system could be detached from reality. This paper develops Systemic Externalism, a framework that grounds coherence in the demonstrated viability of knowledge systems. The framework introduces systemic brittleness, a diagnostic tool for measuring network health through the observable costs of applying propositions. We argue that selective pressure from these costs forces knowledge systems to converge on an emergent structure—the **Apex Network**—representing not pre-existing truth but a bottom-up pattern of maximally viable propositions surviving historical filtering. A claim's justification depends on both its internal coherence and the proven resilience of the public Consensus Network certifying it. This naturalistic theory redefines objective truth as alignment with the Apex Network's emergent structure, explains how Quine's web of belief undergoes pragmatic revision, and grounds a falsifiable research program for assessing epistemic system health. This program is illustrated through preliminary applications to historical cases, such as Ptolemaic astronomy and AI winters, using quantifiable proxies like citation patterns and resource allocation metrics.

## **1. Introduction: From a Static Web to a Dynamic Process**

Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Miasma theory generated catastrophic costs—thousands died in London because public health efforts were misdirected at odors rather than contaminated water—and required accelerating ad hoc modifications to explain anomalies. Miasma theory's brittleness can be illustrated through its high patch velocity (P(t)); historical analyses (e.g., Snow 1855) suggest the need for dozens of ad-hoc modifications by the mid-19th century to explain accumulating anomalies. Germ theory, by contrast, dramatically reduced these costs while explaining diverse phenomena with a unified conceptual framework.

This dynamic illustrates the classic isolation objection to coherentism: as BonJour (1985) noted, a belief system may be perfectly coherent yet entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. This paper develops an alternative response grounding coherence in the demonstrated viability of entire knowledge systems, measured through their capacity to minimize costs over time.

The framework, which we term **Systemic Externalism**, holds that justification requires two conditions: internal coherence within a shared network (a **Consensus Network**—our fallible, historically-situated collective knowledge system) and that network's demonstrated reliability through low **brittleness** (accumulated vulnerability to collapse due to rising systemic costs). This dual requirement provides the externalist constraint coherentism needs while preserving its holistic character.

The model treats inquiry as an evolutionary process cultivating viable public knowledge systems. It is a macro-epistemology focusing on cumulative systems like science and law, where claims build upon previous work and practical consequences provide feedback about performance.

Three important scope limitations should be noted upfront. First, the framework applies primarily to cumulative knowledge systems where pragmatic feedback is measurable—empirical science, engineering, law, and policy. While the framework's core diagnostics apply universally, its extension to abstract domains like pure mathematics—where pragmatic pushback is primarily internal—requires treating brittleness as inefficiency in proof structures or axiomatic consistency, as explored in Section 6.4. Its application to pure mathematics or abstract philosophy remains underdeveloped. Second, the framework operates at macro-historical timescales and may not resolve real-time controversies. Third, in domains where power structures control information flow, the framework's diagnostics may be suppressed or delayed for extended periods.

Three clarifications prevent misunderstanding. First, viability differs from mere endurance. A coercive empire may persist, but it is a textbook case of a high-brittleness system; its longevity is not a sign of health but a measure of the immense energy it wastes suppressing its own instability. True viability means addressing challenges with low, stable systemic costs. Second, the framework incorporates power and contingency as central variables, not exceptions. The exercise of power to maintain a brittle system is itself a primary indicator of non-viability through measurable coercive costs. Third, the model's claims are probabilistic, not deterministic. Systems accumulating brittleness become progressively vulnerable to contingent shocks, offering a falsifiable research program for understanding structural dynamics, not a deterministic theory of history.

## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

**The Deflationary Path: Belief → Proposition → Validated Data → Standing Predicate**

#### **2.1.1 From Private Belief to Public Proposition**

The journey begins with *belief*, the raw material of epistemology. As a private psychological state tied to an individual's consciousness, it is analytically inaccessible for a theory of public knowledge. The first step is therefore to isolate its testable, public content as a *proposition*: a falsifiable, testable claim articulated in language that is subject to collective assessment. Unlike the abstract, language-independent content of sentences in traditional philosophy, our treatment is deliberately deflationary and functional, focusing on propositions as concrete, evaluable statements made by agents within a knowledge network. This transformation from private belief to public proposition is essential because it makes beliefs accessible to collective evaluation and allows them to function within a shared epistemic system.

#### **2.1.2 The Coherence Test**

Next, a candidate proposition must pass a rigorous test for coherence. This is not the thin, formal consistency of logic, but a thick, forward-looking pragmatic assessment—a form of risk analysis. A shared network, as a resource-constrained system, implicitly asks: will integrating this proposition increase or decrease our long-term systemic brittleness? A proposition that successfully passes this test is accepted into the network as validated data—a reliable claim that can be used and cited within the system, awaiting its ultimate functional test.

#### **2.1.3 From Validated Data to Standing Predicate**

When a proposition proves exceptionally useful in reducing systemic costs, its core principle becomes a Standing Predicate—a reusable tool for evaluating new cases. For instance, once "cholera is an infectious disease" proved valuable, the schema "…is an infectious disease" was promoted to Standing Predicate status. Applying it to new cases automatically mobilizes proven strategies—isolating patients, tracing transmission, targeting pathogens. A predicate's status is provisional: if it begins to generate rising brittleness, it can be demoted.

This differs from predicates in formal logic (functions returning truth values) and traditional philosophical predicates (denoting properties). Standing Predicates are defined functionally: by their track record of enabling successful action across diverse applications. They are the units of cultural-epistemic selection—conceptual tools that prove their worth through demonstrated capacity to reduce brittleness, creating a positive feedback loop that reinforces the most reliable inferential patterns.

### **2.2 The Units of Analysis: Predicates, Networks, and Replicators**

Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A *Shared Network*, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011). 

The *Standing Predicate* is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.

The model's deflationary path shifts from private *belief* (psychological state) to public *proposition* (testable claim), potentially becoming a *Standing Predicate* if it reduces costs exceptionally.

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory, as synthesized for the social sciences by Mesoudi (2011). The network’s abstract informational structure—its core Standing Predicates and their relations—functions as the **replicator**: the "code" that is copied and transmitted. The social group and its institutions (the scientific community, legal system, or society) function as the **interactor**: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This distinction is crucial for understanding how knowledge can evolve and persist across different social contexts. It explains how knowledge can persist even when the societies that created it do not; the rediscovery of Roman legal principles during the Renaissance is a prime example of a successful replicator outliving its original interactor. The existence of these countless, independently formed and often nested Shared Networks—all responding to the same landscape of physical and social constraints—supports a crucial inference: they collectively reveal an objective structure that underwrites all successful inquiry. This mosaic of survived predicates, biased toward low-brittleness solutions, anticipates the Apex Network: not a pre-existing truth, but an emergent attractor forged by historical filtering (as detailed in Section 4).

### **2.3 Pragmatic Pushback and Systemic Costs**

A shared network is not a passive library; it is an active system under constant pressure from *pragmatic pushback*—our model’s term for the systemic analogue of what Quine called a "recalcitrant experience" at the periphery of an individual's web of belief. It is the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback is not an argument but a material outcome—a bridge collapses, a treatment fails, a society fragments. Pragmatic pushback generates *first-order costs* (direct failures like excess mortality) and *systemic costs* (secondary burdens, including *conceptual debt* from patches and *coercive overheads* from suppressing dissent), which signal misalignment and drive revision—quantifying health for a falsifiable program (see Sections 2.4–2.5).

It is important to note that "pragmatic pushback" is not limited to direct, material failures. In highly abstract domains, such as theoretical physics or pure mathematics, where direct empirical tests are deferred or unavailable, pushback primarily manifests through the accumulation of costs. A research program that requires an accelerating rate of ad-hoc modifications to maintain internal consistency, or that loses its unifying power, is experiencing a powerful form of pragmatic pushback. These epistemic inefficiencies are real costs that render a network brittle and unproductive, even if it has not been directly falsified by an experiment. The framework's diagnostic lens thus applies universally to all forms of inquiry, measuring viability through either external, material consequences or internal, systemic dysfunction. This universal applicability is crucial for our claim that the framework provides a general solution to the isolation objection across all domains of knowledge.

### **2.4 Gauging Brittleness: An Empirical Toolkit**

A system's brittleness is a measure of its accumulated costs. To avoid overconfidence in formalization, we note that a complete quantitative model remains a goal for empirical research rather than a settled achievement. The following table illustrates potential diagnostic indicators for each component:

| Indicator | Domain | Potential Proxy Metric | Data Collection Method | Example Application |
| :--- | :--- | :--- | :--- | :--- |
| Rate of Ad-Hoc Modification (P(t)) | Scientific Paradigms | A bibliometric ratio calculated as: (Citations to papers primarily resolving known anomalies) / (Citations to papers generating novel, successful predictions). A rising trend indicates degenerative patching. | Academic databases (e.g., Web of Science, arXiv), using NLP to classify paper abstracts by function (anomaly-resolution vs. prediction-generation) | In Lysenkoism, auxiliary hypotheses rose from 10% to 40% of publications (1940-1950) |
| Ratio of Coercion to Production (C(t)) | Socio-Political Networks | Internal security budget / Total productive budget. Values >0.3 indicate high brittleness | Budget data, Seshat Databank, World Bank institutional analyses | Soviet Union pre-collapse: security budgets >30% GDP correlated with fragility (Turchin 2003) |
| Increasing Model Complexity (M(t)) | Computational Systems | Parameter count growth rate / Performance improvement rate. Exponential divergence signals brittleness | arXiv trends, MLPerf benchmarks, computational cost analyses | Deep learning 2018-2023: parameter counts grew 100x while performance gains ~20% |
| Resilience Reserve (R(t)) | Cross-Domain | Number of independent domain confirmations; age and stability of core principles without major revision | Historical records, citation networks, cross-domain literature analysis | Germ theory: confirmed across medicine, sanitation, biology, epidemiology |

Brittleness can be broken down into four measurable dimensions:

- **Patch Velocity (P(t))**: The rate of ad-hoc modifications relative to novel predictions, indicating conceptual debt accumulation.

- **Coercion Ratio (C(t))**: The proportion of resources devoted to suppressing alternatives, measuring normative brittleness.

- **Model Complexity (M(t))**: The escalation of complexity without proportional gains, signaling inefficiency.

- **Resilience Reserve (R(t))**: The breadth of independent confirmations and stability of core principles, providing a buffer against shocks.

This structure allows brittleness to be assessed holistically, with each dimension providing convergent evidence. While some judgment is required (e.g., classifying papers as anomaly-resolution vs. prediction-generation), this can be operationalized through systematic coding protocols and inter-rater reliability checks.

### **2.5 A Worked Example: Measuring Brittleness in Ptolemaic Astronomy**

Consider Ptolemaic astronomy circa 1500 CE. Its predictive machinery required about 80 epicycles to remain accurate, with new observations demanding 2–3 more each decade. Complexity ballooned while returns diminished: predictive accuracy rose only 15% despite a 400% increase in computational burden.

Applying these metrics to contemporary AI development reveals potential brittleness. Deep learning's M(t) shows parameter counts growing 100x from 2018-2023 (e.g., GPT-3 with 175 billion parameters vs. early models with millions), while performance gains on benchmarks like MLPerf have improved only 20-30%. This exponential complexity escalation without proportional returns signals brittleness: each marginal improvement requires vastly more computational resources, leading to unsustainable energy costs and diminishing returns. P(t) is evident in the proliferation of 'alignment' papers—thousands of publications on AI safety in recent years—many of which address anomalies in model behavior rather than generating novel capabilities. R(t) declines as AI remains isolated from broader scientific integration, with limited cross-domain applications beyond narrow tasks. These trends suggest potential warning signs of rising brittleness in deep learning, which may invite cautious comparison to the structural dynamics of past degenerating research programs like late-stage Ptolemaic astronomy.

The operationalization of brittleness faces an unavoidable circularity: measuring systemic costs requires standards for "waste" or "dysfunction" that themselves depend on epistemic commitments. We acknowledge this as a fundamental limitation rather than a fully solvable problem. The framework manages this hermeneutic circle by constraining interpretation through a methodology of triangulation, relying on three principles:

* "**Physical-Biological Anchors:** We anchor measurements in outcomes that register as failures across divergent theoretical perspectives. Demographic collapse, infrastructure failure, and resource depletion provide relatively theory-neutral indicators of breakdown."
* "**Comparative-Diachronic Methods:** The framework's claims are strongest when made comparatively (System A exhibits lower brittleness than System B) or diachronically (System C's brittleness is rising over time). Such judgments can be robust even when absolute standards are contested."
* "**Convergent Evidence Requirements:** A diagnosis of brittleness is only made when multiple, independent indicators converge. Systematic convergence across bibliometric, institutional, and empirical measures becomes progressively difficult to dismiss as mere theoretical bias."

This yields what we call 'constrained interpretation'—structured judgment accountable to evidence. It is sufficient for the framework's purpose: distinguishing degenerating from progressive research programs.

We distinguish degenerative "patches" from progressive hypotheses by assessing explanatory return on investment. Progressive hypotheses offer high returns: small complexity investments yielding novel predictions or unifying phenomena. Degenerative patches offer low returns: high-cost fixes resolving only targeted anomalies while increasing overall complexity. The Higgs boson exemplifies the former, adding one particle but unifying electroweak theory with confirmed novel predictions. Ptolemaic epicycles exemplify the latter, requiring ever-more geometrical complexity to save specific planetary observations without generating testable insights. Operationally, this distinction can be measured through bibliometric analysis: does a modification primarily generate citations for novel predictions and applications, or primarily for managing known anomalies?

To illustrate how this triangulation methodology works in practice, consider the diagnosis of rising brittleness in a hypothetical research program. We would not rely solely on bibliometric indicators (like an increasing ratio of auxiliary modifications to novel predictions) but would look for convergent evidence across multiple domains: institutional indicators (increasing resource allocation to managing dissent within the research community), empirical indicators (declining predictive accuracy or increasing reliance on immunizing strategies), and social indicators (growing difficulty in training new researchers or public support).

Crucially, this convergence requirement does not eliminate the hermeneutic dimension but constrains it. Any single indicator might be explained away through alternative interpretations, but systematic convergence across independent measures becomes increasingly difficult to dismiss. The methodology thus provides what we might call "constrained interpretation"—structured judgment that remains accountable to multiple streams of evidence.

We acknowledge this falls short of mechanical objectivity, but mechanical objectivity was never the goal. The framework aims to make evaluative judgments more systematic, transparent, and accountable to evidence, not to eliminate judgment entirely.

To further address the potential for motivated reasoning, we emphasize that brittleness assessments require inter-rater reliability checks and peer validation. Coding protocols should be developed with clear, operational definitions and tested for consistency across diverse analysts.

Regarding the complexity of causal attribution, we recognize that systemic costs are often confounded by external factors. Our falsifiable research program (Section 7.4) proposes statistical methods to control for these, such as multivariate regression models that include variables for economic conditions, political stability, and environmental factors. While absolute causal isolation is challenging, comparative analyses—e.g., systems with similar contexts but differing brittleness—can provide robust evidence. The framework's probabilistic nature allows for meaningful inferences even in complex historical settings.

### **2.6 Two Modalities of Systemic Brittleness**

The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism. While this unification is a promising extension of the core framework, it relies on controversial metaethical commitments and should be viewed as speculative. The paper's central claims about epistemic justification stand independently of this normative extension.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application.

## **3. The Drivers of Adaptation: The Logic of Systemic Viability**

The framework for assessing systemic health detailed in Section 2 is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems.

### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.

**Instrumental Argument**: The framework makes a falsifiable empirical claim: networks with high, rising brittleness are statistically more likely to collapse or require radical revision. This yields a conditional recommendation: *if* an agent or institution seeks long-term stability and problem-solving capacity, *then* it has evidence-based reason to adopt low-brittleness principles.

This focus on shared, public networks directly addresses the isolation objection. A perfectly coherent but detached system—such as the internal logic of a complex video game or a self-consistent fictional world—might exhibit low internal "costs." However, Systemic Externalism is not a theory of arbitrary systems; it is a theory of public knowledge systems that must operate in a single, shared reality. The ultimate test of viability is not internal coherence but cross-domain application. A system's brittleness becomes truly apparent when its principles are used to act upon the world and interact with other systems. The "pragmatic pushback" from a bridge collapsing or a medical treatment failing is an inter-subjective, reality-based constraint that no isolated system can simulate or evade.

This differs from mere instrumental rationality. The end (viable inquiry) is not an arbitrary preference but a structural necessity for systems participating in cumulative knowledge production. The means (low-brittleness principles) face recursive constraints—they must themselves demonstrate low brittleness, preventing purely expedient solutions. This recursive structure makes norms responsive to objective pragmatic constraints, not mere efficiency.

**Naturalized Proceduralism**: The framework's contribution is best understood as naturalized proceduralism. Like procedural realists (e.g., later Putnam), we ground objectivity in procedural properties rather than direct correspondence. The crucial divergence: where rationalist accounts locate objectivity in idealized discourse norms, our model grounds it in the empirical, historical process of pragmatic selection. The arbiter is not the internal coherence of our reasons but the measurable brittleness of systems those reasons produce. Arguments are disciplined by non-discursive data: systemic success and failure.

When the model describes one network as "better" or identifies "epistemic progress," these are technical descriptions of systemic performance, not subjective values. A "better" network exhibits lower measured brittleness and higher predicted resilience. Viability is a structural precondition for any system entering the historical record.

## **3. Why Some Systems Collapse**

### **3.2 Coherence as Forward-Looking Risk Assessment**

Framed by the logic of viability, the test for coherence is a thick, forward-looking *cost-benefit analysis*: a set of heuristics that a resource-constrained system uses to bet on whether adopting a new proposition will increase or decrease its long-term brittleness. The traditional epistemic virtues are the core principles of this practical calculus:

* **Logical Consistency:** A hedge against the infinite future costs of inferential paralysis.
* **Explanatory Power:** A measure of a proposition’s potential return on investment, reducing future inquiry costs by paying down conceptual debt.
* **Simplicity / Parsimony:** A direct measure of systemic overhead; complex propositions increase long-term maintenance costs.
* **Evidential Support:** An assessment of integrative risk; a well-supported claim is a low-risk investment unlikely to trigger a cascade of costly future revisions.

One might object that this account reduces scientific revolutions to purely pragmatic considerations, ignoring the role of theoretical virtues like explanatory depth or mathematical elegance. However, these virtues are themselves pragmatically valuable in our framework: explanatory depth reduces future conceptual debt by unifying disparate phenomena, while mathematical elegance often signals structural efficiency that minimizes maintenance costs. Rather than eliminating traditional theoretical virtues, our framework explains their pragmatic function within the evolutionary process of knowledge development.

This forward-looking model also explains how revolutionary science is possible. When a dominant Consensus Network exhibits high and rising systemic brittleness—a state corresponding to a Kuhnian 'crisis'—the cost-benefit analysis for new propositions shifts dramatically. A radical new hypothesis, while having low coherence with the existing network's specific principles, may promise a massive long-term reduction in systemic costs. The network, in effect, makes a high-risk, high-reward bet. Coherence, in this dynamic sense, is not a conservative check for conformity, but a pragmatic assessment of a proposition's potential to resolve systemic crisis.

## **4. Convergence and the Emergence of Objective Structures**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We will show how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

Our account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is often of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system—its descent into crippling inefficiency, intellectual stagnation, and institutional decay—provides a clear, non-negotiable data point. This approach differs from traditional foundationalism by building knowledge from what we can confidently reject rather than from what we must indubitably accept.

The systematic analysis of these failures allows us to build the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been empirically invalidated by the catastrophic systemic costs they reliably generate. The *Negative Canon* catalogs failed networks invalidated by costs (e.g., phlogiston), anchoring objectivity by mapping untenable paths and constraining coherence.

E.g., phlogiston chemistry, miasma theory, luminiferous aether, and blank slate psychology, as detailed in our worked examples. While some elements of failed theories may return in modified forms, the Canon identifies core structural patterns that reliably generate costs, not every specific claim.

This canon charts failures of both causal and normative alignment:

* **Failures of Causal Alignment** are characteristic of what we term *Epistemic Brittleness*. Systems like scholastic physics, phlogiston chemistry, and Lysenkoist biology entered the canon because their core principles generated catastrophic causal failures, leading to chronically high rates of ad-hoc modification and predictive collapse. They represent failed blueprints for the causal world.
* **Failures of Normative Alignment** are characteristic of *Normative Brittleness*. Systems like chattel slavery or totalitarianism entered the canon because their core principles violated emergent normative facts about human cooperation. Their unsustainability was demonstrated by the immense and ever-rising coercive overheads required to suppress the normative pushback they generated. These are not condemned by a modern moral judgment alone; they are failed engineering principles for a viable social world.

By charting what demonstrably fails, whether in engineering or social organization, we are not merely learning what to avoid; we are effectively reverse-engineering the constraints of a real territory. The Negative Canon functions like a reef chart for inquiry. It is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and prevents a collapse into relativism.

### **4.2 Toward an Emergent Conception of Truth**

The Apex Network is the emergent structure revealed as unviable systems are eliminated. It is not a pre-existing truth but the structural residue of countless pragmatic filters. Like π, we cannot state it in full, but we know it exists, can approximate it indefinitely, and can judge which approximations are closer.

Formally, if we define V(W) as the viability function (1 for viable, 0 for brittle), the Apex Network A can be expressed as the intersection of all maximally viable worlds: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times—the maximal coherent subset remaining after infinite pragmatic filtering. This echoes Peirce's (1878) notion of truth as the ideal end of inquiry. Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to approximate this structure. Progress means reducing |S_consensus \ A|.

We access it through:

- **Negative knowledge**: The Negative Canon charts what demonstrably fails
- **Progressive approximation**: Successively lower-brittleness systems
- **Comparative judgments**: System A exhibits lower brittleness than System B

The structure's objectivity derives from the mind-independent nature of pragmatic constraints that reliably generate costs for systems violating them, not from metaphysical speculation about its pre-existence.

The mechanism that forges this structure is a bottom-up emergence driven by the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question. The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we might expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

This process operates through the differential success of Standing Predicates across domains. When a predicate proves highly effective in reducing brittleness in one domain, it creates pressure for similar principles to be adopted in related domains. For example, the success of germ theory in medicine created pressure for similar causal approaches in public health and sanitation. This cross-domain propagation of successful predicates is a key mechanism driving the emergence of the Apex Network, as the most viable conceptual tools gradually spread across different knowledge systems.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge. The "pursuit of truth," then, is the practical, fallible project of refining our current Consensus Network to better align with this real, objective structure.

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state; they simply try to solve local problems and reduce costs. The Apex Network is the name we give to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

The Apex Network's status is therefore dual, a distinction critical to our fallibilist realism. Ontologically, it is real: it is the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, however, it remains a regulative ideal. We can never achieve a final, God's-eye view to confirm that our Consensus Network is a perfect map of it; our knowledge of the structure is necessarily incomplete and fallible. Its existence is what grounds our realism and prevents a collapse into relativism, while our epistemic limitations are what make the project of inquiry a permanent and progressive one. It is the necessary concept that makes our comparative judgments of "more" or "less" brittle meaningful.

To further clarify the ontological commitments of the Apex Network, we contrast it with alternative positions. Unlike platonic realism, which posits timeless ideal forms existing independently of history, the Apex Network is emergent from pragmatic processes, not a pre-existing metaphysical entity. It differs from social constructivism by being mind-independent: its structure constrains successful inquiry regardless of cultural beliefs. This view aligns with Peirce's (1878) conception of truth as the ideal end of inquiry, but naturalizes it as the convergent outcome of pragmatic selection rather than a transcendental ideal. The Apex Network is 'real' in the sense that it exists as a stable attractor in the landscape of viability, discoverable through eliminative methods, much like π is real as the limit of successive approximations.

### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
* **Level 1: Objective Truth.** The ultimate, regulative ideal of the process. A proposition is objectively true if its principles are part of the real, emergent **Apex Network**—the objective structure of viable solutions. While this structure is never fully mapped, it functions as the formal standard that makes our comparative judgments of "more" or "less" brittle meaningful. It is the structure toward which the reduction of systemic costs forces our knowledge systems to converge.

This layered framework avoids a simplistic "Whig history" by recognizing that **Justified Truth** is a historically-situated achievement. Newtonian mechanics earned its Level 2 status by being a maximally low-brittleness system for its problem-space for over two centuries. Its replacement by relativity does not retroactively invalidate that status; it shows the evolutionary process at work, where an expanding problem-space revealed pragmatic constraints that required a new, more viable system. This allows for sharp, non-anachronistic historical judgments: a claim can be justifiably true in its time (Level 2) yet still be objectively false (not Level 1) when judged against the Apex Network from the perspective of a more resilient successor.

This three-level framework can be understood as a naturalistic reinterpretation of traditional correspondence and coherence theories of truth. Level 3 corresponds to a form of coherence theory, but with the crucial external constraint of brittleness assessment that prevents isolation from reality. Level 2 incorporates elements of correspondence theory, as justification depends on the system's track record of successful engagement with reality, but it avoids the traditional correspondence theory's problematic assumption of direct access to facts by grounding correspondence in the system's pragmatic performance. Level 1 represents an idealized correspondence with the full structure of reality, but understood as an emergent pattern rather than a pre-existing state of affairs. By integrating these traditional insights within a dynamic evolutionary framework, we preserve their intuitive appeal while addressing their historical limitations. This synthesis offers a more nuanced understanding of truth that recognizes both its coherence within systems and its correspondence with reality, without reducing it to either exclusively.

### **4.4 The Evolving Structure of Knowledge: Convergent Core and Pluralist Frontier**

The historical process of pragmatic filtering gives our evolving Consensus Networks a discernible structure, which can be understood as having two distinct epistemic zones. This distinction is not about the nature of reality itself, but describes the justificatory status of our claims at a given time. It reflects the uneven distribution of pragmatic pressures across different domains of inquiry, with some areas facing stronger constraints that drive convergence, while others remain more open to viable alternatives.

* **The Convergent Core:** This represents the load-bearing foundations of our current knowledge. It comprises domains where the relentless pressure of pragmatic selection has eliminated all known rival formulations, leaving a single, or functionally identical, set of low-brittleness principles. Principles reside in this core—such as the laws of thermodynamics or the germ theory of disease—not because they are dogmatically held or self-evident, but because all tested alternatives have been relegated to the Negative Canon after generating catastrophically high systemic costs. While no claim is immune to revision in principle, the principles in the Convergent Core are functionally unrevisable in practice, as doing so would require dismantling the most successful and resilient knowledge structures we have ever built. A claim from this core achieves the highest degree of justification we can assign, approaching our standard for Objective Truth (Level 1).
* **The Pluralist Frontier:** This describes the domains of active research where our current evidence is insufficient to decide between multiple, competing, and viable reconstructions of the landscape of viability. Here, rival systems (e.g., different interpretations of quantum mechanics or competing models of consciousness) may co-exist, each with a demonstrably low and stable degree of brittleness. It is crucial to distinguish this constrained, evidence-based pluralism from relativism. The frontier is not an "anything goes" zone; it is a highly restricted space strictly bounded on all sides by the Negative Canon. A system based on phlogiston is not a "viable contender" on the frontier of chemistry; it is a demonstrably failed research program. This pluralism is therefore a sign of epistemic underdetermination—a feature of our map's current limitations, not reality's supposed indifference. The key difference from relativism is that the pluralism here is temporary and defeasible: as evidence accumulates and pragmatic pressures increase, some of these competing systems will likely demonstrate lower brittleness and move toward the Convergent Core, while others will be relegated to the Negative Canon. This position resonates with pragmatist accounts of functional pluralism (e.g., Price 1992), which treat different conceptual frameworks as tools whose legitimacy is determined by their utility within a specific practice. Within this frontier, the core claims of each viable competing system can be granted the status of Justified Truth (Level 2). This is also the zone where non-epistemic factors, such as institutional power or contingent path dependencies, can play their most significant role, sometimes artificially constraining the range of options explored or creating temporary monopolies on what is considered justified.

### **4.5 Illustrative Cases of Convergence and Brittleness**

The transition from Newtonian to relativistic physics offers a canonical example of this framework's diagnostic application. After centuries of viability, the Newtonian system began to accumulate significant systemic costs in the late 19th century. These manifested as first-order predictive failures, such as its inability to account for the perihelion of Mercury, and as rising conceptual debt in the form of ad-hoc modifications like the Lorentz-FitzGerald contraction hypothesis. In terms of our brittleness metrics, the Newtonian system showed increasing patch velocity (P(t)) as more auxiliary hypotheses were needed to preserve the core theory, and declining explanatory returns (M(t)) as these modifications grew in complexity without proportional gains in predictive power. The Einsteinian system proved to be a more resilient solution, reducing this conceptual debt and substantially lowering the systemic costs of inquiry in physics.

A more contemporary case can be found in the recent history of artificial intelligence, which illustrates how a brittleness assessment might function in real time. The periodic "AI winters" can be understood as the collapse of high-brittleness paradigms, such as symbolic AI, which suffered from a high rate of ad-hoc modification when faced with novel challenges. While the subsequent deep learning paradigm proved to be a low-brittleness solution for many specific tasks, it may now be showing signs of rising systemic costs, such as unsustainable escalations in computational resources for marginal performance gains and an accelerating research focus on auxiliary 'alignment patches' rather than on foundational architectural advances. This situation illustrates the Pluralist Frontier in action, as rival architectures might now be competing to become the next low-brittleness solution.



### **4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, are both examples. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability, therefore, requires a balance between low-cost stability and adaptive capacity.



## **5. The Dynamism of the Web: How Knowledge Becomes Entrenched**

Quine's "Web of Belief" provided a powerful static model of a knowledge system, but it has been criticized for lacking a corresponding dynamic account of its formation and change. This section provides that dynamic physiology, detailing the naturalistic process by which a successful discovery migrates from the tentative "periphery" of the web to its load-bearing "core."

A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Systemic Externalism and Its Relations

This section clarifies Systemic Externalism's position within contemporary epistemology by examining its relationship to coherentism, evolutionary epistemology, and neopragmatism.

### 6.1 Resolving Coherentism's Isolation Problem

Contemporary coherentist theories face BonJour's (1985) isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality. Internalist responses (Kvanvig 2012; Carlson 2015) explain *why* some beliefs are more central but not *how* that centrality is earned through external discipline.

Systemic Externalism provides this missing externalist constraint. It complements Carlson's (2015) internalist reconstruction showing core beliefs are functionally indispensable by providing the causal, evolutionary explanation: principles become core by surviving historical pragmatic filtering demonstrating their role in low-brittleness networks. Justification requires both internal coherence and demonstrated network reliability through low systemic brittleness. This grounds the web's structure in objective, externalist history.

### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.

**Relation to Lakatos and Laudan**: While Lakatos (1970) describes degenerating research programmes qualitatively, our framework provides the underlying causal mechanism. Brittleness measures accumulated systemic costs causing degeneration, offering quantifiable proxies (P(t), M(t), C(t)) where Lakatos gave binary classification. Unlike Laudan's (1977) retrospective problem-solving effectiveness, brittleness provides forward-looking risk assessment, detecting vulnerability before crisis.

### 6.3 A Realist Corrective to Neopragmatism

The framework retains pragmatism's anti-foundationalist spirit while offering a corrective to neopragmatists (Rorty 1979; Brandom 1994) vulnerable to reducing objectivity to social consensus. These accounts of justification as linguistic practice lack robust, non-discursive external constraints, leaving inadequate resources for cases where communities converge on unviable beliefs through well-managed discourse.

Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.

(2) The epistemological question (answered by the eliminative process of pragmatic selection): Our framework provides the specific causal mechanism for convergence that structural realism often lacks. We discover these robust structures not through a mysterious act of intellectual insight, but through the brutal, eliminative process of pragmatic selection. High-brittleness networks—those whose posited structures misalign with the real landscape of viability—generate unsustainable costs, collapse, and enter the **Negative Canon**. Low-brittleness networks survive and are built upon. Over historical time, this failure-driven selective pressure is precisely the engine that forces our **Consensus Networks** to conform to the objective, relational structure of the Apex Network. Unlike Ladyman and Ross (2007), who posit ontic structures, our Apex Network emerges via pragmatic selection, evidenced by cross-domain predicate propagation (e.g., causality from physics to biology).

### 6.4 Scope, Limitations, and Research Directions

The framework operates at the macro-historical level, suited to cumulative knowledge systems with clear practical consequences. Key limitations:

**Scope**: Best applies where pragmatic constraint is direct and measurable (empirical science, engineering, policy). Application to pure mathematics or highly theoretical domains requires further development. In pure mathematics, where external feedback is minimal, brittleness may manifest as internal inefficiency: increasingly complex proofs yielding diminishing unifying power, or axiomatic systems proliferating without integration. For example, frameworks that require accelerating ad-hoc fixes to maintain consistency are brittle, much like late-stage Ptolemaic astronomy. This extension is speculative and requires case studies—such as comparing Euclidean and non-Euclidean geometries—through metrics like proof complexity and cross-domain applicability.

**Measurement**: Operationalizing brittleness metrics non-circularly remains challenging. Proposed measures are heuristic guides for a research program, not algorithmic solutions (see Section 2.4's reflective equilibrium defense).

**Power Dynamics**: While acknowledging power's role in maintaining brittle systems, fuller accounts of coercive mechanism interactions with epistemic selection require development.

**Potential Limitations and Future Research**: The framework's probabilistic claims may be challenged by cases where brittle systems endure due to contingent factors, requiring more refined models of shock sensitivity. Additionally, the metrics' interpretive nature invites research into automated brittleness detection using machine learning on bibliometric data. Cross-cultural applications could test the universality of viability landscapes, addressing whether pragmatic constraints vary across societies. Finally, integrating micro-epistemological foundations—such as how individual justification relates to macro-viability—remains underdeveloped and could strengthen the model's normative force.

These point toward productive research while indicating appropriate applications.

### **6.5 Relation to Other Externalist Approaches**

Systemic Externalism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Systemic Externalism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Systemic Externalism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Systemic Externalism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**

### **7.1 Coherent Fictions and Incommensurable Paradigms**

Conspiracy theories and Kuhnian incommensurable paradigms challenge the framework differently. Conspiracy theories typically exhibit diagnostic brittleness signatures: accelerating ad-hoc modifications protecting core tenets, high maintenance costs suppressing dissent, and epistemic parasitism (rationalizing mainstream successes without generating novel research). Whether these constitute decisive refutation depends on objective measurement, which remains challenging.

Incommensurable paradigms present subtler difficulties. Direct theoretical comparison may be impossible, but structural performance indicators (complexity-to-prediction ratios, resource escalation) can supplement traditional empirical and theoretical considerations when paradigms compete within overlapping domains. This reframes some philosophical impasses as tractable empirical questions, though interpretive challenges remain significant.

### **7.2 Historical Endurance and Real-Time Application**

Flawed systems can endure for centuries, and applying viability standards to live controversies risks hindsight bias. The framework distinguishes mere endurance from viability (as clarified in Section 1): brittle systems persist by paying immense measurable costs. Ptolemaic cosmology's longevity confirms the model by demonstrating the high institutional and intellectual price of insulating flawed cores from pragmatic feedback, making such systems vulnerable to efficient competitors.

Power and oppression cases illustrate this. Slavery appeared stable to beneficiaries but exhibited objective brittleness through measurable indicators: coercive overheads (patrols, legal apparatus), chronic instability (rebellions), and opportunity costs (suppressed productivity). The exercise of power doesn't negate brittleness; coercive costs become primary diagnostic indicators (the C(t) metric). While the framework predicts statistical tendencies rather than deterministic outcomes, potential counterexamples exist where apparently brittle systems temporarily prevailed due to contingent factors. However, these cases often reveal underlying brittleness through higher long-term costs or eventual collapse.

Real-time application aims at epistemic risk management, not deterministic prediction. Retrospective analysis calibrates diagnostic tools by studying known failures' empirical signatures. These calibrated tools then inform forward-looking questions: Does exponential computational cost escalation in AI signal degeneration despite short-term performance gains? Do proliferating alignment fixes represent mounting conceptual debt? Rising brittleness indicators don't prove falsehood but signal higher-risk, potentially degenerating programs.

### **7.3 Macro-Epistemology and Individual Justification**

As a macro-epistemology explaining long-term viability of public knowledge systems, the framework doesn't primarily solve micro-epistemological problems (Gettier cases, perceptual justification). Instead, it bridges levels through higher-order evidence: diagnosed system health provides powerful defeaters or corroborators for individual beliefs.

Formally, diagnosed brittleness determines rational priors. Low-brittleness networks (IPCC reports) warrant high priors; high-brittleness networks (denialist documentaries) warrant low priors. Following Kelly (2005), source properties matter. Even powerful first-order evidence from high-brittleness sources yields low posterior confidence due to extremely low priors. Macro-level diagnosis thus provides rational, quantitative trust allocation.

### **7.4 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.*

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.

**Falsifiability**: If broad, methodologically sound historical analysis revealed no significant correlation between systemic cost indicators and subsequent fragility, the theory's causal engine would be severely undermined. The framework's ultimate test lies in prospective application: diagnosing rising brittleness in current paradigms yields falsifiable predictions about statistical likelihood of supersession when facing novel challenges.


## **8. Conclusion**

This paper develops Systemic Externalism, which resolves coherentism's isolation objection by grounding coherence in demonstrated viability—measured through systemic costs. This naturalistic account of truth avoids both internalist solipsism and foundationalist implausibility.

Systemic Externalism reframes truth as viability: coherence proven by resilience against systemic costs. Three key contributions: (1) Brittleness metrics providing falsifiable diagnostics (patch velocity, coercion ratios, model complexity). (2) Standing Predicates as functional units of cultural-epistemic selection. (3) The Apex Network as an emergent structure forged through historical filtering. Empirical pilots (illustrated through Ptolemaic astronomy, AI development brittleness, and historical case studies) demonstrate the framework's utility, inviting interdisciplinary testing. While the operationalization of these metrics represents a significant empirical program, even their partial application—as in the worked example of Ptolemaic astronomy—can provide valuable diagnostic insight. Future work will focus on pilot studies to refine these proxy measures.

The ultimate arbiter is not the elegance of a theory or the consensus of its adherents, but the trail of consequences it leaves in the world. While this framework operates at a high level of abstraction, its primary data streams originate from the ground up. As we have argued, systemic costs are ultimately experienced by individuals as suffering, instability, and the frustration of human goals. Dissent, friction, and protest are therefore not merely political problems; they are primary sources of epistemological data about a system's rising brittleness. This framework, in this light, is best understood as a tool for a democratic public to hold its own knowledge-generating systems accountable. By making the indicators of systemic health part of a shared, evidence-based language, it helps us ask the most pragmatic question of all: "Is this way of thinking, this way of organizing ourselves, still working for us?"


