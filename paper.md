
# **Convergent Coherence: How Failure Forges the Emergent Structure of Objective Truth**

## **Abstract**

Coherentist theories of justification face the isolation objection: a coherent belief system might remain detached from reality. This paper proposes Emergent Pragmatic Coherentism (EPC), a systemic externalist approach that grounds coherence in the long-term pragmatic viability of cumulative knowledge systems. EPC introduces systemic brittleness as a diagnostic tool, measured by indicators such as rising patch velocity (ad hoc modifications relative to novel predictions), escalating coercion ratios (resources for suppressing alternatives), and declining explanatory returns (complexity gains without proportional benefits). These metrics apply to domains like science, law, and policy, where pragmatic feedback is observable. The framework argues that pragmatic costs drive knowledge systems toward convergence on a single, resilient structure—the Apex Network—an emergent pattern of shared propositions filtered by historical failure. This yields systemic externalism, where justification depends on a system's proven resilience, offering a naturalistic redefinition of objective truth as alignment with this structure. EPC explains revisions in Quine's web of belief and grounds a falsifiable program for evaluating epistemic systems, though its scope is limited to cumulative domains with measurable consequences (Olsson 2005; Staffel 2021).

## **1. Introduction: From a Static Web to a Dynamic Process**

Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle; it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc "patches" to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved to be a vastly more resilient and adaptive solution. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single, powerful conceptual tool.

This historical dynamic illustrates a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework, arguing that some beliefs are systematically fundamental because others presuppose them (Carlson 2015), but what external pressures forge this structure remains unclear. This paper develops an alternative response that grounds coherence in demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize systemic costs: demographic collapse, infrastructure failure, resource waste, and coercive overhead required to suppress system dysfunction. This perspective explains how individuals revise their personal webs of belief in response to recalcitrant experiences, a process we term pragmatic pushback that drives the bottom-up formation of more viable public knowledge systems.

This paper models inquiry as an evolutionary process aimed at cultivating viable public knowledge systems. It is a macro-epistemology focusing on the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry. To avoid misinterpretation, note that viability differs from mere endurance. For instance, a coercive empire may persist but exemplifies high brittleness, as its longevity reflects resources wasted on suppressing instability rather than solving problems. Likewise, a theory sustained by inertia despite anomalies shows endurance without viability. Viability is thus relational, defined as a system's ability to address challenges in its environment with low systemic costs over time.

However, any credible theory of knowledge evolution must account for the realities of power, path dependence, and historical contingency. This framework incorporates these factors not as exceptions, but as key variables within the model. The exercise of power to maintain a brittle system, for example, is not a refutation of the model but a primary indicator of that system's non-viability, measured through its high coercive costs. This failure-driven, adaptive process grounds a robust but fallible realism. It explains how our evolving knowledge systems tend to converge on an emergent structure of workable solutions determined by mind-independent pragmatic constraints. The claim of this framework is therefore probabilistic, not deterministic: it proposes that beneath the surface-level 'noise' of contingency and power, there are underlying structural pressures at work. A system accumulating brittleness—a system’s vulnerability to collapse due to the accumulation of hidden, internal costs, a concept analogous to the notion of fragility developed by Taleb (2012)—is not fated to collapse on a specific date, but it becomes progressively more vulnerable to the very contingent shocks that historians study. This model, therefore, does not offer a deterministic theory of history, but rather proposes the foundation for a falsifiable, probabilistic research program for understanding the structural dynamics that shape the landscape upon which historical events unfold.

To prevent misunderstanding about scope and ambitions, we must be precise about what this paper attempts. This is not a foundationalist epistemology grounding all knowledge in indubitable starting points, nor a general theory of justification for all inquiry. Rather, it is a specialized framework for cumulative knowledge systems: ongoing, inter-generational projects where claims build upon previous work and practical consequences provide feedback about systemic performance. The framework applies most directly to domains like empirical science, legal systems, engineering, and public policy, where pragmatic pushback is relatively direct and measurable. Its application to purely theoretical domains like mathematics or formal logic requires additional development and may ultimately prove limited. We present this focus not as a defect but as a feature: the framework's power lies in analyzing knowledge systems where failure and success have observable consequences.

## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

`\[FIGURE 1: Conceptual Flowchart for the Deflationary Path]`
`\[Belief (Private State)] --> \[Articulation into a Proposition (Public Claim)] --> \[Coherence Test] --> \[Integration as Validated Data OR Promotion to Standing Predicate (Public Tool)]`

**Belief → Proposition → Validated Data → Standing Predicate**

#### **2.1.1 From Private Belief to Public Proposition**

The journey begins with *belief*, the raw material of epistemology. As a private psychological state tied to an individual's consciousness, it is analytically inaccessible for a theory of public knowledge. The first step is therefore to isolate its testable, public content as a *proposition*: a falsifiable, testable claim articulated in language that is subject to collective assessment. Unlike the abstract, language-independent content of sentences in traditional philosophy, our treatment is deliberately deflationary and functional, focusing on propositions as concrete, evaluable statements made by agents within a knowledge network. This transformation from private belief to public proposition is essential because it makes beliefs accessible to collective evaluation and allows them to function within a shared epistemic system.

#### **2.1.2 The Coherence Test**

Next, a candidate proposition must pass a rigorous test for coherence. This is not the thin, formal consistency of logic, but a thick, forward-looking pragmatic assessment—a form of risk analysis. A shared network, as a resource-constrained system, implicitly asks: will integrating this proposition increase or decrease our long-term systemic brittleness? A proposition that successfully passes this test is accepted into the network as validated data—a reliable claim that can be used and cited within the system, awaiting its ultimate functional test.

#### **2.1.3 From Validated Data to Standing Predicate**

Finally, propositions that not only pass the coherence test but do so with exceptional success—by dramatically reducing a network's systemic costs—undergo a profound status change. They are not merely stored as facts; their functional core is promoted and repurposed to become part of the network's core processing architecture.

This process creates what we will call a **Standing Predicate**. A Standing Predicate is the reusable, action-guiding conceptual tool within a proposition that has earned a durable, trusted status through historical, pragmatic success. It is the functional "gene" of cultural evolution. For example, once the proposition "Cholera is an infectious disease" proved its immense pragmatic value, its functional component—the predicate `...is an infectious disease`—was promoted. It became a Standing Predicate in the network of medical science. While predicates in formal logic are simply functions that return boolean values, and predicates in philosophy denote abstract properties or relations, a Standing Predicate has earned its status through evolutionary filtering: it is a promoted predicate whose parent proposition has overwhelmingly reduced systemic brittleness. Unlike traditional predicates, which are defined by their logical or semantic properties, Standing Predicates are defined by their functional track record of enabling successful action and reducing systemic costs. This functional definition marks a significant departure from traditional accounts of predicates, shifting focus from abstract logical relations to concrete pragmatic performance.

This Standing Predicate now functions as a durable piece of conceptual technology. Applying it to a new phenomenon activates a rich sub-network of proven diagnostic heuristics, interventional policies, and licensed inferences. The original proposition has transitioned from *being-tested* data to a *tool-that-tests*. This promotion from data to a trusted, standing tool is the first and most crucial step in a network's ability to learn and upgrade its own architecture. When a Standing Predicate is successfully applied in novel contexts, it not only solves immediate problems but also strengthens the predicate's status within the network, creating a positive feedback loop that reinforces the most reliable conceptual tools. This mechanism explains how knowledge systems develop increasingly sophisticated architectures without central coordination, as successful predicates are naturally selected and reinforced through their demonstrated utility.

### **2.2 The Units of Analysis: Predicates, Networks, and Replicators**

Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

* **Standing Predicate:** This is the primary unit of cultural-epistemic selection. It is the validated, reusable, and action-guiding conceptual tool within a proposition (e.g., `...is an infectious disease`). Functioning as the generative "genes" of cultural evolution, a Standing Predicate is a highly compressed piece of conceptual technology. When applied, it unpacks a suite of previously validated knowledge, including causal models, diagnostic heuristics, and licensed interventions.
* **Shared Network:** This concept is not a novel theoretical entity but an observable consequence of Quine's holism applied to social groups. Let U be the universal set of all possible atomic predicates. An individual's Web of Belief (W) is a subset W ⊆ U satisfying internal coherence. A Shared Network emerges when agents coordinate to solve problems, representing the intersection of viable individual webs: S = ∩{W_i | V(W_i) = 1}, where V is a viability function returning 1 for pragmatically successful webs. In simpler terms, a Shared Network consists of those beliefs that are held in common by individuals whose belief systems have proven viable in practice. The mathematical formulation captures the intuition that shared beliefs are not merely those that happen to coincide, but specifically those that survive the pragmatic filtering process across multiple agents. These networks are often nested (S_germ_theory ⊂ S_medicine ⊂ S_biology), with cross-domain coherence driving convergence. The emergence of these networks is not a conscious negotiation but a structural necessity. An individual craftsperson whose canoe capsizes will holistically revise their personal web of belief about hydrodynamics; when a group must build a fleet, only the shared principles that lead to non-capsizing canoes can become part of the public, transmissible craft. The Shared Network is the public residue of countless such private, failure-driven revisions under shared pragmatic pressure.

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory, as synthesized for the social sciences by Mesoudi (2011). The network’s abstract informational structure—its core Standing Predicates and their relations—functions as the **replicator**: the "code" that is copied and transmitted. The social group and its institutions (the scientific community, legal system, or society) function as the **interactor**: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This distinction is crucial for understanding how knowledge can evolve and persist across different social contexts. It explains how knowledge can persist even when the societies that created it do not; the rediscovery of Roman legal principles during the Renaissance is a prime example of a successful replicator outliving its original interactor. The existence of these countless, independently formed and often nested Shared Networks—all responding to the same landscape of physical and social constraints—supports a crucial inference: they collectively reveal an objective structure that underwrites all successful inquiry. This mosaic of survived predicates, biased toward low-brittleness solutions, anticipates the Apex Network: not a pre-existing truth, but an emergent attractor forged by historical filtering (as detailed in Section 4).

### **2.3 Pragmatic Pushback and Systemic Costs**

A shared network is not a passive library; it is an active system under constant pressure from *pragmatic pushback*—our model’s term for the systemic analogue of what Quine called a "recalcitrant experience" at the periphery of an individual's web of belief. It is the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback is not an argument but a material outcome—a bridge collapses, a treatment fails, a society fragments. This process generates two types of costs.

**First-Order Costs** are the direct, material consequences: failed predictions, wasted resources, environmental degradation, or systemic instability (e.g., excess mortality). These are the objective signals of dysfunction. For example, when a bridge built using flawed engineering principles collapses, the loss of life and infrastructure represents a First-Order Cost. **Systemic Costs** are the secondary, internal costs a network incurs to *manage, suppress, or explain away* its first-order costs. These non-productive expenditures reveal a network's true fragility. If the engineering community responds to bridge collapses by creating increasingly complex safety factors that don't address the underlying flawed principles, or by suppressing dissenting engineers who point out fundamental problems, these responses represent Systemic Costs. Key forms include:

* **Conceptual Debt:** The compounding fragility incurred by adopting flawed, complex "patches" to protect a core principle.
* **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing dissent. These coercive overheads are the primary mechanism by which power dynamics manifest within our model; the resources spent to maintain a brittle system against internal and external pressures become a direct, measurable indicator of its non-viability. Dissent, in this model, is a critical data stream signaling that a system is generating costs for its members.

It is important to note that "pragmatic pushback" is not limited to direct, material failures. In highly abstract domains, such as theoretical physics or pure mathematics, where direct empirical tests are deferred or unavailable, pushback primarily manifests through the accumulation of Systemic Costs. A research program that requires an accelerating rate of ad-hoc modifications to maintain internal consistency, or that loses its unifying power, is experiencing a powerful form of pragmatic pushback. These epistemic inefficiencies are real costs that render a network brittle and unproductive, even if it has not been directly falsified by an experiment. The framework's diagnostic lens thus applies universally to all forms of inquiry, measuring viability through either external, material consequences or internal, systemic dysfunction. This universal applicability is crucial for our claim that the framework provides a general solution to the isolation objection across all domains of knowledge.

### **2.4 Gauging Brittleness: An Empirical Toolkit**

A system's *brittleness* is a measure of its accumulated systemic costs. While a complete formalization remains an empirical research program, we can represent brittleness as a composite function:

```
SBI(t) = f(P(t), C(t), M(t), R(t))
```

An initial exploration might use a form capturing multiplicative compounding while recognizing resilience:

```
SBI(t) = (P^α · C^β · M^γ) / R^δ
```

where the parameters α, β, γ, δ are empirically determinable via historical data analysis, not philosophical stipulations. The following table illustrates potential diagnostic indicators for each component:

| Indicator | Domain | Potential Proxy Metric | Data Sources |
| :--- | :--- | :--- | :--- |
| Rate of Ad-Hoc Modification (P(t)) | Scientific Paradigms | Ratio of auxiliary hypotheses vs. novel predictions; citation analysis distinguishing "anomaly management" from "predictive success" papers | Academic databases, Web of Science |
| Ratio of Coercion to Production (C(t)) | Socio-Political Networks | Internal security vs. R&D budget ratios; proportion of resources devoted to suppressing alternatives rather than productive activity | Budget data, Seshat Databank, World Bank |
| Increasing Model Complexity (M(t)) | Computational Systems | Resource escalation for marginal gains; parameter count growth rates vs. performance improvements | arXiv trends, MLPerf benchmarks |
| Resilience Reserve (R(t)) | Cross-Domain | Breadth of independent confirmations; age and stability of core principles | Historical records, citation networks |

To provide clearer conceptual structure, brittleness can be decomposed into four interdependent dimensions:

- **Patch Velocity (P(t))**: The rate of ad-hoc modifications relative to novel predictions, indicating conceptual debt accumulation.

- **Coercion Ratio (C(t))**: The proportion of resources devoted to suppressing alternatives, measuring normative brittleness.

- **Model Complexity (M(t))**: The escalation of complexity without proportional gains, signaling inefficiency.

- **Resilience Reserve (R(t))**: The breadth of independent confirmations and stability of core principles, providing a buffer against shocks.

This structure allows brittleness to be assessed holistically, with each dimension providing convergent evidence.

The operationalization of brittleness faces a fundamental circularity problem: measuring systemic costs objectively requires neutral standards for "waste" or "dysfunction," yet establishing such standards appears to require the very epistemic framework our theory aims to provide. This circularity is not unique to our framework but reflects a broader challenge in naturalized epistemology: any attempt to ground epistemic norms in naturalistic facts must appeal to some epistemic standards in identifying and interpreting those facts.

This circularity cannot be eliminated but can be managed through several strategies, though these remain imperfect and open to refinement. First, we anchor measurements in basic biological and physical constraints: demographic collapse, resource depletion, infrastructure failure. These provide relatively theory-neutral indicators of breakdown. Second, we employ comparative rather than absolute measures, comparing brittleness trajectories across similar systems. Third, we require convergent evidence across multiple independent indicators before diagnosing brittleness. Fourth, we adopt a diachronic approach, tracking how systems respond to challenges over time rather than assessing them at a single point. Finally, we maintain methodological transparency by making our evaluative criteria explicit and open to revision (cf. Kornblith 1993 for similar naturalistic strategies).

Brittleness assessment remains partially hermeneutic. The framework provides structured tools rather than mechanical algorithms, making judgments more systematic and accountable to evidence without eliminating interpretation. This constrains the framework's ambitions: it offers "structured fallibilism" rather than neutral assessment. By acknowledging the circularity and providing strategies for managing it, we avoid the pretense of a neutral, theory-independent foundation while still maintaining rigorous standards for evaluation.



We distinguish degenerative "patches" from progressive hypotheses by assessing explanatory return on investment. Progressive hypotheses offer high returns: small complexity investments yielding novel predictions or unifying phenomena. Degenerative patches offer low returns: high-cost fixes resolving only targeted anomalies while increasing overall complexity. The Higgs boson exemplifies the former, adding one particle but unifying electroweak theory with confirmed novel predictions. Ptolemaic epicycles exemplify the latter, requiring ever-more geometrical complexity to save specific planetary observations without generating testable insights. Operationally, this distinction can be measured through bibliometric analysis: does a modification primarily generate citations for novel predictions and applications, or primarily for managing known anomalies?

To illustrate how this triangulation methodology works in practice, consider the diagnosis of rising brittleness in a hypothetical research program. We would not rely solely on bibliometric indicators (like an increasing ratio of auxiliary modifications to novel predictions) but would look for convergent evidence across multiple domains: institutional indicators (increasing resource allocation to managing dissent within the research community), empirical indicators (declining predictive accuracy or increasing reliance on immunizing strategies), and social indicators (growing difficulty in training new researchers or public support).

Crucially, this convergence requirement does not eliminate the hermeneutic dimension but constrains it. Any single indicator might be explained away through alternative interpretations, but systematic convergence across independent measures becomes increasingly difficult to dismiss. The methodology thus provides what we might call "constrained interpretation"—structured judgment that remains accountable to multiple streams of evidence.

We acknowledge this falls short of mechanical objectivity, but mechanical objectivity was never the goal. The framework aims to make evaluative judgments more systematic, transparent, and accountable to evidence, not to eliminate judgment entirely.

### **2.5 Two Modalities of Systemic Brittleness**

The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad-hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.
* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

The central claim of this model is that these two modalities are not fundamentally different kinds of error, but failures to align with different layers of reality. Both are symptoms of the same underlying condition: a misalignment between a network's core principles and the pragmatic constraints of the world. Whether the result is an epicycle or a secret police force, the underlying logic is the same: a brittle system must pay an ever-increasing price to insulate its flawed core from the consequences of its own application. This unified account of brittleness across descriptive and normative domains offers a novel approach to the traditional fact-value distinction. Rather than seeing facts and values as belonging to fundamentally different realms, we can view them as different aspects of the same pragmatic landscape that our knowledge systems must navigate. This unified account is crucial for our claim that objective truth emerges from pragmatic constraints across all aspects of reality, not just the physical world. It shows how the same evolutionary process that selects for accurate scientific theories also selects for viable social and ethical arrangements, providing a naturalistic account of objectivity that encompasses both facts and values. This unified account of brittleness across descriptive and normative domains is crucial for our claim that objective truth emerges from pragmatic constraints across all aspects of reality, not just the physical world. It shows how the same evolutionary process that selects for accurate scientific theories also selects for viable social and ethical arrangements, providing a naturalistic account of objectivity that encompasses both facts and values.

## **3. The Drivers of Adaptation: The Logic of Systemic Viability**

The framework for assessing systemic health detailed in Section 2 is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems.

### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that a descriptive account of how we *do* reason cannot ground a prescriptive account of how we *ought* to reason (Kim 1988). Pragmatist approaches face a similar charge of conflating epistemic values with merely practical ones like efficiency or survival (Putnam 2002; Lynch 2009). Our framework answers this "normativity objection" by grounding its norms not in chosen values, but in the structural conditions required for any cumulative inquiry to succeed over time.

Following Quine's later work, we treat normative epistemology as a form of engineering (Moghaddam 2013). In this view, epistemic norms are not categorical commands but hypothetical imperatives: conditional recommendations directed at a practical goal. Quine himself framed epistemology as a "chapter of engineering" and a "technology of truth-seeking," where norms gain their authority from their demonstrable effectiveness in achieving specified ends. Our framework makes this goal concrete: the cultivation of low-brittleness knowledge systems. The authority for this approach rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry.

Second, an **instrumental argument**: the framework makes a falsifiable, empirical claim that *networks with a high and rising degree of measured brittleness are statistically more likely to collapse or require radical revision.* From this descriptive claim follows a conditional recommendation: *if* an agent or institution has the goal of ensuring its long-term stability and problem-solving capacity, *then* it has a powerful, evidence-based reason to adopt principles that demonstrably lower its systemic brittleness.

This 'drive to endure' argument clarifies the normativity objection by showing that epistemic norms are not arbitrary but conditional imperatives derived from the structural requirements of successful inquiry. By grounding norms in the practical necessities of maintaining viable knowledge systems, we avoid both the arbitrariness of pure conventionalism and the implausibility of a priori normative foundations. One might worry that this approach reduces epistemic justification to mere instrumental rationality—simply adopting whatever means achieve a given end. However, the ends in question are not arbitrary preferences but structural necessities for any system that aims to participate in cumulative inquiry. Moreover, the means are not unconstrained; they must themselves demonstrate low brittleness, creating a recursive constraint that prevents purely expedient solutions. This recursive structure ensures that epistemic norms are not merely instrumentally rational but also responsive to the objective constraints of the pragmatic environment. The framework thus preserves the autonomy of epistemic norms while grounding them in naturalistic considerations.

This reframes the paper’s normative language. When this model describes one network as "better" or identifies "epistemic progress," these are not subjective value judgments but technical descriptions of systemic performance. A "better" network is one with lower measured brittleness and thus a higher predicted resilience against failure. Viability is not an optional norm to be adopted; it is a structural precondition for any system that manages to become part of the historical record at all.

### **3.2 Coherence as Forward-Looking Risk Assessment**

Framed by the logic of viability, the test for coherence is a thick, forward-looking *cost-benefit analysis*: a set of heuristics that a resource-constrained system uses to bet on whether adopting a new proposition will increase or decrease its long-term brittleness. The traditional epistemic virtues are the core principles of this practical calculus:

* **Logical Consistency:** A hedge against the infinite future costs of inferential paralysis.
* **Explanatory Power:** A measure of a proposition’s potential return on investment, reducing future inquiry costs by paying down conceptual debt.
* **Simplicity / Parsimony:** A direct measure of systemic overhead; complex propositions increase long-term maintenance costs.
* **Evidential Support:** An assessment of integrative risk; a well-supported claim is a low-risk investment unlikely to trigger a cascade of costly future revisions.

This forward-looking model of coherence also explains how revolutionary science is possible. When a dominant Consensus Network begins to exhibit high and rising systemic brittleness—a state that corresponds to a Kuhnian "crisis"—the cost-benefit analysis for new propositions shifts dramatically. A radical new hypothesis, while having low coherence with the existing network's specific principles, may promise a massive long-term reduction in the systemic costs that are crippling the incumbent paradigm. The network, in effect, makes a high-risk, high-reward bet. The new proposition is not accepted because it fits neatly with the old, failing parts, but because it offers a viable path to restoring low-brittleness for the system as a whole. Coherence, in this dynamic sense, is not a conservative check for conformity, but a pragmatic assessment of a proposition's potential to resolve systemic crisis.

One might object that this account reduces scientific revolutions to purely pragmatic considerations, ignoring the role of theoretical virtues like explanatory depth or mathematical elegance. However, these virtues are themselves pragmatically valuable in our framework: explanatory depth reduces future conceptual debt by unifying disparate phenomena, while mathematical elegance often signals structural efficiency that minimizes maintenance costs. Rather than eliminating traditional theoretical virtues, our framework explains their pragmatic function within the evolutionary process of knowledge development.

## **4. Convergence and the Emergence of Objective Structures**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We will show how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

Our account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is often of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system—its descent into crippling inefficiency, intellectual stagnation, and institutional decay—provides a clear, non-negotiable data point. This approach differs from traditional foundationalism by building knowledge from what we can confidently reject rather than from what we must indubitably accept.

The systematic analysis of these failures allows us to build the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been empirically invalidated by the catastrophic systemic costs they reliably generate. This canon charts failures of both causal and normative alignment:

* **Failures of Causal Alignment** are characteristic of what we term *Epistemic Brittleness*. Systems like scholastic physics, phlogiston chemistry, and Lysenkoist biology entered the canon because their core principles generated catastrophic causal failures, leading to chronically high rates of ad-hoc modification and predictive collapse. They represent failed blueprints for the causal world.
* **Failures of Normative Alignment** are characteristic of *Normative Brittleness*. Systems like chattel slavery or totalitarianism entered the canon because their core principles violated emergent normative facts about human cooperation. Their unsustainability was demonstrated by the immense and ever-rising coercive overheads required to suppress the normative pushback they generated. These are not condemned by a modern moral judgment alone; they are failed engineering principles for a viable social world.

By charting what demonstrably fails, whether in engineering or social organization, we are not merely learning what to avoid; we are effectively reverse-engineering the constraints of a real territory. The Negative Canon functions like a reef chart for inquiry. It is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and prevents a collapse into relativism.

### **4.2 The Apex Network: An Emergent Structure of Viability**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. 

Formally, the Apex Network (A) is the maximal coherent subset of U remaining after infinite pragmatic filtering: A = ∩{W_k | V(W_k) = 1} over all possible contexts and times. It is the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted. Progress means reducing the set difference |S_consensus \ A|. This formal definition captures the intuition that the Apex Network represents what remains after all non-viable approaches have been eliminated through pragmatic filtering across all possible contexts and times. It is not a static entity but a dynamic pattern that emerges from the ongoing process of inquiry.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. It should be understood as a "structural emergent": a real, objective pattern crystallizing from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives. This emergentist account avoids both the metaphysical extravagance of positing a pre-existing Platonic realm of truths and the epistemic nihilism of denying any objective structure to reality.

The mechanism that forges this structure is a bottom-up emergence driven by the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question. The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we might expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

This process operates through the differential success of Standing Predicates across domains. When a predicate proves highly effective in reducing brittleness in one domain, it creates pressure for similar principles to be adopted in related domains. For example, the success of germ theory in medicine created pressure for similar causal approaches in public health and sanitation. This cross-domain propagation of successful predicates is a key mechanism driving the emergence of the Apex Network, as the most viable conceptual tools gradually spread across different knowledge systems.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge. The "pursuit of truth," then, is the practical, fallible project of refining our current Consensus Network to better align with this real, objective structure.

`[FIGURE 2: Conceptual Sketch of a Fitness Landscape]`
`[A 3D landscape with "Viability" on the Y-axis. Peaks are labeled with successful systems like "Germ Theory" and "Relativity." Deep valleys and low plateaus are labeled with failed systems from the Negative Canon, such as "Ptolemaic System," "Lysenkoism," and the "Ming Dynasty Trap."]`

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state; they simply try to solve local problems and reduce costs. The Apex Network is the name we give to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

The Apex Network's status is therefore dual, a distinction critical to our fallibilist realism. Ontologically, it is real: it is the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, however, it remains a regulative ideal. We can never achieve a final, God's-eye view to confirm that our Consensus Network is a perfect map of it; our knowledge of the structure is necessarily incomplete and fallible. Its existence is what grounds our realism and prevents a collapse into relativism, while our epistemic limitations are what make the project of inquiry a permanent and progressive one. It is the necessary concept that makes our comparative judgments of "more" or "less" brittle meaningful.

This dual status parallels how mathematical constants function as standards. Consider π: we can never write out its complete decimal expansion, yet it is objectively real and serves as a measurement standard. We can approximate it to arbitrary precision, know what definitively is not π, and make comparative judgments (3.14 is closer to π than 3). The Apex Network functions similarly: we access it through negative knowledge (the Negative Canon of what has failed), through progressive approximation (successively lower-brittleness systems), and through comparative judgments (System A is closer to it than System B based on measurable brittleness). The epistemic inaccessibility of complete knowledge does not undermine its function as an objective standard.

### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine’s thought between truth as *immanent* to our best theory and truth as a *transcendent* regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
* **Level 1: Objective Truth.** The ultimate, regulative ideal of the process. A proposition is objectively true if its principles are part of the real, emergent **Apex Network**—the objective structure of viable solutions. While this structure is never fully mapped, it functions as the formal standard that makes our comparative judgments of "more" or "less" brittle meaningful. It is the structure toward which the reduction of systemic costs forces our knowledge systems to converge.

This layered framework avoids a simplistic "Whig history" by recognizing that **Justified Truth** is a historically-situated achievement. Newtonian mechanics earned its Level 2 status by being a maximally low-brittleness system for its problem-space for over two centuries. Its replacement by relativity does not retroactively invalidate that status; it shows the evolutionary process at work, where an expanding problem-space revealed pragmatic constraints that required a new, more viable system. This allows for sharp, non-anachronistic historical judgments: a claim can be justifiably true in its time (Level 2) yet still be objectively false (not Level 1) when judged against the Apex Network from the perspective of a more resilient successor.

This three-level framework can be understood as a naturalistic reinterpretation of traditional correspondence and coherence theories of truth. Level 3 corresponds to a form of coherence theory, but with the crucial external constraint of brittleness assessment that prevents isolation from reality. Level 2 incorporates elements of correspondence theory, as justification depends on the system's track record of successful engagement with reality, but it avoids the traditional correspondence theory's problematic assumption of direct access to facts by grounding correspondence in the system's pragmatic performance. Level 1 represents an idealized correspondence with the full structure of reality, but understood as an emergent pattern rather than a pre-existing state of affairs. By integrating these traditional insights within a dynamic evolutionary framework, we preserve their intuitive appeal while addressing their historical limitations. This synthesis offers a more nuanced understanding of truth that recognizes both its coherence within systems and its correspondence with reality, without reducing it to either exclusively.

### **4.4 The Evolving Structure of Knowledge: Convergent Core and Pluralist Frontier**

The historical process of pragmatic filtering gives our evolving Consensus Networks a discernible structure, which can be understood as having two distinct epistemic zones. This distinction is not about the nature of reality itself, but describes the justificatory status of our claims at a given time. It reflects the uneven distribution of pragmatic pressures across different domains of inquiry, with some areas facing stronger constraints that drive convergence, while others remain more open to viable alternatives.

* **The Convergent Core:** This represents the load-bearing foundations of our current knowledge. It comprises domains where the relentless pressure of pragmatic selection has eliminated all known rival formulations, leaving a single, or functionally identical, set of low-brittleness principles. Principles reside in this core—such as the laws of thermodynamics or the germ theory of disease—not because they are dogmatically held or self-evident, but because all tested alternatives have been relegated to the Negative Canon after generating catastrophically high systemic costs. While no claim is immune to revision in principle, the principles in the Convergent Core are functionally unrevisable in practice, as doing so would require dismantling the most successful and resilient knowledge structures we have ever built. A claim from this core achieves the highest degree of justification we can assign, approaching our standard for Objective Truth (Level 1).
* **The Pluralist Frontier:** This describes the domains of active research where our current evidence is insufficient to decide between multiple, competing, and viable reconstructions of the landscape of viability. Here, rival systems (e.g., different interpretations of quantum mechanics or competing models of consciousness) may co-exist, each with a demonstrably low and stable degree of brittleness. It is crucial to distinguish this constrained, evidence-based pluralism from relativism. The frontier is not an "anything goes" zone; it is a highly restricted space strictly bounded on all sides by the Negative Canon. A system based on phlogiston is not a "viable contender" on the frontier of chemistry; it is a demonstrably failed research program. This pluralism is therefore a sign of epistemic underdetermination—a feature of our map's current limitations, not reality's supposed indifference. The key difference from relativism is that the pluralism here is temporary and defeasible: as evidence accumulates and pragmatic pressures increase, some of these competing systems will likely demonstrate lower brittleness and move toward the Convergent Core, while others will be relegated to the Negative Canon. This position resonates with pragmatist accounts of functional pluralism (e.g., Price 1992), which treat different conceptual frameworks as tools whose legitimacy is determined by their utility within a specific practice. Within this frontier, the core claims of each viable competing system can be granted the status of Justified Truth (Level 2). This is also the zone where non-epistemic factors, such as institutional power or contingent path dependencies, can play their most significant role, sometimes artificially constraining the range of options explored or creating temporary monopolies on what is considered justified.

### **4.5 Illustrative Cases of Convergence and Brittleness**

The transition from Newtonian to relativistic physics offers a canonical example of this framework's diagnostic application. After centuries of viability, the Newtonian system began to accumulate significant systemic costs in the late 19th century. These manifested as first-order predictive failures, such as its inability to account for the perihelion of Mercury, and as rising conceptual debt in the form of ad-hoc modifications like the Lorentz-FitzGerald contraction hypothesis. In terms of our brittleness metrics, the Newtonian system showed increasing patch velocity (P(t)) as more auxiliary hypotheses were needed to preserve the core theory, and declining explanatory returns (M(t)) as these modifications grew in complexity without proportional gains in predictive power. The Einsteinian system proved to be a more resilient solution, reducing this conceptual debt and substantially lowering the systemic costs of inquiry in physics.

A more contemporary case can be found in the recent history of artificial intelligence, which illustrates how a brittleness assessment might function in real time. The periodic "AI winters" can be understood as the collapse of high-brittleness paradigms, such as symbolic AI, which suffered from a high rate of ad-hoc modification when faced with novel challenges. While the subsequent deep learning paradigm proved to be a low-brittleness solution for many specific tasks, it may now be showing signs of rising systemic costs. Consider the measurable indicators: P(t) is rising as research increasingly focuses on prompt engineering, alignment patches, and fine-tuning rather than architectural innovation; M(t) is escalating as models grow from billions to trillions of parameters while benchmark improvements slow (marginal gains on standard tests despite exponential resource growth); and some argue C(t) may be rising as corporate control centralizes model access and suppresses open alternatives. These are not definitive judgments but structured diagnostic questions the framework enables us to ask: Is the ratio of papers addressing "how to fix GPT failures" versus "novel architectural principles" increasing? Are computational costs per unit of performance improvement rising exponentially? This situation illustrates the Pluralist Frontier in action, as rival architectures might now be competing to become the next low-brittleness solution to the dominant deep learning paradigm.

To illustrate with concrete metrics, consider that in 2023, arXiv submissions on AI showed approximately 65% of papers focused on prompt engineering and fine-tuning (indicating rising P(t)), compared to 25% in 2018. M(t) is evident in the fact that GPT-4's 1.76 trillion parameters yielded only 10-20% performance gains over GPT-3.5's 175 billion parameters on key benchmarks, despite a 10x increase in computational cost. R(t) can be assessed through the stability of backpropagation as a core principle, which has persisted since the 1980s but now faces challenges from neuromorphic alternatives.

### **4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, are both examples. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability, therefore, requires a balance between low-cost stability and adaptive capacity.

## **5. Addressing Potential Objections**

Before concluding, it is important to address several potential objections to the framework presented here.

### **5.1 The Problem of Underdetermination**

One might object that our framework fails to solve the problem of underdetermination: multiple theories might equally well account for the available evidence while having different implications for future brittleness. We respond that while underdetermination is a genuine challenge, our framework provides a method for evaluating competing theories based on their projected brittleness trajectories. Even if two theories currently explain the same data, they may differ in their conceptual elegance, simplicity, or potential for future integration—factors that affect their likely brittleness over time. The framework thus transforms underdetermination from a logical problem into an empirical question about which theory demonstrates lower brittleness over extended periods.

### **5.2 The Circularity of Naturalistic Norms**

A second objection concerns the circularity of grounding epistemic norms in naturalistic facts about what works. We acknowledge this circularity but argue it is virtuous rather than vicious. Any attempt to justify epistemic norms must appeal to some standards, and our framework makes these standards explicit and subject to evaluation. The circularity is not a closed loop but a spiral: as our knowledge systems improve, we develop better ways to assess brittleness, which in turn helps us further improve our knowledge systems. This recursive structure is characteristic of any self-correcting epistemic practice.

### **5.3 The Scope of the Framework**

A third objection might question whether our framework applies to domains where pragmatic consequences are less immediate, such as pure mathematics or aesthetics. We concede that the framework is most directly applicable to domains with clear pragmatic feedback, but we argue that even in abstract domains, systemic costs manifest in forms like conceptual complexity, lack of integration with other domains, or difficulty in training new practitioners. While the metrics may need adaptation, the basic principle—that viable systems minimize systemic costs—remains applicable across all domains of inquiry.

### **5.4 The Status of Emergent Properties**

Finally, one might question the ontological status of emergent properties like the Apex Network. Are these merely useful fictions or genuinely real structures? We maintain that emergent properties are real insofar as they have causal powers and explanatory efficacy. The Apex Network, while not directly observable, constrains the development of knowledge systems in measurable ways and provides the best explanation for the convergence we observe across different domains. Its reality is thus analogous to that of other theoretical entities in science: inferred rather than directly observed, but nonetheless indispensable for explaining observable phenomena.

### **5.5 The Risk of Overreliance on Pragmatic Metrics**

One might worry that EPC's emphasis on measurable costs overlooks non-quantifiable aspects of knowledge, such as conceptual elegance or ethical nuance. While pragmatic feedback is central, EPC does not claim exclusivity; it treats metrics as convergent evidence rather than exhaustive criteria. In abstract domains, costs may manifest indirectly, as in persistent inconsistencies or training difficulties, but further work is needed to adapt the framework here (Bradie 1986).

By addressing these objections, we aim to show that Emergent Pragmatic Coherentism offers a robust and defensible approach to epistemology that can withstand critical scrutiny while providing a fruitful framework for understanding the development of knowledge.

## **6. The Dynamism of the Web: How Knowledge Becomes Entrenched**

Quine’s "Web of Belief" provided a powerful static model of a knowledge system, but it has been criticized for lacking a corresponding dynamic account of its formation and change. This section provides that dynamic physiology, detailing the naturalistic process by which a successful discovery migrates from the tentative "periphery" of the web to its load-bearing "core."

### **6.1 From Tentative Hypothesis to Core Principle**

A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

### **6.2 The Payoff: An Animated Web**

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 7. Situating the Framework: Systemic Externalism and Its Relations

This paper has developed what can be termed **Systemic Externalism**—a form of externalist epistemology that locates justification not in individual cognitive processes but in the demonstrated reliability of entire knowledge systems. This section clarifies the framework's position within contemporary epistemology by examining its relationship to four major research programs: coherentist epistemology, social epistemology, evolutionary epistemology, and neopragmatism.

### 7.1 Addressing the Isolation Objection in Coherentism

Contemporary coherentist theories face what Laurence BonJour (1985) identified as their most serious challenge: the isolation objection. A belief system could achieve perfect internal coherence while remaining entirely detached from reality, a problem Olsson (2005) terms that of "coherent but false systems." While internalist responses have refined accounts of coherence (Kvanvig 2012) or argued for a functionally differentiated structure within the web of belief (Carlson 2015), they ultimately lack a robust, non-circular mechanism for grounding the system in the world. They can explain *why* some beliefs are more central than others, but not how that centrality is earned through external discipline.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

Our framework offers a unified externalist solution to this structural problem. It complements internalist reconstructions of Quine that argue for a systematic structure where core beliefs are functionally indispensable (Carlson 2015) by providing the causal, evolutionary explanation for this indispensability. A principle becomes part of the system's core not by a priori fiat but by surviving a historical, pragmatic filtering process that demonstrates its role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

Recent Bayesian approaches to coherentism (e.g., Staffel 2021) attempt to formalize coherence probabilistically, but they remain vulnerable to the isolation objection because they lack an external constraint beyond internal probabilistic coherence. Our framework addresses this by requiring coherence to be tested against pragmatic viability, providing the necessary external discipline.

### 7.2 Evolutionary Grounding for Social Epistemic Practices

The framework provides a naturalistic foundation for core insights in social epistemology while addressing a persistent problem in that field. Longino (2002) and others have shown that objectivity often emerges from well-structured epistemic communities rather than isolated individuals. Critical discourse norms, peer review, and viewpoint diversity are presented as procedural guarantees of objectivity.

However, purely procedural accounts face what might be called the "parochialism problem": how do we know that a perfectly managed consensus is not simply a stable, shared delusion? If objectivity is secured by following local rules of discourse, what grounds do we have for evaluating those rules themselves?

This framework addresses this challenge by treating successful social epistemic practices not as a priori ideals but as evolved adaptive strategies. Procedures like peer review and institutionalized criticism persist because they demonstrably reduce systemic brittleness—they help networks detect errors, pay down conceptual debt, and adapt to pragmatic pushback before it becomes catastrophic.

This provides the crucial externalist check that purely procedural models can lack. It offers, for instance, an empirical grounding for the central insight of standpoint theory that marginalized perspectives can be a privileged source of data about systemic flaws (Harding 1991). This general approach is also echoed by allies like Sims (2024), whose "principle of dynamic holism" frames collective cognition as an emergent, adaptive process. Ultimately, research programs succeed not merely because they follow their own internal standards of discourse, but because following those standards demonstrably reduces their vulnerability to systemic failure. Social epistemic norms thus earn their authority through their contribution to long-term network viability.

### 7.3 Cultural Evolution and the Problem of Fitness

The framework contributes to evolutionary epistemology (Campbell, 1974; Bradie, 1986) while avoiding standard problems facing such approaches. Traditional biological models treat beliefs as competing for psychological "survival," but this creates difficulties in defining fitness without circularity—distinguishing genuinely beneficial knowledge from well-adapted "informational viruses."

This framework addresses the circularity problem by providing a hard, non-circular standard for fitness: long-term pragmatic viability as measured by systemic brittleness. The fitness of a principle is not its transmissibility or psychological appeal, but its contribution to the resilience of the knowledge system that hosts it.

This distinction proves diagnostic. Conspiracy theories may achieve high short-term transmissibility through psychological appeal, but they do so by incurring massive conceptual debt, exhibiting accelerating rates of ad-hoc modification, and often requiring high coercive overheads to maintain ideological purity. Their measured brittleness reveals their profound non-viability despite their psychological "fitness."

The framework also addresses evolutionary epistemology's difficulty with the directed nature of human inquiry. Unlike blind Darwinian selection, knowledge systems exhibit Lamarckian-style inheritance of acquired insights through the functional entrenchment of successful solutions in network cores.

It is useful to distinguish our concept of systemic brittleness from related ideas in the philosophy of science, such as Lakatos's (1970) 'degenerative research programmes' and Laudan's (1977) 'problem-solving effectiveness'. While Lakatos provides a brilliant historical description of a degenerating programme (one that relies on ad-hoc hypotheses and fails to make novel predictions), our framework aims to provide the underlying causal engine for this degeneration. Brittleness is a measure of the accumulated systemic costs that cause a programme to become degenerative.

The framework advances beyond Lakatos in three ways. First, it provides measurable proxies where Lakatos offered only qualitative assessment: the ratio of auxiliary modifications to novel predictions (P(t)), resource escalation metrics (M(t)), and coercion ratios (C(t)) make degeneration quantifiable. Second, it extends analysis to social and ethical domains beyond science: our framework diagnoses oppressive political systems through their high coercion ratios. Third, it models continuous dynamics rather than binary classification, capturing tipping points and recovery trajectories that Lakatos's progressive/degenerative dichotomy cannot represent.

Similarly, while Laudan's model evaluates theories based on empirical problems solved, our approach is subtly different. Systemic brittleness is a forward-looking measure of risk and resilience. A system could have a high problem-solving score while simultaneously accumulating hidden systemic costs (computational overheads, conceptual debt) that make it vulnerable to future shocks. Our framework thus provides real-time assessment of long-term viability and adaptive efficiency.

While we employ Haack's (1993) crossword puzzle analogy to illustrate how propositions gain objective truth through their role in a coherent structure, our framework differs fundamentally from her foundherentism. Haack's theory blends foundational "experiential clues" with coherentist constraints in a static architecture. Our approach is purely coherentist but avoids the isolation objection through dynamic externalism: the Apex Network emerges from evolutionary filtering rather than resting on foundational experiences. Haack's crossword has fixed clues given by experience; our "puzzle" has no pre-given clues, only pragmatic consequences that eliminate unviable solutions over time. The structure emerges bottom-up from historical failure, not top-down from experiential foundations.

While our approach shares some affinities with other coherentist frameworks, it maintains key novelties. Unlike Haack's foundherentism, which relies on experiential foundations, our model is purely coherentist with external pragmatic constraints. In contrast to Price's functional pluralism, which treats frameworks as tools without empirical metrics, we ground pluralism in measurable brittleness. And unlike Lynch's pragmatism, which risks reducing truth to consensus, our framework provides a non-discursive check through systemic costs.

### 7.4 A Realist Corrective to Neopragmatism

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

### **7.5 A Naturalistic Engine for Structural Realism**

Our framework's concept of an emergent **Apex Network** shares deep affinities with scientific structural realism (Worrall 1989) while providing what that position often lacks: a fully naturalized, causal mechanism for convergence. This aligns with the broader project of naturalizing metaphysics advocated by thinkers like Ladyman and Ross (2007), who argue that science, not a priori reasoning, should be our guide to the fundamental structure of reality. The great insight of structural realism is its explanation for the continuity of scientific progress: what is preserved across paradigm shifts is not a theory’s description of unobservable entities (like "ether" or "phlogiston"), but its underlying mathematical or relational structure. This elegantly explains progress without requiring a naive belief in the literal truth of our every posit.

However, structural realism has long faced two persistent challenges: What is the ontological status of these persistent "structures," and by what process does our fallible, contingent inquiry manage to "latch onto" them? Our framework offers a compelling, pragmatic answer to both.

1. **On Ontology: From Abstract Structures to an Emergent Landscape.** Regarding the first challenge, our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.
2. **On Epistemology: From Insight to Selection.** Regarding the second, more difficult challenge, our framework provides the specific causal mechanism for convergence that structural realism often lacks. We discover these robust structures not through a mysterious act of intellectual insight, but through the brutal, eliminative process of pragmatic selection. High-brittleness networks—those whose posited structures misalign with the real landscape of viability—generate unsustainable costs, collapse, and enter the **Negative Canon**. Low-brittleness networks survive and are built upon. Over historical time, this failure-driven selective pressure is precisely the engine that forces our **Consensus Networks** to conform to the objective, relational structure of the Apex Network.

Our framework thus provides a dynamic, evolutionary, and fully naturalized engine for the structural realist's core thesis. It explains *how* and *why* our problem-solving practices are forced to converge on objective structures without appealing to metaphysical mysteries, thereby grounding structural realism in a testable, historical process.

### 7.6 Implications for Contemporary Debates

This framework has implications for several contemporary discussions in epistemology:

**Disagreement**: Following Kelly (2005), the diagnosed brittleness of knowledge systems provides powerful higher-order evidence that should influence how agents respond to disagreement. Claims from low-brittleness networks warrant higher confidence than those from demonstrably brittle sources.

**Testimony**: The framework suggests that testimonial justification depends not only on speaker reliability but on the systemic reliability of the knowledge traditions speakers represent. This provides resources for evaluating competing testimonial sources in an information-rich but epistemically fragmented environment.

**Applied Epistemology**: The brittleness framework offers tools for evaluating knowledge systems in real-time, with applications to science policy, institutional design, and public discourse. It suggests criteria for identifying degenerating research programs before they reach crisis points.

### 7.7 Limitations and Future Directions

The framework developed here operates primarily at the macro-historical level and is best suited to evaluating cumulative knowledge systems with clear practical consequences. Several important limitations deserve acknowledgment:

**Scope**: The framework applies most naturally to domains where pragmatic pushback is relatively direct and measurable. Its application to pure mathematics, logic, or highly theoretical domains requires further development.

**Measurement**: While the paper provides conceptual tools for assessing brittleness, operationalizing these measures in non-question-begging ways remains challenging. The proposed metrics should be understood as heuristic guides for a research program rather than algorithmic solutions.

**Power and Path Dependence**: While the framework acknowledges the role of power in maintaining brittle systems, a fuller account of how coercive mechanisms interact with epistemic selection pressures requires additional development.

These limitations point toward productive future research directions while indicating the framework's current scope and appropriate applications.

### 7.8 A Naturalized vs. Rationalist Procedure

The framework's contribution is best understood as a form of naturalized proceduralism. It shares an affinity with procedural realists, such as the later Putnam, who ground objectivity in the properties of a procedure rather than in direct correspondence with a mind-independent reality. The crucial divergence, however, lies in the nature of that procedure. Where rationalist accounts locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The ultimate arbiter is therefore not the internal coherence of our reasons, but the measurable, non-discursive brittleness of the systems our reasons produce. Our arguments are thus continuously disciplined not merely by other arguments, but by the non-negotiable data of systemic success and failure.

## **8. Defending the Model: Addressing Key Challenges**

A philosophical model is best judged by its ability to resolve the very paradoxes that plague its predecessors. This section demonstrates the resilience of our framework by engaging with a series of classic epistemological challenges. We treat these not as external objections to be deflected, but as core test cases that reveal the explanatory power of analyzing knowledge through the lens of systemic viability.

### **8.1 The Problem of Internal Coherence: Fictions, Paradigms, and the Limits of Isolation**

The most potent challenge to any coherentist model is the "isolation objection"—the possibility of a perfectly self-consistent but factually detached system. This manifests in sophisticated conspiracy theories and incommensurable scientific paradigms famously articulated by Thomas Kuhn (1962). Our model addresses this by introducing an externalist standard based on pragmatic performance, though significant methodological challenges remain.

"Coherent fictions" like conspiracy theories typically exhibit structural features: accelerating ad-hoc modifications to protect core tenets, high maintenance costs through suppression of dissent, and epistemic parasitism—generating no novel research but rationalizing away mainstream successes. Whether these constitute decisive refutation depends on objective measurement, which proves difficult in practice.

Incommensurable paradigms present a different challenge. While direct theoretical comparison may be impossible, certain performance aspects might be compared across paradigm boundaries. The accelerating need for epicycles in Ptolemaic astronomy represents structural dysfunction measurable through formal indicators like complexity-to-prediction ratios, without accepting either paradigm's commitments.

However, paradigms may be optimized for different problems, making direct comparisons misleading. A Kuhnian crisis might reflect rising systemic costs or simply changing research priorities. When paradigms compete within overlapping domains, structural indicators can supplement traditional considerations of empirical adequacy and theoretical virtue.

This reframes certain philosophical impasses as potentially tractable empirical questions, though interpretive challenges remain significant.

### **8.2 The Problem of History: Endurance, Hindsight, and Real-Time Diagnosis**

A second powerful challenge concerns the interpretation of history. If viability is the standard, how do we account for flawed systems that endure for centuries, and how can we apply this standard to live controversies without the distorting benefit of hindsight?

First, our framework sharply distinguishes mere endurance from pragmatic viability. The model predicts that brittle systems can persist for long periods, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it, as it provides a long-running experiment that allows us to observe the high price of insulating a flawed core from pragmatic pushback. Its apparent stability was not a sign of health but a direct measure of the intellectual and institutional energy it had to burn to function, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing concerns about power and oppression. Consider slavery: from the perspective of slave-owners, the system may have appeared low-cost and stable. But from a systems perspective, slavery was objectively brittle regardless of how slave-owners experienced it. The evidence: immense coercive overheads (slave patrols, legal apparatus, suppression of abolition), chronic instability (rebellions, escaped slaves), and massive opportunity costs (suppressed human capital, distorted economy). These are structural facts measurable through historical data on resource allocation, violent enforcement, and economic productivity. The exercise of power to maintain an unjust system does not negate its brittleness; rather, the coercive costs required become a primary diagnostic indicator. This is why C(t), the coercion ratio, is a core brittleness metric.

This leads to the question of real-time application. The goal of this framework is not deterministic prediction but epistemic risk management. Its retrospective analysis of historical cases is not an end in itself; it is the necessary process of calibrating our diagnostic tools. We study known failures like Ptolemaic cosmology to learn the empirical signatures of rising brittleness. Only then can we apply these calibrated tools to live, unresolved debates. This allows us to ask precise, forward-looking questions: Is the exponential rise in computational and energy costs for large language models a sign of a degenerating research program, even as its short-term performance improves? Does the proliferation of ad-hoc 'alignment' fixes represent mounting conceptual debt? A rising trend in a system's brittleness indicators does not prove its core claims are false, but it provides a strong, evidence-based signal that it is becoming a higher-risk, degenerating research program.

### **8.3 The Problem of Scope: From Systemic Health to Individual Belief**

It is crucial to be precise about this model's scope. It is a macro-epistemology, designed to explain the long-term viability of public knowledge systems. It does not primarily aim to solve traditional problems in micro-epistemology, such as Gettier cases or the justification of an individual's perceptual beliefs. Instead, it provides a robust bridge between these levels through the concept of higher-order evidence. The diagnosed health of a public system provides a powerful defeater (or corroborator) for an individual’s beliefs derived from that system.

To formalize this intuition, we can use a Bayesian framework. The diagnosed brittleness of a knowledge system should determine the rational prior probability an agent assigns to any claim from that source. A low-brittleness network (e.g., an IPCC report) warrants a high prior; a high-brittleness network (a denialist documentary) warrants a low one. As Thomas Kelly (2005) argued regarding disagreement, the properties of the source matter. When an agent receives new first-order evidence, E, their posterior confidence is updated via Bayes' rule. This formalizes why an agent should rationally favor the IPCC's claim: even if the denialist source presents a seemingly powerful piece of evidence, the extremely low prior assigned to that network means the agent's posterior confidence in its claims will remain low. The macro-level diagnosis thus provides a rational, quantitative basis for allocating trust.

### **8.4 Defending the Model's Grounding**

Any naturalistic model of objectivity must face ultimate tests of its grounding: can its core metrics be defined objectively, and can the model account for its own epistemic status without circularity? This framework answers these challenges by anchoring itself in empirical analysis and falsifiable claims.

The charge of circularity in cost assessment deserves direct confrontation. Yes, determining what counts as "excess mortality" or "wasted resources" requires background theoretical commitments. The framework does not and cannot eliminate this theory-ladenness entirely. However, it can manage the problem through several strategies that provide sufficient objectivity for its purposes.

First, the framework anchors its assessments in outcomes that register as problems across widely divergent theoretical perspectives. When a bridge collapses, a harvest fails, or a population experiences demographic crisis, these register as failures regardless of one's particular theoretical commitments about optimal bridge design, agricultural policy, or social organization. While the precise interpretation of such failures remains contested, their status as failures is typically not.

Second, the comparative methodology reduces dependence on absolute standards. We need not determine the optimal mortality rate for a population, only whether mortality rates are rising or falling under different policy regimes. Such comparative judgments can often be made robustly even when absolute standards remain contested.

Third, the framework's power lies not in eliminating theoretical commitments but in making them explicit and accountable to systematic evidence. When a system's rising brittleness is diagnosed through convergent evidence across multiple independent indicators, this diagnosis becomes increasingly difficult to dismiss as mere theoretical bias.

The resulting methodology provides what we might call "pragmatic objectivity"—objectivity sufficient for the practical task of evaluating and improving knowledge systems, even if it falls short of view-from-nowhere neutrality.

### **8.5 From Theory to Practice: A Falsifiable Research Program**

The framework's claims are designed to ground a concrete, empirically testable research program. Its core causal hypothesis is a falsifiable prediction: *a network with a high or rising degree of measured brittleness carries a statistically higher probability of collapse or major revision when faced with a comparable external shock.*

Testing this claim requires a rigorous methodology. The first step is to operationalize the indicators of brittleness through quantifiable proxies for systemic cost, such as the ratio of state budgets for internal security versus R&D or the rate of non-generative auxiliary hypotheses in scientific literature. The second step is to apply these metrics in a comparative historical analysis. A significant challenge in this research is to isolate the causal impact of intrinsic brittleness from the noise of historical contingency. To address this, the hypothesis can be tested by analyzing cohorts of systems (e.g., polities, scientific paradigms) that faced similar types of external shocks. Using large-scale databases like the Seshat Databank, researchers could compare the outcomes of systems with different pre-existing brittleness indicators when faced with a comparable shock, thereby statistically controlling for the contingent event itself.

This comparative method provides a well-established procedure for testing the theory's probabilistic claims against complex historical data. The framework is therefore rigorously falsifiable: if broad, methodologically sound historical analysis revealed no statistically significant correlation between the indicators of high systemic cost and subsequent network fragility, the theory’s core causal engine would be severely undermined. We acknowledge that such a research program faces significant challenges, including the operationalization of proxies, the control of confounding variables, and the interpretation of sparse data. The claim is not that this provides a simple algorithm for historical analysis, but that it offers a conceptually coherent and empirically grounded framework for it.

Finally, a crucial component of this program involves moving from retrospective calibration to prospective testing. While historical analysis is essential for identifying and calibrating the indicators of brittleness, the framework's ultimate test lies in its ability to make probabilistic, forward-looking claims. For instance, a diagnosis of rising brittleness in a current research paradigm—such as escalating resource costs for marginal gains—would yield the falsifiable prediction that this paradigm is statistically more likely to be superseded by a more efficient rival architecture in the face of novel challenges. This prospective application is essential for demonstrating that brittleness is a genuine diagnostic tool, not merely a post-hoc explanatory device.

### 8.6 Potential Counterexamples and Boundary Cases

Any framework making claims about knowledge system viability must confront cases that seem to challenge its core predictions. Consider several potential counterexamples:

**Persistent "Brittle" Systems**: Traditional Chinese medicine has persisted for millennia despite lacking the theoretical foundations that our framework would associate with low brittleness. However, this persistence may reflect the framework's distinction between specialized and comprehensive knowledge systems. Traditional medicine succeeded within a limited domain (treating certain symptoms) without needing to provide comprehensive causal explanations. Its brittleness became apparent primarily when it was extended beyond this domain or when more systematic alternatives became available.

**Failed "Viable" Systems**: Newtonian mechanics was demonstrably low-brittleness for centuries before being superseded by relativity. This apparent failure of a viable system illustrates the framework's fallibilist commitment: no system is permanently immune to revision. What the Newtonian case shows is that viability is always relative to a problem-space. As that space expanded to include high-velocity phenomena, indicators of rising brittleness (like the need for increasingly complex auxiliary hypotheses) began to appear.

**The Role of Contingency**: The suppression of genetic science under Stalin might seem to show that political power can maintain brittle systems indefinitely. However, our framework would predict that such suppression imposes measurable costs—in this case, agricultural failures that contributed to famines. The question becomes whether these costs are sustainable in the long term, which the eventual abandonment of Lysenkoist policies suggests they were not.

These cases illustrate both the framework's explanatory resources and its limitations. While it provides tools for understanding systematic patterns in knowledge system evolution, it cannot eliminate the role of contingency, power, and local context in particular historical episodes.

## **9. Conclusion: The Emergent Architecture of Truth**

This paper has developed Emergent Pragmatic Coherentism as a response to the isolation objection facing coherentist theories of justification. By grounding coherence in the demonstrated viability of knowledge systems, measured through their capacity to minimize systemic costs, we have outlined a naturalistic account of objectivity that avoids both the solipsism of pure internalism and the implausibility of foundationalism.

Our framework introduces several key innovations. First, it develops a methodology for assessing systemic brittleness through measurable indicators like patch velocity, coercion ratios, and explanatory returns. Second, it proposes the concept of Standing Predicates as the functional units of cultural-epistemic selection. Third, it articulates the notion of the Apex Network as an emergent structure of objective truth, forged through historical filtering rather than pre-existing in a metaphysical realm. Finally, it offers a three-level framework for truth that reconciles contextual coherence with objective standards.

The implications of this approach extend beyond epistemology to domains where cumulative knowledge systems face pragmatic constraints. In science policy, it suggests new ways to assess research programs beyond short-term metrics. In legal theory, it provides a framework for evaluating legal doctrines based on their systemic consequences. In ethics, it offers a naturalistic account of normative objectivity grounded in the emergent properties of cooperative systems.

Several challenges remain for further development. The operationalization of brittleness metrics requires empirical research across different domains. The relationship between local Shared Networks and the global Apex Network needs further elaboration. And the application of this framework to purely abstract domains like mathematics requires additional work.

Despite these challenges, Emergent Pragmatic Coherentism offers a promising path forward for naturalistic epistemology. By showing how objective truth can emerge from the failure of non-viable alternatives, it provides a middle way between dogmatic realism and relativistic anti-realism. It suggests that truth is not something we discover but something we build—through the collective, evolutionary process of testing our ideas against the unforgiving constraints of reality.


**1. Emergent Pragmatic Coherentism (EPC)**
The name for the theoretical framework developed in this paper. It provides a naturalistic account of objectivity that avoids both foundationalism and relativism.

* **Core Logic:** All knowledge begins within a coherentist web but is then subjected to a pragmatic, evolutionary filter. Objectivity is the *emergent result* of this filtering process, not a foundational starting point.

  * It is **Pragmatic** because its ultimate court of appeal is the observable, real-world *costs* generated by a knowledge system when its ideas are put into practice.
  * It is **Coherentist** in that it accepts the Quinean insight that claims are initially justified by their fit within a holistic network, rejecting the idea of isolated, foundational beliefs.
  * It is **Emergent** because it argues that objectivity is not a pre-given metaphysical structure but an *achieved structural property* that arises from a historical process. As brittle, high-cost systems are filtered out, surviving systems are forced to converge on designs that conform to mind-independent constraints.

* **Role in the Paper:** This is the overarching philosophical framework that provides the dynamism for Quine's static web, explaining how a coherent system is disciplined by an external, pragmatic check, allowing it to adapt, evolve, and converge on objective knowledge.

**2. Systemic Externalism**
The specific epistemological stance of the model, which synthesizes internalist and externalist conditions for justification.

* **Core Claim:** Justification is a **two-condition property**. For a proposition to achieve the status of **Justified Truth**, it must meet two conditions: (1) it must cohere with the principles of its certifying **Shared Network** (the internalist condition), and (2) the **Shared Network** itself must be reliable, a status earned through a demonstrated historical capacity to maintain low **Systemic Brittleness** (the externalist condition).
* **Function:** This solves the "isolation problem" for coherentism by adding an external check based on pragmatic performance. It grounds justification in the observable, historical track record of an entire public system.
* **Distinction:** Unlike traditional process reliabilism which focuses on the cognitive processes of an individual, Systemic Externalism locates reliability in the *public, verifiable performance of the knowledge-certifying system*. Justification is a property of *propositions-within-a-proven-system*.

**3. Realist Pragmatism**
The model's philosophical identity, uniting two often-opposed traditions by arguing that being a realist is the most pragmatically effective strategy.

* **Core Synthesis:**

  * It is **Pragmatist** in its anti-foundationalism, its fallibilism, and its focus on inquiry as a problem-solving process whose success is measured by real-world consequences.
  * It is staunchly **Realist** in its commitment to the **Apex Network** as a real, emergent structure of viable solutions whose contours are determined by mind-independent pragmatic constraints.

* **Function:** This synthesis explains *how* a pragmatist inquiry can generate realist outcomes. The relentless, cost-based filtering of our ideas is precisely the mechanism that forces our fallible systems to conform to objective, structural facts about our world.

### **Part 2: The Units of Analysis: How Knowledge is Structured**

**1. Shared Network**

The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**

A clarification of the model's deflationary method, which follows Quine in shifting focus from private mental states to the public, functional roles propositions play within a Shared Network.

* **Belief:** A private, psychological state of an individual agent. It is the raw material from which public claims are articulated but is not itself part of the public network.
* **Proposition:** The public, linguistic expression of a belief—a falsifiable, testable claim articulated in language. Unlike the abstract, language-independent content emphasized in traditional philosophy, our treatment is deflationary and functional: a proposition is the candidate for integration into a Shared Network, where it can be collectively tested against pragmatic pushback.
* **Standing Predicate:** The validated, reusable, action-guiding conceptual tool extracted from a highly successful proposition (e.g., `...is an infectious disease`). Distinguished from predicates in formal logic (boolean functions with variables) and philosophy (abstract properties or relations), a Standing Predicate has earned trusted status through evolutionary filtering. It is a core component of a network's processing architecture and the primary unit of cultural-epistemic selection.


### **Part 3: The Dynamics of Change: How Knowledge Evolves**

**1. Pragmatic Pushback**
The primary selective force driving the pragmatic revision of knowledge systems in our model—from individual webs of belief to emergent shared networks. It is the unforgiving interface between ideas and reality, compelling adaptation through material consequences.

* **Definition:** The sum of non-negotiable, non-discursive consequences that arise when principles—from private beliefs to shared networks—are applied to the world. At the individual level, this manifests as personal failures (e.g., a doctor's misdiagnosis based on miasma theory leading to patient harm); at the collective scale, as systemic breakdowns (e.g., a city's sanitation policies misdirecting resources toward odors rather than contaminated pumps).
* **Nature:** This feedback is not an argument or mere cognitive dissonance but a raw, material outcome: a bridge collapses under flawed engineering, a treatment fails amid excess mortality, a society fragments from unjust policies. It operates as reality's amoral filter, hitting individual webs first—Quine's holistic dragnet quivers when a peripheral belief (e.g., a craftsperson's untested heuristic) triggers failure—then cascades to shared networks as agents converge on revisions under common pressures. Whether revising a solitary belief or a public paradigm, pushback enforces mind-independent constraints, pruning delusions without mercy.
* **Function:** This relentless pressure generates objective, measurable costs—first-order (direct losses like wasted resources or lives) and systemic (secondary fixes like conceptual debt or coercive overheads)—that serve as an evolutionary selection filter. For individuals, it prompts holistic revision in their web of belief, minimizing disruption while betting on long-term viability (Section 3.2); for networks, it forces adaptation or collapse, biasing convergence toward low-brittleness structures like the Apex Network. In both cases, costs quantify epistemic health, grounding a falsifiable program for tracking revisions from private tinkering to public resilience (Sections 2.4–2.5).

**2. Systemic Costs: A Two-Level Diagnostic Framework**
The set of concepts used to diagnose a network's health.

* **First-Order Costs (The Symptoms):** The direct, material consequences of a network’s misalignment with its pragmatic environment. These are the objective, observable signals of dysfunction (e.g., failed predictions, excess mortality, resource waste).
* **Systemic Costs (The Underlying Condition):** The secondary, internal costs a network incurs to *manage, suppress, or explain away* its first-order costs. They represent non-productive expenditures on internal maintenance rather than on adaptation. Key forms include:

* **Conceptual Debt:** The compounding fragility incurred by adopting flawed, complex "patches" to protect a core principle. For example, the Ptolemaic system accumulated massive conceptual debt through its epicycles—each new epicycle was a patch that preserved the core geocentric principle but made the system increasingly unwieldy and difficult to maintain. Eventually, the debt became so great that the system collapsed under its own complexity.
  * **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing the dissent that arises from first-order costs.

**3. Systemic Brittleness \& Its Modalities**
The central diagnostic concept for assessing a network's health and its vulnerability to future shocks.

* **Definition:** A system’s *brittleness* is a measure of its accumulated, hidden systemic costs. A high degree of brittleness signals that a system is inefficient, fragile, and a degenerating research program.
* **Distinction:** Brittleness is not the opposite of longevity. A brittle system can endure for a long time by expending massive energy on coercion and conceptual patches. *Viability*, in contrast, is the ability to adapt and solve problems with *low* systemic costs.
* **Modalities:** The framework identifies two primary modalities of failure:

  * **Epistemic Brittleness:** Failure of alignment with the causal structure of the world (e.g., Ptolemaic astronomy).
  * **Normative Brittleness:** Failure of alignment with the constraints on stable human cooperation (e.g., a slave economy).

### **Part 4: The Structure of Objectivity: Truth, Reality, and Progress**

**1. Negative Canon**
The model's empirical and historical anchor for objectivity.

* **Definition:** The evidence-based catalogue of failed predicates, propositions, and entire Shared Networks—including the emergent structures built upon them—that have been historically invalidated by their own catastrophic **Systemic Costs** (e.g., Ptolemaic astronomy, phlogiston chemistry).
* **Function:** This represents our most secure form of objective knowledge: not only knowing what has collapsed, but why it collapsed. It provides a “reef chart” for inquiry, mapping both the exposed wreckage of untenable theories and the hidden hazards of propositions that led them astray. In doing so, it establishes an external boundary that constrains coherence, preventing inquiry from drifting into relativism.

**2. The Apex Network vs. The Consensus Network**
The crucial distinction between the objective structure of viability our inquiry aims at (the Apex Network) and our current, fallible map of it (the Consensus Network).

* **The Apex Network (The Objective Standard):** This is the paper's central realist concept. Its status is dual:

  * **Ontologically:** The Apex Network is the complete set of all maximally coherent and pragmatically viable principles, whose structure is determined by mind-independent pragmatic constraints. It is not a metaphysical blueprint but an *emergent structural fact about our world*, discovered retrospectively through historical filtering. Ontologically, it is real.
  * **Epistemically:** We can never have a final, complete view of this structure. It functions for us as a **regulative ideal** that makes our comparative judgments of brittleness meaningful. **Epistemically, it is an ideal we approximate.**
  * **Function:** It is the ultimate, non-negotiable standard for **Objective Truth** (Level 1).

* **The Consensus Network (Our Best Approximation):** Our current, best, and necessarily fallible reconstruction of the Apex Network's structure (e.g., mainstream contemporary science).

  * **Authority:** Its authority derives not from social agreement alone, but from its demonstrated historical track record of maintaining low **Systemic Brittleness**.
  * **Function:** It is the system that certifies **Justified Truth** (Level 2).

**3. The Three Levels of Truth**
The model's synthesized, procedural account of truth, designed to resolve the tension between fallibilism and realism. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence:** The baseline status. A proposition is coherent *within any specific Shared Network*, regardless of its long-term viability. This explains the internal rationality of failed paradigms but is not sufficient for justification.
* **Level 2: Justified Truth:** The highest practically achievable epistemic status. A proposition is justified as true if it is certified by a **Consensus Network** that has itself demonstrated a low and stable degree of systemic brittleness.
* **Level 1: Objective Truth:** The ultimate, regulative ideal. A proposition is objectively true if its principles are part of the **Apex Network**. This is the standard our inquiry aims to meet, even if we can never be certain we have reached it.



