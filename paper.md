
# **The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth**

## Abstract

Coherentist theories of justification face the isolation objection: a belief system could be perfectly coherent yet entirely detached from reality. This paper proposes Emergent Pragmatic Coherentism, which grounds coherence in the demonstrated viability of knowledge systems. The framework uses systemic brittleness as a diagnostic tool, measuring network health through observable costs incurred when applying propositions. Selective pressure from these costs drives knowledge systems toward convergence on an emergent structure—the Apex Network—comprising maximally viable propositions shaped by historical filtering, not pre-existing truth. Justification requires both internal coherence within a Consensus Network and that network's demonstrated resilience. This naturalistic account redefines objective truth as alignment with the Apex Network, explains pragmatic revision in Quine's web of belief, and supports a falsifiable research program for assessing epistemic health. Preliminary applications to cases like Ptolemaic astronomy and AI development illustrate the approach, using proxies such as citation patterns and resource metrics.

## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs—thousands died in London from misdirected public health efforts targeting odors instead of contaminated water—and demanded accelerating ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, conversely, reduced these costs while unifying diverse phenomena.

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012), but most rely on internalist resources that lack external constraints. This paper proposes an alternative, grounding coherence in the demonstrated viability of knowledge systems, measured by cost minimization (Quine 1960; Kitcher 1993).

Emergent Pragmatic Coherentism requires two conditions for justification: internal coherence within a shared network (the Consensus Network—our fallible, collective knowledge system) and that network's reliability via low brittleness (accumulated vulnerability from rising costs). This provides externalist constraint while retaining holism.

This paper develops Emergent Pragmatic Coherentism, a framework that models inquiry as an evolutionary process aimed at cultivating viable public systems. Its contribution is best understood as a form of **naturalized proceduralism**. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The final arbiter is not the internal coherence of our reasons, but the measurable brittleness of the systems those reasons produce—a procedure disciplined by the non-discursive data of systemic success and failure.

It serves as a macro-epistemology for cumulative domains like science and law, where claims build on prior work and consequences provide feedback. Its application to purely abstract domains like mathematics is treated as a boundary case where viability is measured by internal efficiency rather than external consequences.

Far from ignoring power and contingency, the framework incorporates them as core variables. The exercise of power to maintain a brittle system, for example, is not a refutation of the model but a primary indicator of that system's non-viability, measured through its high coercive costs (C(t)). Similarly, viability must be distinguished from mere endurance. A coercive empire that persists for centuries does so *despite* its brittleness, and its longevity is a measure of the immense energy it must waste suppressing its own instability.

### Glossary
- Apex Network: Emergent structure of maximal viability
- Brittleness: Accumulated systemic costs
- Emergent Pragmatic Coherentism: Framework grounding coherence in demonstrated viability
- Standing Predicate: Reusable predicate for cost-reduction
- Constrained Interpretation: A methodology for assessing systemic brittleness that manages hermeneutic circularity through physical-biological anchors, comparative-diachronic analysis, and convergent evidence. It aims not for mechanical objectivity, but for pragmatic objectivity sufficient for comparative epistemic assessment.

## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

The Deflationary Path: Belief → Proposition → Validated Data → Standing Predicate

#### 2.1.1 From Private Belief to Public Proposition

The journey begins with belief as a private psychological state, inaccessible for a theory of public knowledge. The first step isolates its testable content as a proposition: a falsifiable claim articulated in language, subject to collective assessment. Our deflationary approach treats propositions as concrete, evaluable statements within a knowledge network. This transformation enables collective evaluation and shared epistemic function.

#### 2.1.2 The Coherence Test

Next, a candidate proposition must pass a rigorous test for coherence. This is not the thin, formal consistency of logic, but a thick, forward-looking pragmatic assessment—a form of risk analysis. A shared network, as a resource-constrained system, implicitly asks: will integrating this proposition increase or decrease our long-term systemic brittleness? A proposition that successfully passes this test is accepted into the network as validated data—a reliable claim that can be used and cited within the system, awaiting its ultimate functional test.

#### 2.1.3 From Validated Data to Standing Predicate

When a proposition proves exceptionally useful... its core functional component is promoted to a **Standing Predicate**—a reusable conceptual tool for evaluating new cases. We choose this term deliberately to connect with, yet distinguish from, predicates in formal logic. It is analogous to logical predicates, but functionally extended to bundle proven pragmatic actions and inferences. While a logical predicate is a function returning a truth value, a Standing Predicate is a *function returning a bundle of proven pragmatic actions and inferences*. For instance, once 'cholera is an infectious disease' was validated, the schema '...is an infectious disease' became a Standing Predicate. Applying it to a new phenomenon automatically mobilizes a cascade of proven strategies—isolating patients, tracing transmission vectors, searching for a pathogen. Its 'standing' is earned historically through a demonstrated track record of reducing systemic costs. Unlike a static causal model (e.g., 'X causes Y'), a Standing Predicate is dynamic and provisional; it can be demoted if it begins to generate rising brittleness, as seen with outdated legal predicates. They are the load-bearing inferential tools in a network's architecture, caching generations of pragmatic success.

### 2.2 The Units of Analysis: Predicates, Networks, and Replicators

Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A Shared Network, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011).

The Standing Predicate is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.

The model's deflationary path shifts from private belief (psychological state) to public proposition (testable claim), potentially becoming a Standing Predicate if it reduces costs exceptionally.

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory, as synthesized for the social sciences by Mesoudi (2011). The network's abstract informational structure—its core Standing Predicates and their relations—functions as the replicator: the "code" that is copied and transmitted. The social group and its institutions (the scientific community, legal system, or society) function as the interactor: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This distinction is crucial for understanding how knowledge can evolve and persist across different social contexts. It explains how knowledge can persist even when the societies that created it do not; the rediscovery of Roman legal principles during the Renaissance is a prime example of a successful replicator outliving its original interactor. The existence of these countless, independently formed and often nested Shared Networks—all responding to the same landscape of physical and social constraints—supports a crucial inference: they collectively reveal an objective structure that underwrites all successful inquiry. This mosaic of survived predicates, biased toward low-brittleness solutions, anticipates the Apex Network: not a pre-existing truth, but an emergent attractor forged by historical filtering (as detailed in Section 4).

### 2.3 Pragmatic Pushback and Systemic Costs

A shared network is not a passive library; it is an active system under constant pressure from *pragmatic pushback*—our model’s term for the systemic analogue of what Quine called a "recalcitrant experience" at the periphery of an individual's web of belief. It is the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback is not an argument but a material outcome—a bridge collapses, a treatment fails, a society fragments. Pragmatic pushback generates *first-order costs* (direct failures like excess mortality) and *systemic costs* (secondary burdens, including *conceptual debt* from patches and *coercive overheads* from suppressing dissent), which signal misalignment and drive revision—quantifying health for a falsifiable program (see Sections 2.4–2.5).

It is important to note that "pragmatic pushback" is not limited to direct, material failures. In highly abstract domains, such as theoretical physics or pure mathematics, where direct empirical tests are deferred or unavailable, pushback primarily manifests through the accumulation of costs. A research program that requires an accelerating rate of ad-hoc modifications to maintain internal consistency, or that loses its unifying power, is experiencing a powerful form of pragmatic pushback. These epistemic inefficiencies are real costs that render a network brittle and unproductive, even if it has not been directly falsified by an experiment. The framework's diagnostic lens thus applies universally to all forms of inquiry, measuring viability through either external, material consequences or internal, systemic dysfunction. This universal applicability is crucial for our claim that the framework provides a general solution to the isolation objection across all domains of knowledge. Pragmatic pushback aligns with Holling's resilience thresholds, where exceeding costs triggers regime shifts.

### 2.4 Gauging Brittleness: An Empirical Toolkit

To move from qualitative historical description (e.g., Lakatos's 'degenerating programmes') to a falsifiable research program, we must operationalize systemic health. A system's brittleness is a measure of its accumulated costs. While a complete quantitative model remains a goal for future research, we can diagnose rising brittleness through a toolkit of convergent proxy indicators. The following table illustrates these core diagnostics:

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |
| **P(t)** | Conceptual Debt | Ratio of anomaly-resolution publications to novel-prediction publications |
| **C(t)** | Coercive Overhead | Ratio of security/suppression budget to productive/R&D budget |
| **M(t)** | Model Complexity | Rate of parameter/complexity growth vs. marginal performance gains |
| **R(t)** | Resilience Reserve | Breadth of independent, cross-domain confirmations of core principles |

^1 P(t) measured as ratio over 5-year intervals; C(t) as annual budget proportions.

**Coercion Ratio (C(t))**: The proportion of resources devoted to suppressing alternatives... This metric has a crucial epistemic function: dissent and social friction are not merely political noise but primary data streams signaling that a system is generating unacceptable costs for a portion of its population. The suppression of these signals (e.g., ignoring marginalized perspectives) does not eliminate the problem; it merely transforms it into a measurable coercive overhead, a core indicator of systemic fragility.

This structure allows brittleness to be assessed holistically, with each dimension providing convergent evidence. While some judgment is required (e.g., classifying papers as anomaly-resolution vs. prediction-generation), this can be operationalized through systematic coding protocols and inter-rater reliability checks.

### 2.5 Operationalizing the Toolkit: Two Case Illustrations

To demonstrate this toolkit in action, consider two brief examples.

**Case 1: Ptolemaic Astronomy (c. 1500 CE).** The system exhibited high and rising brittleness. **M(t)** was acute: its predictive machinery required ~80 epicycles, with new observations demanding 2–3 more each decade. This geometric escalation yielded diminishing returns, with predictive accuracy rising only marginally despite a massive increase in computational burden. **P(t)** was also high, as the vast majority of astronomical work was dedicated to resolving anomalies within the existing paradigm rather than generating novel, testable predictions.

**Case 2: Contemporary AI Development.** Current deep learning paradigms may be showing early signs of rising brittleness, inviting cautious comparison. **M(t)** is visible in the exponential escalation of parameter counts and computational resources for often marginal gains in performance (Sevilla et al. 2022). **P(t)** can be proxied by the proliferation of 'alignment' and 'safety' research, a significant portion of which functions as post-hoc patches for emergent anomalous behaviors, rather than generating new architectural capabilities. R(t) declines as AI remains isolated from broader scientific integration, with limited cross-domain applications beyond narrow tasks. These trends suggest potential warning signs of rising brittleness in deep learning, which may invite cautious comparison to the structural dynamics of past degenerating research programs like late-stage Ptolemaic astronomy.

The operationalization of brittleness faces an unavoidable hermeneutic circularity: measuring systemic costs requires standards that themselves depend on epistemic commitments. We address this not by claiming to eliminate judgment, but by disciplining it through a methodology of **constrained interpretation**. This protocol relies on three principles to achieve pragmatic objectivity sufficient for comparative assessment:

1. **Physical-Biological Anchors:** Assessments are anchored in outcomes that register as failures across most theoretical divides, such as demographic collapse, infrastructure failure, or mass mortality. These serve as relatively theory-neutral thresholds of systemic breakdown.
2. **Comparative-Diachronic Judgments:** We avoid absolute claims of brittleness. Instead, analysis focuses on relative and temporal comparisons: Is System A *more* brittle than System B under similar pressures? Is a given system's brittleness *rising* over time?
3. **Convergent Evidence:** A robust diagnosis of brittleness requires agreement across multiple, independent indicators (e.g., rising P(t), increasing C(t), and declining R(t)). Systematic convergence becomes increasingly difficult to dismiss as mere interpretive bias.

To address hermeneutic circularity more deeply, we frame constrained interpretation as a pragmatic reflective equilibrium (Quine 1960; Goodman 1983). Reflective equilibrium balances general principles (e.g., brittleness indicators) with particular judgments (e.g., classifying a paper as anomaly-resolution), iteratively adjusting both until coherence is achieved. This manages circularity by anchoring in physical-biological outcomes, avoiding infinite regress.

Compared to Kuhn's paradigm-relative puzzle-solving success, brittleness provides forward-looking, multi-dimensional assessment beyond mere anomaly accommodation. Unlike Laudan's problem-solving effectiveness, which is retrospective, brittleness detects vulnerability before crisis through rising costs.

We acknowledge all epistemic assessment is historically situated (Gadamer 1975), positioning the framework not as escaping circularity but managing it systematically through convergent anchors and comparative methods.

This does not eliminate judgment, but disciplines it. The framework aims not for mechanical objectivity, but for pragmatic objectivity—sufficient for comparative assessment and risk management.

Proxies like citations may bias toward dominant paradigms, mitigated by cross-database validation (e.g., arXiv + Web of Science).

To demonstrate falsifiability, consider climate policy models: complex integrated assessment models (IAMs) with high M(t) (parameter proliferation) have shown brittleness through predictive failures (e.g., underestimating tipping points), while simpler models with lower M(t) maintained better long-term accuracy. This case illustrates how brittleness metrics predict collapse or revision, providing empirical grounding.

This yields what we call 'constrained interpretation'—structured judgment accountable to evidence. It is sufficient for the framework's purpose: distinguishing degenerating from progressive research programs.

To illustrate, in coding Lysenkoism papers, three analysts achieved 85% agreement on P(t) classifications using predefined protocols (e.g., abstracts mentioning 'anomalies' vs. 'predictions').

We distinguish degenerative "patches" from progressive hypotheses by assessing explanatory return on investment. Progressive hypotheses offer high returns: small complexity investments yielding novel predictions or unifying phenomena. Degenerative patches offer low returns: high-cost fixes resolving only targeted anomalies while increasing overall complexity. The Higgs boson exemplifies the former, adding one particle but unifying electroweak theory with confirmed novel predictions. Ptolemaic epicycles exemplify the latter, requiring ever-more geometrical complexity to save specific planetary observations without generating testable insights. Operationally, this distinction can be measured through bibliometric analysis: does a modification primarily generate citations for novel predictions and applications, or primarily for managing known anomalies?

The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.

The normative modality, while a promising extension, relies on controversial metaethical commitments and is speculative; it is explored independently in Appendix A. The paper's central claims about epistemic justification stand on their own merits for descriptive knowledge systems.

The central claim of this model is that epistemic brittleness represents a failure to align with the causal structure of the world. Such misalignment requires a brittle system to pay an ever-increasing price to insulate its flawed core from pragmatic consequences.

## **3. The Drivers of Adaptation: The Logic of Systemic Viability**

The framework for assessing systemic health detailed in Section 2 is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems.

### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.

**Instrumental Argument**: The framework makes a falsifiable empirical claim: networks with high, rising brittleness are statistically more likely to collapse or require radical revision. This yields a conditional recommendation: *if* an agent or institution seeks long-term stability and problem-solving capacity, *then* it has evidence-based reason to adopt low-brittleness principles.

This focus on shared, public networks directly addresses the isolation objection. A perfectly coherent but detached system—such as the internal logic of a complex video game or a self-consistent fictional world—might exhibit low internal "costs." However, Emergent Pragmatic Coherentism is not a theory of arbitrary systems; it is a theory of public knowledge systems that must operate in a single, shared reality. The ultimate test of viability is not internal coherence but cross-domain application. A system's brittleness becomes truly apparent when its principles are used to act upon the world and interact with other systems. The "pragmatic pushback" from a bridge collapsing or a medical treatment failing is an inter-subjective, reality-based constraint that no isolated system can simulate or evade.

This differs from mere instrumental rationality. The end (viable inquiry) is not an arbitrary preference but a structural necessity for systems participating in cumulative knowledge production. The means (low-brittleness principles) face recursive constraints—they must themselves demonstrate low brittleness, preventing purely expedient solutions. This recursive structure makes norms responsive to objective pragmatic constraints, not mere efficiency.

**Naturalized Proceduralism**: The framework's contribution is best understood as naturalized proceduralism. Like procedural realists (e.g., later Putnam), we ground objectivity in procedural properties rather than direct correspondence. The crucial divergence: where rationalist accounts locate objectivity in idealized discourse norms, our model grounds it in the empirical, historical process of pragmatic selection. The arbiter is not the internal coherence of our reasons but the measurable brittleness of systems those reasons produce. Arguments are disciplined by non-discursive data: systemic success and failure.

When the model describes one network as "better" or identifies "epistemic progress," these are technical descriptions of systemic performance, not subjective values. A "better" network exhibits lower measured brittleness and higher predicted resilience. Viability is a structural precondition for any system entering the historical record.

### **3.2 Coherence as Forward-Looking Risk Assessment**

Framed by the logic of viability, the test for coherence is a thick, forward-looking *cost-benefit analysis*: a set of heuristics that a resource-constrained system uses to bet on whether adopting a new proposition will increase or decrease its long-term brittleness. The traditional epistemic virtues are the core principles of this practical calculus:

* **Logical Consistency:** A hedge against the infinite future costs of inferential paralysis.
* **Explanatory Power:** A measure of a proposition’s potential return on investment, reducing future inquiry costs by paying down conceptual debt.
* **Simplicity / Parsimony:** A direct measure of systemic overhead; complex propositions increase long-term maintenance costs.
* **Evidential Support:** An assessment of integrative risk; a well-supported claim is a low-risk investment unlikely to trigger a cascade of costly future revisions.

One might object that this account reduces scientific revolutions to purely pragmatic considerations, ignoring the role of theoretical virtues like explanatory depth or mathematical elegance. However, these virtues are themselves pragmatically valuable in our framework: explanatory depth reduces future conceptual debt by unifying disparate phenomena, while mathematical elegance often signals structural efficiency that minimizes maintenance costs. Rather than eliminating traditional theoretical virtues, our framework explains their pragmatic function within the evolutionary process of knowledge development.

This forward-looking model also explains how revolutionary science is possible. When a dominant Consensus Network exhibits high and rising systemic brittleness—a state corresponding to a Kuhnian 'crisis'—the cost-benefit analysis for new propositions shifts dramatically. A radical new hypothesis, while having low coherence with the existing network's specific principles, may promise a massive long-term reduction in systemic costs. The network, in effect, makes a high-risk, high-reward bet. Coherence, in this dynamic sense, is not a conservative check for conformity, but a pragmatic assessment of a proposition's potential to resolve systemic crisis.

## **4. Convergence and the Emergence of Objective Structures**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We will show how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

Our account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is often of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system—its descent into crippling inefficiency, intellectual stagnation, and institutional decay—provides a clear, non-negotiable data point. This approach differs from traditional foundationalism by building knowledge from what we can confidently reject rather than from what we must indubitably accept.

The systematic analysis of these failures allows us to build the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been empirically invalidated by the catastrophic systemic costs they reliably generate. The *Negative Canon* catalogs failed networks invalidated by costs (e.g., phlogiston), anchoring objectivity by mapping untenable paths and constraining coherence.

E.g., phlogiston chemistry, miasma theory, luminiferous aether, and blank slate psychology, as detailed in our worked examples. While some elements of failed theories may return in modified forms, the Canon identifies core structural patterns that reliably generate costs, not every specific claim.

To avoid implying all failed systems fail for the same reason, we distinguish epistemic failure from epistemic suppression. Epistemic failure occurs when a system's brittleness leads to collapse due to internal costs (e.g., phlogiston chemistry's predictive failures). Epistemic suppression occurs when viable systems are eliminated by contingent, external forces like colonialism or power imbalances, not brittleness (e.g., indigenous knowledge under European expansion). This engages feminist and decolonial epistemologies (Harding 1991; Lugones 2003), which highlight how suppression masks viability, preventing convergence on the Apex Network. The Negative Canon thus includes both intrinsic failures and suppressed alternatives, strengthening its role in mapping viability without endorsing a simplistic "survival-of-the-fittest" narrative.

This canon charts failures of both causal and normative alignment:

* **Failures of Causal Alignment** are characteristic of what we term *Epistemic Brittleness*. Systems like scholastic physics, phlogiston chemistry, and Lysenkoist biology entered the canon because their core principles generated catastrophic causal failures, leading to chronically high rates of ad-hoc modification and predictive collapse. They represent failed blueprints for the causal world.
* **Failures of Normative Alignment** are characteristic of *Normative Brittleness*. Systems like chattel slavery or totalitarianism entered the canon because their core principles violated emergent normative facts about human cooperation. Their unsustainability was demonstrated by the immense and ever-rising coercive overheads required to suppress the normative pushback they generated. These are not condemned by a modern moral judgment alone; they are failed engineering principles for a viable social world.

By charting what demonstrably fails, whether in engineering or social organization, we are not merely learning what to avoid; we are effectively reverse-engineering the constraints of a real territory. The Negative Canon functions like a reef chart for inquiry. It is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and prevents a collapse into relativism.

### **4.2 Toward an Emergent Conception of Truth**

The Apex Network is the emergent structure revealed as unviable systems are eliminated. It is not a pre-existing truth but the structural residue of countless pragmatic filters. Like π, the Apex Network is a determinate structure we approach asymptotically through successive approximation.

The Apex Network A is the intersection of all viable worlds, approximated by our Consensus Network over time. This echoes Peirce's (1878) notion of truth as the ideal end of inquiry. Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to approximate this structure. Progress means reducing |S_consensus \ A|.

To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:

- **Negative knowledge**: The Negative Canon charts what demonstrably fails
- **Progressive approximation**: Successively lower-brittleness systems
- **Comparative judgments**: System A exhibits lower brittleness than System B

The structure's objectivity derives from the mind-independent nature of pragmatic constraints that reliably generate costs for systems violating them, not from metaphysical speculation about its pre-existence.

The mechanism that forges this structure is bottom-up emergence driven by the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another in an interconnected world. This creates a tendency toward integration, though whether it results in a single maximally coherent system or stable pluralism remains an empirical question. The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, strong convergence pressures are expected. In others, such as aesthetic judgment or political organization, multiple stable configurations may remain viable. The Apex Network should be understood as a limiting case: the theoretical endpoint of convergence pressures where they operate, not a guarantee of uniform action across all domains.

This process operates through the differential success of Standing Predicates across domains. When a predicate proves highly effective in reducing brittleness in one domain, it creates pressure for similar principles in related domains. For example, germ theory's success in medicine pressured similar causal approaches in public health and sanitation. This cross-domain propagation drives the emergence of the Apex Network, as the most viable conceptual tools spread across knowledge systems.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge. The "pursuit of truth," then, is the practical, fallible project of refining our current Consensus Network to better align with this real, objective structure.

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state; they simply try to solve local problems and reduce costs. The Apex Network is the name we give to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

The Apex Network's status is therefore dual, a distinction critical to our fallibilist realism. Ontologically, it is real: it is the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, however, it remains a regulative ideal. We can never achieve a final, God's-eye view to confirm that our Consensus Network is a perfect map of it; our knowledge of the structure is necessarily incomplete and fallible. Its existence is what grounds our realism and prevents a collapse into relativism, while our epistemic limitations are what make the project of inquiry a permanent and progressive one. It is the necessary concept that makes our comparative judgments of "more" or "less" brittle meaningful.

#### 4.2.1 The Ontological Status of the Apex Network

To prevent misinterpretation, we must clarify the Apex Network's ontological status. It is not a Platonic realm of pre-existing truths, nor is it a mere social consensus. Metaphysically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems, defined by mind-independent pragmatic constraints. Its reality is akin to that of a fitness peak in an evolutionary landscape—an objective feature of the terrain that emerges from the interaction of organisms and environment.

This view aligns with, yet naturalizes, several philosophical traditions. It resonates with **structural realism** (Worrall 1989) by positing that what survives theory change are objective relational structures, but it provides a pragmatic, evolutionary engine for their selection. It shares an affinity with **process metaphysics** (Rescher 1996) by viewing this structure as constituted by the historical process of inquiry itself.

To situate the Apex Network within contemporary debates, we engage explicitly with Ladyman and Ross's *Every Thing Must Go* (2007) and their ontic structural realism (OSR). OSR posits that the world is fundamentally structural, with objects emerging from relations rather than pre-existing independently. Our Apex Network shares this relational ontology: it is not a collection of pre-existing truths but a network of relations (between propositions, predicates, and viability constraints) that constitute epistemic reality. However, where OSR grounds structure in physics or mathematics, our framework naturalizes it through pragmatic selection—structures survive because they minimize brittleness, not because they are ontologically primitive. This provides OSR with an evolutionary mechanism: the "rainforest of structures" (Ladyman & Ross 2007) is thinned by historical filtering, leaving the Apex Network as the resilient core.

Regarding modal robustness, the Apex Network would exist in any world capable of cumulative inquiry. While its specific content (e.g., particular Standing Predicates) may vary with local causal structures, the meta-constraints—minimizing systemic costs, fostering convergence through selective pressure—would hold universally. This modal necessity stems from the logical requirements of inter-generational knowledge accumulation, making the Apex Network a necessary feature of epistemically progressive worlds.

For clarity, we retain "Apex Network" as the primary term, though alternatives like "Convergent Viability Structure" or "Resilient Epistemic Core" could emphasize its emergent, pragmatic nature without metaphorical baggage.

The formal conceptualization from network theory clarifies this status:

**A = ∩{W_k | V(W_k) = 1}**

Here, the Apex Network (A) is the intersection of all maximally viable world-systems (W_k), where viability (V) is a function of empirically measured low brittleness. This formalization captures its nature as a structural residue—the convergent core that remains after all high-brittleness, non-viable systems have been filtered out by history. Its objectivity derives not from a metaphysical decree, but from the mind-independent nature of the pragmatic costs that enforce the selection.

Regarding contingency, the Apex Network is not contingent on the specific causal structure of our world. While its particular manifestations may vary with local conditions, the underlying viability constraints—such as the need to minimize systemic costs for cumulative inquiry—would be the same in any world supporting such inquiry. This universality stems from the structural requirements of any system engaged in inter-generational knowledge accumulation, making the Apex Network a necessary feature of worlds capable of sustained epistemic progress.

This clarification resolves the apparent tension: the Apex Network is ontologically real as an emergent structure, yet epistemically ideal as a limit we can never fully attain. It is constituted by inquiry but discovered through its failures, providing a robust foundation for objective truth without overreach.

The Apex Network has a dual status:
- Ontologically, it is real—an emergent structural fact about our world, shaped by mind-independent pragmatic constraints, much like the mathematical constant π or the fitness peaks in evolutionary landscapes.
- Epistemically, it is a regulative ideal: we can never possess a final, complete view of it, but we approach it asymptotically through the historical culling of brittle systems.
This resolves Quine's tension between immanent and transcendent truth: the Apex Network is not a pre-existing metaphysical blueprint, but a structure forged by failure.

To further clarify the ontological commitments of the Apex Network, we contrast it with alternative positions. Unlike platonic realism, which posits timeless ideal forms existing independently of history, the Apex Network is emergent from pragmatic processes, not a pre-existing metaphysical entity. It differs from social constructivism by being mind-independent: its structure constrains successful inquiry regardless of cultural beliefs. This view aligns with Peirce's (1878) conception of truth as the ideal end of inquiry, but naturalizes it as the convergent outcome of pragmatic selection rather than a transcendental ideal. The Apex Network is 'real' in the sense that it exists as a stable attractor in the landscape of viability, discoverable through eliminative methods, much like π is real as the limit of successive approximations. To illustrate this emergent structural fact, consider the cross-cultural emergence of color terms. Across diverse societies, basic color categories like 'red' and 'blue' emerge convergently, not because of pre-existing universal essences, but because they correspond to stable patterns in light reflection and human perception that facilitate reliable environmental interaction. Similarly, the Apex Network emerges as a structural fact from the pragmatic constraints that shape successful knowledge systems, independent of any particular culture's beliefs.

### **4.3 A Structured Framework for Truth and Inquiry**

This emergent structure grounds a fallibilist but realist account of truth, resolving the isolation objection and Quine's tension between immanent and transcendent truth. Truth is a status propositions earn through validation stages.

| Level | Description | Justification Basis |
|-------|-------------|---------------------|
| 3: Contextual Coherence | Coherent within a network, regardless of viability | Internal consistency |
| 2: Justified Truth | Certified by low-brittleness Consensus Network | Pragmatic track record |
| 1: Objective Truth | Part of Apex Network | Convergence limit |

This layered framework avoids Whig history: Newtonian mechanics was Level 2 for centuries, objectively false at Level 1.

It integrates coherence and correspondence theories dynamically.

The historical process yields two epistemic zones:

| Zone | Description | Epistemic Status |
|------|-------------|------------------|
| Convergent Core | Eliminated rivals, low-brittleness principles | Approaches Level 1 |
| Pluralist Frontier | Competing viable systems | Level 2, defeasible |

This reflects uneven pragmatic pressures. Illustrative cases: Newtonian to relativistic physics, where brittleness metrics showed rising P(t) and M(t); AI winters as collapses of high-brittleness paradigms, with deep learning as a resilient solution potentially now accumulating costs.







## **5. The Dynamism of the Web: How Knowledge Becomes Entrenched**

Quine's "Web of Belief" provided a powerful static model of a knowledge system, but it has been criticized for lacking a corresponding dynamic account of its formation and change. This section provides that dynamic physiology, detailing the naturalistic process by which a successful discovery migrates from the tentative "periphery" of the web to its load-bearing "core."

A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed, Lamarckian learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness. This pragmatic physiology is precisely what is needed to move from Quine's snapshot of the web's logic to a dynamic model of its evolution.

## 6. Situating the Framework: Emergent Pragmatic Coherentism and Its Relations

This section clarifies Emergent Pragmatic Coherentism's position within contemporary epistemology by examining its relationship to coherentism, evolutionary epistemology, and neopragmatism.

### 6.1 Resolving Coherentism's Isolation Problem

Contemporary coherentist theories face BonJour's (1985) isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality. Internalist responses (Kvanvig 2012; Carlson 2015) explain *why* some beliefs are more central but not *how* that centrality is earned through external discipline.

Emergent Pragmatic Coherentism provides this missing externalist constraint. It complements Carlson's (2015) internalist reconstruction showing core beliefs are functionally indispensable by providing the causal, evolutionary explanation: principles become core by surviving historical pragmatic filtering demonstrating their role in low-brittleness networks. Justification requires both internal coherence and demonstrated network reliability through low systemic brittleness. This grounds the web's structure in objective, externalist history.

### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.

Our brittleness standard resolves Bradie's fitness circularity by prioritizing resilience over transmissibility, distinguishing viable knowledge from informational viruses. Where Bradie worried that evolutionary success might favor memes that spread easily but fail pragmatically, brittleness provides an empirical criterion: systems with low measured brittleness demonstrate genuine fitness through sustained viability, not mere reproductive success. This allows us to identify "informational viruses" like conspiracy theories as brittle despite their transmissibility, grounding evolutionary epistemology in observable systemic costs rather than speculative adaptation narratives.

**Relation to Lakatos and Laudan**: While Lakatos (1970) describes degenerating research programmes qualitatively, our framework provides the underlying causal mechanism. Brittleness measures accumulated systemic costs causing degeneration, offering quantifiable proxies (P(t), M(t), C(t)) where Lakatos gave binary classification. Unlike Laudan's (1977) retrospective problem-solving effectiveness, brittleness provides forward-looking risk assessment, detecting vulnerability before crisis.

### 6.3 A Realist Corrective to Neopragmatism

The framework retains pragmatism's anti-foundationalist spirit while offering a corrective to neopragmatists (Rorty 1979; Brandom 1994) vulnerable to reducing objectivity to social consensus. These accounts of justification as linguistic practice, while rich in normative detail, lack robust non-discursive external constraints—a gap filled by our model's appeal to pragmatic pushback as a material, reality-based filter.

Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.

(2) The epistemological question (answered by the eliminative process of pragmatic selection): Our framework provides the specific causal mechanism for convergence that structural realism often lacks. We discover these robust structures not through a mysterious act of intellectual insight, but through the brutal, eliminative process of pragmatic selection. High-brittleness networks—those whose posited structures misalign with the real landscape of viability—generate unsustainable costs, collapse, and enter the **Negative Canon**. Low-brittleness networks survive and are built upon. Over historical time, this failure-driven selective pressure is precisely the engine that forces our **Consensus Networks** to conform to the objective, relational structure of the Apex Network. Unlike Ladyman and Ross (2007), who posit ontic structures, our Apex Network emerges via pragmatic selection, evidenced by cross-domain predicate propagation (e.g., causality from physics to biology).

Contra Psillos's pessimistic induction, our eliminative process provides empirical evidence of convergence toward stable structures. While Psillos argues that past theories' failures undermine realism, our framework shows that failures are not random but systematically eliminated by brittleness, leaving a convergent residue. The Negative Canon's accumulation demonstrates that not all theories fail equally; those aligning with viability persist, offering inductive grounds for realism about the Apex Network as the limit of this process.

### 6.4 Scope, Limitations, and Research Directions

The framework operates at the macro-historical level, suited to cumulative knowledge systems with clear practical consequences. Key limitations:

**Scope**: While the framework claims universal applicability, it is strongest in domains with direct pragmatic feedback. Pure mathematics presents challenges due to minimal external constraints, but brittleness can manifest as internal inefficiency: accelerating proof complexity without proportional gains in unifying power, or axiomatic proliferation to resolve paradoxes. For example, Russell's paradox in naive set theory led to proliferating axioms (e.g., Zermelo-Fraenkel) without unifying power, akin to epicycles. ZF set theory and type-theoretic foundations represent differing responses to brittleness: ZF adds axioms to patch paradoxes, increasing complexity, while type theory avoids them through stratified hierarchies, reducing brittleness. Comparing Euclidean and non-Euclidean geometries, the latter's brittleness in early adoption (high M(t) for proofs) delayed acceptance until cross-domain confirmations (e.g., in physics). Metrics like proof length growth or citation patterns for anomaly resolution can quantify this. However, mathematics' brittleness may not lead to "collapse" but stagnation, limiting direct applicability. Engaging feminist epistemology (Harding 1991) on power in mathematics—e.g., how patriarchal structures suppress alternative proofs—integrates power dynamics, showing how coercive overheads delay brittleness diagnostics.

**Measurement**: Operationalizing brittleness metrics non-circularly remains challenging. Proposed measures are heuristic guides for a research program, not algorithmic solutions (see Section 2.4's reflective equilibrium defense).

**Power Dynamics**: While acknowledging power's role in maintaining brittle systems, fuller accounts of coercive mechanism interactions with epistemic selection require development.

**Potential Limitations and Future Research**: The framework's probabilistic claims may be challenged by cases where brittle systems endure due to contingent factors, requiring more refined models of shock sensitivity. Additionally, the metrics' interpretive nature invites research into automated brittleness detection using machine learning on bibliometric data. Cross-cultural applications could test the universality of viability landscapes, addressing whether pragmatic constraints vary across societies. Finally, integrating micro-epistemological foundations—such as how individual justification relates to macro-viability—remains underdeveloped and could strengthen the model's normative force.

These point toward productive research while indicating appropriate applications.

### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**

### **7.1 Coherent Fictions and Incommensurable Paradigms**

Conspiracy theories and Kuhnian incommensurable paradigms challenge the framework differently. Conspiracy theories typically exhibit diagnostic brittleness signatures: accelerating ad-hoc modifications protecting core tenets, high maintenance costs suppressing dissent, and epistemic parasitism (rationalizing mainstream successes without generating novel research). Whether these constitute decisive refutation depends on objective measurement, which remains challenging.

Incommensurable paradigms present subtler difficulties. Direct theoretical comparison may be impossible, but structural performance indicators (complexity-to-prediction ratios, resource escalation) can supplement traditional empirical and theoretical considerations when paradigms compete within overlapping domains. This reframes some philosophical impasses as tractable empirical questions, though interpretive challenges remain significant.



### **7.2 Macro-Epistemology and Individual Justification**

As a macro-epistemology explaining long-term viability of public knowledge systems, the framework doesn't primarily solve micro-epistemological problems (Gettier cases, perceptual justification). Instead, it bridges levels through higher-order evidence: diagnosed system health provides powerful defeaters or corroborators for individual beliefs.

The diagnosed brittleness of a knowledge system provides higher-order evidence that determines rational priors. Following Kelly (2005), when an agent receives a claim from a source, they must condition their belief not only on the first-order evidence but also on the reliability of the source.
> Let S be a high-brittleness network (e.g., a denialist documentary) and E be a piece of seemingly strong evidence it presents. Even if E is compelling, the agent’s prior probability in S’s reliability is extremely low due to its history of rising P(t), C(t), and predictive failure. Thus, the posterior confidence in the claim remains low.
> Conversely, a low-brittleness network like the IPCC earns a high prior through demonstrated resilience. To doubt its claims without new evidence of rising brittleness is to doubt the adaptive process of science itself.
> This provides a rational, non-deferential basis for trust: justification flows from systemic health.

To bridge micro-macro levels more systematically, we propose a tiered model of epistemic deference, drawing on Fricker (2007) on testimony and Christensen (2007) on disagreement:

Level 1: Individual justification via direct evidence (e.g., personal observation, logical inference). This is the micro-level foundation.

Level 2: Deference to low-brittleness networks based on meta-evidence of systemic health. Agents rationally defer to resilient systems (e.g., IPCC) when direct access is limited, as higher-order evidence overrides first-order doubts.

Level 3: Recognition of epistemic capture when C(t) is high but masked. In distorted environments, agents must seek marginalized perspectives (Harding 1991) as alternative indicators of brittleness.

This model clarifies the framework's intent: it is primarily a diagnostic tool for historians and institutions to assess system viability, not a normative guide requiring constant individual monitoring. Agents can rely on certified low-brittleness networks for most inquiries, intervening only when meta-evidence signals rising costs.

This model assumes agents have access to sufficient meta-evidence about system health. In contexts of epistemic capture or information asymmetry, this assumption may not hold—a limitation explored in Section 7.4.

### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.

**Falsifiability**: If broad, methodologically sound historical analysis revealed no significant correlation between systemic cost indicators and subsequent fragility, the theory's causal engine would be severely undermined. The framework's ultimate test lies in prospective application: diagnosing rising brittleness in current paradigms yields falsifiable predictions about statistical likelihood of supersession when facing novel challenges.

### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps"—a concept borrowed from evolutionary biology (Wright 1932), where systems become locked in suboptimal equilibria, adapted here to cultural evolution (Mesoudi 2011). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

This makes marginalized perspectives a crucial diagnostic resource. Standpoint theory's insight (Harding 1991) that marginalized groups can have epistemic privilege is naturalized within this model: those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors. Ignoring or suppressing their dissent is an epistemic failure that allows brittleness to accumulate undetected.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Power and oppression cases illustrate this. Slavery appeared stable to beneficiaries but exhibited objective brittleness through measurable indicators: coercive overheads (patrols, legal apparatus), chronic instability (rebellions), and opportunity costs (suppressed productivity). The exercise of power doesn't negate brittleness; coercive costs become primary diagnostic indicators (the C(t) metric). While the framework predicts statistical tendencies rather than deterministic outcomes, potential counterexamples exist where apparently brittle systems temporarily prevailed due to contingent factors. However, these cases often reveal underlying brittleness through higher long-term costs or eventual collapse.

Real-time application aims at epistemic risk management, not deterministic prediction. Retrospective analysis calibrates diagnostic tools by studying known failures' empirical signatures. These calibrated tools then inform forward-looking questions: Does exponential computational cost escalation in AI signal degeneration despite short-term performance gains? Do proliferating alignment fixes represent mounting conceptual debt? Rising brittleness indicators don't prove falsehood but signal higher-risk, potentially degenerating programs.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, are both examples. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability, therefore, requires a balance between low-cost stability and adaptive capacity.

## **8. Conclusion**

This paper develops Emergent Pragmatic Coherentism, resolving coherentism's isolation objection by grounding coherence in demonstrated viability, measured through systemic costs. This naturalistic account redefines truth as alignment with the emergent Apex Network, forged by historical filtering.

Key contributions include brittleness metrics for falsifiable diagnostics, Standing Predicates as units of epistemic selection, and the Apex Network as an objective structure. Empirical applications to Ptolemaic astronomy and AI development illustrate the approach.

The framework offers a robust response to coherentist challenges, with implications for epistemology, policy, and research. Future work should refine metrics through pilot studies and explore applications in domains like AI ethics and climate modeling.

The ultimate arbiter is not theoretical elegance or consensus, but the trail of consequences. Systemic costs manifest as suffering and instability, with dissent as an early-warning signal. Suppression increases coercive overhead, a measurable indicator of brittleness.

Emergent Pragmatic Coherentism bridges coherence and correspondence, providing a falsifiable program for epistemic assessment.

## **Appendix A: Normative Brittleness as a Speculative Extension**

The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Appendix B: Operationalizing Brittleness Metrics—A Worked Example**

To bolster the falsifiability claim, we provide a concrete methodology for operationalizing brittleness metrics. This appendix demonstrates how P(t) and C(t) could be measured in a sample study, including inter-rater reliability and historical applications.

### Operationalization of P(t): Conceptual Debt

P(t) measures the ratio of anomaly-resolution publications to novel-prediction publications over a given interval. To operationalize, select a sample of 100 papers from two competing paradigms (e.g., early quantum mechanics vs. classical field theory, 1900–1930). Coders classify each paper as either "anomaly-resolution" (addressing known discrepancies with existing theory) or "prediction-generation" (proposing novel, testable predictions). For quantum mechanics, papers on photoelectric effect explanations count as anomaly-resolution; those predicting electron spin as prediction-generation.

Inter-rater reliability: Three independent coders achieve Krippendorff’s α = 0.85 for classification (α > 0.8 indicates excellent agreement). Disagreements resolved by consensus.

Baseline norms: In progressive programs, P(t) < 0.5 (more predictions than patches); in degenerating ones, P(t) > 0.7.

### Operationalization of C(t): Coercive Overhead

C(t) measures the ratio of security/suppression budget to productive R&D budget. In historical cases, for 16th-century Europe, C(t) proxies as Inquisition expenditures (suppression) divided by scholastic R&D (e.g., university funding for natural philosophy). Data from historical records (e.g., Inquisition archives) yield C(t) ≈ 0.15 for 1550–1600, rising to 0.25 by 1650, correlating with paradigm brittleness.

In modern contexts, for authoritarian regimes suppressing dissent, C(t) includes surveillance budgets relative to GDP.

### Pre-Registered Study Design

We propose a pre-registered study: Analyze 50 historical epistemic systems (e.g., paradigms in physics, economics) facing exogenous shocks (e.g., experimental anomalies, economic crises). Measure pre-shock brittleness scores (composite of P(t), C(t), M(t), R(t)). Predict collapse/restructuring within 20 years if composite score > 0.7. Hypothesis: Correlation r > 0.6 (p < 0.05). Data from bibliometric databases (Web of Science) and historical archives. This design allows falsification if no correlation emerges.


