
# **The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth**

## Abstract

Coherentist theories of justification are vulnerable to the isolation objection: the possibility that a perfectly coherent belief system could be detached from reality. This paper develops Emergent Pragmatic Coherentism, a framework that grounds coherence in the demonstrated viability of knowledge systems. The framework introduces systemic brittleness, a diagnostic tool for measuring network health through the observable costs of applying propositions. We argue that selective pressure from these costs forces knowledge systems to converge on an emergent structure—the **Apex Network**—representing not pre-existing truth but a bottom-up pattern of maximally viable propositions surviving historical filtering. A claim's justification depends on both its internal coherence and the proven resilience of the public Consensus Network certifying it. This naturalistic theory redefines objective truth as alignment with the Apex Network's emergent structure, explains how Quine's web of belief undergoes pragmatic revision, and grounds a falsifiable research program for assessing epistemic system health. This program is illustrated through preliminary applications to historical cases, such as Ptolemaic astronomy and AI winters, using quantifiable proxies like citation patterns and resource allocation metrics.

## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Miasma theory generated catastrophic costs—thousands died in London because public health efforts were misdirected at odors rather than contaminated water—and required accelerating ad hoc modifications to explain anomalies. Miasma theory's brittleness can be illustrated through its high patch velocity (P(t)); historical analyses (e.g., Snow 1855) suggest the need for dozens of ad-hoc modifications by the mid-19th century to explain accumulating anomalies. Germ theory, by contrast, dramatically reduced these costs while explaining diverse phenomena with a unified conceptual framework.

This dynamic illustrates the classic isolation objection to coherentism: as BonJour (1985) noted, a belief system may be perfectly coherent yet entirely detached from reality. While coherentists have developed various responses (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. This paper develops an alternative response grounding coherence in the demonstrated viability of entire knowledge systems, measured through their capacity to minimize costs over time.

The framework, which we term **Emergent Pragmatic Coherentism**, holds that justification requires two conditions: internal coherence within a shared network (a **Consensus Network**—our fallible, historically situated collective knowledge system) and that network's demonstrated reliability through low **brittleness** (accumulated vulnerability to collapse due to rising systemic costs). This dual requirement provides the externalist constraint coherentism needs while preserving its holistic character.

The model treats inquiry as an evolutionary process cultivating viable public knowledge systems. It is a macro-epistemology focusing on cumulative systems like science and law, where claims build upon previous work and practical consequences provide feedback about performance.

Three important scope limitations should be noted upfront. First, the framework applies primarily to cumulative knowledge systems where pragmatic feedback is measurable—empirical science, engineering, law, and policy. While the framework's core diagnostics apply universally, its extension to abstract domains like pure mathematics—where pragmatic pushback is primarily internal—requires treating brittleness as inefficiency in proof structures or axiomatic consistency, as explored in Section 6.4. For instance, brittleness in mathematics might manifest as accelerating proof complexity or axiomatic inconsistency, as in the shift from naive set theory (vulnerable to paradoxes like Russell's) to Zermelo-Fraenkel set theory (ZFC), which resolved brittleness by providing a more viable axiomatic foundation with lower long-term inconsistency risks. Its application to pure mathematics or abstract philosophy remains underdeveloped. Second, the framework operates at macro-historical timescales and may not resolve real-time controversies. Third, in domains where power structures control information flow, the framework's diagnostics may be suppressed or delayed for extended periods.

Three clarifications prevent misunderstanding. First, viability differs from mere endurance. A coercive empire may persist, but it is a textbook case of a high-brittleness system; its longevity is not a sign of health but a measure of the immense energy it wastes suppressing its own instability. True viability means addressing challenges with low, stable systemic costs. Second, the framework incorporates power and contingency as central variables, not exceptions. The exercise of power to maintain a brittle system is itself a primary indicator of non-viability through measurable coercive costs. Third, the model's claims are probabilistic, not deterministic. Systems accumulating brittleness become progressively vulnerable to contingent shocks, offering a falsifiable research program for understanding structural dynamics, not a deterministic theory of history.

### Glossary
- Apex Network: Emergent structure of maximal viability
- Brittleness: Accumulated systemic costs
- Emergent Pragmatic Coherentism: Framework grounding coherence in demonstrated viability
- Standing Predicate: Reusable predicate for cost-reduction

## **2. A Framework for Assessing Systemic Viability**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

**The Deflationary Path: Belief → Proposition → Validated Data → Standing Predicate**

#### **2.1.1 From Private Belief to Public Proposition**

The journey begins with *belief*, the raw material of epistemology. As a private psychological state tied to an individual's consciousness, it is analytically inaccessible for a theory of public knowledge. The first step is therefore to isolate its testable, public content as a *proposition*: a falsifiable, testable claim articulated in language that is subject to collective assessment. Unlike the abstract, language-independent content of sentences in traditional philosophy, our treatment is deliberately deflationary and functional, focusing on propositions as concrete, evaluable statements made by agents within a knowledge network. This transformation from private belief to public proposition is essential because it makes beliefs accessible to collective evaluation and allows them to function within a shared epistemic system.

#### **2.1.2 The Coherence Test**

Next, a candidate proposition must pass a rigorous test for coherence. This is not the thin, formal consistency of logic, but a thick, forward-looking pragmatic assessment—a form of risk analysis. A shared network, as a resource-constrained system, implicitly asks: will integrating this proposition increase or decrease our long-term systemic brittleness? A proposition that successfully passes this test is accepted into the network as validated data—a reliable claim that can be used and cited within the system, awaiting its ultimate functional test.

#### **2.1.3 From Validated Data to Standing Predicate**

When a proposition proves exceptionally useful in reducing systemic costs, its core principle becomes a Standing Predicate—a reusable tool for evaluating new cases. For instance, once "cholera is an infectious disease" proved valuable, the schema "…is an infectious disease" was promoted to Standing Predicate status. Applying it to new cases automatically mobilizes proven strategies—isolating patients, tracing transmission, targeting pathogens. Unlike a static causal model (e.g., 'X causes Y'), a Standing Predicate like "…confers noncausal powers" (in normative domains) evolves via demotion if costs rise, as in outdated legal predicates. A predicate's status is provisional: if it begins to generate rising brittleness, it can be demoted. Additional examples include the principle of conservation of momentum in physics, which became a Standing Predicate after unifying classical mechanics, and the concept of due process in law, which evolves via demotion when it fails to reduce systemic costs like injustice. Further examples are the periodic table in chemistry, which organizes elements to reduce predictive costs, and the concept of falsifiability in science, which became a tool for evaluating hypotheses. Two additional examples include the double-entry bookkeeping system in accounting, which became a Standing Predicate for tracking financial flows and reducing errors, and the peer review process in academia, which evolved as a tool for quality control but can be demoted if it generates excessive administrative costs.

This differs from predicates in formal logic (functions returning truth values) and traditional philosophical predicates (denoting properties). Standing Predicates are defined functionally: by their track record of enabling successful action across diverse applications. They are the units of cultural-epistemic selection—conceptual tools that prove their worth through demonstrated capacity to reduce brittleness, creating a positive feedback loop that reinforces the most reliable inferential patterns. The term 'Standing Predicate' is philosophically superior because it connects to logical predicates while emphasizing their functional roles in epistemic systems, serving as standing rules for inference that can be demoted if they fail to reduce brittleness.

### **2.2 The Units of Analysis: Predicates, Networks, and Replicators**

Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

A *Shared Network*, the primary unit of public knowledge, emerges as an observable consequence of Quine's holism applied socially: it is the coherent intersection of viable individual webs of belief, often nested (e.g., germ theory within medicine). Agents inherit these networks top-down but revise them bottom-up via pragmatic pushback, functioning as replicators of ideas (Mesoudi 2011). 

The *Standing Predicate* is the validated, reusable tool extracted from successful propositions (e.g., "...is an infectious disease"), serving as the core unit of cultural-epistemic selection. It unpacks causal models and interventions when applied.

The model's deflationary path shifts from private *belief* (psychological state) to public *proposition* (testable claim), potentially becoming a *Standing Predicate* if it reduces costs exceptionally.

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory, as synthesized for the social sciences by Mesoudi (2011). The network’s abstract informational structure—its core Standing Predicates and their relations—functions as the **replicator**: the "code" that is copied and transmitted. The social group and its institutions (the scientific community, legal system, or society) function as the **interactor**: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This distinction is crucial for understanding how knowledge can evolve and persist across different social contexts. It explains how knowledge can persist even when the societies that created it do not; the rediscovery of Roman legal principles during the Renaissance is a prime example of a successful replicator outliving its original interactor. The existence of these countless, independently formed and often nested Shared Networks—all responding to the same landscape of physical and social constraints—supports a crucial inference: they collectively reveal an objective structure that underwrites all successful inquiry. This mosaic of survived predicates, biased toward low-brittleness solutions, anticipates the Apex Network: not a pre-existing truth, but an emergent attractor forged by historical filtering (as detailed in Section 4).

### **2.3 Pragmatic Pushback and Systemic Costs**

A shared network is not a passive library; it is an active system under constant pressure from *pragmatic pushback*—our model’s term for the systemic analogue of what Quine called a "recalcitrant experience" at the periphery of an individual's web of belief. It is the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback is not an argument but a material outcome—a bridge collapses, a treatment fails, a society fragments. Pragmatic pushback generates *first-order costs* (direct failures like excess mortality) and *systemic costs* (secondary burdens, including *conceptual debt* from patches and *coercive overheads* from suppressing dissent), which signal misalignment and drive revision—quantifying health for a falsifiable program (see Sections 2.4–2.5).

It is important to note that "pragmatic pushback" is not limited to direct, material failures. In highly abstract domains, such as theoretical physics or pure mathematics, where direct empirical tests are deferred or unavailable, pushback primarily manifests through the accumulation of costs. A research program that requires an accelerating rate of ad-hoc modifications to maintain internal consistency, or that loses its unifying power, is experiencing a powerful form of pragmatic pushback. These epistemic inefficiencies are real costs that render a network brittle and unproductive, even if it has not been directly falsified by an experiment. The framework's diagnostic lens thus applies universally to all forms of inquiry, measuring viability through either external, material consequences or internal, systemic dysfunction. This universal applicability is crucial for our claim that the framework provides a general solution to the isolation objection across all domains of knowledge. Pragmatic pushback aligns with Holling's resilience thresholds, where exceeding costs triggers regime shifts.

### **2.4 Gauging Brittleness: An Empirical Toolkit**

A system's brittleness is a measure of its accumulated costs. To avoid overconfidence in formalization, we note that a complete quantitative model remains a goal for empirical research rather than a settled achievement. The following table illustrates potential diagnostic indicators for each component:

| Indicator | Domain | Potential Proxy Metric | Data Collection Method | Example Application |
| :--- | :--- | :--- | :--- | :--- |
| Rate of Ad Hoc Modification (P(t)) | Scientific Paradigms | A bibliometric ratio calculated as: (Citations to papers primarily resolving known anomalies) / (Citations to papers generating novel, successful predictions). A rising trend indicates degenerative patching. | Academic databases (e.g., Web of Science, arXiv), using NLP to classify paper abstracts by function (anomaly-resolution vs. prediction-generation) | In Lysenkoism, auxiliary hypotheses rose from 10% to 40% of publications (1940-1950) |
| Ratio of Coercion to Production (C(t)) | Socio-Political Networks | Internal security budget / Total productive budget. Values >0.3 indicate high brittleness | Budget data, Seshat Databank, World Bank institutional analyses | Soviet Union pre-collapse: security budgets >30% GDP correlated with fragility (Turchin 2003) |
| Increasing Model Complexity (M(t)) | Computational Systems | Parameter count growth rate / Performance improvement rate. Exponential divergence signals brittleness | arXiv trends, MLPerf benchmarks, computational cost analyses | Deep learning 2018-2023: parameter counts grew 100x while performance gains ~20% |
| Resilience Reserve (R(t)) | Cross-Domain | Number of independent domain confirmations; age and stability of core principles without major revision | Historical records, citation networks, cross-domain literature analysis | Germ theory: confirmed across medicine, sanitation, biology, epidemiology |

Brittleness can be broken down into four measurable dimensions:

- **Patch Velocity (P(t))**: The rate of ad hoc modifications relative to novel predictions, indicating conceptual debt accumulation.

- **Coercion Ratio (C(t))**: The proportion of resources devoted to suppressing alternatives, measuring normative brittleness.

- **Model Complexity (M(t))**: The escalation of complexity without proportional gains, signaling inefficiency.

- **Resilience Reserve (R(t))**: The breadth of independent confirmations and stability of core principles, providing a buffer against shocks.

This structure allows brittleness to be assessed holistically, with each dimension providing convergent evidence. While some judgment is required (e.g., classifying papers as anomaly-resolution vs. prediction-generation), this can be operationalized through systematic coding protocols and inter-rater reliability checks.

### **2.5 Operationalizing and Modalities of Brittleness**

Consider Ptolemaic astronomy circa 1500 CE. Its predictive machinery required about 80 epicycles to remain accurate, with new observations demanding 2–3 more each decade. Complexity ballooned while returns diminished: predictive accuracy rose only 15% despite a 400% increase in computational burden.

Applying these metrics to contemporary AI development reveals potential brittleness. Drawing on Sevilla et al. (2022), parameter counts escalated 100-fold while FLOPs efficiency gains lagged at 20-30% per MLPerf (2023) cycles, signaling potential M(t) brittleness. P(t) is evident in the proliferation of 'alignment' papers—thousands of publications on AI safety in recent years—many of which address anomalies in model behavior rather than generating novel capabilities. R(t) declines as AI remains isolated from broader scientific integration, with limited cross-domain applications beyond narrow tasks. These trends suggest potential warning signs of rising brittleness in deep learning, which may invite cautious comparison to the structural dynamics of past degenerating research programs like late-stage Ptolemaic astronomy.

The operationalization of brittleness faces an unavoidable circularity: measuring systemic costs requires standards for "waste" or "dysfunction" that themselves depend on epistemic commitments. We acknowledge this as a fundamental limitation rather than a fully solvable problem. The framework manages this hermeneutic circle by constraining interpretation through a methodology of triangulation, relying on four principles:

* "**Physical-Biological Anchors:** We anchor measurements in outcomes that register as failures across divergent theoretical perspectives. Demographic collapse, infrastructure failure, and resource depletion provide relatively theory-neutral indicators of breakdown."
* "**Comparative-Diachronic Methods:** The framework's claims are strongest when made comparatively (System A exhibits lower brittleness than System B) or diachronically (System C's brittleness is rising over time). Such judgments can be robust even when absolute standards are contested."
* "**Convergent Evidence Requirements:** A diagnosis of brittleness is only made when multiple, independent indicators converge. Systematic convergence across bibliometric, institutional, and empirical measures becomes progressively difficult to dismiss as mere theoretical bias."
* "**Minimal Viability Anchors:** To further mitigate circularity, we employ cross-cultural anchors such as resource sustainability (e.g., systems failing to maintain basic subsistence levels) and demographic stability (e.g., population decline without external shocks). These provide a minimal, theory-neutral baseline for assessing dysfunction."

Proxies like citations may bias toward dominant paradigms, mitigated by cross-database validation (e.g., arXiv + Web of Science).

To demonstrate falsifiability, consider climate policy models: complex integrated assessment models (IAMs) with high M(t) (parameter proliferation) have shown brittleness through predictive failures (e.g., underestimating tipping points), while simpler models with lower M(t) maintained better long-term accuracy. This case illustrates how brittleness metrics predict collapse or revision, providing empirical grounding.

This yields what we call 'constrained interpretation'—structured judgment accountable to evidence. It is sufficient for the framework's purpose: distinguishing degenerating from progressive research programs.

To illustrate, in coding Lysenkoism papers, three analysts achieved 85% agreement on P(t) classifications using predefined protocols (e.g., abstracts mentioning 'anomalies' vs. 'predictions').

We distinguish degenerative "patches" from progressive hypotheses by assessing explanatory return on investment. Progressive hypotheses offer high returns: small complexity investments yielding novel predictions or unifying phenomena. Degenerative patches offer low returns: high-cost fixes resolving only targeted anomalies while increasing overall complexity. The Higgs boson exemplifies the former, adding one particle but unifying electroweak theory with confirmed novel predictions. Ptolemaic epicycles exemplify the latter, requiring ever-more geometrical complexity to save specific planetary observations without generating testable insights. Operationally, this distinction can be measured through bibliometric analysis: does a modification primarily generate citations for novel predictions and applications, or primarily for managing known anomalies?

The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Epistemic Brittleness:** This is the modality of failure resulting from a misalignment with the **causal structure of the world**. It is found in descriptive knowledge systems, like scientific paradigms, whose primary function is to predict and manipulate physical reality. It is diagnosed through indicators of failed causal engagement: an accelerating rate of ad hoc modification to explain away predictive failures, increasing model complexity without a corresponding increase in causal power, and high energetic inefficiency. The late-stage Ptolemaic network, accumulating epicycles to manage its failed causal predictions, is the canonical example of a system suffering from acute epistemic brittleness.

The normative modality, while a promising extension, relies on controversial metaethical commitments and is speculative; it is explored independently in Appendix A. The paper's central claims about epistemic justification stand on their own merits for descriptive knowledge systems.

The central claim of this model is that epistemic brittleness represents a failure to align with the causal structure of the world. Such misalignment requires a brittle system to pay an ever-increasing price to insulate its flawed core from pragmatic consequences.

## **3. The Drivers of Adaptation: The Logic of Systemic Viability**

The framework for assessing systemic health detailed in Section 2 is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems.

### **3.1 Grounding Epistemic Norms in Systemic Viability**

A standard objection to naturalistic epistemology is that descriptive accounts of how we *do* reason cannot ground prescriptive accounts of how we *ought* to reason (Kim 1988). Our framework answers this "normativity objection" by grounding its norms in structural conditions required for cumulative inquiry to succeed.

Following Quine, we treat normative epistemology as engineering (Moghaddam 2013). Epistemic norms are hypothetical imperatives—conditional recommendations for achieving specified ends. Our framework makes this goal concrete: cultivating low-brittleness knowledge systems, aligning with recent discussions of epistemic risk (Pritchard 2016). Two arguments establish this norm's authority.

**Constitutive Argument**: Cumulative inquiry requires intergenerational stability. Any system that systematically undermines its own persistence cannot succeed at preserving and transmitting knowledge. Low brittleness is not an optional value but a structural constraint on cumulative inquiry itself. A system cannot be viable if it accumulates costs faster than it solves problems—it will exhaust resources or fragment before completing its project.

**Instrumental Argument**: The framework makes a falsifiable empirical claim: networks with high, rising brittleness are statistically more likely to collapse or require radical revision. This yields a conditional recommendation: *if* an agent or institution seeks long-term stability and problem-solving capacity, *then* it has evidence-based reason to adopt low-brittleness principles.

This focus on shared, public networks directly addresses the isolation objection. A perfectly coherent but detached system—such as the internal logic of a complex video game or a self-consistent fictional world—might exhibit low internal "costs." However, Emergent Pragmatic Coherentism is not a theory of arbitrary systems; it is a theory of public knowledge systems that must operate in a single, shared reality. The ultimate test of viability is not internal coherence but cross-domain application. A system's brittleness becomes truly apparent when its principles are used to act upon the world and interact with other systems. The "pragmatic pushback" from a bridge collapsing or a medical treatment failing is an inter-subjective, reality-based constraint that no isolated system can simulate or evade.

This differs from mere instrumental rationality. The end (viable inquiry) is not an arbitrary preference but a structural necessity for systems participating in cumulative knowledge production. The means (low-brittleness principles) face recursive constraints—they must themselves demonstrate low brittleness, preventing purely expedient solutions. This recursive structure makes norms responsive to objective pragmatic constraints, not mere efficiency.

**Naturalized Proceduralism**: The framework's contribution is best understood as naturalized proceduralism. Like procedural realists (e.g., later Putnam), we ground objectivity in procedural properties rather than direct correspondence. The crucial divergence: where rationalist accounts locate objectivity in idealized discourse norms, our model grounds it in the empirical, historical process of pragmatic selection. The arbiter is not the internal coherence of our reasons but the measurable brittleness of systems those reasons produce. Arguments are disciplined by non-discursive data: systemic success and failure.

When the model describes one network as "better" or identifies "epistemic progress," these are technical descriptions of systemic performance, not subjective values. A "better" network exhibits lower measured brittleness and higher predicted resilience. Viability is a structural precondition for any system entering the historical record.

### **3.2 Coherence as Forward-Looking Risk Assessment**

Framed by the logic of viability, the test for coherence is a thick, forward-looking *cost-benefit analysis*: a set of heuristics that a resource-constrained system uses to bet on whether adopting a new proposition will increase or decrease its long-term brittleness. The traditional epistemic virtues are the core principles of this practical calculus:

* **Logical Consistency:** A hedge against the infinite future costs of inferential paralysis.
* **Explanatory Power:** A measure of a proposition’s potential return on investment, reducing future inquiry costs by paying down conceptual debt.
* **Simplicity / Parsimony:** A direct measure of systemic overhead; complex propositions increase long-term maintenance costs.
* **Evidential Support:** An assessment of integrative risk; a well-supported claim is a low-risk investment unlikely to trigger a cascade of costly future revisions.

One might object that this account reduces scientific revolutions to purely pragmatic considerations, ignoring the role of theoretical virtues like explanatory depth or mathematical elegance. However, these virtues are themselves pragmatically valuable in our framework: explanatory depth reduces future conceptual debt by unifying disparate phenomena, while mathematical elegance often signals structural efficiency that minimizes maintenance costs. Rather than eliminating traditional theoretical virtues, our framework explains their pragmatic function within the evolutionary process of knowledge development.

This forward-looking model also explains how revolutionary science is possible. When a dominant Consensus Network exhibits high and rising systemic brittleness—a state corresponding to a Kuhnian 'crisis'—the cost-benefit analysis for new propositions shifts dramatically. A radical new hypothesis, while having low coherence with the existing network's specific principles, may promise a massive long-term reduction in systemic costs. The network, in effect, makes a high-risk, high-reward bet. Coherence, in this dynamic sense, is not a conservative check for conformity, but a pragmatic assessment of a proposition's potential to resolve systemic crisis.

## **4. Convergence and the Emergence of Objective Structures**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We will show how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

Our account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is often of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system—its descent into crippling inefficiency, intellectual stagnation, and institutional decay—provides a clear, non-negotiable data point. This approach differs from traditional foundationalism by building knowledge from what we can confidently reject rather than from what we must indubitably accept.

The systematic analysis of these failures allows us to build the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been empirically invalidated by the catastrophic systemic costs they reliably generate. The *Negative Canon* catalogs failed networks invalidated by costs (e.g., phlogiston), anchoring objectivity by mapping untenable paths and constraining coherence.

E.g., phlogiston chemistry, miasma theory, luminiferous aether, and blank slate psychology, as detailed in our worked examples. While some elements of failed theories may return in modified forms, the Canon identifies core structural patterns that reliably generate costs, not every specific claim.

This canon charts failures of both causal and normative alignment:

* **Failures of Causal Alignment** are characteristic of what we term *Epistemic Brittleness*. Systems like scholastic physics, phlogiston chemistry, and Lysenkoist biology entered the canon because their core principles generated catastrophic causal failures, leading to chronically high rates of ad-hoc modification and predictive collapse. They represent failed blueprints for the causal world.
* **Failures of Normative Alignment** are characteristic of *Normative Brittleness*. Systems like chattel slavery or totalitarianism entered the canon because their core principles violated emergent normative facts about human cooperation. Their unsustainability was demonstrated by the immense and ever-rising coercive overheads required to suppress the normative pushback they generated. These are not condemned by a modern moral judgment alone; they are failed engineering principles for a viable social world.

By charting what demonstrably fails, whether in engineering or social organization, we are not merely learning what to avoid; we are effectively reverse-engineering the constraints of a real territory. The Negative Canon functions like a reef chart for inquiry. It is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and prevents a collapse into relativism.

### **4.2 Toward an Emergent Conception of Truth**

The Apex Network is the emergent structure revealed as unviable systems are eliminated. It is not a pre-existing truth but the structural residue of countless pragmatic filters. Like π, we cannot state it in full, but we know it exists, can approximate it indefinitely, and can judge which approximations are closer.

The Apex Network A is the intersection of all viable worlds, approximated by our Consensus Network over time. This echoes Peirce's (1878) notion of truth as the ideal end of inquiry. Our Consensus Network S_consensus(t) is a fallible, historically-situated attempt to approximate this structure. Progress means reducing |S_consensus \ A|.

To clarify emergence, maximal viability arises through differential survival: systems reducing brittleness propagate their Standing Predicates across domains, fostering convergence. The Apex Network is domain-specific where pragmatic constraints vary (e.g., tighter in physics than aesthetics), but universal in demanding viability alignment. Convergence is structural (methods like experimentation) rather than purely propositional (specific claims), permitting content pluralism while unifying approaches.

Formally, the Apex Network can be conceptualized using network theory (Newman 2010) as the resilient core of intersecting viable worlds: A = ∩{W_k | V(W_k) = 1}, where W_k represents a viable world-system (such as a scientific paradigm, a legal framework, or an entire society's knowledge base), and V(W_k) is computed via brittleness metrics (e.g., low P(t), C(t), M(t), high R(t)). This formalization highlights how convergence emerges from graph resilience, where edges (Standing Predicates) strengthen through cross-domain propagation, eliminating brittle nodes.

We access it through:

- **Negative knowledge**: The Negative Canon charts what demonstrably fails
- **Progressive approximation**: Successively lower-brittleness systems
- **Comparative judgments**: System A exhibits lower brittleness than System B

The structure's objectivity derives from the mind-independent nature of pragmatic constraints that reliably generate costs for systems violating them, not from metaphysical speculation about its pre-existence.

The mechanism that forges this structure is a bottom-up emergence driven by the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question. The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we might expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

This process operates through the differential success of Standing Predicates across domains. When a predicate proves highly effective in reducing brittleness in one domain, it creates pressure for similar principles to be adopted in related domains. For example, the success of germ theory in medicine created pressure for similar causal approaches in public health and sanitation. This cross-domain propagation of successful predicates is a key mechanism driving the emergence of the Apex Network, as the most viable conceptual tools gradually spread across different knowledge systems.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge. The "pursuit of truth," then, is the practical, fallible project of refining our current Consensus Network to better align with this real, objective structure.

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state; they simply try to solve local problems and reduce costs. The Apex Network is the name we give to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

The Apex Network's status is therefore dual, a distinction critical to our fallibilist realism. Ontologically, it is real: it is the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, however, it remains a regulative ideal. We can never achieve a final, God's-eye view to confirm that our Consensus Network is a perfect map of it; our knowledge of the structure is necessarily incomplete and fallible. Its existence is what grounds our realism and prevents a collapse into relativism, while our epistemic limitations are what make the project of inquiry a permanent and progressive one. It is the necessary concept that makes our comparative judgments of "more" or "less" brittle meaningful.

#### 4.2.1 The Ontological Status of the Apex Network

To address potential oscillations in the interpretation of the Apex Network, we explicitly clarify its ontological status. The Apex Network is not a pre-existing metaphysical blueprint awaiting discovery, but a structure constituted by the bottom-up process of pragmatic filtering. It is discovered retrospectively through the elimination of high-brittleness systems, yet its contours are shaped by the very process of inquiry itself.

Metaphysically, the Apex Network is an emergent structural invariant: a network topology comprising the space of maximally viable Standing Predicates that have survived historical selection. It represents not a fixed set of truths, but a dynamic attractor in the landscape of possible knowledge systems, defined by mind-independent pragmatic constraints.

This view aligns with and diverges from several philosophical traditions. Like Peirce's (1878) regulative ideal, the Apex Network functions as an asymptotic limit we approach through inquiry, but our framework naturalizes this ideal as an emergent outcome of pragmatic selection rather than a transcendental norm. Structural realism (Worrall 1989) emphasizes persistent relational structures across theory changes; we extend this by grounding such structures in the emergent viability landscape, where relations are preserved because they minimize systemic costs. Mechanistic realism, as articulated by Cartwright (1999), treats the world as composed of capacities rather than universal laws; the Apex Network can be seen as the space of viable mechanistic capacities that reliably reduce brittleness across domains. Finally, process metaphysics (Rescher 1996) views reality as fundamentally processual; our conception resonates with this by understanding the Apex Network as an ongoing emergent process, continually refined through the dynamics of pragmatic pushback.

Regarding contingency, the Apex Network is not contingent on the specific causal structure of our world. While its particular manifestations may vary with local conditions, the underlying viability constraints—such as the need to minimize systemic costs for cumulative inquiry—would be the same in any world supporting such inquiry. This universality stems from the structural requirements of any system engaged in inter-generational knowledge accumulation, making the Apex Network a necessary feature of worlds capable of sustained epistemic progress.

This clarification resolves the apparent tension: the Apex Network is ontologically real as an emergent structure, yet epistemically ideal as a limit we can never fully attain. It is constituted by inquiry but discovered through its failures, providing a robust foundation for objective truth without overreach.

To clarify, the Apex Network is ontologically real as an emergent attractor in the viability landscape (mind-independent constraints), yet epistemically ideal as a limit we approximate fallibly—resolving Quine's tension without contradiction.

To further clarify the ontological commitments of the Apex Network, we contrast it with alternative positions. Unlike platonic realism, which posits timeless ideal forms existing independently of history, the Apex Network is emergent from pragmatic processes, not a pre-existing metaphysical entity. It differs from social constructivism by being mind-independent: its structure constrains successful inquiry regardless of cultural beliefs. This view aligns with Peirce's (1878) conception of truth as the ideal end of inquiry, but naturalizes it as the convergent outcome of pragmatic selection rather than a transcendental ideal. The Apex Network is 'real' in the sense that it exists as a stable attractor in the landscape of viability, discoverable through eliminative methods, much like π is real as the limit of successive approximations. To illustrate this emergent structural fact, consider the cross-cultural emergence of color terms. Across diverse societies, basic color categories like 'red' and 'blue' emerge convergently, not because of pre-existing universal essences, but because they correspond to stable patterns in light reflection and human perception that facilitate reliable environmental interaction. Similarly, the Apex Network emerges as a structural fact from the pragmatic constraints that shape successful knowledge systems, independent of any particular culture's beliefs.

### **4.3 A Structured Framework for Truth and Inquiry**

This emergent structure grounds a fallibilist but realist account of truth, resolving the isolation objection and Quine's tension between immanent and transcendent truth. Truth is a status propositions earn through validation stages.

| Level | Description | Justification Basis |
|-------|-------------|---------------------|
| 3: Contextual Coherence | Coherent within a network, regardless of viability | Internal consistency |
| 2: Justified Truth | Certified by low-brittleness Consensus Network | Pragmatic track record |
| 1: Objective Truth | Part of Apex Network | Convergence limit |

This layered framework avoids Whig history: Newtonian mechanics was Level 2 for centuries, objectively false at Level 1.

It integrates coherence and correspondence theories dynamically.

The historical process yields two epistemic zones:

| Zone | Description | Epistemic Status |
|------|-------------|------------------|
| Convergent Core | Eliminated rivals, low-brittleness principles | Approaches Level 1 |
| Pluralist Frontier | Competing viable systems | Level 2, defeasible |

This reflects uneven pragmatic pressures. Illustrative cases: Newtonian to relativistic physics, where brittleness metrics showed rising P(t) and M(t); AI winters as collapses of high-brittleness paradigms, with deep learning as a resilient solution potentially now accumulating costs.







## **5. The Dynamism of the Web: How Knowledge Becomes Entrenched**

Quine's "Web of Belief" provided a powerful static model of a knowledge system, but it has been criticized for lacking a corresponding dynamic account of its formation and change. This section provides that dynamic physiology, detailing the naturalistic process by which a successful discovery migrates from the tentative "periphery" of the web to its load-bearing "core."

A proposition is promoted to the core by demonstrating its immense value in lowering the entire network's systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. Entrenchment functions as systemic caching: networks conserve resources by fixing proven principles in the core. As Herbert Simon argued, real-world agents and systems operate under bounded rationality with finite time, attention, and computational resources (Simon 1972). By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine's static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

## 6. Situating the Framework: Emergent Pragmatic Coherentism and Its Relations

This section clarifies Emergent Pragmatic Coherentism's position within contemporary epistemology by examining its relationship to coherentism, evolutionary epistemology, and neopragmatism.

### 6.1 Resolving Coherentism's Isolation Problem

Contemporary coherentist theories face BonJour's (1985) isolation objection: a belief system could achieve perfect internal coherence while entirely detached from reality. Internalist responses (Kvanvig 2012; Carlson 2015) explain *why* some beliefs are more central but not *how* that centrality is earned through external discipline.

Emergent Pragmatic Coherentism provides this missing externalist constraint. It complements Carlson's (2015) internalist reconstruction showing core beliefs are functionally indispensable by providing the causal, evolutionary explanation: principles become core by surviving historical pragmatic filtering demonstrating their role in low-brittleness networks. Justification requires both internal coherence and demonstrated network reliability through low systemic brittleness. This grounds the web's structure in objective, externalist history.

### 6.2 Evolutionary Epistemology and the Fitness Problem

Evolutionary epistemology (Campbell 1974; Bradie 1986) faces a circularity problem: defining fitness without distinguishing genuinely beneficial knowledge from well-adapted "informational viruses." Our framework provides a non-circular standard: long-term viability measured by systemic brittleness. A principle's fitness is its contribution to system resilience, not its transmissibility or psychological appeal. Recent work in network epistemology (Zollman 2013) complements this by modeling how epistemic networks evolve through communication and division of cognitive labor.

This proves diagnostic. Conspiracy theories achieve high transmissibility but incur massive conceptual debt through accelerating ad-hoc modifications and coercive ideological maintenance. Their measured brittleness reveals non-viability despite psychological "fitness." The framework also addresses evolutionary epistemology's difficulty with directed inquiry by modeling Lamarckian-style inheritance through functional entrenchment of successful solutions.

Our brittleness standard resolves Bradie's fitness circularity by prioritizing resilience over transmissibility, distinguishing viable knowledge from informational viruses. Where Bradie worried that evolutionary success might favor memes that spread easily but fail pragmatically, brittleness provides an empirical criterion: systems with low measured brittleness demonstrate genuine fitness through sustained viability, not mere reproductive success. This allows us to identify "informational viruses" like conspiracy theories as brittle despite their transmissibility, grounding evolutionary epistemology in observable systemic costs rather than speculative adaptation narratives.

**Relation to Lakatos and Laudan**: While Lakatos (1970) describes degenerating research programmes qualitatively, our framework provides the underlying causal mechanism. Brittleness measures accumulated systemic costs causing degeneration, offering quantifiable proxies (P(t), M(t), C(t)) where Lakatos gave binary classification. Unlike Laudan's (1977) retrospective problem-solving effectiveness, brittleness provides forward-looking risk assessment, detecting vulnerability before crisis.

### 6.3 A Realist Corrective to Neopragmatism

The framework retains pragmatism's anti-foundationalist spirit while offering a corrective to neopragmatists (Rorty 1979; Brandom 1994) vulnerable to reducing objectivity to social consensus. These accounts of justification as linguistic practice lack robust, non-discursive external constraints, leaving inadequate resources for cases where communities converge on unviable beliefs through well-managed discourse.

Systemic failure provides the missing constraint. Lysenkoist biology's collapse resulted not from discourse breakdown—that discourse was brutally enforced—but from catastrophic costs no conversational management could prevent. Pragmatic viability is not objectivity's source but the empirical indicator of alignment with the Apex Network's mind-independent structure. Genuine solidarity emerges from low-brittleness systems adapted to pragmatic constraints, making viable knowledge cultivation the secure path to enduring agreement.

**Relation to Structural Realism**: The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions:

(1) The ontological question (answered by the emergent landscape of viability): Our model naturalizes the ontology of these structures. The **Apex Network** *is* the complete set of viable relational structures, but it is not an abstract or metaphysical entity. As argued in Section 4, it is an **emergent structural fact about our world**—a real "landscape of viability" whose contours are determined by mind-independent pragmatic constraints. These structures are not posited a priori; they are discovered retrospectively through the historical process of culling what fails.

(2) The epistemological question (answered by the eliminative process of pragmatic selection): Our framework provides the specific causal mechanism for convergence that structural realism often lacks. We discover these robust structures not through a mysterious act of intellectual insight, but through the brutal, eliminative process of pragmatic selection. High-brittleness networks—those whose posited structures misalign with the real landscape of viability—generate unsustainable costs, collapse, and enter the **Negative Canon**. Low-brittleness networks survive and are built upon. Over historical time, this failure-driven selective pressure is precisely the engine that forces our **Consensus Networks** to conform to the objective, relational structure of the Apex Network. Unlike Ladyman and Ross (2007), who posit ontic structures, our Apex Network emerges via pragmatic selection, evidenced by cross-domain predicate propagation (e.g., causality from physics to biology).

Contra Psillos's pessimistic induction, our eliminative process provides empirical evidence of convergence toward stable structures. While Psillos argues that past theories' failures undermine realism, our framework shows that failures are not random but systematically eliminated by brittleness, leaving a convergent residue. The Negative Canon's accumulation demonstrates that not all theories fail equally; those aligning with viability persist, offering inductive grounds for realism about the Apex Network as the limit of this process.

### 6.4 Scope, Limitations, and Research Directions

The framework operates at the macro-historical level, suited to cumulative knowledge systems with clear practical consequences. Key limitations:

**Scope**: While the framework claims universal applicability, it is strongest in domains with direct pragmatic feedback. Pure mathematics presents challenges due to minimal external constraints, but brittleness can manifest as internal inefficiency: accelerating proof complexity without proportional gains in unifying power, or axiomatic proliferation to resolve paradoxes. For example, Russell's paradox in naive set theory led to proliferating axioms (e.g., Zermelo-Fraenkel) without unifying power, akin to epicycles. Comparing Euclidean and non-Euclidean geometries, the latter's brittleness in early adoption (high M(t) for proofs) delayed acceptance until cross-domain confirmations (e.g., in physics). Metrics like proof length growth or citation patterns for anomaly resolution can quantify this. However, mathematics' brittleness may not lead to "collapse" but stagnation, limiting direct applicability. Engaging feminist epistemology (Harding 1991) on power in mathematics—e.g., how patriarchal structures suppress alternative proofs—integrates power dynamics, showing how coercive overheads delay brittleness diagnostics.

**Measurement**: Operationalizing brittleness metrics non-circularly remains challenging. Proposed measures are heuristic guides for a research program, not algorithmic solutions (see Section 2.4's reflective equilibrium defense).

**Power Dynamics**: While acknowledging power's role in maintaining brittle systems, fuller accounts of coercive mechanism interactions with epistemic selection require development.

**Potential Limitations and Future Research**: The framework's probabilistic claims may be challenged by cases where brittle systems endure due to contingent factors, requiring more refined models of shock sensitivity. Additionally, the metrics' interpretive nature invites research into automated brittleness detection using machine learning on bibliometric data. Cross-cultural applications could test the universality of viability landscapes, addressing whether pragmatic constraints vary across societies. Finally, integrating micro-epistemological foundations—such as how individual justification relates to macro-viability—remains underdeveloped and could strengthen the model's normative force.

These point toward productive research while indicating appropriate applications.

### **6.5 Relation to Other Externalist Approaches**

Emergent Pragmatic Coherentism shares the externalist commitment to grounding justification in factors beyond internal coherence, but it diverges from traditional externalisms by focusing on macro-level systemic viability rather than individual beliefs or processes. Unlike process reliabilism (Goldman 1979), which evaluates belief-forming processes for their tendency to produce true beliefs, Emergent Pragmatic Coherentism assesses entire knowledge networks for their demonstrated resilience against systemic costs, providing a collective, historical constraint. This macro-focus complements reliabilism by explaining why reliable processes emerge and persist in viable systems while unreliable ones are culled.

Compared to virtue epistemology (Zagzebski 1996), which emphasizes intellectual virtues like open-mindedness and intellectual courage, Emergent Pragmatic Coherentism naturalizes these virtues as pragmatic necessities for maintaining low-brittleness networks. Virtues are not innate traits but evolved responses to the selective pressures of cumulative inquiry, where dogmatic systems accumulate coercive costs and fragment. This provides a functional explanation for why virtues correlate with epistemic success, without reducing justification to individual psychology.

The framework also relates to social epistemology (Goldman 1999), extending it by modeling how collective structures evolve through pragmatic selection, not just communication. While social epistemology examines how testimony and division of labor improve individual justification, Emergent Pragmatic Coherentism adds the dimension of systemic health, showing how brittle social structures undermine even well-coordinated epistemic communities.

## **7. Defending the Model: Addressing Key Challenges**

### **7.1 Coherent Fictions and Incommensurable Paradigms**

Conspiracy theories and Kuhnian incommensurable paradigms challenge the framework differently. Conspiracy theories typically exhibit diagnostic brittleness signatures: accelerating ad-hoc modifications protecting core tenets, high maintenance costs suppressing dissent, and epistemic parasitism (rationalizing mainstream successes without generating novel research). Whether these constitute decisive refutation depends on objective measurement, which remains challenging.

Incommensurable paradigms present subtler difficulties. Direct theoretical comparison may be impossible, but structural performance indicators (complexity-to-prediction ratios, resource escalation) can supplement traditional empirical and theoretical considerations when paradigms compete within overlapping domains. This reframes some philosophical impasses as tractable empirical questions, though interpretive challenges remain significant.



### **7.2 Macro-Epistemology and Individual Justification**

As a macro-epistemology explaining long-term viability of public knowledge systems, the framework doesn't primarily solve micro-epistemological problems (Gettier cases, perceptual justification). Instead, it bridges levels through higher-order evidence: diagnosed system health provides powerful defeaters or corroborators for individual beliefs.

Formally, diagnosed brittleness determines rational priors. Low-brittleness networks (IPCC reports) warrant high priors; high-brittleness networks (denialist documentaries) warrant low priors. Following Kelly (2005), source properties matter. Even powerful first-order evidence from high-brittleness sources yields low posterior confidence due to extremely low priors. Macro-level diagnosis thus provides rational, quantitative trust allocation.

### **7.3 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

**Testable Hypothesis**: Using Seshat data, compare 50 historical systems across different domains. We predict a strong positive correlation between high composite brittleness scores (normalized measures combining C(t), P(t), M(t), R(t)) and system collapse or major restructuring within one generation post-shock (p<0.05). This could be formalized as a regression model predicting collapse probability from pre-shock brittleness indicators while controlling for shock magnitude and resource base.

**Falsifiability**: If broad, methodologically sound historical analysis revealed no significant correlation between systemic cost indicators and subsequent fragility, the theory's causal engine would be severely undermined. The framework's ultimate test lies in prospective application: diagnosing rising brittleness in current paradigms yields falsifiable predictions about statistical likelihood of supersession when facing novel challenges.

### **7.4 Power, Contingency, and Diagnostic Challenges**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth; knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions, but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: it proposes that beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date, but it becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012).

The exercise of power presents a fundamental challenge: those who benefit from brittle systems have both the means and motivation to suppress indicators of fragility. Consider how tobacco companies suppressed research on smoking's health effects for decades. The framework addresses this through three mechanisms: (1) Coercive costs eventually become visible in budgets and institutional structures; (2) Suppressed knowledge often persists in marginalized communities, creating measurable tensions; (3) Power-maintained systems show characteristic patterns of innovation stagnation. However, we acknowledge that power can delay recognition of brittleness for generations, making real-time application challenging in politically contested domains.

Marginalized perspectives (per Harding 1991) offer untapped brittleness indicators, e.g., suppressed dissent in power-maintained systems.

This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Power and oppression cases illustrate this. Slavery appeared stable to beneficiaries but exhibited objective brittleness through measurable indicators: coercive overheads (patrols, legal apparatus), chronic instability (rebellions), and opportunity costs (suppressed productivity). The exercise of power doesn't negate brittleness; coercive costs become primary diagnostic indicators (the C(t) metric). While the framework predicts statistical tendencies rather than deterministic outcomes, potential counterexamples exist where apparently brittle systems temporarily prevailed due to contingent factors. However, these cases often reveal underlying brittleness through higher long-term costs or eventual collapse.

Real-time application aims at epistemic risk management, not deterministic prediction. Retrospective analysis calibrates diagnostic tools by studying known failures' empirical signatures. These calibrated tools then inform forward-looking questions: Does exponential computational cost escalation in AI signal degeneration despite short-term performance gains? Do proliferating alignment fixes represent mounting conceptual debt? Rising brittleness indicators don't prove falsehood but signal higher-risk, potentially degenerating programs.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, are both examples. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability, therefore, requires a balance between low-cost stability and adaptive capacity.

## **8. Conclusion**

This paper develops Emergent Pragmatic Coherentism, which resolves coherentism's isolation objection by grounding coherence in demonstrated viability—measured through systemic costs. This naturalistic account of truth avoids both internalist solipsism and foundationalist implausibility.

Emergent Pragmatic Coherentism reframes truth as viability: coherence proven by resilience against systemic costs. Three key contributions: (1) Brittleness metrics providing falsifiable diagnostics (patch velocity, coercion ratios, model complexity). (2) Standing Predicates as functional units of cultural-epistemic selection. (3) The Apex Network as an emergent structure forged through historical filtering. Empirical pilots (illustrated through Ptolemaic astronomy, AI development brittleness, and historical case studies) demonstrate the framework's utility, inviting interdisciplinary testing. While the operationalization of these metrics represents a significant empirical program, even their partial application—as in the worked example of Ptolemaic astronomy—can provide valuable diagnostic insight. Future work will focus on pilot studies to refine these proxy measures.

In summary, Emergent Pragmatic Coherentism offers a robust response to coherentism's challenges, with implications for epistemology, policy, and interdisciplinary research. Future directions include refining metrics through pilot studies and exploring applications in emerging domains like AI ethics.

The ultimate arbiter is not the elegance of a theory or the consensus of its adherents, but the trail of consequences it leaves in the world. While this framework operates at a high level of abstraction, its primary data streams originate from the ground up. As we have argued, systemic costs are ultimately experienced by individuals as suffering, instability, and the frustration of human goals. Dissent, friction, and protest are therefore not merely political problems; they are primary sources of epistemological data about a system's rising brittleness. This framework, in this light, is best understood as a tool for a democratic public to hold its own knowledge-generating systems accountable. By making the indicators of systemic health part of a shared, evidence-based language, it helps us ask the most pragmatic question of all: "Is this way of thinking, this way of organizing ourselves, still working for us?"

In terms of implications for epistemology, Emergent Pragmatic Coherentism bridges the gap between coherence and correspondence by naturalizing truth as alignment with the Apex Network, offering a falsifiable program for assessing epistemic systems. Future research should focus on refining brittleness metrics through cross-domain pilot studies, exploring applications in AI and climate science, and integrating micro-epistemological foundations to strengthen normative claims.

## **Appendix A: Normative Brittleness as a Speculative Extension**

The framework's core focus is epistemic brittleness, but it suggests a parallel modality for normative systems. This extension is speculative and independent of the paper's central claims.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan’s (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring *noncausal powers*. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of *being unjust* confers on an institution the noncausal power to *justify resentment* and *require condemnation*. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and *causal* signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."


