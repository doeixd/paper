# **The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth**

## **Abstract**

Coherentist theories of justification face the isolation objection: a belief system could be perfectly coherent yet entirely detached from reality. This paper proposes Emergent Pragmatic Coherentism, grounding coherence in the demonstrated viability of knowledge systems. The framework introduces **systemic brittleness** as a diagnostic tool, measuring network health through observable costs incurred when applying propositions. We argue that mind-independent pragmatic constraints determine a **necessary structure of optimal solutions—the Apex Network**—which emerges from the topology of reality itself. Selective pressure forces knowledge systems to converge on this structure through historical filtering, a process of **discovery, not creation**. Justification thus requires both internal coherence and the certifying network's demonstrated resilience. This **naturalized proceduralism** redefines objective truth as alignment with the Apex Network and supports a **falsifiable research program** for assessing epistemic health, with preliminary applications to cases from Ptolemaic astronomy to contemporary AI.

## **1. Introduction: From a Static Web to a Dynamic Process**

Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network was demonstrably brittle: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.

This transforms the isolation objection: a coherent system detached from reality isn't truly possible because reality's constraints force convergence toward viable configurations. A perfectly coherent flat-earth cosmology generates catastrophic navigational costs. A coherent phlogiston chemistry generates accelerating conceptual debt. These aren't merely false—they're structurally unstable, misaligned with constraint topology. Failed systems reveal where the constraint landscape drops off, allowing us to map its hazards. This process is not teleological; it does not follow a lighthouse beam toward a known destination. Rather, it is the painstaking construction of a **reef chart** from the empirical data of shipwrecks. Successful systems are those that navigate the safe channels revealed by this chart of failures, triangulating toward the viable peaks that emerge necessarily from how reality is organized. The Apex Network is the structure that remains when all such unstable configurations are eliminated.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology—a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry.

To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not viable in these terms but a textbook case of high brittleness; its longevity measures the immense energy it wastes suppressing its own instability. Viability is therefore not an intrinsic property but a relational one: a system's capacity to solve problems within a given pragmatic environment with sustainably low systemic costs.

This distinction is not merely conceptual; it is empirically measurable. As we will show, the ratio of a state's resources dedicated to coercive control versus productive investment serves as a primary, quantifiable indicator of this non-viability. The framework's claim is therefore probabilistic: while a system accumulating brittleness is not fated to collapse on a specific date, it becomes progressively more vulnerable to the very contingent shocks that historians study.

However, any credible theory of knowledge evolution must account for the realities of power, path dependence, and historical contingency. This framework incorporates these factors not as exceptions but as key variables within the model. The exercise of power to maintain a brittle system is not a refutation of the model but a primary indicator of that system's non-viability, measured through its high coercive costs.

This failure-driven, adaptive process grounds a robust but fallible realism. It explains how evolving knowledge systems are forced to converge on an emergent structure of workable solutions determined by mind-independent pragmatic constraints. The claim is probabilistic, not deterministic: beneath the surface noise of contingency and power, there are underlying structural pressures at work. A system accumulating brittleness—a system's vulnerability to collapse due to the accumulation of hidden, internal costs, a concept analogous to the notion of fragility developed by Taleb (2012)—is not fated to collapse on a specific date, but it becomes progressively more vulnerable to the very contingent shocks that historians study. This model does not offer a deterministic theory of history but rather proposes the foundation for a falsifiable, probabilistic research program for understanding the structural dynamics that shape the landscape upon which historical events unfold.

To prevent misunderstanding about the framework's scope and ambitions, we must be precise about what this paper does and does not attempt. This is not a foundationalist epistemology that aims to ground all knowledge in indubitable starting points, nor is it a general theory of justification applicable to all domains of inquiry. Rather, it is a specialized framework for understanding the evolution and evaluation of cumulative knowledge systems—those engaged in ongoing, inter-generational projects where claims build upon previous work and where practical consequences provide feedback about systemic performance.

The framework applies most directly to domains like empirical science, legal systems, engineering, public policy, and mathematics, where pragmatic pushback—whether external through failed predictions or internal through proof complexity and conceptual debt—provides measurable feedback about systemic performance. We present this focus not as a limitation but as a demonstration of the framework's power to unify apparently disparate domains under a single selective mechanism.

The framework's contribution is best understood as a form of naturalized proceduralism. While sharing the proceduralist commitment to grounding objectivity in process rather than direct correspondence, it diverges sharply from rationalist accounts. Where they locate objectivity in the idealized norms of discourse, our model grounds it in the empirical, historical process of pragmatic selection. The ultimate arbiter is therefore not the internal coherence of our reasons, but the measurable, non-discursive brittleness of the systems our reasons produce. Our arguments are thus continuously disciplined not merely by other arguments, but by the non-negotiable data of systemic success and failure.

**The argument proceeds as a systematic construction in seven steps.**

- Section 2 introduces the diagnostic framework for assessing systemic viability through brittleness metrics and establishes operational protocols for managing circularity.
- Section 3 develops the Negative Canon as the empirically grounded boundary that disciplines inquiry.
- Section 4 presents the Apex Network as an emergent necessary structure determined by pragmatic constraints, articulating the modal claims that ground naturalistic objectivity.
- Section 5 explains how this evolutionary process animates Quine's web of belief with directed learning mechanisms.
- Section 6 situates Systemic Externalism within contemporary epistemology, demonstrating how mathematics serves as a paradigm case of internal brittleness.
- Section 7 defends the framework against key objections while honestly acknowledging scope limitations and bullets bitten.

## **2. The Core Concepts: Units of Epistemic Selection**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

`\[FIGURE 1: Conceptual Flowchart for the Deflationary Path]`  
`\[Belief (Private State)] --> \[Articulation into a Proposition (Public Claim)] --> \[Coherence Test] --> \[Integration as Validated Data OR Promotion to Standing Predicate (Public Tool)]`

**Belief → Proposition → Validated Data → Standing Predicate**

#### **2.1.1 From Private Belief to Public Proposition**

The journey begins with *belief*, the raw material of epistemology. As a private psychological state tied to an individual's consciousness, it is analytically inaccessible for a theory of public knowledge. The first step is therefore to isolate its testable, public content as a *proposition*. A proposition is a falsifiable claim that can be articulated, communicated, and collectively assessed.

#### **2.1.2 The Coherence Test**

Next, a candidate proposition must pass a rigorous test for coherence. This is not the thin, formal consistency of logic, but a thick, forward-looking pragmatic assessment—a form of risk analysis. A shared network, as a resource-constrained system, implicitly asks: will integrating this proposition increase or decrease our long-term systemic brittleness? A proposition that successfully passes this test is accepted into the network as validated data—a reliable claim that can be used and cited within the system, awaiting its ultimate functional test.

#### **2.1.3 From Validated Data to Standing Predicate**

Finally, propositions that not only pass the coherence test but do so with exceptional success—by dramatically reducing a network's systemic costs—undergo a profound status change. They are not merely stored as facts; their functional core is promoted and repurposed to become part of the network's core processing architecture.

This process creates what we call a **Standing Predicate**: the reusable, action-guiding conceptual tool within a proposition that has earned a durable, trusted status. It is the functional "gene" of cultural evolution. For example, once the proposition "Cholera is an infectious disease" proved its immense pragmatic value, its functional component—the predicate `...is an infectious disease`—was promoted. It became a Standing Predicate in the network of medical science.

This Standing Predicate now functions as a durable piece of conceptual technology. Unlike a static causal claim (e.g., 'X causes Y'), a Standing Predicate is a dynamic, action-guiding tool that unpacks a suite of proven interventions and inferential heuristics. Applying it to a new phenomenon activates a rich sub-network of proven diagnostic heuristics, interventional policies, and licensed inferences. The original proposition has transitioned from *being-tested* data to a *tool-that-tests*. This promotion from data to a trusted, standing tool is the first and most crucial step in a network's ability to learn and upgrade its own architecture.

### **2.2 The Units of Analysis: Predicates, Networks, and Replicators**

Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

* **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition (e.g., `...is an infectious disease`). Functioning as the generative "genes" of cultural evolution, a Standing Predicate is a highly compressed piece of conceptual technology. When applied, it unpacks a suite of previously validated knowledge, including causal models, diagnostic heuristics, and licensed interventions.
* **Shared Network:** This concept is not a novel theoretical entity but an observable consequence of Quine's holism applied to social groups. A Shared Network is the emergent, public architecture formed by the coherent subset of propositions and predicates that must be shared across many individual webs of belief for agents to solve problems collectively. These networks are often nested: a specialized network like germ theory forms a coherent subset of propositions within the broader network of modern medicine, which itself must align with the predicates of empirical science. The emergence of these networks is not a conscious negotiation but a structural necessity. An individual craftsperson whose canoe capsizes will holistically revise their personal web of belief about hydrodynamics; when a group must build a fleet, only the shared principles that lead to non-capsizing canoes can become part of the public, transmissible craft. The Shared Network is the public residue of countless such private, failure-driven revisions under shared pragmatic pressure.

To be precise about this evolutionary dynamic, we can adopt a distinction from generalized evolutionary theory, as synthesized for the social sciences by Mesoudi (2011). The network's abstract informational structure—its core Standing Predicates and their relations—functions as the **replicator**: the "code" that is copied and transmitted. The social group and its institutions (the scientific community, legal system, or society) function as the **interactor**: the physical "vessel" through which this informational code is expressed, applied, and tested against real-world consequences. This distinction explains how knowledge can persist even when the societies that created it do not; the rediscovery of Roman legal principles during the Renaissance is a prime example of a successful replicator outliving its original interactor. The existence of these countless, independently formed and often nested Shared Networks—all responding to the same landscape of physical and social constraints—supports a crucial inference: they collectively reveal an objective structure that underwrites all successful inquiry. This mosaic of survived predicates, biased toward low-brittleness solutions, anticipates the Apex Network: a constraint-determined structure of necessary optimal solutions that emerges from and is revealed by historical filtering (as detailed in Section 4).

### **2.3 Pragmatic Pushback and Systemic Costs**

A shared network is not a passive library but an active system under constant pressure from pragmatic pushback: our model's term for the systemic analogue of what Quine called a "recalcitrant experience" at the periphery of an individual's web of belief. It is the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback is not an argument but a material outcome: a bridge collapses, a treatment fails, a society fragments. This process generates two types of costs.

First-Order Costs are the direct, material consequences: failed predictions, wasted resources, environmental degradation, or systemic instability (e.g., excess mortality). These are the objective signals of dysfunction. Systemic Costs are the secondary, internal costs a network incurs to manage, suppress, or explain away its first-order costs. These non-productive expenditures reveal a network's true fragility. Key forms include:

* **Conceptual Debt:** The compounding fragility incurred by adopting flawed, complex patches to protect a core principle.
* **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing dissent. These coercive overheads are the primary mechanism by which power dynamics manifest within our model: the resources spent to maintain a brittle system against internal and external pressures become a direct, measurable indicator of its non-viability. Dissent, in this model, is a critical data stream signaling that a system is generating costs for its members.

Pragmatic pushback is not limited to direct, material failures. In highly abstract domains like theoretical physics or pure mathematics, where direct empirical tests are deferred or unavailable, pushback primarily manifests through the accumulation of Systemic Costs. A research program that requires an accelerating rate of ad hoc modifications to maintain internal consistency, or that loses its unifying power, is experiencing a powerful form of pragmatic pushback. These epistemic inefficiencies are real costs that render a network brittle and unproductive, even if it has not been directly falsified by experiment. The framework's diagnostic lens thus applies to all forms of inquiry, measuring viability through either external, material consequences or internal, systemic dysfunction.

To operationalize these concepts, we introduce a set of diagnostic indicators for tracking brittleness over time:

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |
| **P(t)** | Conceptual Debt | Ratio of anomaly-resolution publications to novel-prediction publications |
| **C(t)** | Coercive Overhead | Ratio of security/suppression budget to productive/R&D budget |
| **M(t)** | Model Complexity | Rate of parameter/complexity growth vs. marginal performance gains |
| **R(t)** | Resilience Reserve | Breadth of independent, cross-domain confirmations of core principles |

To illustrate this toolkit in action, consider two brief examples. **Case 1: Ptolemaic Astronomy (c. 1500 CE).** The system exhibited high and rising brittleness. **M(t)** was acute, with model complexity escalating through added epicycles and equants to maintain predictive accuracy. **P(t)** was also high, as anomaly-resolution papers (e.g., explaining retrograde motion) outnumbered novel predictions. **C(t)** manifested in the intellectual suppression of heliocentric alternatives, delaying convergence. **R(t)** was low, with confirmations limited to narrow astronomical domains without broader integration.

**Case 2: Contemporary AI Development.** Current deep learning paradigms may be showing early signs of rising brittleness. **M(t)** is visible in the exponential escalation of parameter counts (e.g., from GPT-2's 1.5B to GPT-4's estimated 1.7T) for diminishing returns in generalization. **P(t)** can be proxied by the proliferation of 'alignment' and 'safety' research, which often functions as ad hoc patches for ethical and reliability anomalies rather than core architectural advances. **C(t)** appears in the high computational and energy costs required for training, diverting resources from fundamental research. **R(t)** is moderate but potentially declining, as models excel in narrow tasks but struggle with cross-domain transfer.



## **3. The Methodology of Brittleness Assessment**

### **3.1 The Challenge of Objectivity: Preempting the Circularity Objection**

We face a fundamental circularity problem in operationalizing brittleness: measuring systemic costs objectively requires neutral standards for "waste" or "dysfunction," yet establishing such standards appears to require the very epistemic framework our theory aims to provide.

This circularity cannot be eliminated but can be managed through several strategies. First, we anchor measurements in basic biological and physical constraints: demographic collapse, resource depletion, infrastructure failure. These provide relatively theory-neutral indicators of breakdown. Second, we employ comparative rather than absolute measures, comparing brittleness trajectories across similar systems. Third, we require convergent evidence across multiple independent indicators before diagnosing brittleness.

Brittleness assessment remains partially hermeneutic. The framework provides structured tools rather than mechanical algorithms, making judgments more systematic and accountable to evidence without eliminating interpretation. This constrains the framework's ambitions: it offers "structured fallibilism" rather than neutral assessment.

This methodology provides pragmatic objectivity sufficient for comparative assessment. **A detailed protocol for operationalizing these metrics, including a worked example of inter-rater reliability, is provided in Appendix A.**

### **3.2 A Tiered Diagnostic Framework for Cost Objectivity**

To clarify how objective cost assessment is possible without appealing to contested values, we can organize brittleness indicators into a tiered diagnostic framework, moving from the most foundational and least contestable to the more domain-specific.

* **Tier 1: Foundational Bio-Social Costs:** At the most fundamental level are the direct, material consequences of a network's misalignment with the conditions for its own persistence. These are not abstract values but objective bio-demographic facts, measurable through historical and bioarchaeological data. They include:
  * Excess mortality and morbidity rates (relative to contemporaneous peers with similar constraints)
  * Widespread malnutrition and resource depletion
  * Demographic collapse or unsustainable fertility patterns
  * Chronic physical suffering and injury rates

* **Tier 2: Systemic Costs of Internal Friction:** The second tier measures the non-productive resources a system must expend on internal control rather than productive adaptation. These systemic costs are often directly quantifiable:
  * **The Coercion Ratio (C(t)):** In socio-political networks, this can be measured by analyzing the ratio of a state's resources allocated to internal security and suppression versus resources for public health, infrastructure, and R&D.
  * **Information Suppression Costs:** Resources dedicated to censorship or documented suppression of minority viewpoints, and resulting innovation lags when compared to more open rival systems.

* **Tier 3: Domain-Specific Epistemic Costs:** The third tier addresses more abstract domains like theoretical science where "costs" are defined by the constitutive goals of the domain itself. They manifest not as mortality but as crippling inefficiency:
  * **Patch Velocity (P(t)):** The rate at which non-generative auxiliary hypotheses must be produced to protect core theory (measurable through literature analysis).
  * **Model Complexity Inflation (M(t)):** Theories requiring constant addition of free parameters to accommodate existing data without increasing novel predictive power (quantifiable through parameter-to-prediction ratios).
  * **Proof Complexity Escalation:** In mathematics, increasing proof length without proportional explanatory gain.

### **3.3 The Triangulation Method: A Protocol for Constrained Interpretation**

No single indicator is immune to interpretive bias. Therefore, a robust diagnosis of brittleness requires **triangulation across independent baselines**. This protocol provides a concrete method for achieving pragmatic objectivity.

* **Baseline 1: Comparative-Historical Analysis:** We compare a system's metrics against contemporaneous peers with similar technological, resource, and environmental constraints. For example, 17th-century France exhibited higher excess mortality from famine than England, not because of worse climate, but because of a more brittle political-economic system that hindered food distribution. The baseline is what was demonstrably achievable at the time.

* **Baseline 2: Diachronic Trajectory Analysis:** We measure the direction and rate of change within a single system over time. A society where life expectancy is falling, or a research program where the ratio of ad-hoc patches to novel predictions is rising, is exhibiting increasing brittleness regardless of its performance relative to others.

* **Baseline 3: Biological Viability Thresholds:** Some thresholds are determined by non-negotiable biological facts. A society with a Total Fertility Rate sustainably below 2.1 is, by definition, demographically unviable without immigration. A system generating chronic malnutrition in over 40% of its population is pushing against fundamental biological limits.

**A diagnosis of high brittleness is only made when these independent methods converge.** For instance, a system is robustly diagnosed as brittle if it shows rising mortality over time (Baseline 2), performs worse than its peers (Baseline 1), and approaches biological stress thresholds (Baseline 3). This convergence makes the diagnosis difficult to dismiss as mere theoretical bias.

### **3.4 A Protocol for Constrained Interpretation**

Critics object that classifying spending as "productive" vs. "coercive" requires prior normative commitments, making the framework circular. The causal hierarchy provides an operational solution through trajectory analysis rather than categorical definition.

**The Operational Protocol:**

**Step 1: Measurement Without Classification**
Track resource allocation over time without labeling it:
- Proportion to internal security/surveillance/enforcement (S)
- Proportion to infrastructure/health/education/R&D (P)
- Total resource base (R)

**Step 2: Correlate With First-Order Indicators**
Measure demographic and economic trajectories:
- Mortality rates (rising/stable/falling)
- Morbidity indicators
- Economic output per capita
- Innovation metrics (patents, new technologies, productivity gains)
- Population stability

**Step 3: Apply Diagnostic Rules**

A spending category functions as coercive overhead when increasing allocation correlates with rising First-Order Costs, the system requires accelerating investment to maintain baseline stability (diminishing returns), or reduction correlates with improved outcomes.

A spending category functions as productive investment when increasing allocation correlates with falling First-Order Costs, returns are constant or increasing, and it generates positive spillovers to other domains.

**Concrete Example: Criminal Justice Spending**

Society A doubles police budget (year 1: 2% GDP → year 10: 4% GDP):
- Crime rates: -40%
- Incarceration rate: -20%
- Homicide rate: -60%
- Community survey trust: +35%
- Recidivism: -25%
**Diagnosis:** Productive investment. Rising S correlates with falling First-Order Costs.

Society B doubles police budget (year 1: 2% GDP → year 10: 4% GDP):
- Crime rates: +5%
- Incarceration rate: +300%
- Homicide rate: -10%
- Community survey trust: -50%
- Social instability indicators: +60%
- By year 10, requires 6% GDP to maintain control
**Diagnosis:** Coercive overhead. Rising S correlates with rising total systemic costs despite some metrics improving.

**Why This Isn't Circular:**

The classification emerges from empirical correlation patterns, not from a priori definitions. We don't need to know "what policing essentially is" before measuring; we observe what specific spending patterns do to measurable outcomes over time.

The robustness comes from convergent evidence. A single metric (e.g., crime rate) can be ambiguous, but when demographic indicators, economic output, innovation rates, stability metrics, and coercive spending ratios all move in the same direction, the diagnosis becomes robust to interpretive variation.

This is standard scientific methodology: identifying causal patterns through correlation across independent measurement streams, not through defining essences.

### **3.3 The Triangulation Method**

The "excess mortality" indicator faces an apparent circularity: excess relative to what baseline? The framework resolves this through triangulation across three independent baseline types whose convergence provides robust diagnosis.

**Baseline Type 1: Comparative-Historical**

Compare a system's metrics against contemporaneous peers with similar:
- Technology levels (agricultural, medical, industrial)
- Resource endowments (arable land, climate, minerals)
- Disease environments (endemic pathogens, epidemic exposure)
- Population density and urbanization rates

**Example:** 17th century Europe
- Society A: 45% child mortality (age 0-5)
- Society B: 28% child mortality
- Same technology level, similar climate, comparable urbanization
- **Diagnosis:** Society A exhibits excess mortality relative to demonstrated pragmatic constraints of the era

### **3.4 Illustrating the Triangulation Method: Three Baselines**

This baseline is theory-neutral regarding "optimal" mortality. It measures: what's achievable given constraint set X? Systems failing to achieve demonstrated performance under comparable constraints exhibit excess costs.

**Baseline Type 2: Trajectory Analysis**

Measure direction and rate of change:
- Rising mortality = degrading system
- Stable mortality = maintaining system
- Falling mortality = improving system

**Example:** Two societies, Year 1 to Year 50
- Society C: child mortality 40% → 35% → 30% (improving)
- Society D: child mortality 30% → 35% → 42% (degrading)
- **Diagnosis:** D exhibits rising brittleness despite better initial conditions

This baseline requires no absolute standard. Relative trajectories under comparable external conditions (no plague, no war, no famine) reveal system health without defining "optimal."

**Baseline Type 3: Biological Viability Thresholds**

Some thresholds are determined by reproductive biology, not social norms:
- Total Fertility Rate < 2.1 = population decline (absent immigration)
- Infant mortality > 400 per 1000 = demographic crisis (population can't replace itself)
- Life expectancy < 25 years = structural reproduction crisis
- Chronic malnutrition > 40% = biological stress threshold

These aren't normative claims about what mortality "should" be—they're structural facts about what mortality rates are compatible with population reproduction.

**Convergent Evidence Protocol:**

Don't rely on a single baseline. Assess brittleness through convergence:

Society X shows:
- Worse mortality than peers with similar constraints? (+1)
- Deteriorating trajectory over time? (+1)
- Below biological viability thresholds? (+1)
- High coercive spending correlated with mortality management? (+1)
- Innovation rates declining? (+1)

Brittleness Score: 5/5 = high confidence diagnosis
Brittleness Score: 1/5 = insufficient evidence, requires investigation

This methodology parallels how climate science establishes anthropogenic warming: no single measurement type is definitive, but convergence across ice cores, tree rings, direct temperature records, sea level rise, and atmospheric composition provides robust diagnosis despite measurement uncertainties in each stream.

The framework achieves pragmatic objectivity: sufficient for comparative assessment and institutional evaluation without requiring view-from-nowhere neutrality or theory-neutral measurement.

### **3.5 A Tiered Diagnostic Framework for Cost-Shifting**

To clarify how objective cost assessment is possible without appealing to contested values, we can organize brittleness indicators into a tiered diagnostic framework, moving from the most foundational and least contestable to the more domain-specific.

**Tier 1: Foundational Bio-Social Costs**

At the most fundamental level are the direct, material consequences of a network's misalignment with the conditions for its own persistence. These are not abstract values but objective bio-demographic facts, measurable through historical and bioarchaeological data. They include:
- Excess mortality and morbidity rates (relative to contemporaneous peers with similar constraints)
- Widespread malnutrition and resource depletion
- Demographic collapse or unsustainable fertility patterns
- Chronic physical suffering and injury rates

A system that generates higher death or disease rates than viable alternatives under comparable constraints is incurring a measurable, non-negotiable first-order cost. These metrics are grounded in biological facts about human survival and reproduction, not in contested normative frameworks.

**Tier 2: Systemic Costs of Internal Friction**

The second tier measures the non-productive resources a system must expend on internal control rather than productive adaptation. These are the energetic and informational prices a network pays to manage the dissent and dysfunction generated by its Tier 1 costs. These systemic costs are often directly quantifiable:

- **The Coercion Ratio:** In socio-political networks, this can be measured by analyzing the ratio of a state's resources allocated to internal security and suppression versus resources for public health, infrastructure, and R&D
- **Information Suppression Costs:** Resources dedicated to censorship or documented suppression of minority viewpoints, and resulting innovation lags when compared to more open rival systems
- **Enforcement Escalation:** Rising expenditure required to maintain baseline stability, indicating diminishing returns and structural fragility

**Tier 3: Domain-Specific Epistemic Costs**

The third tier addresses more abstract domains like theoretical science where "costs" are defined by the constitutive goals of the domain itself. They manifest not as mortality but as crippling inefficiency, measured by brittleness indicators:

- **Accelerating Ad-Hoc Modification:** In scientific paradigms, the rate at which non-generative auxiliary hypotheses must be produced to protect core theory (measurable through literature analysis)
- **Increased Model Complexity:** Theories requiring constant addition of free parameters to accommodate existing data without increasing novel predictive power (quantifiable through parameter-to-prediction ratios)
- **Proof Complexity Escalation:** In mathematics, increasing proof length without proportional explanatory gain
- **Conceptual Debt Accumulation:** Rising cognitive load and descriptive inelegance without compensating unifying power

While the interpretation of these costs is a normative matter for agents within a system, their existence and magnitude are empirical questions. The framework's core causal claim is falsifiable and descriptive: a network with high or rising brittleness across these tiers carries a statistically higher probability of systemic failure or major revision when faced with external shocks.

**The Diagnostic Power of Cost-Shifting**

This tiered framework reveals a crucial diagnostic pattern: cost-shifting. A system might prove highly efficient at the epistemic level (Tier 3), generating immense technological progress, while simultaneously generating catastrophic bio-social costs (Tier 1), such as environmental degradation or demographic stress.

The framework does not offer a simple formula for aggregating these heterogeneous costs. Instead, the tension itself is the diagnostic signal. A system that systematically optimizes for one type of cost by exporting massive costs to another domain is exhibiting hidden, long-term brittleness. Such a system is not holistically viable but merely defers costs onto its social or ecological substrate, making it vulnerable to collapse when those substrates can no longer absorb the burden.

The goal of diagnosis is not to produce a single brittleness score but to identify these unsustainable trade-offs and rising trajectories of risk across all three tiers.

## **4. The Emergent Structure of Objectivity**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We will show how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

Our account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is often of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system—its descent into crippling inefficiency, intellectual stagnation, and institutional decay—provides a clear, non-negotiable data point.

The systematic analysis of these failures allows us to build the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been empirically invalidated by the catastrophic systemic costs they reliably generate. This canon charts failures of both causal and normative alignment:

* **Failures of Causal Alignment** are characteristic of what we term *Epistemic Brittleness*. Systems like scholastic physics, phlogiston chemistry, and Lysenkoist biology entered the canon because their core principles generated catastrophic causal failures, leading to chronically high rates of ad-hoc modification and predictive collapse. They represent failed blueprints for the causal world.
* **Failures of Normative Alignment** are characteristic of *Normative Brittleness*. Systems like chattel slavery or totalitarianism entered the canon because their core principles violated emergent normative facts about human cooperation. Their unsustainability was demonstrated by the immense and ever-rising coercive overheads required to suppress the normative pushback they generated. These are not condemned by a modern moral judgment alone; they are failed engineering principles for a viable social world.

By charting what demonstrably fails, whether in engineering or social organization, we are not merely learning what to avoid; we are effectively reverse-engineering the constraints of a real territory. The Negative Canon functions like a reef chart for inquiry. It is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and prevents a collapse into relativism.

This process must be understood as eliminative, not teleological. It does not follow a lighthouse beam pulling inquiry toward a known destination. Instead, it is the painstaking construction of a **reef chart**. The Negative Canon is the record of shipwrecks—the empirical data of what has failed. Our knowledge of the safe channel (the Apex Network) is built retrospectively by mapping the hazards. Progress is measured not by our proximity to a distant light, but by the growing accuracy of our map of what to avoid.

### **4.2 The Apex Network: An Emergent Structure of Modal Necessity**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

The mechanism that forges this structure is a bottom-up emergence driven by the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question.

The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge.

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state; they simply try to solve local problems and reduce costs. The Apex Network is the name we give to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

The Apex Network's status is therefore dual, a distinction critical to our fallibilist realism. Ontologically, it is real: it is the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, however, it remains a regulative ideal. We can never achieve a final, God's-eye view to confirm that our Consensus Network is a perfect map of it; our knowledge of the structure is necessarily incomplete and fallible. Its existence is what grounds our realism and prevents a collapse into relativism, while our epistemic limitations are what make the project of inquiry a permanent and progressive one. It is the necessary concept that makes our comparative judgments of "more" or "less" brittle meaningful.

Thus, the Apex Network should not be misconstrued as a single, final theory of everything. Rather, it is the complete set of maximally viable configurations—a high-altitude plateau on the fitness landscape. While some domains may have a single sharp peak, others may permit a constrained pluralism of equally low-brittleness systems. The convergence is *away from the vast valleys of failure* documented in the Negative Canon, and *toward this resilient plateau* of viable solutions.

#### **4.2.1 The Necessity Argument**

**Premise 1: Reality Imposes Non-Negotiable Constraints**

Physical laws (thermodynamics, resource scarcity, causation), biological facts (human needs, mortality, development timescales, pain), logical requirements (non-contradiction, inference rules), and coordination necessities (collective action problems, information asymmetries, trust requirements).

These constraints are mind-independent. They exist whether we recognize them or not. A society can believe food is optional, but starvation will constrain them regardless.

**Premise 2: Constraints Generate a Fitness Landscape**

These constraints form a topology of possible configurations. This topology necessarily generates peaks and valleys: some paths through configuration space minimize energy expenditure and brittleness (viable, sustainable), while others lead to unsustainable cost accumulation (catastrophic, collapse-prone).

This landscape emerges from constraint interaction with objective structure. Just as physical constraints generate the lowest-energy molecular configurations, pragmatic constraints generate optimal social/epistemic configurations. The structure isn't imposed from outside—it emerges necessarily from how the constraints are organized.

**Premise 3: Optimal Solutions Emerge Necessarily**

Given a constrained system, optimal solutions (or a compact set of them) emerge necessarily from the constraint topology, existing whether anyone has calculated them. The shortest path between two points emerges from geometric constraints whether anyone has walked it. The most efficient heat engine design emerges from thermodynamic constraints whether anyone has built it. These aren't constructed—they emerge as factual consequences of how the system is structured.

**Premise 4: The Apex Network IS That Optimal Structure**

The Apex Network is the complete configuration space of maximally viable solutions to the constraint problem reality poses. It is:
- The set of predicates that minimize brittleness
- The relational structure that optimally navigates constraints
- The "peaks" in the fitness landscape

**Conclusion: The Apex Network Emerges Necessarily, Independent of Discovery**

Just as π emerges necessarily from geometric constraints whether we've calculated it, and the lowest-energy molecular configuration emerges necessarily from quantum mechanics whether we've modeled it, the Apex Network emerges necessarily from pragmatic constraints whether we've discovered it. The structure isn't constructed by inquiry—it emerges as an objective fact from how reality's constraints are organized. Discovery reveals this emergent structure; it does not create it.

In short, the Apex Network is not constructed by inquiry but emerges as an objective, structural fact from the problem-space that reality's constraints impose. Inquiry discovers this structure; it does not invent it.

#### **4.2.2 Discovery vs. Creation: Resolving the Ambiguity**

Historical filtering is the **discovery process**, not the creation mechanism.

**What History Does:**
- Runs experiments exploring configuration space
- Reveals where the landscape drops off (failed systems)
- Triangulates toward peaks (successful systems)
- Maps the constraint topology through trial and error

**What History Doesn't Do:**
- Create the constraint structure (constraints are mind-independent)
- Determine which solutions are optimal (determined by constraints)
- Make the Apex Network exist (it exists necessarily given constraints)

**Analogy:** Mathematical Discovery

Mathematicians discovered π through measurement and calculation. Different cultures discovered it independently (Babylonians, Egyptians, Greeks, Chinese, Indians). The discovery process was contingent (which civilization, which method, when). But π itself is necessary (determined by geometric constraints). Its value would be discovered in any sufficiently sophisticated mathematical tradition.

**Parallel:** Moral/Epistemic Discovery

Societies discover low-brittleness predicates through experimentation. Different cultures discover similar principles independently (reciprocity norms, property rights, harm prohibitions). The discovery process is contingent (which society, which crisis, when). But the Apex Network is necessary (determined by pragmatic constraints). Its structure would be discovered in any sufficiently long-running civilization.

#### **4.2.3 Counterfactual Stability**

If human history had unfolded completely differently:
- Different societies running different experiments
- Different specific failures populating the Negative Canon
- Different paths of discovery
- Different terminology and conceptual frameworks

**The Apex Network structure would remain the same** because it's determined by constraints, not by historical path.

**Evidence for this claim:**
- Independent emergence of similar low-brittleness principles across isolated cultures
- Convergent evolution toward comparable solutions when facing similar problems
- Failed systems fail in structurally similar ways (high coercive costs, demographic stress, innovation suppression)
- Mathematical convergence: different traditions discover same theorems

This counterfactual stability is what makes the Apex Network an objective standard, not a historical artifact.

#### **4.2.4 Ontological Clarification: Structural Realism**

**The Apex Network is:**
- **Real:** Exists as objective structure that emerges from mind-independent constraints
- **Structural:** Consists of relational patterns, not substances or Platonic forms
- **Emergent:** Arises necessarily from constraint topology—not constructed by inquiry, not reducible to individual constraints, but emerging from their interaction as systems seek lowest-cost paths through constraint space
- **Necessary:** Would emerge in any world with similar constraint topology
- **Discoverable:** Accessible through empirical investigation of what minimizes brittleness over time

**The Apex Network is NOT:**
- A Platonic form in a separate realm
- A historical accident determined by which societies survived
- A social construction or consensus
- A teleological endpoint toward which history progresses
- A complete, final theory we could ever fully possess

**Formal Characterization:**

A = ∩{W_k | V(W_k) = 1}

Where:
- A = Apex Network
- W_k = possible world-systems (configurations of predicates)
- V(W_k) = viability function (determined by brittleness metrics)
- ∩ = intersection (common structure across all viable systems)

The intersection of all maximally viable configurations reveals their shared structure. This shared structure is what survives all possible variations in historical path—it's the emergent, constraint-determined necessity that arises from how reality is organized.

**This resolves the isolation objection:** A coherent system detached from the Apex Network isn't merely false—it's structurally unstable. It will generate rising brittleness until it either adapts toward the Apex Network or collapses. Coherence alone is insufficient because reality's constraints force convergence.

#### **4.2.5 Epistemological Status: Regulative Ideal**

Ontologically real, epistemically regulative.

**We can never know with certainty** that our current Consensus Network perfectly maps the Apex Network. Our knowledge is:
- Fallible (any specific claim might be wrong)
- Incomplete (Apex Network likely contains structure we haven't discovered)
- Approximative (our map has granularity limits)

**Yet we can know with high confidence:**
- Some configurations are definitely not in the Apex Network (the Negative Canon)
- We're progressively better approximating it (reducing brittleness over time)
- Some principles are nearly certain to be correct (independently discovered across cultures, survived exhaustive testing)

**The regulative function:** The Apex Network is the objective standard that makes our comparative judgments meaningful. When we say System A is "more aligned with truth" than System B, we mean: A better approximates the Apex Network structure as evidenced by lower brittleness across convergent metrics.

This combination—ontologically real, epistemically ideal—enables robust fallibilist realism. We can be realists about moral/epistemic truth without claiming infallibility or final knowledge.

#### **4.2.6 Cross-Domain Convergence and Pluralism**

The mechanism that drives discovery of this structure is bottom-up emergence through the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question.

The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge.

`[FIGURE 2: Conceptual Sketch of a Fitness Landscape]`  
`[A 3D landscape with "Viability" on the Y-axis. Peaks are labeled with successful systems like "Germ Theory" and "Relativity." Deep valleys and low plateaus are labeled with failed systems from the Negative Canon, such as "Ptolemaic System," "Lysenkoism," and the "Ming Dynasty Trap."]`

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state but simply try to solve local problems and reduce costs. The Apex Network is the name we give to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
* **Level 1: Objective Truth.** The ultimate, regulative ideal of the process. A proposition is objectively true if its principles are part of the real, emergent Apex Network: the objective structure of viable solutions. While this structure is never fully mapped, it functions as the formal standard that makes our comparative judgments of "more" or "less" brittle meaningful. It is the structure toward which the reduction of systemic costs forces our knowledge systems to converge.

This layered framework avoids a simplistic "Whig history" by recognizing that Justified Truth is a historically-situated achievement. Newtonian mechanics earned its Level 2 status by being a maximally low-brittleness system for its problem-space for over two centuries. Its replacement by relativity does not retroactively invalidate that status but shows the evolutionary process at work, where an expanding problem-space revealed pragmatic constraints that required a new, more viable system. This allows for sharp, non-anachronistic historical judgments: a claim can be justifiably true in its time (Level 2) yet still be objectively false (not Level 1) when judged against the Apex Network from the perspective of a more resilient successor.

### **4.3.1 How Propositions Become Truth Itself: Deflationism and the Hard Core**

The three-level framework reveals a profound insight about the nature of truth: propositions do not merely "correspond" to truth as an external standard but become constitutive of truth itself through a process of functional transformation and entrenchment.

**The Deflationary Insight**

Our framework aligns with deflationary theories of truth in a crucial respect: there is no substantial metaphysical property "truth" that exists independently of the propositions we validate. When we say "the proposition P is true," we are not attributing some mysterious non-natural property to P. Rather, we are acknowledging P's status within a well-functioning epistemic system.

However, our framework is not purely deflationary. It provides robust, naturalistic content to truth-attributions through the three-level structure:
- To say P is true (Level 2) is to say P is certified by a low-brittleness Consensus Network
- To say P is objectively true (Level 1) is to say P aligns with the emergent, constraint-determined Apex Network

This transforms deflationism from a purely linguistic doctrine into a substantive epistemological claim: truth is what survives systematic pragmatic filtering. The predicate "is true" tracks functional role within viable knowledge systems, not correspondence to a Platonic realm.

**From Validated Data to Constitutive Core: The Progression**

A proposition's journey to becoming truth itself follows a systematic progression through functional transformation:

1. **Initial Hypothesis (Being-Tested):** The proposition begins as a tentative claim within some Shared Network, subject to coherence constraints and empirical testing. It is data to be evaluated.

2. **Validated Data (Locally Proven):** Through repeated application without generating significant brittleness, the proposition earns trust. Its predictions are confirmed; its applications succeed. It transitions from hypothesis to validated data—something the network can build upon.

3. **Standing Predicate (Tool-That-Tests):** The proposition's functional core—its reusable predicate—is promoted to Standing Predicate status. It becomes conceptual technology: a tool for evaluating new phenomena rather than something being evaluated. "...is an infectious disease" becomes a diagnostic standard, not a claim under test.

4. **Convergent Core Entry (Functionally Unrevisable):** As all rival formulations are relegated to the Negative Canon after generating catastrophic costs, the proposition migrates to the Convergent Core. Here it achieves Level 2 status: Justified Truth. To doubt it now is to doubt the entire system's demonstrated viability.

5. **Hard Core (Constitutive of Inquiry Itself):** In the most extreme cases, a proposition becomes so deeply entrenched that it functions as a constitutive condition for inquiry within its domain. This is Quine's hard core—the principles so fundamental that their removal would collapse the entire edifice.

**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:

- **Logic and Basic Mathematics:** Revising logic requires using logic to evaluate the revision (infinite regress). Revising basic arithmetic requires abandoning the conceptual tools needed to track resources, measure consequences, or conduct any systematic inquiry. These exhibit maximal brittleness-if-removed.

- **Thermodynamics:** The laws of thermodynamics undergird all engineering, chemistry, and energy policy. Revising them would invalidate centuries of validated applications and require reconstructing vast swaths of applied knowledge. The brittleness cost is astronomical.

- **Germ Theory:** After decades of successful interventions, public health infrastructure, medical training, and pharmaceutical development all presuppose germ theory's core claims. Revision would collapse these systems, generating catastrophic first-order costs.

**The Paradox Resolved: Fallibilism Without Relativism**

This creates an apparent paradox: how can we be fallibilists who acknowledge all claims are revisable in principle, while simultaneously treating hard core propositions as effectively unrevisable in practice?

The resolution lies in recognizing that "revisable in principle" means: *if we encountered sufficient pragmatic pushback, we would revise even hard core claims*. The qualification "sufficient" is doing crucial work. For hard core propositions, the threshold is extraordinarily high—perhaps requiring a wholesale breakdown of the systems they support. But it is not infinitely high.

This is precisely what makes the framework naturalistic rather than foundationalist. Hard core status is not metaphysical bedrock but functional achievement. A proposition becomes "truth itself" not by corresponding to eternal forms but by proving so indispensable to viable inquiry that its removal would be catastrophically costly.

**Truth as Emergent, Achieved Status**

On our account, truth is not discovered in a Platonic realm but achieved through historical filtering. Propositions become true by:
1. Surviving systematic application without generating brittleness
2. Migrating from peripheral hypotheses to core infrastructure
3. Becoming functionally indispensable to ongoing inquiry
4. Aligning with the emergent, constraint-determined Apex Network

This is why our framework can embrace both deflationary insights (truth is not a substantial property) and robust realism (some propositions align with objective constraint-determined structure). The predicate "is true" tracks functional role in viable systems, but viability itself is determined by mind-independent pragmatic constraints.

When we say "thermodynamics is true," we are not making a claim about correspondence to metaphysical facts. We are acknowledging that thermodynamics has survived exhaustive testing, migrated to the hard core of physics, and aligned with the constraint-determined structure that any viable physics must approximate. Its truth is its achieved, demonstrated indispensability within maximally viable inquiry.

This resolves the classical tension between Quine's holism (all claims are revisable) and the practical unrevisability of core principles: both are true because they describe different aspects of the same evolutionary process through which propositions earn their status by proving their viability under relentless pragmatic pressure.

### **4.4 The Evolving Structure of Knowledge: Convergent Core and Pluralist Frontier**

The historical process of pragmatic filtering gives our evolving Consensus Networks a discernible structure, which can be understood as having two distinct epistemic zones. This distinction is not about the nature of reality itself, but describes the justificatory status of our claims at a given time.

* **The Convergent Core:** This represents the load-bearing foundations of our current knowledge. It comprises domains where the relentless pressure of pragmatic selection has eliminated all known rival formulations, leaving a single, or functionally identical, set of low-brittleness principles. Principles reside in this core—such as the laws of thermodynamics or the germ theory of disease—not because they are dogmatically held or self-evident but because all tested alternatives have been relegated to the Negative Canon after generating catastrophically high systemic costs. While no claim is immune to revision in principle, the principles in the Convergent Core are functionally unrevisable in practice, as doing so would require dismantling the most successful and resilient knowledge structures we have ever built. A claim from this core achieves the highest degree of justification we can assign, approaching our standard for Objective Truth (Level 1).
* **The Pluralist Frontier:** This describes the domains of active research where our current evidence is insufficient to decide between multiple, competing, and viable reconstructions of the landscape of viability. Here, rival systems (e.g., different interpretations of quantum mechanics or competing models of consciousness) may coexist, each with a demonstrably low and stable degree of brittleness. It is crucial to distinguish this constrained, evidence-based pluralism from relativism. The frontier is not an "anything goes" zone but a highly restricted space strictly bounded on all sides by the Negative Canon. A system based on phlogiston is not a "viable contender" on the frontier of chemistry but a demonstrably failed research program. This pluralism is therefore a sign of epistemic underdetermination: a feature of our map's current limitations, not reality's supposed indifference. This position resonates with pragmatist accounts of functional pluralism (e.g., Price 1992), which treat different conceptual frameworks as tools whose legitimacy is determined by their utility within a specific practice. Within this frontier, the core claims of each viable competing system can be granted the status of Justified Truth (Level 2). This is also the zone where non-epistemic factors, such as institutional power or contingent path dependencies, can play their most significant role, sometimes artificially constraining the range of options explored or creating temporary monopolies on what is considered justified.

### **4.5 Illustrative Cases of Convergence and Brittleness**

The transition from Newtonian to relativistic physics offers a canonical example of this framework's diagnostic application. After centuries of viability, the Newtonian system began to accumulate significant systemic costs in the late 19th century. These manifested as first-order predictive failures, such as its inability to account for the perihelion of Mercury, and as rising conceptual debt in the form of ad-hoc modifications like the Lorentz-FitzGerald contraction hypothesis. The Einsteinian system proved a more resilient solution, reducing this conceptual debt and substantially lowering the systemic costs of inquiry in physics.

A more contemporary case can be found in the recent history of artificial intelligence, which illustrates how a brittleness assessment might function in real time. The periodic "AI winters" can be understood as the collapse of high-brittleness paradigms, such as symbolic AI, which suffered from a high rate of ad-hoc modification when faced with novel challenges. While the subsequent deep learning paradigm proved a low-brittleness solution for many specific tasks, it may now be showing signs of rising systemic costs. These can be described conceptually as, for example, potentially unsustainable escalations in computational and energy resources for marginal performance gains, or an accelerating research focus on auxiliary, post-hoc modifications rather than on foundational architectural advances. This situation illustrates the Pluralist Frontier in action, as rival architectures might now be seen as competing to become the next low-brittleness solution.

### **4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, exemplifies this state. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability therefore requires a balance between low-cost stability and adaptive capacity.

## **5. Applications and Dynamics**

### **5.1 Animating the Web of Belief**

Quine’s "Web of Belief" provided a powerful static model of a knowledge system, but it has been criticized for lacking a corresponding dynamic account of its formation and change. This section provides that dynamic physiology, detailing the naturalistic process by which a successful discovery migrates from the tentative "periphery" of the web to its load-bearing "core."

A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis on the periphery of physics. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, making its revision increasingly costly. Finally, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This entire process is driven by a powerful, naturalistic pressure. As the cognitive scientist Herbert Simon argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources (Simon 1972). The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem. When a core principle is certified by a Consensus Network with low demonstrated brittleness, it achieves the status of Justified Truth (Level 2).

This process provides the two missing mechanisms needed to animate Quine’s static web, transforming it from a purely confirmational holism into a system with a robust, functional structure (Carlson 2015). First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, solving the isolation objection. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time, a process akin to what Imre Lakatos described in the development of a research programme's "hard core."

### **5.2 Mathematics as a Paradigm Case of Internal Brittleness**

Naturalistic epistemologies often treat mathematics as a boundary problem—how can we account for a priori knowledge without appealing to Platonic forms? This framework shows mathematics is not a problematic edge case but a **paradigm demonstration** of how pragmatic selection operates in purely abstract domains.

**The Core Insight:** Mathematical frameworks face pragmatic pushback through internal inefficiency rather than external falsification.

#### **5.2.1 Operationalizing Mathematical Brittleness**

The abstract costs in mathematics can be operationalized using our diagnostic toolkit.

* **M(t) - Proof Complexity Escalation:** The historical shift from Newton's intuitive infinitesimals to the rigorous epsilon-delta formalism of Weierstrass can be seen as a response to rising brittleness. While initially more complex, the epsilon-delta method drastically reduced the long-term conceptual debt and paradoxes generated by the more brittle infinitesimal approach.

* **P(t) - Axiom Proliferation:** The response to Russell's paradox provides a perfect case study. Naive set theory collapsed under infinite brittleness. Zermelo-Fraenkel set theory (ZF) can be seen as a principled "patch"—adding axioms like Replacement and Foundation to block the paradox. Competing systems, like Russell's own Type Theory, offered a different trade-off profile. The mathematical community's convergence on ZF reflects a collective, pragmatic assessment of which system offered greater long-term viability with lower maintenance costs.

#### **5.2.2 Mathematical Brittleness Indicators**

**M(t): Proof Complexity Escalation**
- Increasing proof length without proportional explanatory gain
- Measured as: average proof length for theorems of comparable scope over time
- Rising M(t) signals degenerating research program

**P(t): Axiom Proliferation**
- Ad-hoc modifications to patch paradoxes or anomalies
- Measured as: ratio of new axioms added to resolve contradictions vs. axioms generating novel theorems
- High P(t) indicates conceptual debt accumulation

**C(t): Contradiction Suppression Costs**
- Resources devoted to preventing or managing paradoxes
- Measured as: proportion of research addressing known anomalies vs. extending theory
- High C(t) reveals underlying fragility

**R(t): Unification Power**
- Ability to integrate diverse mathematical domains under common framework
- Measured as: breadth of cross-domain applicability
- Declining R(t) signals fragmentation and loss of coherence

#### **5.2.2 Historical Case Study: Russell's Paradox**

**Naive Set Theory (pre-1901):**
- M(t): Moderate—proofs reasonably concise
- R(t): Exceptional—unified logic, number theory, analysis
- Apparent low brittleness

**Russell's Paradox (1901):**
- Revealed infinite brittleness: the theory could derive contradiction
- All inference paralyzed (if A and ¬A both derivable, anything follows)
- Complete systemic collapse

**Response 1: ZF Set Theory (Zermelo-Fraenkel + Axiom of Choice)**
- Added axioms (Replacement, Foundation, etc.) to block paradox
- M(t): Increased (more axioms, more complex proofs)
- P(t): Moderate (axioms serve multiple purposes beyond patching)
- C(t): Low (paradox resolved, no ongoing suppression needed)
- R(t): High (maintained unifying power)
- **Diagnosis:** Successful low-brittleness resolution through principled modification

**Response 2: Type Theory (Russell/Whitehead)**
- Stratified hierarchy preventing self-reference
- M(t): High (complicated type restrictions on proofs)
- P(t): Low (structural solution, not ad-hoc)
- C(t): Low (paradox structurally impossible)
- R(t): Moderate (some mathematical domains resist type stratification)
- **Diagnosis:** Alternative low-brittleness solution with different trade-offs

**Response 3: Paraconsistent Logic**
- Accept contradictions but control explosion
- M(t): Variable (depends on implementation)
- P(t): Very High (many special rules to prevent collapse)
- C(t): Very High (constant management of contradictions)
- R(t): Low (marginal adoption, limited domains)
- **Diagnosis:** Higher brittleness—requires ongoing suppression costs

**Historical Outcome:** Mathematical community converged primarily on ZF, with Type Theory for specific domains. Paraconsistent approaches remain peripheral. This convergence reflects differential brittleness, not arbitrary preference.

#### **5.2.3 Coercive Overheads in Mathematics: Power and Suppression**

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches (e.g., career punishment, publication barriers for heterodox foundations), this incurs measurable **Coercive Overheads (C(t))**. These costs manifest as innovation lags, fragmentation into splinter communities, and delayed discoveries as useful insights are marginalized for sociological, not technical, reasons. The historical resistance to non-standard analysis, despite its applications, is a potential example of such a brittleness-inducing suppression.

#### **5.2.4 Why Logic Occupies the Core**

Logic isn't metaphysically privileged—it's functionally indispensable.

**The Entrenchment Argument:**
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
- Some quantum logics represent such revisions
- But the cost threshold is exceptionally high
- Most "apparent" logic violations turn out to be scope restrictions rather than genuine revisions

#### **5.2.4 Mathematical Progress as Brittleness Reduction**

**Non-Euclidean Geometry:**
- Euclidean geometry: high brittleness for curved space applications
- Required elaborate patches (epicycles in astronomy)
- Non-Euclidean alternatives: lower brittleness for cosmology, general relativity
- **Pattern:** Replace high-brittleness framework with lower-brittleness alternative when domain expands

**Calculus Foundations:**
- Infinitesimals: intuitive but theoretically brittle (paradoxes of the infinite)
- Epsilon-delta formalism: higher initial complexity but lower long-term brittleness
- Historical adoption pattern follows brittleness reduction

**Category Theory:**
- More abstract/complex than set theory
- But lower brittleness for certain domains (algebraic topology, theoretical computer science)
- Adoption follows domain-specific viability assessment

#### **5.2.5 Power and Suppression in Mathematics**

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
- Publication barriers for alternative foundations
- Curriculum monopolization by dominant frameworks
- Citation exclusion of rival approaches

**Measurable Costs:**
- **Innovation lag:** Talented mathematicians driven from field when approaches rejected for sociological rather than technical reasons
- **Fragmentation:** Splinter communities forming alternative journals, departments
- **Inefficiency:** Duplication of effort as alternative approaches can't build on dominant framework's results
- **Delayed discoveries:** Useful insights suppressed for decades (e.g., non-standard analysis resisted despite applications)

**The Brittleness Signal:** When a mathematical community requires high coercive costs to maintain orthodoxy against persistent alternatives, this signals underlying brittleness—the dominant framework may not be optimally viable.

**Historical Example:** Intuitionist vs. classical mathematics
- Intuitionists demonstrated genuine technical alternatives
- Classical community initially suppressed through institutional power
- High coercive costs (career barriers, publication difficulties)
- Eventual accommodation as constructive methods proved valuable
- **Diagnosis:** Initial suppression revealed brittleness in classical community's claim to unique optimality

#### **5.2.6 The General Principle**

Mathematics demonstrates the framework's universality. All domains—physical, social, mathematical—face pragmatic selection. The feedback mechanism varies:
- **Physical sciences:** External prediction vs. observation
- **Social systems:** Demographic/economic/stability costs
- **Mathematics:** Internal coherence, proof efficiency, unification power

But the underlying filter is identical: systems accumulating brittleness are replaced by more viable alternatives. The Apex Network spans all domains because constraint-determined optimal structures exist in all domains.

Mathematics is not a special case requiring different epistemology—it's a pure case showing how pragmatic selection operates when feedback is entirely internal to the system.

## **6. Situating the Framework in Contemporary Debates**

This paper has developed what can be termed **Systemic Externalism**—a form of externalist epistemology that locates justification not in individual cognitive processes but in the demonstrated reliability of entire knowledge systems. This section clarifies the framework's position within contemporary epistemology by examining its relationship to four major research programs: coherentist epistemology, social epistemology, evolutionary epistemology, and neopragmatism.

### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

While internalist coherentists like Carlson (2015) have successfully shown *that* the web must have a functionally indispensable core, they lack the resources to explain *why* that core is forged by external discipline. Systemic Externalism provides this missing causal engine, grounding Carlson's internal structure in an externalist history of pragmatic selection.

This epistemological challenge is a precise structural analogue to a long-standing dilemma in metaphysics, articulated by Bennett-Hunter (2015). Emergentist theories must balance a property's *dependence* on its physical base with its genuine *novelty*. An overemphasis on dependence collapses into reductionism, while an overemphasis on novelty risks a slide into dualism. The core problem in both domains is the same: how can a system's internal architecture, whether of beliefs or properties, be reliably connected to a world outside that system?

This paper's framework, which we term *Systemic Externalism*, offers a unified externalist solution to this structural problem. It provides the evolutionary backstory for the web's functional structure, arguing that a principle becomes functionally indispensable, as described by Carlson (2015), precisely because it has survived a historical filtering process based on pragmatic consequences. Justification is therefore a two-level property: it requires not only a proposition's internal coherence within a network but also the demonstrated reliability of the network itself, measured through its historical capacity to maintain low systemic brittleness. This dynamic, failure-driven approach also distinguishes our model from much of network epistemology, which often analyzes information flow within static network structures (Zollman 2013; Rosenstock et al. 2017). By examining how entire networks evolve under the selective pressure of pragmatic pushback, we ground the web’s internal structure in an objective, externalist history, thereby resolving the isolation objection.

#### **6.1.1 A Naturalistic Engine for Structural Realism**

The Apex Network shares affinities with scientific structural realism (Worrall 1989) while providing a naturalistic engine for structural realism by answering two key questions: how do our contingent historical practices reliably converge on objective relational structures, and what is the causal mechanism that drives this convergence? Structural realism persuasively argues that what survives paradigm shifts are relational structures, not descriptions of unobservable entities. However, it has struggled to explain the non-miraculous process by which our fallible inquiries converge on these structures.

Our framework provides this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our theories to align with the objective relational structure of the Apex Network. Theories fail not randomly but systematically when they accumulate brittleness, entering the Negative Canon. Low-brittleness networks survive and converge. This counters pessimistic induction: the pattern of failure is not arbitrary but reveals the constraint-determined structure that any viable theory must approximate.

Ontologically, the Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology. Epistemologically, we discover this structure through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs. Low-brittleness networks survive, triangulating toward the objective structure. This provides the causal mechanism structural realism lacks, grounding its claims in a testable, evolutionary process.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

### **6.3 Distinguishing from Lakatos and Laudan**

It is useful to distinguish our concept of systemic brittleness from related ideas in the philosophy of science, such as Lakatos's (1970) 'degenerative research programmes' and Laudan's (1977) 'problem-solving effectiveness'. While Lakatos provides a brilliant historical description of a degenerating programme—one that relies on ad-hoc hypotheses and fails to make novel predictions—our framework aims to provide the underlying causal engine for this degeneration. Brittleness is a measure of the accumulated systemic costs that cause a programme to become degenerative. It is a diagnostic of a system's structural health, not just a historical description of its output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of risk and resilience. A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## **7. Final Defense and Principled Limitations**

A philosophical model is best judged by its ability to resolve the very paradoxes that plague its predecessors. This section demonstrates the resilience of our framework by engaging with a series of classic epistemological challenges. We treat these not as external objections to be deflected, but as core test cases that reveal the explanatory power of analyzing knowledge through the lens of systemic viability.

### **7.1 The Falsifiable Research Program**

The framework's claims are designed to ground a concrete, empirically testable research program. Its core causal hypothesis is a falsifiable prediction: *a network with a high or rising degree of measured brittleness carries a statistically higher probability of collapse or major revision when faced with a comparable external shock.*

Testing this claim requires a rigorous methodology. The first step is to operationalize the indicators of brittleness through quantifiable proxies for systemic cost, such as the ratio of state budgets for internal security versus R&D or the rate of non-generative auxiliary hypotheses in scientific literature. The second step is to apply these metrics in a comparative historical analysis. A significant challenge in this research is to isolate the causal impact of intrinsic brittleness from the noise of historical contingency. To address this, the hypothesis can be tested by analyzing cohorts of systems (e.g., polities, scientific paradigms) that faced similar types of external shocks. Using large-scale databases like the Seshat Databank, researchers could compare the outcomes of systems with different pre-existing brittleness indicators when faced with a comparable shock, thereby statistically controlling for the contingent event itself.

This comparative method provides a well-established procedure for testing the theory's probabilistic claims against complex historical data. The framework is therefore rigorously falsifiable: if broad, methodologically sound historical analysis revealed no statistically significant correlation between the indicators of high systemic cost and subsequent network fragility, the theory’s core causal engine would be severely undermined. We acknowledge that such a research program faces significant challenges, including the operationalization of proxies, the control of confounding variables, and the interpretation of sparse data. The claim is not that this provides a simple algorithm for historical analysis, but that it offers a conceptually coherent and empirically grounded framework for it.

Finally, a crucial component of this program involves moving from retrospective calibration to prospective testing. While historical analysis is essential for identifying and calibrating the indicators of brittleness, the framework's ultimate test lies in its ability to make probabilistic, forward-looking claims. For instance, a diagnosis of rising brittleness in a current research paradigm—such as escalating resource costs for marginal gains—would yield the falsifiable prediction that this paradigm is statistically more likely to be superseded by a more efficient rival architecture in the face of novel challenges. This prospective application is essential for demonstrating that brittleness is a genuine diagnostic tool, not merely a post-hoc explanatory device.

### **7.2 Scope, Limitations, and Genuine Bullets Bitten**

Philosophical honesty requires acknowledging not just what a framework can do, but what it cannot. This section confronts the framework's real limitations and the genuine costs of its commitments. These are not flaws to be apologized for but principled choices about scope and method.

#### **7.2.1 Species-Specific Objectivity**

**The Limitation:** Moral and epistemic truths are objective for creatures with our biological and social architecture (extended childhood requiring cooperation, limited cognition requiring trust, biological constraints of mortality and suffering, social coordination problems). Hypothetical beings with radically different structures would face different constraints and discover different optimal configurations.

**Example:** Beings that reproduce through fission (no childhood, no parenting), have perfect telepathic communication (no trust problems), experience no pain (no suffering to minimize), or have perfect memory (no institutional needs) would discover different Apex Network structure. Their moral truths would differ from ours.

**Why We Accept This:** This is relativism at the species level, not cultural level: analogous to how chemistry is objective within baryonic matter but might differ for exotic matter. It's appropriate epistemic modesty. Claims are grounded in actual evidence about actual constraint structures we face, not speculation about hypothetical beings.

**What This Preserves:** Objectivity within our domain—all humans across all cultures face the same constraint structure and will discover the same Apex Network. Cross-cultural convergence on reciprocity norms, harm prohibitions, and property conventions reflects this constraint-determined necessity.

#### **7.2.2 Learning Through Catastrophic Failure**

**The Limitation:** We learn moral truths primarily through catastrophic failure. The Negative Canon is written in blood. We could not know slavery's wrongness without the historical experiment generating centuries of suffering. Future moral knowledge will require future suffering to generate data.

**Why We Accept This:** This is how empirical knowledge works in complex domains. Medicine required harmful experiments before ethical review boards. Engineering required catastrophic bridge failures before developing safety factors. There is no shortcut bypassing human cost—complex system behavior cannot be fully predicted from first principles.

**What This Implies:**
- Moral learning is necessarily slow
- Each generation will make errors future generations recognize
- We should be epistemically humble about current certainties
- But we can have high confidence in Negative Canon entries (exhaustively tested failures)

#### **7.2.3 Floor Not Ceiling**

**The Limitation:** The framework maps necessary constraints (the floor), not sufficient conditions for flourishing (the ceiling). It cannot address what makes life meaningful beyond sustainable, supererogatory virtue and moral excellence, aesthetic value and beauty, or the difference between a decent life and an exemplary one.

**Why We Accept This:** Appropriate scope limitation. The framework does what it does well rather than overreaching. It identifies catastrophic failures and boundary conditions, leaving substantial space for legitimate value pluralism above the floor.

**What This Implies:** The framework provides necessary but not sufficient conditions. Thick theories of the good life must build on this foundation. The Pluralist Periphery is real: multiple flourishing forms exist, but all must respect the floor (avoid Negative Canon predicates).

#### **7.2.4 Expert Dependence and Democratic Legitimacy**

**The Limitation:** Accurate brittleness assessment requires technical expertise in historical analysis, statistics, comparative methods, systems theory. This creates epistemic inequality: not everyone has equal access to moral knowledge.

**Why We Accept This:** Similar to scientific expertise generally. Complex systems require specialized knowledge to evaluate. Nuclear safety, climate science, and epidemiology all face the same challenge.

**The Democratic Challenge:** If moral truth requires expert assessment, how do we maintain democratic legitimacy?

**Partial Mitigation Strategies:**
- Public accessibility of evidence (data transparency)
- Distributed expertise (community-based participatory research)
- Standpoint epistemology (marginalized groups as expert witnesses to brittleness)
- Institutional design (independent assessment boards, adversarial review)
- Education (improving general systems literacy)

**Honest Acknowledgment:** These strategies reduce but do not eliminate the problem. Some epistemic inequality is unavoidable when evaluating complex systems.

#### **7.2.5 Discovery Requires Empirical Testing**

**The Limitation:** While the Apex Network exists as a determined structure, discovering it requires empirical data. We cannot deduce optimal social configurations from first principles alone—we need historical experiments to reveal constraint topology.

**Why We Accept This:** Even in physics and mathematics, we need empirical input. Pure reason can explore logical possibilities, but determining which possibilities are actual requires observation or experiment. For complex social systems with feedback loops and emergent properties, this dependence is stronger.

**What This Allows:** Prospective guidance through constraint analysis. We can reason about likely optimal solutions by analyzing constraint structure, but we need empirical validation. This is stronger than pure retrospection but weaker than complete a priori knowledge.

#### **7.2.6 The Viable Evil Possibility**

**The Limitation:** If a deeply repugnant system achieved genuinely low brittleness (minimal coercive costs, stable demographics, innovation, adaptation), the framework would have to acknowledge it as viable, though not necessarily just by other standards.

**Example:** Hypothetical perfectly internalized caste system where:
- Lower castes genuinely accept their position (minimal coercion)
- No measurable demographic stress
- Stable innovation rates
- Low enforcement costs
- But intuitively morally repugnant

**Why We Accept This:** Intellectual honesty. The framework is incomplete—it maps pragmatic viability, not all moral dimensions. If such a system existed (we doubt it does—internalization itself has costs), it would fall in Pluralist Periphery, not Negative Canon.

**Empirical Bet:** We predict such systems do not exist because true internalization without coercion is rare (requires ongoing socialization costs), caste systems historically show high brittleness (demographic stratification, innovation suppression), and stable oppression usually masks hidden costs that emerge under stress. For instance, historical caste systems like those in ancient India or feudal Europe exhibited rising coercive overheads (e.g., enforcement of purity laws, suppression of mobility) and eventual collapse under shocks like famines or invasions, revealing underlying brittleness (Turchin 2003). Similarly, the Soviet Union's apparent stability under Stalin masked immense hidden costs (purges, famines, innovation lags), leading to rapid disintegration in 1991.

**But:** If empirics proved otherwise, we would acknowledge the framework's incompleteness rather than deny evidence.

#### **7.2.7 What We Claim**

These limitations do not undermine the framework's contribution—they define appropriate scope. EPC excels at:

**Strong Claims:**
- Identifying catastrophic systemic failures
- Explaining moral progress as empirically detectable debugging
- Grounding realism naturalistically without non-natural properties
- Providing empirical tools for institutional evaluation
- Offering prospective guidance through constraint analysis
- Unifying descriptive and normative epistemology

**Modest Claims:**
- Does not provide complete ethics
- Does not solve all normative disagreements
- Does not eliminate need for judgment
- Does not achieve view-from-nowhere objectivity
- Does not offer categorical imperatives independent of systemic goals

**The Value Proposition:** A powerful but limited diagnostic tool for systemic health. Use it for what it does well. Supplement with other resources for what it cannot address. Do not expect a complete theory of human flourishing—expect robust tools for avoiding catastrophic failure and identifying progressive change.

This honest accounting strengthens rather than weakens the framework's philosophical contribution.

## **8. Conclusion: The Practical Payoff**

This paper has developed Emergent Pragmatic Coherentism as a response to the isolation objection facing coherentist theories of justification. By grounding coherence in the long-term viability of knowledge systems rather than internal consistency alone, the framework provides the external constraint that coherentism requires while preserving its holistic insights. The concept of systemic brittleness offers a naturalistic diagnostic tool for evaluating epistemic systems, while the notion of a constraint-determined Apex Network explains how objective knowledge can arise from fallible human practices.

Our framework for assessing systemic brittleness makes this process analyzable. By systematically studying the record of failed systems, we can begin to discern the contours of the Apex Network: the emergent set of maximally convergent, pragmatically indispensable principles that successful inquiry is forced to discover. The result is a form of Systemic Externalism that offers a novel approach for resolving long-standing problems in post-Quinean epistemology by grounding justification in the non-discursive filter of systemic consequences.

This model is not presented as a final, complete system but as the foundation for a progressive and falsifiable research program. Critical future challenges remain, such as fully modeling the role of power asymmetries in creating path-dependent 'fitness traps' and applying the framework to purely aesthetic or mathematical domains. These are not flaws in the model but precisely the kinds of tractable, empirical questions it is designed to generate.

We began with the challenge of distinguishing viable knowledge from brittle dogma in an era of epistemic crisis. The model we have developed suggests the ultimate arbiter is not the elegance of a theory or the consensus of its adherents but the trail of consequences it leaves in the world. While this framework operates at a high level of abstraction, its primary data streams originate from the ground up. As we have argued, systemic costs are ultimately experienced by individuals as suffering, instability, and the frustration of human goals. Dissent, friction, and protest are therefore not merely political problems but primary sources of epistemological data about a system's rising brittleness. This framework, in this light, is best understood as a tool for a democratic public to hold its own knowledge-generating systems accountable. By making the indicators of systemic health part of a shared, evidence-based language, it helps us ask the most pragmatic question of all: "Is this way of thinking, this way of organizing ourselves, still working for us?"

Ultimately, the brittleness toolkit isn't just for philosophers; it could and should be used by institutional designers, science funders, and policy analysts to assess the structural health of our most critical knowledge-producing institutions in real time. This positions the paper not just as a solution to an old philosophical puzzle, but as a practical and urgent research program for the 21st century.

## **Appendix A: Operationalizing Brittleness Metrics**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

## **Appendix B: Normative Brittleness as a Speculative Extension**

The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## **Glossary**

### **Part 1: The Core Framework \& Philosophical Stance**

**1. Emergent Pragmatic Coherentism (EPC)**
The name for the theoretical framework developed in this paper. It provides a naturalistic account of objectivity that avoids both foundationalism and relativism.

* **Core Logic:** All knowledge begins within a coherentist web but is then subjected to a pragmatic, evolutionary filter. Objectivity is the *emergent result* of this filtering process, not a foundational starting point.

  * It is **Pragmatic** because its ultimate court of appeal is the observable, real-world *costs* generated by a knowledge system when its ideas are put into practice.
  * It is **Coherentist** in that it accepts the Quinean insight that claims are initially justified by their fit within a holistic network, rejecting the idea of isolated, foundational beliefs.
  * It is **Emergent** because it argues that objectivity is not a pre-given metaphysical structure but an *achieved structural property* that arises from a historical process. As brittle, high-cost systems are filtered out, surviving systems are forced to converge on designs that conform to mind-independent constraints.

* **Role in the Paper:** This is the overarching philosophical framework that provides the dynamism for Quine's static web, explaining how a coherent system is disciplined by an external, pragmatic check, allowing it to adapt, evolve, and converge on objective knowledge.

**2. Systemic Externalism**
The specific epistemological stance of the model, which synthesizes internalist and externalist conditions for justification.

* **Core Claim:** Justification is a **two-condition property**. For a proposition to achieve the status of **Justified Truth**, it must meet two conditions: (1) it must cohere with the principles of its certifying **Shared Network** (the internalist condition), and (2) the **Shared Network** itself must be reliable, a status earned through a demonstrated historical capacity to maintain low **Systemic Brittleness** (the externalist condition).
* **Function:** This solves the "isolation problem" for coherentism by adding an external check based on pragmatic performance. It grounds justification in the observable, historical track record of an entire public system.
* **Distinction:** Unlike traditional process reliabilism which focuses on the cognitive processes of an individual, Systemic Externalism locates reliability in the *public, verifiable performance of the knowledge-certifying system*. Justification is a property of *propositions-within-a-proven-system*.

**3. Realist Pragmatism**
The model's philosophical identity, uniting two often-opposed traditions by arguing that being a realist is the most pragmatically effective strategy.

* **Core Synthesis:**

  * It is **Pragmatist** in its anti-foundationalism, its fallibilism, and its focus on inquiry as a problem-solving process whose success is measured by real-world consequences.
  * It is staunchly **Realist** in its commitment to the **Apex Network** as a real, emergent structure of viable solutions whose contours are determined by mind-independent pragmatic constraints.

* **Function:** This synthesis explains *how* a pragmatist inquiry can generate realist outcomes. The relentless, cost-based filtering of our ideas is precisely the mechanism that forces our fallible systems to conform to objective, structural facts about our world.

### **Part 2: The Units of Analysis: How Knowledge is Structured**

**1. Shared Network**

The primary unit of public knowledge in our model. The concept is not a novel theoretical entity but is presented as an observable consequence of Quine's holism: the public architecture that emerges when individual webs of belief must align under shared pragmatic pressure. A Shared Network is the coherent subset of propositions and Standing Predicates that must be shared across many individual webs for collective problem-solving to succeed. These networks are often nested, with specialized domains like germ theory forming coherent subsets within broader ones like modern medicine, which must itself align with the predicates of empirical science.

While the network itself evolves through a bottom-up process of failure-driven revision, it is experienced by individuals in a top-down manner. For any agent, acquiring a personal web of belief is largely a process of inheriting the structure of their community's dominant Shared Networks. This inherited web is then revised at the margins through personal "recalcitrant experiences," or what our model terms pragmatic pushback. As the vehicle for cumulative, inter-generational knowledge, a Shared Network functions as a replicator (Mesoudi 2011) of successful ideas. The pressure for coherence *between* these nested networks is what drives the entire system toward convergence on the Apex Network.

**2. The Deflationary Path: Belief → Proposition → Standing Predicate**

A clarification of the model's deflationary method, which follows Quine in shifting focus from private mental states to the public, functional roles propositions play within a Shared Network.

* **Belief:** A private, psychological state of an individual agent. It is the raw material from which public claims are articulated but is not itself part of the public network.
* **Proposition:** The public, linguistic expression of a belief. It is the candidate for integration into a Shared Network, where it can be collectively tested against pragmatic pushback.
* **Standing Predicate:** The validated, reusable, action-guiding conceptual tool extracted from a highly successful proposition (e.g., `...is an infectious disease`). It is a core component of a network's processing architecture and the primary unit of cultural-epistemic selection.

### **Part 3: The Dynamics of Change: How Knowledge Evolves**

**1. Pragmatic Pushback**
The primary selective force driving the pragmatic revision of knowledge systems in our model—from individual webs of belief to emergent shared networks. It is the unforgiving interface between ideas and reality, compelling adaptation through material consequences.

* **Definition:** The sum of non-negotiable, non-discursive consequences that arise when principles—from private beliefs to shared networks—are applied to the world. At the individual level, this manifests as personal failures (e.g., a doctor's misdiagnosis based on miasma theory leading to patient harm); at the collective scale, as systemic breakdowns (e.g., a city's sanitation policies misdirecting resources toward odors rather than contaminated pumps).
* **Nature:** This feedback is not an argument or mere cognitive dissonance but a raw, material outcome: a bridge collapses under flawed engineering, a treatment fails amid excess mortality, a society fragments from unjust policies. It operates as reality's amoral filter, hitting individual webs first—Quine's holistic dragnet quivers when a peripheral belief (e.g., a craftsperson's untested heuristic) triggers failure—then cascades to shared networks as agents converge on revisions under common pressures. Whether revising a solitary belief or a public paradigm, pushback enforces mind-independent constraints, pruning delusions without mercy.
* **Function:** This relentless pressure generates objective, measurable costs—first-order (direct losses like wasted resources or lives) and systemic (secondary fixes like conceptual debt or coercive overheads)—that serve as an evolutionary selection filter. For individuals, it prompts holistic revision in their web of belief, minimizing disruption while betting on long-term viability (Section 3.2); for networks, it forces adaptation or collapse, biasing convergence toward low-brittleness structures like the Apex Network. In both cases, costs quantify epistemic health, grounding a falsifiable program for tracking revisions from private tinkering to public resilience (Sections 2.4–2.5).

**2. Systemic Costs: A Two-Level Diagnostic Framework**
The set of concepts used to diagnose a network's health.

* **First-Order Costs (The Symptoms):** The direct, material consequences of a network’s misalignment with its pragmatic environment. These are the objective, observable signals of dysfunction (e.g., failed predictions, excess mortality, resource waste).
* **Systemic Costs (The Underlying Condition):** The secondary, internal costs a network incurs to *manage, suppress, or explain away* its first-order costs. They represent non-productive expenditures on internal maintenance rather than on adaptation. Key forms include:

  * **Conceptual Debt:** The compounding fragility incurred by adopting flawed, complex "patches" to protect a core principle.
  * **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing the dissent that arises from first-order costs.

**3. Systemic Brittleness \& Its Modalities**
The central diagnostic concept for assessing a network's health and its vulnerability to future shocks.

* **Definition:** A system’s *brittleness* is a measure of its accumulated, hidden systemic costs. A high degree of brittleness signals that a system is inefficient, fragile, and a degenerating research program.
* **Distinction:** Brittleness is not the opposite of longevity. A brittle system can endure for a long time by expending massive energy on coercion and conceptual patches. *Viability*, in contrast, is the ability to adapt and solve problems with *low* systemic costs.
* **Modalities:** The framework identifies two primary modalities of failure:

  * **Epistemic Brittleness:** Failure of alignment with the causal structure of the world (e.g., Ptolemaic astronomy).
  * **Normative Brittleness:** Failure of alignment with the constraints on stable human cooperation (e.g., a slave economy).

### **Part 4: The Structure of Objectivity: Truth, Reality, and Progress**

**1. Negative Canon**
The model's empirical and historical anchor for objectivity.

* **Definition:** The evidence-based catalogue of failed predicates, propositions, and entire Shared Networks—including the emergent structures built upon them—that have been historically invalidated by their own catastrophic **Systemic Costs** (e.g., Ptolemaic astronomy, phlogiston chemistry).
* **Function:** This represents our most secure form of objective knowledge: not only knowing what has collapsed, but why it collapsed. It provides a “reef chart” for inquiry, mapping both the exposed wreckage of untenable theories and the hidden hazards of propositions that led them astray. In doing so, it establishes an external boundary that constrains coherence, preventing inquiry from drifting into relativism.

**2. The Apex Network vs. The Consensus Network**
The crucial distinction between the objective structure of viability our inquiry aims at (the Apex Network) and our current, fallible map of it (the Consensus Network).

* **The Apex Network (The Objective Standard):** This is the paper's central realist concept. Its status is dual:

  * **Ontologically:** The Apex Network is the complete set of all maximally coherent and pragmatically viable principles, whose structure is determined by mind-independent pragmatic constraints. It is not a metaphysical blueprint but an *emergent structural fact about our world*—constraint-determined (the structure exists necessarily given the constraints) yet discovered through history (we identify it via empirical filtering). Ontologically, it is real.
  * **Epistemically:** We can never have a final, complete view of this structure. It functions for us as a **regulative ideal** that makes our comparative judgments of brittleness meaningful. **Epistemically, it is an ideal we approximate.**
  * **Function:** It is the ultimate, non-negotiable standard for **Objective Truth** (Level 1).

* **The Consensus Network (Our Best Approximation):** Our current, best, and necessarily fallible reconstruction of the Apex Network's structure (e.g., mainstream contemporary science).

  * **Authority:** Its authority derives not from social agreement alone, but from its demonstrated historical track record of maintaining low **Systemic Brittleness**.
  * **Function:** It is the system that certifies **Justified Truth** (Level 2).

**3. The Three Levels of Truth**
The model's synthesized, procedural account of truth, designed to resolve the tension between fallibilism and realism. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence:** The baseline status. A proposition is coherent *within any specific Shared Network*, regardless of its long-term viability. This explains the internal rationality of failed paradigms but is not sufficient for justification.
* **Level 2: Justified Truth:** The highest practically achievable epistemic status. A proposition is justified as true if it is certified by a **Consensus Network** that has itself demonstrated a low and stable degree of systemic brittleness.
* **Level 1: Objective Truth:** The ultimate, regulative ideal. A proposition is objectively true if its principles are part of the **Apex Network**. This is the standard our inquiry aims to meet, even if we can never be certain we have reached it.

## **References**

Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business.

Baggio, Guido, and Andrea Parravicini. 2019. "Introduction to Pragmatism and Theories of Emergence." *European Journal of Pragmatism and American Philosophy* XI-2. https://doi.org/10.4000/ejpap.1251.

Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.

Bennett-Hunter, Guy. 2015. "Emergence, Emergentism and Pragmatism." *Theology and Science* 13, no. 3: 337–57. https://doi.org/10.1080/14746700.2015.1053760.

Berlin, Brent, and Paul Kay. 1969. *Basic Color Terms: Their Universality and Evolution*. Berkeley: University of California Press.

BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.

Bradie, Michael. 1986. "Assessing Evolutionary Epistemology." *Biology \& Philosophy* 1, no. 4: 401–59. https://doi.org/10.1007/BF00140962.

Brandom, Robert B. 1994. *Making It Explicit: Reasoning, Representing, and Discursive Commitment*. Cambridge, MA: Harvard University Press.

Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.

Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.

El-Hani, Charbel N., and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3, no. 2, article 3. http://commons.pacificu.edu/eip/vol3/iss2/3.

Gadamer, Hans-Georg. 1975. *Truth and Method*. New York: Seabury Press.

Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.

Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.

Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.

Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.

Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press.

Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.

Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.

Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.

Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press.

Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press. Originally published 1962.

Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.

Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.

Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific Research Programmes." In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.

Laudan, Larry. 1977. *Progress and Its Problems: Towards a Theory of Scientific Growth*. Berkeley: University of California Press.

Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.

Lynch, Michael P. 2009. *Truth as One and Many*. Oxford: Oxford University Press.

Mallapaty, Smriti. 2020. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.

Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.

Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.

Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.

Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.

Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press.

Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press. Originally published 1878.

Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37, no. 7: 1948–70. https://doi.org/10.1080/09515089.2023.2236120.

Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.

Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.

Price, Huw. 1992. "Metaphysical Pluralism." *Journal of Philosophy* 89, no. 8: 387–409. https://doi.org/10.2307/2940975.

Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.

Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60, no. 1: 20–43. https://doi.org/10.2307/2181906.

Quine, W. V. O. 1960. *Word and Object*. Cambridge, MA: MIT Press.

Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press.

Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.

Rosenstock, Sarita, Cailin O'Connor, and Justin Bruner. 2017. "In Epistemic Networks, Is Less Really More?" *Philosophy of Science* 84, no. 2: 234–52. https://doi.org/10.1086/690641.

Rottschaefer, William A. 2012. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.

Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.

Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.

Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.

Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.

Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.

Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding, and Selection in Evolution." In *Proceedings of the Sixth International Congress of Genetics*, edited by Donald F. Jones, 356–66. Ithaca, NY: Brooklyn Botanic Garden.

Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.

Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.

Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.

Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.