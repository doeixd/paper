# **The Architecture of Failure: How Systemic Brittleness Drives Convergent Coherence to Forge Objective Truth**

## **Abstract**

Coherentist theories of justification remain vulnerable to the isolation objection: a belief system could be perfectly coherent yet entirely detached from reality. This paper proposes Emergent Pragmatic Coherentism, grounding coherence in the demonstrated viability of knowledge systems. The framework introduces **systemic brittleness** as a diagnostic tool, measuring network health through observable costs incurred when applying propositions. We argue that mind-independent pragmatic constraints determine a **necessary structure of optimal solutions—the Apex Network**—which emerges from the topology of reality itself. Selective pressure forces knowledge systems to converge on this structure through historical filtering, a process of **discovery, not creation**. Justification thus requires both internal coherence and the certifying network's demonstrated resilience. This **naturalized proceduralism** redefines objective truth as alignment with the Apex Network and supports a **falsifiable research program** for assessing epistemic health, with preliminary applications to cases from Ptolemaic astronomy to contemporary AI.

## **1. Introduction: From a Static Web to a Dynamic Process**

Why did germ theory replace miasma theory? While a standard answer points to superior evidence, a deeper analysis reveals a story about systemic viability. Although miasma theory's focus on sanitation had some positive public health effects, its core principles were degenerating. The miasma network demonstrated its brittleness: it generated catastrophic real-world costs—thousands died in London because public health efforts were misdirected at odors—and it required an accelerating number of ad hoc patches to explain anomalies, such as why the "bad air" was only deadly near a specific water pump. The germ theory network, by contrast, proved vastly more resilient and adaptive. It dramatically reduced these costs by enabling effective interventions and explained a wide range of phenomena with a single conceptual tool.

This historical dynamic highlights a persistent challenge for contemporary coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) acknowledged, a belief system could achieve perfect internal coherence while remaining entirely detached from reality. While coherentists have developed various responses to this objection (Olsson 2005; Kvanvig 2012), most rely on internalist resources that fail to provide the external constraint coherentism requires. Scholars have made compelling cases for a more structured, asymmetrical web of belief from within Quine's own framework (e.g., Carlson 2015), but the question of what external pressures forge this structure remains. This paper develops an alternative response that grounds coherence in the demonstrated viability of entire knowledge systems, measured through their historical capacity to minimize what we will term "systemic costs." Drawing inspiration from resilience theory in systems ecology (Holling 1973), this perspective explains how the holistic revisions individuals make to their personal webs of belief in response to recalcitrant experiences—a process we generalize as pragmatic pushback—drive the bottom-up formation of more viable, less fragile public knowledge systems.

This paper's response is distinctive: it grounds coherence not in historical accident but in emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—form a topology that necessarily generates optimal configurations for navigating those constraints. These optimal structures emerge from the constraint landscape itself, existing whether we've discovered them or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether we've calculated it. What we call "objective truth" is alignment with these emergent, constraint-determined structures. Historical filtering of failed systems is how we discover this emergent topology, not how we create it. Failed systems reveal where the constraint landscape drops off; successful systems triangulate toward the peaks that emerge necessarily from how reality is organized.

This transforms the isolation objection: a coherent system detached from reality isn't truly possible because reality's constraints force convergence toward viable configurations. A perfectly coherent flat-earth cosmology generates catastrophic navigational costs. A coherent phlogiston chemistry generates accelerating conceptual debt. These aren't merely false—they're structurally unstable, misaligned with constraint topology. Failed systems reveal where the constraint landscape drops off, allowing us to map its hazards. This process is not teleological; it does not follow a lighthouse beam toward a known destination. Rather, it is the painstaking construction of a **reef chart** from the empirical data of shipwrecks. Successful systems are those that navigate the safe channels revealed by this chart of failures, triangulating toward the viable peaks that emerge necessarily from how reality is organized. This 'reef chart' epistemology is the central theme of our framework. The Apex Network is the structure that remains when all such unstable configurations are eliminated.

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology—a theory about the long-term viability of cumulative systems like science and law. The model proposes a Lamarckian-style mechanism of directed adaptation through learning, rather than purely Darwinian selection, to account for the intentional nature of inquiry.

To pre-empt a common misinterpretation, we distinguish viability from mere endurance. A brutal empire that persists through coercion is not viable but a textbook case of high brittleness; its longevity measures the immense energy wasted suppressing instability. Viability is relational: a system's capacity to solve problems with sustainably low systemic costs. This distinction is empirically measurable—the ratio of coercive to productive resources quantifies non-viability.

Furthermore, this framework incorporates power, path dependence, and historical contingency not as exceptions but as key variables. The exercise of power to maintain a brittle system is not a refutation of the model but a primary indicator of its non-viability, measured through high coercive costs. The framework's claims are therefore probabilistic, not deterministic: brittleness increases vulnerability to contingent shocks, but collapse is not inevitable.

This failure-driven process grounds fallible realism. Knowledge systems converge on emergent structures determined by mind-independent constraints, yielding a falsifiable research program for historical dynamics.

To clarify its scope, this framework is not foundationalist nor a general theory of justification. It is a specialized epistemology for cumulative knowledge systems—such as science, law, and engineering—where inter-generational claims and pragmatic feedback provide the basis for evaluation.¹

¹It provides macro-level foundations for individual higher-order evidence (Section 7), not solutions to Gettier cases or basic perception (cf. Goldman 1979; Zagzebski 1996).

This focus demonstrates unification under pragmatic selection. The framework is naturalized proceduralism: objectivity arises from historical brittleness assessment, not idealized discourse norms. Arguments are disciplined by systemic success data.

The argument proceeds systematically. Section 2 develops the diagnostic framework for assessing systemic brittleness; Section 3 establishes the logic of viability; Section 4 introduces the Negative Canon and the Apex Network; Section 5 animates Quine's web of belief; Section 6 situates the framework within contemporary epistemological debates; Section 7 defends against objections and clarifies principled limitations.

## **2. The Core Concepts: Units of Epistemic Selection**

To understand how some knowledge systems evolve and thrive while others stagnate and collapse, we need a way to assess their structural health. A naturalistic theory requires functional, precise tools for this analysis, moving beyond mere internal consistency to gauge a system's resilience against real-world pressures. In this, our approach shares a deep affinity with the diagnostic ethos of complex systems theory (Meadows 2008). This section develops such a framework by tracing how a private belief becomes a public, functional component of a knowledge system.

### **2.1 Forging the Instruments: From Private Belief to Public Tool**

Following standard practice in naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts focus from private psychological states to public, functional structures. This methodological move serves two purposes: it makes the analysis tractable by focusing on observable phenomena, and it addresses epistemic systems that transcend individual cognition. The analysis begins with beliefs as private psychological states but quickly moves to their public expression as propositions subject to collective evaluation.

`\[FIGURE 1: Conceptual Flowchart for the Deflationary Path]`  
`\[Belief (Private State)] --> \[Articulation into a Proposition (Public Claim)] --> \[Coherence Test] --> \[Integration as Validated Data OR Promotion to Standing Predicate (Public Tool)]`

**Belief → Proposition → Validated Data → Standing Predicate**

#### **2.1.1 From Private Belief to Public Proposition**

The journey begins with *belief*, the raw material of epistemology. Belief, as a private mental state, remains analytically inaccessible for a theory of public knowledge. The first step is therefore to isolate its testable, public content as a *proposition*. A proposition is a falsifiable claim that can be articulated, communicated, and collectively assessed.

#### **2.1.2 The Coherence Test**

Next, a candidate proposition must pass a rigorous test for coherence. This is not the thin, formal consistency of logic, but a thick, forward-looking pragmatic assessment—a form of risk analysis. A shared network, as a resource-constrained system, implicitly asks: will integrating this proposition increase or decrease our long-term systemic brittleness? A proposition that successfully passes this test is accepted into the network as validated data—a reliable claim that can be used and cited within the system, awaiting its ultimate functional test.

#### **2.1.3 From Validated Data to Standing Predicate**

Finally, propositions that not only pass the coherence test but do so with exceptional success—by dramatically reducing a network's systemic costs—undergo a profound status change. They are not merely stored as facts; their functional core is promoted and repurposed to become part of the network's core processing architecture.

This creates a **Standing Predicate**: a reusable, action-guiding conceptual tool that has earned durable status. It functions as the "gene" of cultural evolution. For instance, "Cholera is an infectious disease" promoted the predicate `...is an infectious disease` to a Standing Predicate in medical science.

Unlike static claims, Standing Predicates are dynamic tools. Applying one unpacks proven interventions, diagnostics, and inferences. The proposition evolves from tested data to a testing tool, enabling networks to learn and adapt.

### **2.2 The Units of Analysis: Predicates, Networks, and Replicators**

Having established the journey from private belief to public tool, we can now define the model's core analytical units. Our analysis makes a deflationary move: we shift focus from the psychology of individual agents to the public, functional structures that emerge as a necessary consequence when multiple Quinean webs of belief are forced to align under pragmatic pressure.

* **Standing Predicate:** This is the primary unit of cultural-epistemic selection: the validated, reusable, and action-guiding conceptual tool within a proposition (e.g., `...is an infectious disease`). Functioning as the generative "genes" of cultural evolution, a Standing Predicate is a highly compressed piece of conceptual technology. When applied, it unpacks a suite of previously validated knowledge, including causal models, diagnostic heuristics, and licensed interventions.
* **Shared Network:** An emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (e.g., germ theory within medicine within science). Their emergence is structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary theory (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as the **replicator**—copied code—while social groups are the **interactor**—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

### **2.3 Pragmatic Pushback and Systemic Costs**

A shared network is not a passive library but an active system under constant pressure from pragmatic pushback: our model's term for the systemic analogue of what Quine called a "recalcitrant experience" at the periphery of an individual's web of belief. It is the sum of the concrete, non-negotiable consequences that arise when a network's principles are applied. This feedback is not an argument but a material outcome: a bridge collapses, a treatment fails, a society fragments. This process generates two types of costs.

First-Order Costs are the direct, material consequences: failed predictions, wasted resources, environmental degradation, or systemic instability (e.g., excess mortality). These are the objective signals of dysfunction. Systemic Costs are the secondary, internal costs a network incurs to manage, suppress, or explain away its first-order costs. These non-productive expenditures reveal a network's true fragility. Key forms include:

* **Conceptual Debt Accumulation:** The compounding fragility incurred by adopting flawed, complex patches to protect a core principle.
* **Coercive Overheads:** The measurable resources allocated to enforcing compliance and managing dissent. These coercive overheads are the primary mechanism by which power dynamics manifest within our model: the resources spent to maintain a brittle system against internal and external pressures become a direct, measurable indicator of its non-viability. Dissent, in this model, is a critical data stream signaling that a system is generating costs for its members.

Pragmatic pushback is not limited to direct, material failures. In highly abstract domains like theoretical physics or pure mathematics, where direct empirical tests are deferred or unavailable, pushback primarily manifests through the accumulation of Systemic Costs. A research program that requires an accelerating rate of ad hoc modifications to maintain internal consistency, or that loses its unifying power, is experiencing a powerful form of pragmatic pushback. These epistemic inefficiencies are real costs that render a network brittle and unproductive, even if it has not been directly falsified by experiment. The framework's diagnostic lens thus applies to all forms of inquiry, measuring viability through either external, material consequences or internal, systemic dysfunction.

To operationalize these concepts, we introduce a set of diagnostic indicators for tracking brittleness over time:

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |
| **P(t)** | Conceptual Debt Accumulation | Ratio of anomaly-resolution publications to novel-prediction publications |
| **C(t)** | Coercive Overhead | Ratio of security/suppression budget to productive/R&D budget |
| **M(t)** | Model Complexity | Rate of parameter/complexity growth vs. marginal performance gains |
| **R(t)** | Resilience Reserve | Breadth of independent, cross-domain confirmations of core principles |

To illustrate this toolkit in action, consider two brief examples.

**Case 1: Ptolemaic Astronomy (c. 1500 CE).** The system exhibited high and rising brittleness. **M(t)** was acute, as model complexity escalated through added epicycles and equants to maintain predictive accuracy. **P(t)** was also high, as most astronomical work was dedicated to resolving anomalies rather than generating novel, testable predictions. **C(t)** manifested in the intellectual and institutional resources used to suppress nascent heliocentric alternatives, and **R(t)** remained low, with the model's principles finding little application outside a narrow predictive domain.

**Case 2: Contemporary AI Development.** Current deep learning paradigms may be showing early signs of rising brittleness. **M(t)** is visible in the exponential escalation of parameter counts and computational resources for often marginal gains in performance. **P(t)** can be proxied by the proliferation of 'alignment' and 'safety' research, much of which functions as post-hoc patches for emergent anomalous behaviors. These trends serve as potential warning signs that invite cautious comparison to the structural dynamics of past degenerating research programs.



## **3. The Methodology of Brittleness Assessment**

### **3.1 The Challenge of Objectivity: Preempting the Circularity Objection**

We face a fundamental circularity problem in operationalizing brittleness: measuring systemic costs objectively requires neutral standards for "waste" or "dysfunction," yet establishing such standards appears to require the very epistemic framework our theory aims to provide.

This circularity cannot be eliminated but can be managed through several strategies. First, we anchor measurements in basic biological and physical constraints: demographic collapse, resource depletion, infrastructure failure. These provide relatively theory-neutral indicators of breakdown. Second, we employ comparative rather than absolute measures, comparing brittleness trajectories across similar systems. Third, we require convergent evidence across multiple independent indicators before diagnosing brittleness.

Brittleness assessment remains partially hermeneutic. The framework provides structured tools rather than mechanical algorithms, making judgments more systematic and accountable to evidence without eliminating interpretation. This constrains the framework's ambitions: it offers "structured fallibilism" rather than neutral assessment.

This methodology provides pragmatic objectivity sufficient for comparative assessment. **A detailed protocol for operationalizing these metrics, including a worked example of inter-rater reliability, is provided in Appendix A.**

### **3.2 The Solution: A Tiered Diagnostic Framework**

To clarify how objective cost assessment is possible without appealing to contested values, we can organize brittleness indicators into a tiered diagnostic framework, moving from the most foundational and least contestable to the more domain-specific.

* **Tier 1: Foundational Bio-Social Costs:** At the most fundamental level are the direct, material consequences of a network's misalignment with the conditions for its own persistence. These are not abstract values but objective bio-demographic facts, measurable through historical and bioarchaeological data. They include:
  * Excess mortality and morbidity rates (relative to contemporaneous peers with similar constraints)
  * Widespread malnutrition and resource depletion
  * Demographic collapse or unsustainable fertility patterns
  * Chronic physical suffering and injury rates

A system that generates higher death or disease rates than viable alternatives under comparable constraints is incurring a measurable, non-negotiable first-order cost. These metrics are grounded in biological facts about human survival and reproduction, not in contested normative frameworks.

* **Tier 2: Systemic Costs of Internal Friction:** The second tier measures the non-productive resources a system must expend on internal control rather than productive adaptation. These are the energetic and informational prices a network pays to manage the dissent and dysfunction generated by its Tier 1 costs. These systemic costs are often directly quantifiable:
  * **The Coercion Ratio (C(t)):** In socio-political networks, this can be measured by analyzing the ratio of a state's resources allocated to internal security and suppression versus resources for public health, infrastructure, and R&D.
  * **Information Suppression Costs:** Resources dedicated to censorship or documented suppression of minority viewpoints, and resulting innovation lags when compared to more open rival systems.

* **Tier 3: Domain-Specific Epistemic Costs:** The third tier addresses abstract domains like science and mathematics, where costs manifest as inefficiency:
  * **Conceptual Debt Accumulation (P(t)):** Rate of auxiliary hypotheses to protect core theory (literature analysis).
  * **Model Complexity Inflation (M(t)):** Parameter growth without predictive gains (parameter-to-prediction ratios).
  * **Proof Complexity Escalation:** Increasing proof length without explanatory gain (mathematics).

While the interpretation of these costs is a normative matter for agents within a system, their existence and magnitude are empirical questions. The framework's core causal claim is falsifiable and descriptive: a network with high or rising brittleness across these tiers carries a statistically higher probability of systemic failure or major revision when faced with external shocks.

While this tiered framework provides conceptual clarity on the different types of costs, robustly measuring them requires a disciplined methodology. We therefore turn to the triangulation method, which provides the practical protocol for achieving pragmatic objectivity.

**Cost-Shifting as Diagnostic Signal**

This framework reveals cost-shifting: systems may excel in one tier (e.g., epistemic efficiency) while incurring catastrophic costs in another (e.g., bio-social harm). Such trade-offs signal hidden brittleness, as deferred costs accumulate vulnerability. Diagnosis identifies unsustainable patterns across tiers, not a single score.

### **3.3 The Triangulation Method**

No single indicator is immune to interpretive bias. Therefore, a robust diagnosis of brittleness requires **triangulation across independent baselines**. This protocol provides a concrete method for achieving pragmatic objectivity.

* **Baseline 1: Comparative-Historical Analysis:** We compare a system's metrics against contemporaneous peers with similar technological, resource, and environmental constraints. For example, 17th-century France exhibited higher excess mortality from famine than England, not because of worse climate, but because of a more brittle political-economic system that hindered food distribution. The baseline is what was demonstrably achievable at the time.

* **Baseline 2: Diachronic Trajectory Analysis:** We measure the direction and rate of change within a single system over time. A society where life expectancy is falling, or a research program where the ratio of ad-hoc patches to novel predictions is rising, is exhibiting increasing brittleness regardless of its performance relative to others.

* **Baseline 3: Biological Viability Thresholds:** Some thresholds are determined by non-negotiable biological facts. A society with a Total Fertility Rate sustainably below 2.1 is, by definition, demographically unviable without immigration. A system generating chronic malnutrition in over 40% of its population is pushing against fundamental biological limits.

Diagnosis requires convergent baselines: e.g., rising mortality (diachronic), peer underperformance (comparative), and biological thresholds. This parallels climate science's multi-evidence convergence, achieving pragmatic objectivity for comparative evaluations. This triangulation method is the practical tool for constructing our reef chart, systematically mapping the hazards of brittleness to reveal the navigable channels of viability.

### **3.4 Protocol in Action: An Operational Test for Coercive Overheads**

To avoid circularity, classify spending via empirical trajectories, not definitions.

**Protocol:**
1. Track allocations: security/suppression (S) vs. productive (P).
2. Correlate with outcomes: mortality, economic output, innovation, stability.
3. Diagnose: Coercive if rising S correlates with worsening outcomes and diminishing returns; productive if correlated with improvement and spillovers.

**Example:** Society A doubles police spending, crime falls 40%, trust rises—productive. Society B doubles spending, crime rises, instability increases—coercive.

Classification emerges from patterns, ensuring robustness through convergent metrics.



## **4. The Emergent Structure of Objectivity**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We will show how the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

### **4.1 A Negative Methodology: Charting What Fails**

The first step in constructing our reef chart is to systematically catalogue the shipwrecks. Our account of objectivity begins not with a speculative vision of a final truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is often of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system—its descent into crippling inefficiency, intellectual stagnation, and institutional decay—provides a clear, non-negotiable data point.

Systematic failure analysis builds the **Negative Canon**: an evidence-based catalogue of invalidated principles. It distinguishes:

* **Epistemic Brittleness:** Causal failures (e.g., scholastic physics, phlogiston) generating ad-hoc patches and predictive collapse.
* **Normative Brittleness:** Social failures (e.g., slavery, totalitarianism) requiring rising coercive overheads to suppress dissent.

Charting failures reverse-engineers viability constraints, providing external discipline against relativism. This eliminative process constructs a reef chart: mapping hazards retrospectively, not pursuing a teleological goal. Progress accrues through better hazard maps.

### **4.2 The Apex Network: An Emergent Structure of Modal Necessity**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the Apex Network. To be precise about its status, it is not a pre-existing metaphysical blueprint awaiting discovery, nor the territory of reality itself, nor is it merely our current consensus. The Apex Network is the name for the theoretical limit-point of this process of convergence, a concept with a deep affinity to the classical pragmatist notion of truth as the ideal end of inquiry (Peirce 1878). Our Consensus Network is a fallible, historically-situated attempt to chart this structure; the Apex Network is the objective structure being charted.

The ontological status of the Apex Network requires careful specification to avoid both foundationalist overreach and relativist collapse. We propose it should be understood as a "structural emergent": a real, objective pattern that crystallizes from the interaction between inquiry practices and environmental resistance. To clarify this naturalized ontological status, consider how objective structural facts can emerge from seemingly subjective domains, such as color perception. While an individual's color preference is contingent, a non-random, cross-cultural pattern emerges from aggregated data: a striking convergence on the color blue. This pattern is not an accident but an emergent structural fact demanding a naturalistic explanation. The "pragmatic pushback" shaping this landscape is the deep history of evolutionary selection on our shared biology. Human color vision was forged by the selective pressures of navigating a terrestrial environment, where efficiently tracking ecologically critical signals—the safety of clear water, the ripeness of fruit—conferred a viability advantage (Berlin and Kay 1969; Henrich 2015). A proposition like `'{associating blue with positive, stable conditions} is a viable perceptual default'` is not a metaphysical rule, but a point of maximal, stable convergence—a principle widely shared because it is a highly viable, low-cost solution for a species with our evolutionary history. The Apex Network has the same ontological status: it is not found, but formed. It is the objective, structural residue left after a long history of pragmatic filtering has eliminated less viable alternatives.

The mechanism that forges this structure is a bottom-up emergence driven by the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question.

The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge.

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state; they simply try to solve local problems and reduce costs. The Apex Network is the name we give to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

The Apex Network's status is therefore dual, a distinction critical to our fallibilist realism. Ontologically, it is real: it is the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, however, it remains a regulative ideal. We can never achieve a final, God's-eye view to confirm that our Consensus Network is a perfect map of it; our knowledge of the structure is necessarily incomplete and fallible. Its existence is what grounds our realism and prevents a collapse into relativism, while our epistemic limitations are what make the project of inquiry a permanent and progressive one. It is the necessary concept that makes our comparative judgments of "more" or "less" brittle meaningful.

Thus, the Apex Network should not be misconstrued as a single, final theory of everything. Rather, it is the complete set of maximally viable configurations—a high-altitude plateau on the fitness landscape. While some domains may have a single sharp peak, others may permit a constrained pluralism of equally low-brittleness systems. The convergence is *away from the vast valleys of failure* documented in the Negative Canon, and *toward this resilient plateau* of viable solutions.

#### **4.2.1 The Modal Necessity of the Apex Network**

The Apex Network's objectivity stems not from historical contingency but from modal necessity. This can be established through a four-premise argument:

**Premise 1:** Reality imposes mind-independent constraints (physical laws, biological facts, logical requirements, coordination necessities).

**Premise 2:** Constraints generate a fitness landscape with peaks (viable solutions) and valleys (brittle failures), emerging necessarily from constraint interactions.

**Premise 3:** Optimal solutions emerge necessarily from this topology, existing independently of discovery (e.g., shortest path from geometry, efficient engine from thermodynamics).

**Premise 4:** The Apex Network is that optimal structure—the complete configuration space of maximally viable solutions.

**Conclusion:** The Apex Network emerges necessarily from constraints, independent of discovery—revealed, not created by inquiry.

This crucial distinction—that historical filtering is a **discovery process, not a creation mechanism**—resolves the ambiguity. History reveals the landscape: experiments map hazards (failures) and peaks (successes) through trial-and-error, but does not create constraints or optimal solutions.

**Analogy:** Mathematical Discovery. Mathematicians discovered π through measurement and calculation. Different cultures discovered it independently. The discovery process was contingent, but π itself is necessary (determined by geometric constraints). Its value would be discovered in any sufficiently sophisticated mathematical tradition.

**Parallel:** Moral/Epistemic Discovery. Societies discover low-brittleness predicates through experimentation. Different cultures discover similar principles independently (reciprocity norms, property rights, harm prohibitions). The discovery process is contingent, but the Apex Network is necessary (determined by pragmatic constraints). Its structure would be discovered in any sufficiently long-running civilization.

Consequently, the Apex Network's structure is **counterfactually stable**: any sufficiently comprehensive exploration of the constraint landscape, across any possible history, would be forced to converge upon it. Evidence includes independent emergence of similar low-brittleness principles across isolated cultures, convergent evolution toward comparable solutions, structurally similar failures (high coercive costs, demographic stress), and mathematical convergence.

This counterfactual stability makes the Apex Network an objective standard, not a historical artifact.

#### **4.2.4 Ontological Clarification: Structural Realism**

**The Apex Network is:**
- **Real:** Exists as objective structure that emerges from mind-independent constraints
- **Structural:** Consists of relational patterns, not substances or Platonic forms
- **Emergent:** Arises necessarily from constraint topology—not constructed by inquiry, not reducible to individual constraints, but emerging from their interaction as systems seek lowest-cost paths through constraint space
- **Necessary:** Would emerge in any world with similar constraint topology
- **Discoverable:** Accessible through empirical investigation of what minimizes brittleness over time

**The Apex Network is NOT:**
- A Platonic form in a separate realm
- A historical accident determined by which societies survived
- A social construction or consensus
- A teleological endpoint toward which history progresses
- A complete, final theory we could ever fully possess

**Formal Characterization:**

A = ∩{W_k | V(W_k) = 1}

Where:
- A = Apex Network
- W_k = possible world-systems (configurations of predicates)
- V(W_k) = viability function (determined by brittleness metrics)
- ∩ = intersection (common structure across all viable systems)

The intersection of all maximally viable configurations reveals their shared structure. This shared structure is what survives all possible variations in historical path—it's the emergent, constraint-determined necessity that arises from how reality is organized.

**This resolves the isolation objection:** A coherent system detached from the Apex Network isn't merely false—it's structurally unstable. It will generate rising brittleness until it either adapts toward the Apex Network or collapses. Coherence alone is insufficient because reality's constraints force convergence.

#### **4.2.5 Epistemological Status: Regulative Ideal**

Ontologically real, epistemically regulative.

**We can never know with certainty** that our current Consensus Network perfectly maps the Apex Network. Our knowledge is:
- Fallible (any specific claim might be wrong)
- Incomplete (Apex Network likely contains structure we haven't discovered)
- Approximative (our map has granularity limits)

**Yet we can know with high confidence:**
- Some configurations are definitely not in the Apex Network (the Negative Canon)
- We're progressively better approximating it (reducing brittleness over time)
- Some principles are nearly certain to be correct (independently discovered across cultures, survived exhaustive testing)

**The regulative function:** The Apex Network is the objective standard that makes our comparative judgments meaningful. When we say System A is "more aligned with truth" than System B, we mean: A better approximates the Apex Network structure as evidenced by lower brittleness across convergent metrics.

This combination—ontologically real, epistemically ideal—enables robust fallibilist realism. We can be realists about moral/epistemic truth without claiming infallibility or final knowledge.

#### **4.2.6 Cross-Domain Convergence and Pluralism**

The mechanism that drives discovery of this structure is bottom-up emergence through the need for cross-domain consistency. Local Shared Networks, developed to solve specific problems, face pressure to cohere with one another because they operate in an interconnected world. This pressure creates a tendency toward integration, though whether this results in a single maximally coherent system or a stable pluralism remains an empirical question.

The framework makes no a priori claims about universal convergence. In domains with tight pragmatic constraints, such as basic engineering or medicine, we expect strong convergence pressures. In others, such as aesthetic judgment or political organization, we might find that multiple stable configurations remain viable. The Apex Network concept should thus be understood as a limiting case: it represents the theoretical endpoint of convergence pressures where they operate, not a guarantee that such pressures will act uniformly across all domains of inquiry.

The Apex Network's function as a standard for objective truth follows from this status. The dynamic can be understood through Susan Haack's (1993) crossword puzzle analogy. A proposition is not objectively true because it corresponds to an isolated fact, but because it is an indispensable component of the unique, fully completed, and maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" in the form of pragmatic pushback. A proposition's truth is thus secured by its necessary and irreplaceable role in the overall structure of the most viable system of knowledge.

`[FIGURE 2: Conceptual Sketch of a Fitness Landscape]`  
`[A 3D landscape with "Viability" on the Y-axis. Peaks are labeled with successful systems like "Germ Theory" and "Relativity." Deep valleys and low plateaus are labeled with failed systems from the Negative Canon, such as "Ptolemaic System," "Lysenkoism," and the "Ming Dynasty Trap."]`

This process must be understood as retrospective and eliminative, not teleological. Individual agents and networks do not aim for a final, integrated state but simply try to solve local problems and reduce costs. The Apex Network is the name we give to the objective, convergent pattern that emerges as an unintended consequence of these countless local efforts to survive the filter of failure. Its objectivity arises not from a purpose but from the mind-independent nature of the pragmatic constraints that reliably generate costs for any system that violates them.

### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check—the assessment of systemic brittleness—prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.
* **Level 1: Objective Truth.** The ultimate, regulative ideal of the process. A proposition is objectively true if its principles are part of the real, emergent Apex Network: the objective structure of viable solutions. While this structure is never fully mapped, it functions as the formal standard that makes our comparative judgments of "more" or "less" brittle meaningful. It is the structure toward which the reduction of systemic costs forces our knowledge systems to converge.

This layered framework avoids a simplistic "Whig history" by recognizing that Justified Truth is a historically-situated achievement. Newtonian mechanics earned its Level 2 status by being a maximally low-brittleness system for its problem-space for over two centuries. Its replacement by relativity does not retroactively invalidate that status but shows the evolutionary process at work, where an expanding problem-space revealed pragmatic constraints that required a new, more viable system. This allows for sharp, non-anachronistic historical judgments: a claim can be justifiably true in its time (Level 2) yet still be objectively false (not Level 1) when judged against the Apex Network from the perspective of a more resilient successor.

### **4.3.1 How Propositions Become Truth Itself: Deflationism and the Hard Core**

The three-level framework reveals a profound insight about the nature of truth: propositions do not merely "correspond" to truth as an external standard but become constitutive of truth itself through a process of functional transformation and entrenchment.

**The Deflationary Insight**

Our framework aligns with deflationary theories of truth in a crucial respect: there is no substantial metaphysical property "truth" that exists independently of the propositions we validate. When we say "the proposition P is true," we are not attributing some mysterious non-natural property to P. Rather, we are acknowledging P's status within a well-functioning epistemic system.

However, our framework is not purely deflationary. It provides robust, naturalistic content to truth-attributions through the three-level structure:
- To say P is true (Level 2) is to say P is certified by a low-brittleness Consensus Network
- To say P is objectively true (Level 1) is to say P aligns with the emergent, constraint-determined Apex Network

This transforms deflationism from a purely linguistic doctrine into a substantive epistemological claim: truth is what survives systematic pragmatic filtering. The predicate "is true" tracks functional role within viable knowledge systems, not correspondence to a Platonic realm.

**From Validated Data to Constitutive Core: The Progression**

A proposition's journey to becoming truth itself follows a systematic progression through functional transformation:

1. **Initial Hypothesis (Being-Tested):** The proposition begins as a tentative claim within some Shared Network, subject to coherence constraints and empirical testing. It is data to be evaluated.

2. **Validated Data (Locally Proven):** Through repeated application without generating significant brittleness, the proposition earns trust. Its predictions are confirmed; its applications succeed. It transitions from hypothesis to validated data—something the network can build upon.

3. **Standing Predicate (Tool-That-Tests):** The proposition's functional core—its reusable predicate—is promoted to Standing Predicate status. It becomes conceptual technology: a tool for evaluating new phenomena rather than something being evaluated. "...is an infectious disease" becomes a diagnostic standard, not a claim under test.

4. **Convergent Core Entry (Functionally Unrevisable):** As all rival formulations are relegated to the Negative Canon after generating catastrophic costs, the proposition migrates to the Convergent Core. Here it achieves Level 2 status: Justified Truth. To doubt it now is to doubt the entire system's demonstrated viability.

5. **Hard Core (Constitutive of Inquiry Itself):** In the most extreme cases, a proposition becomes so deeply entrenched that it functions as a constitutive condition for inquiry within its domain. This is Quine's hard core—the principles so fundamental that their removal would collapse the entire edifice.

**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (Simon 1972).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:

- **Logic and Basic Mathematics:** Revising logic requires using logic to evaluate the revision (infinite regress). Revising basic arithmetic requires abandoning the conceptual tools needed to track resources, measure consequences, or conduct any systematic inquiry. These exhibit maximal brittleness-if-removed.

- **Thermodynamics:** The laws of thermodynamics undergird all engineering, chemistry, and energy policy. Revising them would invalidate centuries of validated applications and require reconstructing vast swaths of applied knowledge. The brittleness cost is astronomical.

- **Germ Theory:** After decades of successful interventions, public health infrastructure, medical training, and pharmaceutical development all presuppose germ theory's core claims. Revision would collapse these systems, generating catastrophic first-order costs.

**The Paradox Resolved: Fallibilism Without Relativism**

This creates an apparent paradox: how can we be fallibilists who acknowledge all claims are revisable in principle, while simultaneously treating hard core propositions as effectively unrevisable in practice?

The resolution lies in recognizing that "revisable in principle" means: *if we encountered sufficient pragmatic pushback, we would revise even hard core claims*. The qualification "sufficient" is doing crucial work. For hard core propositions, the threshold is extraordinarily high—perhaps requiring a wholesale breakdown of the systems they support. But it is not infinitely high.

This is precisely what makes the framework naturalistic rather than foundationalist. Hard core status is not metaphysical bedrock but functional achievement. A proposition becomes "truth itself" not by corresponding to eternal forms but by proving so indispensable to viable inquiry that its removal would be catastrophically costly.

**Truth as Emergent, Achieved Status**

On our account, truth is not discovered in a Platonic realm but achieved through historical filtering. Propositions become true by:
1. Surviving systematic application without generating brittleness
2. Migrating from peripheral hypotheses to core infrastructure
3. Becoming functionally indispensable to ongoing inquiry
4. Aligning with the emergent, constraint-determined Apex Network

This is why our framework can embrace both deflationary insights (truth is not a substantial property) and robust realism (some propositions align with objective constraint-determined structure). The predicate "is true" tracks functional role in viable systems, but viability itself is determined by mind-independent pragmatic constraints.

When we say "thermodynamics is true," we are not making a claim about correspondence to metaphysical facts. We are acknowledging that thermodynamics has survived exhaustive testing, migrated to the hard core of physics, and aligned with the constraint-determined structure that any viable physics must approximate. Its truth is its achieved, demonstrated indispensability within maximally viable inquiry.

This resolves the classical tension between Quine's holism (all claims are revisable) and the practical unrevisability of core principles: both are true because they describe different aspects of the same evolutionary process through which propositions earn their status by proving their viability under relentless pragmatic pressure.

### **4.4 The Evolving Structure of Knowledge: Convergent Core and Pluralist Frontier**

The historical process of pragmatic filtering gives our evolving Consensus Networks a discernible structure, which can be understood as having two distinct epistemic zones. This distinction is not about the nature of reality itself, but describes the justificatory status of our claims at a given time.

* **The Convergent Core:** This represents the load-bearing foundations of our current knowledge. It comprises domains where the relentless pressure of pragmatic selection has eliminated all known rival formulations, leaving a single, or functionally identical, set of low-brittleness principles. Principles reside in this core—such as the laws of thermodynamics or the germ theory of disease—not because they are dogmatically held or self-evident but because all tested alternatives have been relegated to the Negative Canon after generating catastrophically high systemic costs. While no claim is immune to revision in principle, the principles in the Convergent Core are functionally unrevisable in practice, as doing so would require dismantling the most successful and resilient knowledge structures we have ever built. A claim from this core achieves the highest degree of justification we can assign, approaching our standard for Objective Truth (Level 1).
* **The Pluralist Frontier:** This describes the domains of active research where our current evidence is insufficient to decide between multiple, competing, and viable reconstructions of the landscape of viability. Here, rival systems (e.g., different interpretations of quantum mechanics or competing models of consciousness) may coexist, each with a demonstrably low and stable degree of brittleness. It is crucial to distinguish this constrained, evidence-based pluralism from relativism. The frontier is not an "anything goes" zone but a highly restricted space strictly bounded on all sides by the Negative Canon. A system based on phlogiston is not a "viable contender" on the frontier of chemistry but a demonstrably failed research program. This pluralism is therefore a sign of epistemic underdetermination: a feature of our map's current limitations, not reality's supposed indifference. This position resonates with pragmatist accounts of functional pluralism (e.g., Price 1992), which treat different conceptual frameworks as tools whose legitimacy is determined by their utility within a specific practice. Within this frontier, the core claims of each viable competing system can be granted the status of Justified Truth (Level 2). This is also the zone where non-epistemic factors, such as institutional power or contingent path dependencies, can play their most significant role, sometimes artificially constraining the range of options explored or creating temporary monopolies on what is considered justified.

### **4.5 Illustrative Cases of Convergence and Brittleness**

The transition from Newtonian to relativistic physics offers a canonical example of this framework's diagnostic application. After centuries of viability, the Newtonian system began to accumulate significant systemic costs in the late 19th century. These manifested as first-order predictive failures, such as its inability to account for the perihelion of Mercury, and as rising conceptual debt in the form of ad-hoc modifications like the Lorentz-FitzGerald contraction hypothesis. The Einsteinian system proved a more resilient solution, reducing this conceptual debt and substantially lowering the systemic costs of inquiry in physics.

A more contemporary case can be found in the recent history of artificial intelligence, which illustrates how a brittleness assessment might function in real time. The periodic "AI winters" can be understood as the collapse of high-brittleness paradigms, such as symbolic AI, which suffered from a high rate of ad-hoc modification when faced with novel challenges. While the subsequent deep learning paradigm proved a low-brittleness solution for many specific tasks, it may now be showing signs of rising systemic costs. These can be described conceptually as, for example, potentially unsustainable escalations in computational and energy resources for marginal performance gains, or an accelerating research focus on auxiliary, post-hoc modifications rather than on foundational architectural advances. This situation illustrates the Pluralist Frontier in action, as rival architectures might now be seen as competing to become the next low-brittleness solution.

### **4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power**

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape. Powerful institutions can define what counts as a legitimate problem, control research funding to suppress rival networks, and entrench the very path dependencies that reinforce a fitness trap. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, exemplifies this state. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability therefore requires a balance between low-cost stability and adaptive capacity. This evolutionary perspective completes our reef chart, not as a finished map, but as an ongoing process of hazard detection and channel discovery.

## **5. Applications and Dynamics**

### **5.1 Animating the Web of Belief**

Quine's static "Web of Belief" lacks dynamics; this section provides the physiology. Successful propositions migrate from periphery to core by reducing brittleness. For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now catastrophically costly.

Driven by bounded rationality (Simon 1972), this "systemic caching" entrenches proven principles to avoid re-derivation costs. Core principles achieve Justified Truth (Level 2) via low-brittleness certification.

This animates Quine's web: pragmatic pushback provides externalist grounding, entrenchment explains core construction (Carlson 2015). Together, they transform the static web into a dynamic reef chart, where propositions earn their place through demonstrated navigational success.

### **5.2 Mathematics as a Paradigm Case of Internal Brittleness**

Naturalistic epistemologies often treat mathematics as a boundary problem—how can we account for a priori knowledge without appealing to Platonic forms? This framework shows mathematics is not a problematic edge case but a **paradigm demonstration** of how pragmatic selection operates in purely abstract domains.

**The Core Insight:** Mathematical frameworks face pragmatic pushback through internal inefficiency rather than external falsification.

#### **5.2.1 Diagnostic Indicators of Mathematical Brittleness**

**M(t): Proof Complexity Escalation**
- Increasing proof length without proportional explanatory gain
- Measured as: average proof length for theorems of comparable scope over time
- Rising M(t) signals degenerating research program

**P(t): Conceptual Debt Accumulation (proxied by Axiom Proliferation)**
- Ad-hoc modifications to patch paradoxes or anomalies
- Measured as: ratio of new axioms added to resolve contradictions vs. axioms generating novel theorems
- High P(t) indicates conceptual debt accumulation

**C(t): Contradiction Suppression Costs**
- Resources devoted to preventing or managing paradoxes
- Measured as: proportion of research addressing known anomalies vs. extending theory
- High C(t) reveals underlying fragility

**R(t): Unification Power**
- Ability to integrate diverse mathematical domains under common framework
- Measured as: breadth of cross-domain applicability
- Declining R(t) signals fragmentation and loss of coherence

The abstract costs in mathematics can be operationalized using our diagnostic toolkit.

To illustrate these metrics in action, consider brief examples of mathematical progress as brittleness reduction:

**Non-Euclidean Geometry:**
- Euclidean geometry: high brittleness for curved space applications
- Required elaborate patches (epicycles in astronomy)
- Non-Euclidean alternatives: lower brittleness for cosmology, general relativity
- **Pattern:** Replace high-brittleness framework with lower-brittleness alternative when domain expands

**Calculus Foundations:**
- Infinitesimals: intuitive but theoretically brittle (paradoxes of the infinite)
- Epsilon-delta formalism: higher initial complexity but lower long-term brittleness
- Historical adoption pattern follows brittleness reduction

**Category Theory:**
- More abstract/complex than set theory
- But lower brittleness for certain domains (algebraic topology, theoretical computer science)
- Adoption follows domain-specific viability assessment

#### **5.2.2 Case Study: Brittleness Reduction in Foundations—The Crisis of Russell's Paradox**

**Naive Set Theory (pre-1901):**
- M(t): Moderate—proofs reasonably concise
- R(t): Exceptional—unified logic, number theory, analysis
- Apparent low brittleness

**Russell's Paradox (1901):**
- Revealed infinite brittleness: the theory could derive contradiction
- All inference paralyzed (if A and ¬A both derivable, anything follows)
- Complete systemic collapse

**Response 1: ZF Set Theory (Zermelo-Fraenkel + Axiom of Choice)**
- Added axioms (Replacement, Foundation, etc.) to block paradox
- M(t): Increased (more axioms, more complex proofs)
- P(t): Moderate (axioms serve multiple purposes beyond patching)
- C(t): Low (paradox resolved, no ongoing suppression needed)
- R(t): High (maintained unifying power)
- **Diagnosis:** Successful low-brittleness resolution through principled modification

**Response 2: Type Theory (Russell/Whitehead)**
- Stratified hierarchy preventing self-reference
- M(t): High (complicated type restrictions on proofs)
- P(t): Low (structural solution, not ad-hoc)
- C(t): Low (paradox structurally impossible)
- R(t): Moderate (some mathematical domains resist type stratification)
- **Diagnosis:** Alternative low-brittleness solution with different trade-offs

**Response 3: Paraconsistent Logic**
- Accept contradictions but control explosion
- M(t): Variable (depends on implementation)
- P(t): Very High (many special rules to prevent collapse)
- C(t): Very High (constant management of contradictions)
- R(t): Low (marginal adoption, limited domains)
- **Diagnosis:** Higher brittleness—requires ongoing suppression costs

**Historical Outcome:** Mathematical community converged primarily on ZF, with Type Theory for specific domains. Paraconsistent approaches remain peripheral. This convergence reflects differential brittleness, not arbitrary preference.

#### **5.2.3 Power and Suppression in Mathematical Practice**

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches (e.g., career punishment, publication barriers for heterodox foundations), this incurs measurable **Coercive Overheads (C(t))**. These costs manifest as innovation lags, fragmentation into splinter communities, and delayed discoveries as useful insights are marginalized for sociological, not technical, reasons. The historical resistance to non-standard analysis, despite its applications, is a potential example of such a brittleness-inducing suppression.

Logic isn't metaphysically privileged—it's functionally indispensable.

**The Entrenchment Argument:**
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (Simon 1972) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could be revised if we encountered genuine pragmatic pressure
- Some quantum logics represent such revisions
- But the cost threshold is exceptionally high
- Most "apparent" logic violations turn out to be scope restrictions rather than genuine revisions

Addressing feminist epistemology (Harding 1991), mathematical communities can suppress alternatives through institutional power, generating measurable brittleness indicators:

**Coercive Overhead in Mathematics:**
- Career punishment for heterodox approaches
- Publication barriers for alternative foundations
- Curriculum monopolization by dominant frameworks
- Citation exclusion of rival approaches

**Measurable Costs:**
- **Innovation lag:** Talented mathematicians driven from field when approaches rejected for sociological rather than technical reasons
- **Fragmentation:** Splinter communities forming alternative journals, departments
- **Inefficiency:** Duplication of effort as alternative approaches can't build on dominant framework's results
- **Delayed discoveries:** Useful insights suppressed for decades (e.g., non-standard analysis resisted despite applications)

**The Brittleness Signal:** When a mathematical community requires high coercive costs to maintain orthodoxy against persistent alternatives, this signals underlying brittleness—the dominant framework may not be optimally viable.

**Historical Example:** Intuitionist vs. classical mathematics
- Intuitionists demonstrated genuine technical alternatives
- Classical community initially suppressed through institutional power
- High coercive costs (career barriers, publication difficulties)
- Eventual accommodation as constructive methods proved valuable
- **Diagnosis:** Initial suppression revealed brittleness in classical community's claim to unique optimality

Mathematics demonstrates the framework's universality. All domains—physical, social, mathematical—face pragmatic selection. The feedback mechanism varies:
- **Physical sciences:** External prediction vs. observation
- **Social systems:** Demographic/economic/stability costs
- **Mathematics:** Internal coherence, proof efficiency, unification power

But the underlying filter is identical: systems accumulating brittleness are replaced by more viable alternatives. The Apex Network spans all domains because constraint-determined optimal structures exist in all domains.

Mathematics is not a special case requiring different epistemology—it's a pure case showing how pragmatic selection operates when feedback is entirely internal to the system.



## **6. Situating the Framework in Contemporary Debates**

This paper has developed what can be termed **Systemic Externalism**—a form of externalist epistemology that locates justification not in individual cognitive processes but in the demonstrated reliability of entire knowledge systems. This section clarifies the framework's position within contemporary epistemology by examining its relationship to four major research programs: coherentist epistemology, social epistemology, evolutionary epistemology, and neopragmatism.

### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

While internalist coherentists like Carlson (2015) have successfully shown *that* the web must have a functionally indispensable core, they lack the resources to explain *why* that core is forged by external discipline. Systemic Externalism provides this missing causal engine, grounding Carlson's internal structure in an externalist history of pragmatic selection.

This epistemological challenge mirrors metaphysics' emergentist dilemma (Bennett-Hunter 2015): balancing dependence on base with genuine novelty. Systemic Externalism resolves it by grounding web structure in historical pragmatic selection. Justification requires coherence plus network reliability via low brittleness. Unlike static network epistemology (Zollman 2013; Rosenstock et al. 2017), our model examines evolving networks under pushback, providing objective externalist history.

#### **6.1.1 A Naturalistic Engine for Structural Realism**

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive, countering pessimistic induction.

This approach provides an evolutionary, pragmatic engine for the core claims of Ontic Structural Realism (OSR) (cf. Ladyman & Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how and why our scientific practices are forced to converge on these objective structures. The 'rainforest of structures' is selectively thinned by pragmatic filtering, with only the low-brittleness, viable structures (the Apex Network) surviving in the long run. Our framework thus naturalizes OSR's metaphysical claims by grounding them in a testable, historical process.

Ontologically, the Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology. Epistemologically, we discover this structure through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs. Low-brittleness networks survive, triangulating toward the objective structure. This provides the causal mechanism structural realism lacks, grounding its claims in a testable, evolutionary process.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). While structural realism persuasively argues that relational structures are preserved across paradigm shifts, it has struggled to provide a non-miraculous, causal mechanism for how our contingent historical practices reliably converge on these objective structures. Emergent Pragmatic Coherentism provides precisely this missing engine. The eliminative process of pragmatic filtering is the naturalistic mechanism that forces our fallible theories to align with the objective relational structure of the Apex Network. This counters pessimistic induction: theories don't fail randomly; the Negative Canon shows systematic elimination of high-brittleness systems, yielding convergent improvement. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through mysterious insight, but through pragmatic selection. High-brittleness networks misalign with viability, generating unsustainable costs and entering the Negative Canon. Low-brittleness networks survive. Over time, this selective pressure forces Consensus Networks to conform to the objective structure.

### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected.

### **6.3 Distinguishing from Lakatos and Laudan**

While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of risk and resilience. A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

## **7. Final Defense and Principled Limitations**

A philosophical model is best judged by its ability to resolve the very paradoxes that plague its predecessors. Before defending against external objections, we must first clarify the relationship between this macro-epistemology and individual justification. The framework does not primarily aim to solve traditional micro-epistemological problems; instead, it provides a robust theory of **higher-order evidence**. The diagnosed brittleness of a knowledge system provides a powerful defeater (or corroborator) for an individual's beliefs derived from that system. In a Bayesian framework, the diagnosed health of a source network should determine an agent's rational prior probability. A claim from a low-brittleness network (e.g., the IPCC) warrants a high prior; a claim from a high-brittleness network (a conspiracy theory) warrants a low one. The macro-level diagnosis thus provides a rational, non-circular basis for an individual's allocation of epistemic trust. Having established this bridge, we now turn to the framework's falsifiability and principled limitations.

### **7.1 A Falsifiable Research Program**

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Turchin 2003), support this link.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Mallapaty 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

### **7.2 Principled Limitations and Scope**

Philosophical honesty requires acknowledging not just what a framework can do, but what it cannot. This section confronts the framework's real limitations and the genuine costs of its commitments. These are not flaws to be apologized for but principled choices about scope and method.

#### **7.2.1 Species-Specific Objectivity**

**The Limitation:** Moral and epistemic truths are objective for creatures with our biological and social architecture (extended childhood requiring cooperation, limited cognition requiring trust, biological constraints of mortality and suffering, social coordination problems). Hypothetical beings with radically different structures would face different constraints and discover different optimal configurations.

**Example:** Beings that reproduce through fission (no childhood, no parenting), have perfect telepathic communication (no trust problems), experience no pain (no suffering to minimize), or have perfect memory (no institutional needs) would discover different Apex Network structure. Their moral truths would differ from ours.

**Why We Accept This:** This is relativism at the species level, not cultural level: analogous to how chemistry is objective within baryonic matter but might differ for exotic matter. It's appropriate epistemic modesty. Claims are grounded in actual evidence about actual constraint structures we face, not speculation about hypothetical beings.

**What This Preserves:** Objectivity within our domain—all humans across all cultures face the same constraint structure and will discover the same Apex Network. Cross-cultural convergence on reciprocity norms, harm prohibitions, and property conventions reflects this constraint-determined necessity.

#### **7.2.2 Learning Through Catastrophic Failure**

**The Limitation:** We learn moral truths primarily through catastrophic failure. The Negative Canon is written in blood. We could not know slavery's wrongness without the historical experiment generating centuries of suffering. Future moral knowledge will require future suffering to generate data.

**Why We Accept This:** This is how empirical knowledge works in complex domains. Medicine required harmful experiments before ethical review boards. Engineering required catastrophic bridge failures before developing safety factors. There is no shortcut bypassing human cost—complex system behavior cannot be fully predicted from first principles.

**What This Implies:**
- Moral learning is necessarily slow
- Each generation will make errors future generations recognize
- We should be epistemically humble about current certainties
- But we can have high confidence in Negative Canon entries (exhaustively tested failures)

#### **7.2.3 Floor Not Ceiling**

**The Limitation:** The framework maps necessary constraints (the floor), not sufficient conditions for flourishing (the ceiling). It cannot address what makes life meaningful beyond sustainable, supererogatory virtue and moral excellence, aesthetic value and beauty, or the difference between a decent life and an exemplary one.

**Why We Accept This:** Appropriate scope limitation. The framework does what it does well rather than overreaching. It identifies catastrophic failures and boundary conditions, leaving substantial space for legitimate value pluralism above the floor.

**What This Implies:** The framework provides necessary but not sufficient conditions. Thick theories of the good life must build on this foundation. The Pluralist Periphery is real: multiple flourishing forms exist, but all must respect the floor (avoid Negative Canon predicates).

#### **7.2.4 Expert Dependence and Democratic Legitimacy**

**The Limitation:** Accurate brittleness assessment requires technical expertise in historical analysis, statistics, comparative methods, systems theory. This creates epistemic inequality: not everyone has equal access to moral knowledge.

**Why We Accept This:** Similar to scientific expertise generally. Complex systems require specialized knowledge to evaluate. Nuclear safety, climate science, and epidemiology all face the same challenge.

**The Democratic Challenge:** If moral truth requires expert assessment, how do we maintain democratic legitimacy?

**Partial Mitigation Strategies:**
- Public accessibility of evidence (data transparency)
- Distributed expertise (community-based participatory research)
- Standpoint epistemology (marginalized groups as expert witnesses to brittleness)
- Institutional design (independent assessment boards, adversarial review)
- Education (improving general systems literacy)

**Honest Acknowledgment:** These strategies reduce but do not eliminate the problem. Some epistemic inequality is unavoidable when evaluating complex systems.

#### **7.2.5 Discovery Requires Empirical Testing**

**The Limitation:** While the Apex Network exists as a determined structure, discovering it requires empirical data. We cannot deduce optimal social configurations from first principles alone—we need historical experiments to reveal constraint topology.

**Why We Accept This:** Even in physics and mathematics, we need empirical input. Pure reason can explore logical possibilities, but determining which possibilities are actual requires observation or experiment. For complex social systems with feedback loops and emergent properties, this dependence is stronger.

**What This Allows:** Prospective guidance through constraint analysis. We can reason about likely optimal solutions by analyzing constraint structure, but we need empirical validation. This is stronger than pure retrospection but weaker than complete a priori knowledge.

#### **7.2.6 The Viable Evil Possibility**

**The Limitation:** If a deeply repugnant system achieved genuinely low brittleness (minimal coercive costs, stable demographics, innovation, adaptation), the framework would have to acknowledge it as viable, though not necessarily just by other standards.

**Example:** Hypothetical perfectly internalized caste system where:
- Lower castes genuinely accept their position (minimal coercion)
- No measurable demographic stress
- Stable innovation rates
- Low enforcement costs
- But intuitively morally repugnant

**Why We Accept This:** Intellectual honesty. The framework is incomplete—it maps pragmatic viability, not all moral dimensions. If such a system existed (we doubt it does—internalization itself has costs), it would fall in Pluralist Periphery, not Negative Canon.

**Empirical Bet:** We predict that such systems are not merely repugnant but are, in fact, inherently brittle. Critics might point to the apparent long-term stability of systems like the Ottoman Empire's devşirme system or historical caste systems in India as counterexamples. However, our framework predicts, and historical analysis confirms, that such systems exhibit high underlying brittleness masked by coercive power. Their apparent stability was purchased at the cost of massive **Coercive Overheads (C(t))**—such as the resources spent on enforcing purity laws, suppressing revolts, and managing internal dissent—and they consistently demonstrated innovation lags and fragility in the face of external shocks, confirming their position on the landscape of non-viability (cf. Acemoglu & Robinson 2012; Turchin 2003). True internalization without coercion is rare and resource-intensive, while oppression reliably generates hidden costs that emerge under stress.

**But:** If empirics proved otherwise, we would acknowledge the framework's incompleteness rather than deny evidence.

#### **7.2.7 What We Claim**

These limitations do not undermine the framework's contribution—they define appropriate scope. EPC excels at:

**Strong Claims:**
- Identifying catastrophic systemic failures
- Explaining moral progress as empirically detectable debugging
- Grounding realism naturalistically without non-natural properties
- Providing empirical tools for institutional evaluation
- Offering prospective guidance through constraint analysis
- Unifying descriptive and normative epistemology

**Modest Claims:**
- Does not provide complete ethics
- Does not solve all normative disagreements
- Does not eliminate need for judgment
- Does not achieve view-from-nowhere objectivity
- Does not offer categorical imperatives independent of systemic goals

**The Value Proposition:** A powerful but limited diagnostic tool for systemic health. Use it for what it does well. Supplement with other resources for what it cannot address. Do not expect a complete theory of human flourishing—expect robust tools for avoiding catastrophic failure and identifying progressive change.

This honest accounting strengthens rather than weakens the framework's philosophical contribution.

## **8. Conclusion: The Practical Payoff**

This paper has developed Emergent Pragmatic Coherentism as a response to the isolation objection facing coherentist theories of justification. By grounding coherence in the long-term viability of knowledge systems rather than internal consistency alone, the framework provides the external constraint that coherentism requires while preserving its holistic insights. The concept of systemic brittleness offers a naturalistic diagnostic tool for evaluating epistemic systems, while the notion of a constraint-determined Apex Network explains how objective knowledge can arise from fallible human practices.

Our framework for assessing systemic brittleness makes this process analyzable. By systematically studying the record of failed systems, we can begin to discern the contours of the Apex Network: the emergent set of maximally convergent, pragmatically indispensable principles that successful inquiry is forced to discover. The result is a form of Systemic Externalism that offers a novel approach for resolving long-standing problems in post-Quinean epistemology by grounding justification in the non-discursive filter of systemic consequences.

This model is not presented as a final, complete system but as the foundation for a progressive and falsifiable research program. Critical future challenges remain, such as fully modeling the role of power asymmetries in creating path-dependent 'fitness traps' and applying the framework to purely aesthetic or mathematical domains. These are not flaws in the model but precisely the kinds of tractable, empirical questions it is designed to generate.

We began with the challenge of distinguishing viable knowledge from brittle dogma in an era of epistemic crisis. The model we have developed suggests the ultimate arbiter is not the elegance of a theory or the consensus of its adherents but the trail of consequences it leaves in the world. While this framework operates at a high level of abstraction, its primary data streams originate from the ground up. As we have argued, systemic costs are ultimately experienced by individuals as suffering, instability, and the frustration of human goals. Dissent, friction, and protest are therefore not merely political problems but primary sources of epistemological data about a system's rising brittleness. This framework, in this light, is best understood as a tool for a democratic public to hold its own knowledge-generating systems accountable. By making the indicators of systemic health part of a shared, evidence-based language, it helps us ask the most pragmatic question of all: "Is this way of thinking, this way of organizing ourselves, still working for us?"

Ultimately, this framework calls for a form of epistemic humility. The practical payoff of this framework is a more reliable method for navigating our epistemic landscape. It provides the tools to build a better reef chart, allowing us to steer clear of the hazards that have wrecked past inquiries and to more confidently explore the channels of viable knowledge.

Furthermore, the brittleness toolkit isn't just for philosophers; it could and should be used by institutional designers, science funders, and policy analysts to assess the structural health of our most critical knowledge-producing institutions in real time. This positions the paper not just as a solution to an old philosophical puzzle, but as a practical and urgent research program for the 21st century.

## **Appendix A: Normative Brittleness as a Speculative Extension**

The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers. While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."



## **References**

Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business.

Baggio, Guido, and Andrea Parravicini. 2019. "Introduction to Pragmatism and Theories of Emergence." *European Journal of Pragmatism and American Philosophy* XI-2. https://doi.org/10.4000/ejpap.1251.

Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110, no. 1: 1–20. https://doi.org/10.1111/phpr.70057.

Bennett-Hunter, Guy. 2015. "Emergence, Emergentism and Pragmatism." *Theology and Science* 13, no. 3: 337–57. https://doi.org/10.1080/14746700.2015.1053760.

Berlin, Brent, and Paul Kay. 1969. *Basic Color Terms: Their Universality and Evolution*. Berkeley: University of California Press.

BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press.

Bradie, Michael. 1986. "Assessing Evolutionary Epistemology." *Biology \& Philosophy* 1, no. 4: 401–59. https://doi.org/10.1007/BF00140962.

Brandom, Robert B. 1994. *Making It Explicit: Reasoning, Representing, and Discursive Commitment*. Cambridge, MA: Harvard University Press.

Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.

Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3, no. 5: 1–27. https://doi.org/10.22329/jhap.v3i5.3142.

El-Hani, Charbel N., and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3, no. 2, article 3. http://commons.pacificu.edu/eip/vol3/iss2/3.

Gadamer, Hans-Georg. 1975. *Truth and Method*. New York: Seabury Press.

Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel.

Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press.

Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell.

Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press.

Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press.

Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.

Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.

Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.

Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press.

Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press. Originally published 1962.

Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50, no. 1: 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.

Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press.

Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific Research Programmes." In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.

Laudan, Larry. 1977. *Progress and Its Problems: Towards a Theory of Scientific Growth*. Berkeley: University of California Press.

Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press.

Lynch, Michael P. 2009. *Truth as One and Many*. Oxford: Oxford University Press.

Mallapaty, Smriti. 2020. "What the COVID Pandemic Reveals About the Paper-Thin Line Between ‘Data’ and ‘Models’." *Nature* 583: 501–2. https://doi.org/10.1038/d41586-020-02276-1.

Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing.

Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press.

Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press.

Moghaddam, Soroush. 2013. "Confronting the Normativity Objection: W.V. Quine’s Engineering Model and Michael A. Bishop and J.D. Trout’s Strategic Reliabilism." Master's thesis, University of Victoria.

Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press.

Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press. Originally published 1878.

Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37, no. 7: 1948–70. https://doi.org/10.1080/09515089.2023.2236120.

Pritchard, Duncan. 2016. *Epistemic Risk*. Oxford: Oxford University Press.

Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson. Originally published 1934.

Price, Huw. 1992. "Metaphysical Pluralism." *Journal of Philosophy* 89, no. 8: 387–409. https://doi.org/10.2307/2940975.

Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press.

Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60, no. 1: 20–43. https://doi.org/10.2307/2181906.

Quine, W. V. O. 1960. *Word and Object*. Cambridge, MA: MIT Press.

Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press.

Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press.

Rosenstock, Sarita, Cailin O'Connor, and Justin Bruner. 2017. "In Epistemic Networks, Is Less Really More?" *Philosophy of Science* 84, no. 2: 234–52. https://doi.org/10.1086/690641.

Rottschaefer, William A. 2012. "The Moral Realism of Pragmatic Naturalism." *Analyse \& Kritik* 34, no. 1: 141–56. https://doi.org/10.1515/ak-2012-0107.

Simon, Herbert A. 1972. "Theories of Bounded Rationality." In *Decision and Organization*, edited by C. B. McGuire and Roy Radner, 161–76. Amsterdam: North-Holland Publishing Company.

Snow, John. 1855. *On the Mode of Communication of Cholera*. London: John Churchill.

Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91, no. 2: 430–48. https://doi.org/10.1017/psa.2023.104.

Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House.

Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä.

Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding, and Selection in Evolution." In *Proceedings of the Sixth International Congress of Genetics*, edited by Donald F. Jones, 356–66. Ithaca, NY: Brooklyn Botanic Garden.

Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press.

Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press.

Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43, no. 1–2: 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.

Zollman, Kevin J. S. 2013. "Network Epistemology: Communication in the History of Science." *Philosophy Compass* 8, no. 1: 15–27. https://doi.org/10.1111/phc3.12021.