# **Convergent Coherence: A Pragmatic, Externalist Account of Objectivity**

**Abstract**

Coherentist theories of justification face the isolation objection: a belief system could be internally consistent yet detached from reality. This paper develops Emergent Pragmatic Coherentism, an externalist framework that resolves this by grounding coherence in demonstrated long-term viability. We introduce *systemic brittleness*—the observable costs a system incurs when applying its principles—as a diagnostic for assessing knowledge system health. Persistent, mind-independent pragmatic constraints create selective pressure that filters out brittle systems. This eliminative process discovers, rather than creates, an objective structure of viable principles, termed the **Apex Network**. This structure emerges necessarily from reality's constraint topology, existing independently of discovery. Justification requires both internal coherence within a collective system and that system's demonstrated resilience against pragmatic consequences. This systemic externalism provides a naturalistic account of objectivity, where truth aligns with this emergent structure of viability.

### **1. Introduction: From a Static Web to a Dynamic Process**

#### **1.1 The Isolation Objection and the Quinean Web**

Why did germ theory replace miasma theory? Standard accounts cite superior evidence, but a deeper view reveals systemic viability. Miasma theory's sanitation focus yielded some benefits, yet its principles were failing. The miasma network proved brittle: it caused catastrophic costs—thousands died in London from misdirected efforts against odors—and demanded accelerating ad hoc fixes for anomalies, like why "bad air" killed only near certain pumps. Germ theory, however, was resilient and adaptive, slashing costs through effective interventions and explaining phenomena with one tool.

This historical dynamic illustrates a persistent challenge for coherentist theories of justification: the isolation objection. As Laurence BonJour (1985) influentially argued, a belief system could achieve perfect internal consistency while remaining entirely detached from reality, a problem that purely internalist resources seem unable to resolve. While some coherentists have developed sophisticated internalist responses (Olsson 2005; Kvanvig 2012), and others have argued for a functionally differentiated structure within Quine's web of belief where some beliefs are systematically fundamental (Carlson 2015), the question of what *external pressures* forge this structure in a non-circular way remains unanswered.

#### **1.2 The Pragmatic Turn: A Proposal for Systemic Externalism**

This paper offers an externalist, pragmatic response. Knowledge system justification is disciplined by long-term viability, assessed via real-world costs from applying principles. This preserves coherentism's holism while adding external checks. Justification demands two elements: internal coherence within a shared network, and that network's demonstrated reliability through low systemic brittleness.

The framework's key claim is not just historical. Pragmatic filtering discovers, not creates, an objective structure emerging necessarily from reality's constraint topology. Constraints—physical laws, biological limits, logic, social necessities—form a solution space with optimal configurations. These, the **Apex Network**, exist independently, like a physical system's lowest energy state.

This reframes the isolation objection. A coherent system detached from reality is not just false but unstable, misaligned with constraint topology. Flat-earth cosmology incurs navigational costs; phlogiston chemistry builds conceptual debt. Inquiry filters out brittle systems, converging fallible knowledge on Apex Network approximations.

To clarify, viability differs from mere endurance. A coercive empire persisting is not viable but brittle; its longevity measures wasted energy suppressing instability. Viability is relational: capacity to solve problems with sustainably low costs. The framework treats power and contingency as variables, not exceptions. Power maintaining brittleness indicates non-viability via high coercive costs.

#### **1.3 Roadmap of the Argument**

This paper models inquiry as an evolutionary process aimed at cultivating viable, less fragile public knowledge systems. It is a macro-epistemology, a theory about the long-term viability of cumulative systems like science and law. The argument proceeds as a systematic construction. Section 2 introduces the diagnostic framework for assessing systemic health through the concept of brittleness and defines our core analytical units. Section 3 develops the logic of viability that drives this evolutionary process, grounding epistemic norms in pragmatic constraints. Section 4 presents the Apex Network as an emergent necessary structure, articulating the modal claims that ground this account of naturalistic objectivity. Section 5 situates the framework within contemporary epistemology, showing how it refines or corrects related research programs. Finally, Section 6 defends the framework against key objections while acknowledging its principled limitations.

### **2. A Diagnostic Framework for Systemic Health**

To explain why some knowledge systems evolve while others stagnate, we need tools to assess structural health. A naturalistic theory demands precise diagnostics beyond internal consistency, measuring resilience to real-world pressures. Our approach aligns with complex systems theory (Meadows 2008). This section builds the framework by tracing private beliefs into public tools.

#### **2.1 The Units of Analysis: From Belief to Public Tool**

Following naturalized epistemology (Goldman 1979; Kitcher 1993), we focus on public structures rather than private states. This shift enables tractable analysis of observable phenomena in systems beyond individual cognition. The process deflates from private beliefs to public tools.

The process starts with *belief*, a private state inaccessible to public knowledge theories. First, isolate testable content as a *proposition*: a falsifiable, linguistic claim for collective assessment. It must pass coherence testing—not mere logical consistency, but thick pragmatic risk analysis. A resource-constrained network asks: will this proposition raise or lower long-term brittleness?

Successful propositions become validated data. Exceptionally successful ones—dramatically cutting costs—gain status as **Standing Predicates**: reusable, trusted conceptual tools. Unlike formal logic predicates, these are pragmatic technologies unpacking proven actions. For example, validating 'cholera is infectious' created the predicate '...is infectious', mobilizing isolation, tracing, and pathogen searches. Standing is earned by historical cost reduction, turning tested data into testing tools.

These predicates form **Shared Networks**, observable from Quine's holism in social groups. A Shared Network is the emergent public architecture of coherent propositions and predicates shared for collective problem-solving. Networks nest; germ theory is a subset of modern medicine. Individual belief revisions yield public networks under pragmatic pressure.

#### **2.2 Pragmatic Pushback and Systemic Costs**

A shared network is active, pressured by *pragmatic pushback*—the systemic version of Quine's recalcitrant experience. It sums concrete consequences from applying principles: collapsing bridges, failed treatments, fragmented societies. This material feedback, not argument, creates two cost types.

**First-Order Costs** are direct consequences: failed predictions, wasted resources, instability (e.g., excess mortality)—signals of environmental misalignment. **Systemic Costs** are internal expenses managing first-order costs, revealing fragility. Key forms:

*   **Conceptual Debt:** Fragility from complex patches protecting flawed principles.
*   **Coercive Overheads:** Resources enforcing compliance and handling dissent, a data stream of unacceptable costs.

This diagnostic lens applies even in abstract domains. In theoretical mathematics, for example, pragmatic pushback manifests not as a collapsing bridge but as the accumulation of internal systemic costs, such as escalating proof complexity for marginal explanatory gain or the proliferation of ad-hoc axioms to manage paradoxes.

To operationalize brittleness assessment, we employ a toolkit of indicators that can be measured empirically:

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |
| **P(t)** | Conceptual Debt | Ratio of anomaly-resolution publications to novel-prediction publications |
| **C(t)** | Coercive Overhead | Ratio of security/suppression budget to productive/R&D budget |
| **M(t)** | Model Complexity | Rate of parameter/complexity growth vs. marginal performance gains |
| **R(t)** | Resilience Reserve | Breadth of independent, cross-domain confirmations of core principles |

To illustrate this toolkit in action, consider two brief examples.

**Case 1: Ptolemaic Astronomy (c. 1500 CE).** The system exhibited high and rising brittleness. **M(t)** was acute: its predictive machinery required ~80 epicycles, with new observations demanding 2–3 more each decade. This geometric escalation yielded diminishing returns, with predictive accuracy rising only marginally despite a massive increase in computational burden. **P(t)** was also high, as the vast majority of astronomical work was dedicated to resolving anomalies within the existing paradigm rather than generating novel, testable predictions.

**Case 2: Contemporary AI Development.** Current deep learning paradigms may be showing early signs of rising brittleness, inviting cautious comparison. **M(t)** is visible in the exponential escalation of parameter counts and computational resources for often marginal gains in performance (Sevilla et al. 2022). **P(t)** can be proxied by the proliferation of 'alignment' and 'safety' research, a significant portion of which functions as post-hoc patches for emergent anomalous behaviors, rather than generating new architectural capabilities. R(t) declines as AI remains isolated from broader scientific integration, with limited cross-domain applications beyond narrow tasks. These trends suggest potential warning signs of rising brittleness in deep learning, which may invite cautious comparison to the structural dynamics of past degenerating research programs like late-stage Ptolemaic astronomy.

While our framework focuses on epistemic brittleness in descriptive knowledge systems, the concept could be extended to normative domains as "normative brittleness"—a measure of misalignment with the constraints on stable human cooperation. This extension, while promising, introduces controversial metaethical commitments and is reserved for future research.



#### **2.3 Preempting the Circularity Objection: A Methodology of Constrained Interpretation**

The operationalization of this framework faces a significant objection: measuring systemic costs objectively—distinguishing "waste" from "investment," or "excess" mortality from a baseline—appears to require the very normative standards our theory aims to provide, creating a vicious circularity.

This circularity cannot be eliminated entirely, as all empirical assessment is theory-laden to some degree. However, it can be managed through a disciplined methodology we term **constrained interpretation**. This approach does not aim for an impossible, view-from-nowhere neutrality but for *pragmatic objectivity*: a level of objectivity sufficient for comparative assessment and institutional evaluation, achieved through a protocol that disciplines interpretive judgment. This protocol relies on three principles:

1.  **Physical-Biological Anchors:** Assessments are anchored in outcomes that register as failures across widely divergent theoretical frameworks. Demographic collapse, catastrophic infrastructure failure, or sustained mass mortality serve as relatively theory-neutral indicators of systemic breakdown. While the *interpretation* of these failures is contestable, their *status as failures* generally is not.

2.  **Comparative-Diachronic Judgments:** The methodology avoids absolute claims about a system's health. Instead, its power lies in relative and temporal comparisons. The key diagnostic questions are comparative: Is System A *more* brittle than its contemporary, System B, when facing similar pressures? Or they are diachronic: Is System A's brittleness *rising* over time? These judgments can be made robustly even when absolute standards are unavailable.

3.  **Convergent Evidence:** A robust diagnosis of brittleness requires agreement across multiple, independent indicators. For example, a research program is plausibly degenerating only if it exhibits rising conceptual debt (e.g., an accelerating rate of ad-hoc modifications), increasing model complexity for diminishing returns, *and* a growing need for institutional resources to suppress rival paradigms. Systematic convergence across these measures becomes increasingly difficult to dismiss as mere interpretive bias.

This methodology does not eliminate judgment, but it makes that judgment systematic, transparent, and accountable to multiple streams of evidence. It provides structured tools for a fallibilistic research program, not a mechanical algorithm for truth.

##### **2.3.1 How the Causal Hierarchy Addresses the Circularity Objection**

Critics object that classifying spending as "productive" vs. "coercive" requires prior normative commitments, making the framework circular. The causal hierarchy provides an operational solution through trajectory analysis rather than categorical definition.

**The Operational Protocol:**

**Step 1: Measurement Without Classification**
Track resource allocation over time without labeling it:
- Proportion to internal security/surveillance/enforcement (S)
- Proportion to infrastructure/health/education/R&D (P)
- Total resource base (R)

**Step 2: Correlate With First-Order Indicators**
Measure demographic and economic trajectories:
- Mortality rates (rising/stable/falling)
- Morbidity indicators
- Economic output per capita
- Innovation metrics (patents, new technologies, productivity gains)
- Population stability

**Step 3: Apply Diagnostic Rules**

A spending category functions as coercive overhead when increasing allocation correlates with rising First-Order Costs, the system requires accelerating investment to maintain baseline stability (diminishing returns), or reduction correlates with improved outcomes.

A spending category functions as productive investment when increasing allocation correlates with falling First-Order Costs, returns are constant or increasing, and it generates positive spillovers to other domains.

**Concrete Example: Criminal Justice Spending**

Society A doubles police budget (year 1: 2% GDP → year 10: 4% GDP):
- Crime rates: -40%
- Incarceration rate: -20%
- Homicide rate: -60%
- Community survey trust: +35%
- Recidivism: -25%
**Diagnosis:** Productive investment. Rising S correlates with falling First-Order Costs.

Society B doubles police budget (year 1: 2% GDP → year 10: 4% GDP):
- Crime rates: +5%
- Incarceration rate: +300%
- Homicide rate: -10%
- Community survey trust: -50%
- Social instability indicators: +60%
- By year 10, requires 6% GDP to maintain control
**Diagnosis:** Coercive overhead. Rising S correlates with rising total systemic costs despite some metrics improving.

**Why This Isn't Circular:**

The classification emerges from empirical correlation patterns, not from a priori definitions. We don't need to know "what policing essentially is" before measuring; we observe what specific spending patterns do to measurable outcomes over time.

The robustness comes from convergent evidence. A single metric (e.g., crime rate) can be ambiguous, but when demographic indicators, economic output, innovation rates, stability metrics, and coercive spending ratios all move in the same direction, the diagnosis becomes robust to interpretive variation.

##### **2.3.2 Triangulation Across Independent Baselines**

To further address the circularity, we employ triangulation across independent baselines for assessing systemic costs, particularly in domains where direct measurement is challenging.

**1. Comparative-Historical Baseline:** Compare outcomes across contemporaneous societies with similar technology, resources, and environmental constraints. A society with 50% child mortality when peers achieve 30% exhibits measurable excess relative to its era's pragmatic constraints.

**2. Trajectory Analysis:** Track whether key indicators are rising (degrading), stable (maintaining), or falling (improving). When System A shows rising mortality while System B shows falling mortality under comparable conditions, we have empirical grounds for diagnosis without absolute baselines.

**3. Demographic Viability Thresholds:** Some thresholds are biologically determinate: Total Fertility Rate < 2.1 indicates population decline; infant mortality > 30% signals demographic stress; life expectancy < 30 indicates crisis. These are structural constraints, not normative judgments.

**Triangulation Methodology:** Diagnose "excess" through convergent evidence across baselines. Systematic convergence across independent indicators provides robust diagnosis, as in empirical science where multiple experimental paradigms confirm a theory.

### **3. The Logic of Viability: A Naturalistic Engine for Inquiry**

The diagnostic framework detailed in Section 2 is not an arbitrary set of metrics. Its indicators are effective because they are the observable outputs of the selective pressures that drive the evolution of knowledge systems. This section explains the logic of that selective process, showing how it grounds epistemic norms in pragmatic necessity and transforms the abstract notion of coherence into a concrete tool for risk assessment.

#### **3.1 Grounding Epistemic Norms in Pragmatic Constraints**

Naturalistic epistemology faces the normativity objection: descriptive accounts of reasoning cannot prescribe how we ought to reason (Kim 1988). Pragmatism is accused of conflating epistemic with practical values like efficiency (Putnam 2002). Our framework grounds norms in structural conditions for cumulative inquiry success, not chosen values.

Following Quine, normative epistemology is engineering, with norms as hypothetical imperatives for practical goals (Moghaddam 2013). Our goal: cultivating low-brittleness systems. Authority rests on two arguments.

First, a **constitutive argument**: any system engaged in a cumulative, inter-generational project, such as science or law, must maintain sufficient stability to preserve and transmit knowledge. A system that systematically undermines its own persistence cannot, by definition, succeed at this project. The pressure to maintain a low-brittleness design is therefore not an optional value but an inescapable structural constraint on the practice of cumulative inquiry itself. Just as an architect cannot coherently reject the constraints of gravity, a community of inquirers cannot coherently adopt principles that reliably lead to the dissolution of that community.

Second, an **instrumental argument**: the framework makes a falsifiable, empirical claim that networks with a high and rising degree of measured brittleness are statistically more likely to collapse or require radical revision. From this descriptive claim follows a conditional recommendation: *if* an agent or institution has the goal of ensuring its long-term stability and problem-solving capacity, *then* it has a powerful, evidence-based reason to adopt principles that demonstrably lower its systemic brittleness.

This grounding goes deeper than mere instrumentalism. The end—viable inquiry—is not an arbitrary preference but a structural precondition for any system to participate in cumulative knowledge production. The means—low-brittleness principles—are themselves recursively constrained, as they must also demonstrate their own viability. This prevents purely expedient solutions and ensures that the resulting epistemic norms are responsive to objective pragmatic constraints. When the model describes one network as "better" or identifies "epistemic progress," these are not subjective value judgments but technical descriptions of systemic performance. A "better" network is one with lower measured brittleness and thus a higher predicted resilience against failure.

#### **3.2 Coherence as Forward-Looking Risk Assessment**

Under viability logic, coherence is thick, forward-looking cost-benefit analysis. Resource-constrained systems use heuristics to assess if a proposition raises or lowers brittleness. Traditional virtues are practical calculus principles:

*   **Logical Consistency:** A hedge against the infinite future costs of inferential paralysis.
*   **Explanatory Power:** A measure of a proposition’s potential return on investment, reducing future inquiry costs by paying down conceptual debt.
*   **Simplicity:** A direct measure of systemic overhead; overly complex propositions increase long-term maintenance costs.
*   **Evidential Support:** An assessment of integrative risk; a well-supported claim is a low-risk investment, unlikely to trigger a cascade of costly future revisions.

One might object that this account reduces scientific judgment to purely pragmatic considerations, ignoring theoretical virtues like explanatory depth or mathematical elegance. However, these virtues are themselves pragmatically valuable within our framework. Explanatory depth reduces future conceptual debt by unifying disparate phenomena under a single principle, while mathematical elegance often signals a structural efficiency that minimizes long-term maintenance costs. Our framework does not eliminate traditional theoretical virtues but rather explains their pragmatic function within the evolutionary process of knowledge development.

This forward-looking model of coherence also explains how revolutionary science is possible. When a dominant network begins to exhibit high and rising systemic brittleness—a state that corresponds to what Thomas Kuhn (1962) described as a "crisis"—the cost-benefit analysis for new propositions shifts dramatically. A radical new hypothesis, while having low coherence with the *specific principles* of the existing network, may promise a massive long-term reduction in the *systemic costs* that are crippling the incumbent paradigm. The new proposition is not accepted because it fits neatly with the old, failing parts, but because it offers a viable path to restoring low-brittleness for the system as a whole. Coherence, in this dynamic sense, is not a conservative check for conformity, but a pragmatic assessment of a proposition's potential to resolve systemic crisis.

### **4. The Emergent Architecture of Objectivity**

The logic of viability detailed in Section 3 provides the selective pressure that drives the evolution of our knowledge systems. This section builds the theory of objectivity that this dynamic process makes possible. We argue that the descriptive project of identifying high-brittleness systems provides a rigorous, empirical, and fundamentally negative methodology for charting the landscape of what is pragmatically viable.

#### **4.1 A Negative Methodology: Charting the Landscape of Failure**

Our account of objectivity begins not with a speculative vision of a final, positive truth, but with the most unambiguous form of empirical evidence available: large-scale systemic failure. Following a broadly Popperian insight, our most secure knowledge is often of what is demonstrably unworkable. While a single failed experiment can be debated, the collapse of an entire knowledge system—its descent into crippling inefficiency, intellectual stagnation, or institutional decay—provides a clear and non-negotiable data point.

The systematic analysis of these failures allows us to build what we term the **Negative Canon**: a robust, evidence-based catalogue of principles and network designs that have been empirically invalidated by the catastrophic systemic costs they reliably generate. This canon includes systems exhibiting *epistemic brittleness*, such as phlogiston chemistry or Lysenkoist biology, whose core principles generated catastrophic causal failures. It also includes systems exhibiting *normative brittleness*, such as chattel slavery, whose unsustainability was demonstrated by the immense and ever-rising coercive overheads required to manage the normative pushback they generated.

By charting what demonstrably fails, we are not merely learning what to avoid; we are effectively reverse-engineering the constraints of a real territory. The Negative Canon functions like a reef chart for inquiry. It is our empirically verified map of known hazards, marking the impassable terrain on the landscape of viability. This provides a hard, external boundary that disciplines all forms of inquiry and prevents a collapse into relativism. Crucially, the Canon reveals necessary features of the constraint landscape, not just historical accidents. When a structural principle, such as organizing labor through chattel ownership, generates catastrophic brittleness across maximally different societies, this reveals a deep fact about the constraints on viable human cooperation. The Canon thus provides prospective guidance by mapping the structural features of failure.

#### **4.2 The Apex Network: An Emergent Structure of Modal Necessity**

The historical process of filtering out high-brittleness systems is not merely destructive. As unviable designs are relegated to the Negative Canon, this pragmatic selection constructively reveals the contours of an objective structure that all successful inquiry is forced to approximate. This emergent structure is what we term the **Apex Network**. The isolation objection to coherentism assumes that a belief system could float free of reality. Our framework denies this possibility through a claim about emergent necessity: pragmatic constraints form a topology that necessarily generates optimal structures, which any viable system must approximate.

This argument proceeds in four steps:
1.  **Reality Imposes Non-Negotiable Constraints.** These constraints are mind-independent and diverse, including physical laws (e.g., thermodynamics), biological facts (e.g., human nutritional needs), logical requirements (e.g., non-contradiction), and the necessities of social coordination (e.g., solving collective action problems).
2.  **Constraints Generate a Solution Space.** This set of constraints forms a "solution space" or "fitness landscape" of possible configurations for knowledge systems. This landscape is not flat; it necessarily contains peaks of high viability (low-cost, sustainable configurations) and valleys of high brittleness (catastrophic, collapse-prone configurations).
3.  **Optimal Solutions Emerge Necessarily.** Given a constrained system, an optimal solution or a compact set of optimal solutions emerges necessarily from the constraint topology itself, existing whether or not anyone has calculated or discovered it. The most efficient design for a heat engine emerges from thermodynamic constraints; the shortest path between two points emerges from geometric constraints. These are not constructed by inquiry but are factual consequences of how the system is structured.
4.  **The Apex Network *is* that Optimal Structure.** The Apex Network is the name we give to this complete configuration space of maximally viable solutions to the constraint problem that reality poses.

On this view, historical filtering is the **discovery process**, not the creation mechanism. Failed systems are experiments that reveal where the landscape of viability drops off; successful systems triangulate toward the peaks. This makes the objectivity of our knowledge analogous to mathematical objectivity. Mathematicians discovered π through various contingent historical paths, but π itself is a necessary feature of Euclidean geometry. Similarly, different cultures may discover the principles of the Apex Network through different historical experiments, but the structure itself is a necessary feature of our world's constraint topology.

This gives the Apex Network counterfactual stability. If human history had unfolded differently, we would have discovered the same structure through alternative paths, because it is determined by the constraints, not by the contingent historical route taken. This is what makes the Apex Network an objective standard, not a historical artifact. Ontologically, it is best understood as an **emergent structural invariant**: a stable topology within the space of possible knowledge systems. It is real, but its reality is relational and structural, not substantial or Platonic.

#### **4.3 A Three-Level Framework for Truth**

This emergent structure grounds a fallibilist but realist account of truth, resolving a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but are two necessary components of a naturalistic epistemology, reframing truth as a status propositions earn through increasingly rigorous stages of validation.

*   **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent *within a specific Shared Network*, regardless of that network’s long-term viability. This level explains the internal rationality of failed systems, but it is insufficient for justification.
*   **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a **Consensus Network** that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism.
*   **Level 1: Objective Truth.** The ultimate, regulative ideal of the process. A proposition is objectively true if its principles are part of the real, emergent **Apex Network**. While this structure is never fully mapped, it functions as the formal standard that makes our comparative judgments of "more" or "less" brittle meaningful. It is the structure toward which the reduction of systemic costs forces our knowledge systems to converge.

This layered framework avoids a simplistic "Whig history." Newtonian mechanics earned its Level 2 status by being a maximally low-brittleness system for its problem-space for over two centuries. Its replacement by relativity does not retroactively invalidate that status; it shows the evolutionary process at work, where an expanding problem-space revealed pragmatic constraints that required a new, more viable system. A claim can thus be justifiably true in its time (Level 2) yet still be objectively false (not Level 1) when judged against the Apex Network from the perspective of a more resilient successor.

### **5. Animating the Web of Belief**

Quine’s "Web of Belief" provided a powerful, static model of confirmational holism, but it has been criticized for lacking a corresponding dynamic account of its formation and change. This section provides that dynamic physiology. It details the naturalistic process by which a successful discovery migrates from the tentative "periphery" of the web to its load-bearing "core," a process driven by the pragmatic pressures outlined in our framework.

A proposition is promoted to the core by demonstrating its immense value in lowering the entire network’s systemic brittleness. The principle of the Conservation of Energy, for example, began as a contested hypothesis. It migrated inward as it proved its indispensable explanatory power across mechanics, chemistry, and electromagnetism, unifying disparate domains and dramatically reducing the conceptual debt of nineteenth-century physics. As its revision became increasingly costly, it became a default assumption embedded in the very infrastructure of science—its formalisms, instruments, and pedagogy. Its position in the core is now a direct measure of the catastrophic rise in systemic brittleness that its removal would cause.

This process is driven by a powerful, naturalistic pressure. As Herbert Simon (1972) argued, real-world agents and systems operate under bounded rationality; they have finite time, attention, and computational resources. The migration of proven principles to the core is a form of systemic caching. By entrenching its most successful discoveries as default assumptions, a resource-constrained system avoids the crippling cost of re-deriving everything from first principles for every new problem.

This pragmatic physiology provides the two mechanisms needed to animate Quine’s static web. First, it supplies a robust externalist filter—pragmatic pushback—that grounds the web in a world of non-discursive consequences, decisively solving the isolation objection that haunts purely internalist readings. Second, it provides a directed learning mechanism—the entrenchment of pragmatically indispensable principles—that explains how the core of the web is systematically constructed over time. This answers the charge that Quine's model lacks a principle of directed change, showing how the web's structure is not arbitrary but is forged by the historical pressure to minimize systemic brittleness.

### **6. Situating the Framework: A Systemic, Pragmatic Externalism**

The framework developed in this paper can be termed **Systemic Externalism**—a form of externalist epistemology that locates the reliability condition for justification not in an individual's cognitive processes, but in the demonstrated, historical viability of public knowledge systems. This section clarifies this position by examining its relationship to several major philosophical research programs.

#### **6.1 A Grounded Coherentism and a Naturalized Structural Realism**

Our framework offers a direct response to the isolation objection that has long challenged coherentist theories of justification (BonJour 1985). While internalist accounts can explain *why* some beliefs are more central to a web of belief than others (Carlson 2015), they lack a robust, non-circular mechanism to explain how that centrality is earned through external discipline. Systemic Externalism provides this mechanism. A principle becomes part of a system's core precisely because it has survived a historical filtering process that has demonstrated its indispensable role in cultivating a low-brittleness network. Justification is therefore a two-level property: it requires not only a proposition's internal coherence but also the demonstrated reliability of the certifying network, measured through its historical capacity to maintain low systemic brittleness.

This approach also provides a naturalistic engine for the core claims of scientific **structural realism** (Worrall 1989). Structural realism elegantly explains the continuity of scientific progress by arguing that what is preserved across paradigm shifts is a theory's underlying mathematical or relational structure. It has, however, faced persistent questions about the ontological status of these structures and the process by which our fallible inquiry manages to "latch onto" them. Our framework offers a pragmatic answer to both. Ontologically, the **Apex Network** *is* the complete set of viable relational structures, understood not as abstract entities but as an emergent structural fact about our world's constraint topology. Epistemologically, we discover this structure not through a mysterious act of intellectual insight, but through the eliminative process of pragmatic selection. High-brittleness networks—those whose posited structures misalign with the landscape of viability—generate unsustainable costs and enter the Negative Canon. Low-brittleness networks survive. Over historical time, this failure-driven selective pressure is the engine that forces our Consensus Networks to conform to the objective, relational structure of the Apex Network.

#### **6.2 A Realist Corrective to Neopragmatism and Social Epistemology**

While retaining the anti-foundationalist spirit of pragmatism, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of reducing objectivity to social consensus (e.g., Rorty 1979). Accounts of justification as a purely linguistic or social practice lack a robust, non-discursive external constraint. Our framework provides this missing check through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal discourse—that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs.

#### **6.3 Mathematics as a Paradigm Case of Internal Brittleness**

Naturalistic epistemologies often treat mathematics as a difficult boundary case. Our framework, however, treats it as a paradigm demonstration of its core mechanisms, where pragmatic selection operates on a purely internal landscape. Mathematical frameworks face pragmatic pushback not through external falsification but through rising *internal inefficiency*.

The discovery of Russell's paradox in naive set theory, for instance, revealed a state of infinite brittleness, as it paralyzed all inference within the system. The subsequent development of Zermelo-Fraenkel set theory and Type Theory were competing attempts to create a new, low-brittleness foundation, each making different trade-offs between conceptual complexity and restrictive power. The mathematical community's eventual convergence on ZF set theory for most purposes reflects a collective, pragmatic assessment of which system offered greater long-term viability. Similarly, a mathematical research program that requires proofs of escalating complexity for diminishing explanatory returns is exhibiting the classic signs of a degenerating, high-brittleness system.

This shows the universality of the framework. All domains of inquiry—physical, social, and mathematical—face pragmatic selection. The feedback mechanism varies, from external prediction failures to internal incoherence, but the underlying filter is the same: systems that accumulate unsustainable brittleness are eventually replaced by more viable alternatives.

### **7. Objections, Limitations, and Principled Scope**

A philosophical model must be tested against its most difficult cases and its own conceptual boundaries. This section addresses key challenges to the framework, not as external objections to be deflected, but as core test cases that clarify its explanatory power and define its appropriate scope.

#### **7.1 The Problem of History: Endurance, Power, and Hindsight**

A powerful challenge concerns the interpretation of history. If viability is the standard, how do we account for flawed systems that endure for centuries, and how can we apply this standard to live controversies without the benefit of hindsight?

First, our framework sharply distinguishes mere *endurance* from pragmatic *viability*. The model predicts that brittle systems can persist, but only by paying immense and measurable systemic costs. The longevity of a system like Ptolemaic cosmology is not a refutation of the model but a confirmation of it; its apparent stability was not a sign of health but a measure of the intellectual and institutional energy it had to expend, making it profoundly vulnerable to a more efficient competitor.

This distinction is critical for addressing the role of power. A system can become locked into a high-brittleness "fitness trap" by coercive institutions (Acemoglu and Robinson 2012). A slave economy, for instance, is a classic example. While objectively brittle, it creates structures that make escaping the trap prohibitively costly in the short term. The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the *costs of maintaining that power* become a primary diagnostic indicator of it. The immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must waste to resist the structural pressures pushing it toward collapse. This makes marginalized perspectives a crucial diagnostic resource. As standpoint theory suggests (Harding 1991), those who bear the disproportionate first-order costs of a brittle system are positioned to be its most sensitive detectors.

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps." This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics. The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state's productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, exemplifies this state. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability therefore requires a balance between low-cost stability and adaptive capacity.

Finally, the concern about hindsight is best addressed by clarifying the framework's goal: it is not deterministic prediction but epistemic risk management. Retrospective analysis of historical cases is the necessary process of calibrating our diagnostic tools. We study known failures to learn the empirical signatures of rising brittleness before applying these tools to live, unresolved debates. A rising trend in a system's brittleness indicators does not prove its core claims are false, but it provides a strong, evidence-based signal that it is becoming a higher-risk, degenerating research program.

#### **7.2 The Problem of Scope: From Systemic Health to Individual Belief**

It is crucial to be precise about this model's scope. It is a macro-epistemology designed to explain the long-term viability of public knowledge systems. It does not primarily aim to solve traditional problems in micro-epistemology, such as Gettier cases or the justification of an individual's perceptual beliefs. Instead, it provides a robust bridge between these levels through the concept of higher-order evidence. The diagnosed health of a public system provides a powerful defeater (or corroborator) for an individual’s beliefs derived from that system.

The diagnosed brittleness of a knowledge system should determine the rational prior probability an agent assigns to any claim from that source. A claim from a low-brittleness network (e.g., a report from the Intergovernmental Panel on Climate Change) warrants a high prior; a claim from a high-brittleness network (e.g., a conspiracy theory documentary) warrants a low one. As Thomas Kelly (2005) has argued regarding disagreement, the properties of the epistemic source matter. The macro-level diagnosis of a system's health thus provides a rational, non-circular basis for an individual's allocation of trust.

#### **7.3 Principled Limitations**

Philosophical honesty requires acknowledging not just what a framework can do, but what it cannot. The following are not flaws to be rebutted but principled limitations that define the theory's appropriate scope.

*   **Species-Specific Objectivity:** The truths discovered through this process are objective for creatures with our biological and social architecture. Hypothetical beings with radically different constraints would discover a different Apex Network. This is relativism at the species level, not the cultural level, and represents a form of appropriate epistemic modesty.
*   **The "Tragic" Nature of Epistemic Progress:** The framework suggests that much of our most secure objective knowledge, particularly in the normative domain, is discovered through the analysis of large-scale systemic failures. The Negative Canon is, in effect, a record of historical suffering. This raises the difficult conclusion that our knowledge of what constitutes a viable social system seems to depend on the data generated by unviable ones.
*   **A Floor, Not a Ceiling:** The framework is most powerful at identifying catastrophic failures and mapping necessary constraints (the "floor" of viability). It is not designed to provide sufficient conditions for human flourishing (the "ceiling"). It leaves substantial space for legitimate value pluralism above the floor of what is demonstrably unworkable.
*   **Expert Dependence:** Accurate brittleness assessment requires technical expertise in domains like historical analysis, statistics, and systems theory. This creates an epistemic inequality similar to that in other specialized scientific fields, which in turn creates challenges for democratic legitimacy that require institutional solutions (such as data transparency and adversarial review).

These limitations do not undermine the framework's contribution. Instead, they define it as a powerful but specialized diagnostic tool for assessing the health of our collective knowledge systems.

---

### **8. Conclusion**

This paper has developed Emergent Pragmatic Coherentism as a response to the isolation objection that has long troubled coherentist theories of justification. By grounding coherence in the demonstrated, long-term viability of knowledge systems, our framework provides the external constraint that coherentism requires while preserving its holistic insights. The concept of systemic brittleness offers a naturalistic diagnostic tool for evaluating epistemic systems, while the notion of a constraint-determined Apex Network explains how objective knowledge can arise from our fallible, historical practices.

The result is a form of Systemic Externalism that offers a novel approach for resolving long-standing problems in post-Quinean epistemology. It explains how Quine's web of belief is dynamically revised, grounds a robust but fallibilist realism, and provides the conceptual foundation for a falsifiable research program into the health of our most critical knowledge-generating institutions.

This model is a framework for future inquiry, not a complete system. We started with distinguishing viable knowledge from brittle dogma. The theory posits that the ultimate arbiter is not system elegance or consensus, but its consequences. Operating at high abstraction, its data comes from ground-level experiences: suffering, instability, frustrated goals. Dissent and protest are epistemological data on rising brittleness. Ultimately, it's a tool for democratic accountability, asking: "Is this way of thinking, organizing ourselves, still working?"