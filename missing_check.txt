
================================================================================
Citation Extraction Run - 2025-10-20 11:00:53
Files scanned: final.md
Total citations: 96
================================================================================


################################################################################
FILE: final.md
################################################################################

Citation 1 [PARENTHETICAL]:
Line: 9
Citation: (Snow 1855)

Context:
## 1. Introduction: From a Static Web to a Dynamic Process

Why did germ theory replace miasma theory? A standard explanation cites superior evidence, but a deeper view reveals systemic viability. Miasma theory incurred catastrophic costs, such as misdirected public health efforts in London, and demanded constant ad hoc modifications to address anomalies. Its brittleness is evident in high patch velocity (P(t)); historical analyses (Snow 1855) indicate dozens of modifications by the mid-19th century. Germ theory, by contrast, reduced these costs while unifying diverse phenomena.

This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

Reference:
Snow, John. 1855. *On the Mode of Communication of Cholera*. 2nd ed. London: John Churchill. Reprinted in *International Journal of Epidemiology* 42, no. 6 (2013): 1543–1552. https://doi.org/10.1093/ije/dyt193.
------------------------------------------------------------

Citation 2 [PARENTHETICAL]:
Line: 13
Citation: (BonJour 1985)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
BonJour, Laurence. 1985. *The Structure of Empirical Knowledge*. Cambridge, MA: Harvard University Press. ISBN 978-0674843813.
------------------------------------------------------------

Citation 3 [PARENTHETICAL]:
Line: 13
Citation: (Olsson 2005)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
Olsson, Erik J. 2005. *Against Coherence: Truth, Probability, and Justification*. Oxford: Oxford University Press. ISBN 978-0199279999.
------------------------------------------------------------

Citation 4 [PARENTHETICAL]:
Line: 13
Citation: (Kvanvig 2012)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
Kvanvig, Jonathan L. 2012. "Coherentism and Justified Inconsistent Beliefs: A Solution." *Southern Journal of Philosophy* 50(1): 21–41. https://doi.org/10.1111/j.2041-6962.2011.00090.x.
------------------------------------------------------------

Citation 5 [PARENTHETICAL]:
Line: 13
Citation: (Krag 2015)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
Krag, Erik. 2015. "Coherentism and Belief Fixation." *Logos & Episteme* 6, no. 2: 187–199. https://doi.org/10.5840/logos-episteme20156211. ISSN 2069-0533.
------------------------------------------------------------

Citation 6 [PARENTHETICAL]:
Line: 13
Citation: (Quine 1960)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
Quine, W. V. O. 1960. *Word and Object*. Cambridge, MA: MIT Press. ISBN 978-0262670012.
------------------------------------------------------------

Citation 7 [PARENTHETICAL]:
Line: 13
Citation: (Kitcher 1993)

Context:
This raises the fundamental question: how do we distinguish genuine knowledge from coherent delusions? A Ptolemaic astronomer's system was internally coherent, its practitioners trained in sophisticated techniques, yet it was systematically misaligned with reality. What external constraint prevents coherent systems from floating free?

This shift exemplifies the isolation objection to coherentism: a belief system might be coherent yet detached from reality (BonJour 1985). Coherentists have offered responses (Olsson 2005; Kvanvig 2012; Krag 2015), but most rely on internalist resources lacking external constraints. We propose Emergent Pragmatic Coherentism, grounding coherence in demonstrated viability of knowledge systems measured by cost minimization (Quine 1960; Kitcher 1993).

### 1.1 Lineage and Departure

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 8 [POSSESSIVE]:
Line: 17
Citation: Quine's (1969)

Context:
### 1.1 Lineage and Departure

Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

Reference:
Quine, W. V. 1969. "Epistemology Naturalized." In *Ontological Relativity and Other Essays*, 69–90. New York: Columbia University Press. https://doi.org/10.7312/quin92204-004. ISBN 9780231083577, 9780231171991.
------------------------------------------------------------

Citation 9 [POSSESSIVE]:
Line: 17
Citation: Davidson's (1986)

Context:
### 1.1 Lineage and Departure

Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

Reference:
Davidson, Donald. 1986. "A Coherence Theory of Truth and Knowledge." In *Truth and Interpretation: Perspectives on the Philosophy of Donald Davidson*, edited by Ernest LePore, 307–19. Oxford: Blackwell.
------------------------------------------------------------

Citation 10 [POSSESSIVE]:
Line: 17
Citation: Kitcher's (1993)

Context:
### 1.1 Lineage and Departure

Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 11 [POSSESSIVE]:
Line: 17
Citation: Longino's (1990)

Context:
### 1.1 Lineage and Departure

Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

Reference:
Longino, Helen E. 1990. *Science as Social Knowledge: Values and Objectivity in Scientific Inquiry*. Princeton, NJ: Princeton University Press. ISBN 978-0691020518.
------------------------------------------------------------

Citation 12 [POSSESSIVE]:
Line: 19
Citation: Thagard's (1989)

Context:
Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

EPC thus transforms coherence from a metaphor of fit into a measurable function of cost and constraint. Its realism is not the correspondence of propositions to an independent world, but the emergent stability of constraint satisfaction across iterated cycles of error and repair. This is a post-Quinean naturalism that grounds justification in demonstrated viability. It provides the crucial, non-discursive external filter of systemic consequence that these otherwise powerful social, psychological, and internalist accounts ultimately require.

Reference:
Thagard, Paul. 1989. "Explanatory Coherence." *Behavioral and Brain Sciences* 12(3): 435–502. https://doi.org/10.1017/S0140525X00057046.
------------------------------------------------------------

Citation 13 [POSSESSIVE]:
Line: 19
Citation: Zollman's (2007)

Context:
Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

EPC thus transforms coherence from a metaphor of fit into a measurable function of cost and constraint. Its realism is not the correspondence of propositions to an independent world, but the emergent stability of constraint satisfaction across iterated cycles of error and repair. This is a post-Quinean naturalism that grounds justification in demonstrated viability. It provides the crucial, non-discursive external filter of systemic consequence that these otherwise powerful social, psychological, and internalist accounts ultimately require.

Reference:
Zollman, Kevin J. S. 2007. "The Communication Structure of Epistemic Communities." *Philosophy of Science* 74(5): 574–87. https://doi.org/10.1086/508684.
------------------------------------------------------------

Citation 14 [POSSESSIVE]:
Line: 19
Citation: Rescher's (1973)

Context:
Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

EPC thus transforms coherence from a metaphor of fit into a measurable function of cost and constraint. Its realism is not the correspondence of propositions to an independent world, but the emergent stability of constraint satisfaction across iterated cycles of error and repair. This is a post-Quinean naturalism that grounds justification in demonstrated viability. It provides the crucial, non-discursive external filter of systemic consequence that these otherwise powerful social, psychological, and internalist accounts ultimately require.

Reference:
Rescher, Nicholas. 1973. *The Coherence Theory of Truth*. Oxford: Clarendon Press. ISBN 978-0198244011.
------------------------------------------------------------

Citation 15 [POSSESSIVE]:
Line: 19
Citation: Kitcher's (1993)

Context:
Emergent Pragmatic Coherentism extends a tradition beginning with Quine's (1969) naturalized epistemology, Davidson's (1986) coherence theory, Kitcher's (1993) evolutionary model, and Longino's (1990) social epistemology. EPC accepts this inheritance but makes a crucial departure: replacing static coherence with dynamic viability under constraint.

Where Quine anchored epistemology in psychology and Davidson in internal coherence, EPC operationalizes Thagard's (1989) connectionist ECHO, modeling coherence as activation harmony (coherence through network equilibrium) in constraint networks and extending it with pragmatic inhibitory weights derived from brittleness. Unlike Zollman's (2007) abstract Bayesian graphs, which model topology effects (how network structure affects information flow) on belief propagation, EPC injects real-world shocks—Tier 1 demographic collapse, institutional failure—to simulate paradigm fragility empirically. Rescher's (1973) systematicity criteria become quantified in our SBI(t) metrics, while Kitcher's (1993) credit-driven evolution gains failure diagnostics via the Negative Canon. Belief-formation and revision are adaptive processes that minimize brittleness under environmental feedback. Davidson's internal coherence becomes structural homeostasis maintained through pragmatic constraint. Kitcher's and Longino's insights into social calibration are operationalized: the intersubjective circulation of critique becomes a mechanism for reducing systemic fragility.

EPC thus transforms coherence from a metaphor of fit into a measurable function of cost and constraint. Its realism is not the correspondence of propositions to an independent world, but the emergent stability of constraint satisfaction across iterated cycles of error and repair. This is a post-Quinean naturalism that grounds justification in demonstrated viability. It provides the crucial, non-discursive external filter of systemic consequence that these otherwise powerful social, psychological, and internalist accounts ultimately require.

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 16 [PARENTHETICAL]:
Line: 25
Citation: (Holling 1973)

Context:
Our response is distinctive: coherence rests not on historical accident but on emergent necessary structure. Reality's pragmatic constraints—physical laws, biological limits, logical requirements, coordination necessities—create a constraint landscape that necessarily generates optimal configurations. These structures emerge from the constraint landscape itself, existing whether discovered or not, just as the lowest-energy state of a molecule emerges from quantum mechanics whether calculated or not. Objective truth is alignment with these emergent, constraint-determined structures.

We ground coherence in demonstrated viability of entire knowledge systems, measured through their capacity to minimize systemic costs. Drawing from resilience theory (Holling 1973), we explain how individuals' holistic revisions to personal webs of belief in response to recalcitrant experiences, which we term pragmatic pushback, drive bottom-up formation of viable public knowledge systems.

This transforms the isolation objection: a coherent system detached from reality isn't truly possible because constraints force convergence toward viable configurations. A perfectly coherent flat-earth cosmology generates catastrophic navigational costs. A coherent phlogiston chemistry generates accelerating conceptual debt. These aren't merely false but structurally unstable, misaligned with constraint topology. The process resembles constructing a navigation chart by systematically mapping shipwrecks. Successful systems navigate safe channels revealed by failures, triangulating toward viable peaks. The Apex Network is the structure remaining when all unstable configurations are eliminated. Crucially, this historical filtering is a discovery process, not a creation mechanism; the territory is revealed by the map of failures, not created by it.

Reference:
Holling, C. S. 1973. "Resilience and Stability of Ecological Systems." *Annual Review of Ecology and Systematics* 4: 1–23. https://doi.org/10.1146/annurev.es.04.110173.000245.
------------------------------------------------------------

Citation 17 [PARENTHETICAL]:
Line: 50
Citation: (Meadows 2008)

Context:
## 2. The Core Concepts: Units of Epistemic Selection

Understanding how knowledge systems evolve and thrive while others collapse requires assessing their structural health. A naturalistic theory needs functional tools for this analysis, moving beyond internal consistency to gauge resilience against real-world pressures. Following complex systems theory (Meadows 2008), this section traces how private belief becomes a public, functional component of knowledge systems.

### 2.1 Forging the Instruments: From Private Belief to Public Tool

Reference:
Meadows, Donella H. 2008. *Thinking in Systems: A Primer*. Edited by Diana Wright. White River Junction, VT: Chelsea Green Publishing. ISBN 978-1603580557.
------------------------------------------------------------

Citation 18 [PARENTHETICAL]:
Line: 54
Citation: (Goldman 1979)

Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Sinclair 2007), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 19 [PARENTHETICAL]:
Line: 54
Citation: (Kitcher 1993)

Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Sinclair 2007), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 20 [PARENTHETICAL]:
Line: 54
Citation: (Sinclair 2007)

Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Sinclair 2007), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate

Reference:
Sinclair, Robert. 2007. "Quine's Naturalized Epistemology and the Third Dogma of Empiricism." *Southern Journal of Philosophy* 45, no. 3: 455–472. https://doi.org/10.1111/j.2041-6962.2007.tb00060.x.
------------------------------------------------------------

Citation 21 [POSSESSIVE]:
Line: 54
Citation: Kim's (1988)

Context:
### 2.1 Forging the Instruments: From Private Belief to Public Tool

Following naturalized epistemology (Goldman 1979; Kitcher 1993), this framework shifts from private psychological states to public, functional structures. This makes analysis tractable through observable phenomena while addressing epistemic systems transcending individual cognition. By grounding epistemic norms in the demonstrated viability of knowledge systems, the framework addresses Kim's (1988) normativity objection: normative force emerges from the pragmatic consequences of misalignment with constraint-determined structures. Following Quine's engineering model (Sinclair 2007), epistemic norms function as hypothetical imperatives—if your goal is sustainable knowledge production, then minimize systemic brittleness.

**The Progression:** Belief → Proposition → Validated Data → Standing Predicate

Reference:
Kim, Jaegwon. 1988. "What Is 'Naturalized Epistemology'?" *Philosophical Perspectives* 2: 381–405. https://doi.org/10.2307/2214082.
------------------------------------------------------------

Citation 22 [PARENTHETICAL]:
Line: 76
Citation: (Campbell 1974)

Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is a structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

#### Conceptual Architecture

Reference:
Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.
------------------------------------------------------------

Citation 23 [PARENTHETICAL]:
Line: 76
Citation: (Mesoudi 2011)

Context:
**Shared Network:** Emergent public architecture of coherent propositions and predicates shared across individual belief webs for collective problem-solving. Networks nest hierarchically (germ theory within medicine within science). Their emergence is a structural necessity, not negotiation: failure-driven revisions converge on viable principles, forming transmissible public knowledge.

Drawing from evolutionary epistemology (Campbell 1974; Bradie 1986) and cultural evolution (Mesoudi 2011), networks' informational structure (Standing Predicates) acts as replicator—copied code—while social groups are interactor—physical vessels for testing. This explains knowledge persistence beyond societies (e.g., rediscovered Roman law). Independently formed networks reveal an objective structure underwriting successful inquiry, anticipating the Apex Network (Section 4).

#### Conceptual Architecture

Reference:
Mesoudi, Alex. 2011. *Cultural Evolution: How Darwinian Theory Can Explain Human Culture and Synthesize the Social Sciences*. Chicago: University of Chicago Press. ISBN 978-0226520445.
------------------------------------------------------------

Citation 24 [POSSESSIVE]:
Line: 104
Citation: Taleb's (2012)

Context:
These epistemic inefficiencies are real costs rendering networks brittle and unproductive, even without direct experimental falsification. The diagnostic lens thus applies to all inquiry forms, measuring viability through external material consequences or internal systemic dysfunction.

To operationalize these concepts, we introduce diagnostic indicators tracking brittleness over time. Drawing from systems theory, brittleness denotes vulnerability from hidden interdependencies and cascading failures. It parallels Taleb's (2012) fragility but diagnoses structural cause (accumulated systemic weakness) rather than symptom (vulnerability to shocks). This distinction enables predictive diagnosis rather than mere description.

In evolutionary epistemology (Campbell 1974; Popper 1972), falsification acts as a selection pressure on beliefs. EPC extends this: brittleness functions analogously to free energy (Friston 2010), quantifying deviation between a network's internal model and its constraining environment. Reducing brittleness thus constitutes epistemic adaptation. Zollman (2010) shows how transient diversity increases long-term epistemic robustness, providing theoretical justification for our pluralist frontier.

Reference:
Taleb, Nassim Nicholas. 2012. *Antifragile: Things That Gain from Disorder*. New York: Random House. ISBN 978-1400067824.
------------------------------------------------------------

Citation 25 [PARENTHETICAL]:
Line: 106
Citation: (Campbell 1974)

Context:
To operationalize these concepts, we introduce diagnostic indicators tracking brittleness over time. Drawing from systems theory, brittleness denotes vulnerability from hidden interdependencies and cascading failures. It parallels Taleb's (2012) fragility but diagnoses structural cause (accumulated systemic weakness) rather than symptom (vulnerability to shocks). This distinction enables predictive diagnosis rather than mere description.

In evolutionary epistemology (Campbell 1974; Popper 1972), falsification acts as a selection pressure on beliefs. EPC extends this: brittleness functions analogously to free energy (Friston 2010), quantifying deviation between a network's internal model and its constraining environment. Reducing brittleness thus constitutes epistemic adaptation. Zollman (2010) shows how transient diversity increases long-term epistemic robustness, providing theoretical justification for our pluralist frontier.

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |

Reference:
Campbell, Donald T. 1974. "Evolutionary Epistemology." In *The Philosophy of Karl R. Popper*, edited by Paul A. Schilpp, 413–63. La Salle, IL: Open Court.
------------------------------------------------------------

Citation 26 [PARENTHETICAL]:
Line: 106
Citation: (Popper 1972)

Context:
To operationalize these concepts, we introduce diagnostic indicators tracking brittleness over time. Drawing from systems theory, brittleness denotes vulnerability from hidden interdependencies and cascading failures. It parallels Taleb's (2012) fragility but diagnoses structural cause (accumulated systemic weakness) rather than symptom (vulnerability to shocks). This distinction enables predictive diagnosis rather than mere description.

In evolutionary epistemology (Campbell 1974; Popper 1972), falsification acts as a selection pressure on beliefs. EPC extends this: brittleness functions analogously to free energy (Friston 2010), quantifying deviation between a network's internal model and its constraining environment. Reducing brittleness thus constitutes epistemic adaptation. Zollman (2010) shows how transient diversity increases long-term epistemic robustness, providing theoretical justification for our pluralist frontier.

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |

Reference:
Popper, Karl R. 1972. *Objective Knowledge: An Evolutionary Approach*. Oxford: Clarendon Press. ISBN 978-0198750246.
------------------------------------------------------------

Citation 27 [PARENTHETICAL]:
Line: 106
Citation: (Friston 2010)

Context:
To operationalize these concepts, we introduce diagnostic indicators tracking brittleness over time. Drawing from systems theory, brittleness denotes vulnerability from hidden interdependencies and cascading failures. It parallels Taleb's (2012) fragility but diagnoses structural cause (accumulated systemic weakness) rather than symptom (vulnerability to shocks). This distinction enables predictive diagnosis rather than mere description.

In evolutionary epistemology (Campbell 1974; Popper 1972), falsification acts as a selection pressure on beliefs. EPC extends this: brittleness functions analogously to free energy (Friston 2010), quantifying deviation between a network's internal model and its constraining environment. Reducing brittleness thus constitutes epistemic adaptation. Zollman (2010) shows how transient diversity increases long-term epistemic robustness, providing theoretical justification for our pluralist frontier.

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |

Reference:
Friston, Karl J. 2010. "The Free-Energy Principle: A Unified Brain Theory?" *Nature Reviews Neuroscience* 11 (2): 127–138. https://doi.org/10.1038/nrn2787.
------------------------------------------------------------

Citation 28 [IN-PROSE]:
Line: 106
Citation: Zollman (2010)

Context:
To operationalize these concepts, we introduce diagnostic indicators tracking brittleness over time. Drawing from systems theory, brittleness denotes vulnerability from hidden interdependencies and cascading failures. It parallels Taleb's (2012) fragility but diagnoses structural cause (accumulated systemic weakness) rather than symptom (vulnerability to shocks). This distinction enables predictive diagnosis rather than mere description.

In evolutionary epistemology (Campbell 1974; Popper 1972), falsification acts as a selection pressure on beliefs. EPC extends this: brittleness functions analogously to free energy (Friston 2010), quantifying deviation between a network's internal model and its constraining environment. Reducing brittleness thus constitutes epistemic adaptation. Zollman (2010) shows how transient diversity increases long-term epistemic robustness, providing theoretical justification for our pluralist frontier.

| Indicator | Dimension | Proxy Metric |
| :--- | :--- | :--- |

Reference:
Zollman, Kevin J. S. 2010. "The Epistemic Benefit of Transient Diversity." *Erkenntnis* 72: 17–35. https://doi.org/10.1007/s10670-009-9194-6. ISSN 0165-0106.
------------------------------------------------------------

Citation 29 [PARENTHETICAL]:
Line: 127
Citation: (Gingerich 1993)

Context:
**M(t) - Model Complexity:** Measurable through parameter count.
- c. 150 CE (Almagest): ~40 circles (epicycles + deferents) for 7 celestial bodies
- c. 1500 CE: ~80+ circles documented in astronomical tables (Gingerich 1993)
- Complexity doubling over 1350 years with marginal accuracy gains

**P(t) - Patch Velocity:** Ratio of anomaly-resolution to novel predictions.

Reference: NOT FOUND for 'Gingerich 1993'
------------------------------------------------------------

Citation 30 [POSSESSIVE]:
Line: 131
Citation: Gingerich's (1993)

Context:
- Complexity doubling over 1350 years with marginal accuracy gains

**P(t) - Patch Velocity:** Ratio of anomaly-resolution to novel predictions.
- Based on Gingerich's (1993) analysis of astronomical publications 1450-1550:
- Estimated ~85% of astronomical work addressed known anomalies (planetary position discrepancies, precession corrections)
- Only ~15% proposed genuinely novel predictions or applications
- P(t) ≈ 5.7 (ratio of patch work to novel work)

Reference: NOT FOUND for 'Gingerich 1993'
------------------------------------------------------------

Citation 31 [POSSESSIVE]:
Line: 195
Citation: Kitcher's (1993)

Context:
- **Model Complexity Inflation (M(t)):** Parameter growth without predictive gains (parameter-to-prediction ratios).
  - **Proof Complexity Escalation:** Increasing proof length without explanatory gain (mathematics).

While interpreting these costs is normative for agents within a system, their existence and magnitude are empirical questions. The framework's core causal claim is falsifiable and descriptive: networks with high or rising brittleness across these tiers carry statistically higher probability of systemic failure or major revision when faced with external shocks. This operationalizes Kitcher's (1993) 'significant problems' pragmatically: Tier 1 bio-costs define significance without credit cynicism, while C(t) measures coercive monopolies masking failures.

Robustly measuring these costs requires disciplined methodology. The triangulation method provides practical protocol for achieving pragmatic objectivity.

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 32 [PARENTHETICAL]:
Line: 257
Citation: (Popper 1959)

Context:
### 4.1 A Negative Methodology: Charting What Fails

Our account of objectivity is not the pursuit of a distant star but the painstaking construction of a reef chart from the empirical data of shipwrecks. It begins not with visions of final truth, but with our most secure knowledge: the clear, non-negotiable data of large-scale systemic failure. Following Popperian insight (Popper 1959), our most secure knowledge is often of what is demonstrably unworkable. While single failed experiments can be debated, entire knowledge system collapse—descent into crippling inefficiency, intellectual stagnation, institutional decay—provides clear, non-negotiable data.

Systematic failure analysis builds the Negative Canon: an evidence-based catalogue of invalidated principles distinguishing:

Reference:
Popper, Karl. 1959. *The Logic of Scientific Discovery*. London: Hutchinson (originally 1934). ISBN 978-0415278447.
------------------------------------------------------------

Citation 33 [PARENTHETICAL]:
Line: 269
Citation: (Ladyman and Ross 2007)

Context:
### 4.2 The Apex Network: Ontological and Epistemic Status

As the Negative Canon catalogs failures, pragmatic selection reveals the contours of the **Apex Network**: not a pre-existing blueprint, nor our current consensus, but the objective structure of maximally viable solutions all successful inquiry must approximate. The Apex maximizes Thagard's global constraint satisfaction under Zollman's optimal topology (sparse diversity bonus), emerging as Kitcher's adaptive peak on Rescher's systematicity landscape. This network is not a pre-existing metaphysical blueprint but a structural emergent, the asymptotic intersection of all low-brittleness models (Ladyman and Ross 2007). Its status resonates with the pragmatist ideal end of inquiry (Peirce 1878). Our Consensus Network is our fallible map of this objective structure, which is stabilized through adaptive feedback from mind-independent constraints.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not an accident but an emergent structural fact demanding naturalistic explanation.

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press. ISBN 978-0199276196.
------------------------------------------------------------

Citation 34 [PARENTHETICAL]:
Line: 269
Citation: (Peirce 1878)

Context:
### 4.2 The Apex Network: Ontological and Epistemic Status

As the Negative Canon catalogs failures, pragmatic selection reveals the contours of the **Apex Network**: not a pre-existing blueprint, nor our current consensus, but the objective structure of maximally viable solutions all successful inquiry must approximate. The Apex maximizes Thagard's global constraint satisfaction under Zollman's optimal topology (sparse diversity bonus), emerging as Kitcher's adaptive peak on Rescher's systematicity landscape. This network is not a pre-existing metaphysical blueprint but a structural emergent, the asymptotic intersection of all low-brittleness models (Ladyman and Ross 2007). Its status resonates with the pragmatist ideal end of inquiry (Peirce 1878). Our Consensus Network is our fallible map of this objective structure, which is stabilized through adaptive feedback from mind-independent constraints.

The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not an accident but an emergent structural fact demanding naturalistic explanation.

Reference:
Peirce, Charles S. 1992. "How to Make Our Ideas Clear." In *The Essential Peirce: Selected Philosophical Writings*, vol. 1 (1867–1893), edited by Nathan Houser and Christian Kloesel, 124–41. Bloomington: Indiana University Press (originally 1878).
------------------------------------------------------------

Citation 35 [PARENTHETICAL]:
Line: 273
Citation: (Berlin and Kay 1969)

Context:
The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not an accident but an emergent structural fact demanding naturalistic explanation.

Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals, such as safe water and ripe fruit, conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.

The mechanism forging this structure is bottom-up emergence driven by cross-domain consistency needs. Local Shared Networks, developed to solve specific problems, face pressure to cohere because they operate in an interconnected world. This pressure creates tendency toward integration, though whether this yields a single maximally coherent system or stable pluralism remains empirical.

Reference:
Berlin, Brent, and Paul Kay. 1969. *Basic Color Terms: Their Universality and Evolution*. Berkeley: University of California Press. ISBN 978-1575861623
------------------------------------------------------------

Citation 36 [PARENTHETICAL]:
Line: 273
Citation: (Henrich 2015)

Context:
The Apex Network's ontological status requires careful specification to avoid foundationalist overreach and relativist collapse. We propose understanding it as a "structural emergent": a real, objective pattern crystallizing from interaction between inquiry practices and environmental resistance. Consider how objective structural facts can emerge from seemingly subjective domains: while individual color preference is contingent, cross-cultural data shows striking convergence on blue. This pattern is not an accident but an emergent structural fact demanding naturalistic explanation.

Pragmatic pushback shaping this landscape is evolutionary selection on shared biology. Human color vision was forged by navigating terrestrial environments, where efficiently tracking ecologically critical signals, such as safe water and ripe fruit, conferred viability advantage (Berlin and Kay 1969; Henrich 2015). The Apex Network has the same ontological status: not found but formed, the objective structural residue after pragmatic filtering has eliminated less viable alternatives.

The mechanism forging this structure is bottom-up emergence driven by cross-domain consistency needs. Local Shared Networks, developed to solve specific problems, face pressure to cohere because they operate in an interconnected world. This pressure creates tendency toward integration, though whether this yields a single maximally coherent system or stable pluralism remains empirical.

Reference:
Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press. ISBN 978-0691178431.
------------------------------------------------------------

Citation 37 [POSSESSIVE]:
Line: 279
Citation: Haack's (1993)

Context:
The framework makes no a priori claims about universal convergence. Domains with tight pragmatic constraints (basic engineering, medicine) show strong convergence pressures. Others (aesthetic judgment, political organization) may support multiple stable configurations. The Apex Network concept is thus a limiting case: the theoretical endpoint of convergence pressures where they operate, not a guarantee of uniform action across all inquiry domains.

The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

Reference:
Haack, Susan. 1993. *Evidence and Inquiry: Towards Reconstruction in Epistemology*. Oxford: Blackwell. ISBN 978-0631196792.
------------------------------------------------------------

Citation 38 [PARENTHETICAL]:
Line: 281
Citation: (Rescher 1996)

Context:
The Apex Network's function as standard for objective truth follows from this status. Using Susan Haack's (1993) crossword puzzle analogy: a proposition is objectively true because it is an indispensable component of the unique, fully completed, maximally coherent solution to the entire puzzle—a solution disciplined by thousands of external "clues" as pragmatic pushback.

This process is retrospective and eliminative, not teleological. Individual agents and networks solve local problems and reduce costs. The Apex Network is the objective, convergent pattern emerging as unintended consequence of countless local efforts to survive the failure filter. Its objectivity arises from the mind-independent nature of pragmatic constraints reliably generating costs for violating systems. This view resonates with process metaphysics (Rescher 1996), understanding the objective structure as constituted by the historical process of inquiry itself, not as a pre-existing static form.

The Apex Network's status is dual, a distinction critical to our fallibilist realism. Ontologically, it is real: the objective, mind-independent structure of viability that exists whether we correctly perceive it or not. Epistemically, it remains a regulative ideal. We can never achieve final confirmation our Consensus Network perfectly maps it; our knowledge is necessarily incomplete and fallible. Its existence grounds our realism and prevents collapse into relativism, while our epistemic limitations make inquiry a permanent and progressive project.

Reference:
Rescher, Nicholas. 1996. *Process Metaphysics: An Introduction to Process Philosophy*. Albany: State University of New York Press. ISBN 978-0791428184.
------------------------------------------------------------

Citation 39 [PARENTHETICAL]:
Line: 316
Citation: (Newman 2010)

Context:
#### 4.2.1 Formal Characterization

Drawing on network theory (Newman 2010), we can formally characterize the Apex Network as:

A = ∩{W_k | V(W_k) = 1}

Reference:
Newman, Mark. 2010. *Networks: An Introduction*. Oxford: Oxford University Press. ISBN 978-0199206650.
------------------------------------------------------------

Citation 40 [PARENTHETICAL]:
Line: 369
Citation: (Tauriainen 2017)

Context:
### 4.3 A Three-Level Framework for Truth

This emergent structure grounds a fallibilist but realist account of truth. It resolves the isolation objection and clarifies a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our framework shows these are not contradictory but two necessary components of a naturalistic epistemology. It reframes truth as a status propositions earn through increasingly rigorous stages of validation.

* **Level 3: Contextual Coherence.** The baseline status for any claim. A proposition is coherent within a specific Shared Network, regardless of that network's long-term viability. This level explains the internal rationality of failed or fictional systems, but the framework's externalist check, the assessment of systemic brittleness, prevents this from being mistaken for justified truth.
* **Level 2: Justified Truth.** The highest epistemic status practically achievable. A proposition is justified as true if it is certified by a Consensus Network that has a demonstrated track record of low systemic brittleness. For all rational purposes, we are licensed to treat such claims as true. The diagnosed health of the certifying network provides powerful higher-order evidence that functions as a defeater for radical skepticism. To doubt a claim at this level, without new evidence of rising brittleness, is to doubt the entire adaptive project of science itself.

Reference:
Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä, Department of Social Sciences and Philosophy. https://urn.fi/URN:NBN:fi:jyu-201705312584.
------------------------------------------------------------

Citation 41 [PARENTHETICAL]:
Line: 399
Citation: (March 1978)

Context:
**Quine's Hard Core and Functional Entrenchment**

Quine famously argued that no claim is immune to revision in principle, yet some claims are practically unrevisable because revising them would require dismantling too much of our knowledge structure. Our framework explains this tension through the concept of functional entrenchment driven by bounded rationality (March 1978).

A proposition migrates to the hard core not through metaphysical necessity but through pragmatic indispensability. The costs of revision become effectively infinite:

Reference:
March, James G. 1978. "Bounded Rationality, Ambiguity, and the Engineering of Choice." *The Bell Journal of Economics* 9, no. 2: 587–608. https://doi.org/10.2307/3003600.
------------------------------------------------------------

Citation 42 [PARENTHETICAL]:
Line: 419
Citation: (Quine 1951)

Context:
**Animating Quine's Web: From Static Structure to Dynamic Process**

Quine's "Web of Belief" (Quine 1951, 1960) provided a powerful static model of confirmational holism, but it has been criticized for lacking a dynamic account of its formation and change. Our framework provides the missing mechanisms.

First, pragmatic pushback supplies the externalist filter that grounds the web in mind-independent reality, decisively solving the isolation objection that haunts purely internalist readings. This relentless, non-discursive filter of real-world consequences prevents the web from floating free of constraints.

Reference:
Quine, W. V. O. 1951. "Two Dogmas of Empiricism." *Philosophical Review* 60(1): 20–43. https://doi.org/10.2307/2181906.
------------------------------------------------------------

Citation 43 [PARENTHETICAL]:
Line: 423
Citation: (March 1978)

Context:
First, pragmatic pushback supplies the externalist filter that grounds the web in mind-independent reality, decisively solving the isolation objection that haunts purely internalist readings. This relentless, non-discursive filter of real-world consequences prevents the web from floating free of constraints.

Second, the entrenchment of pragmatically indispensable principles in the system's core provides a directed learning mechanism. A proposition migrates to the core not by convention but because it has demonstrated immense value in lowering the entire network's systemic brittleness, making its revision catastrophically costly. This process, driven by bounded rationality (March 1978), functions as systemic caching: proven principles are preserved to avoid re-derivation costs.

For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now prohibitively expensive.

Reference:
March, James G. 1978. "Bounded Rationality, Ambiguity, and the Engineering of Choice." *The Bell Journal of Economics* 9, no. 2: 587–608. https://doi.org/10.2307/3003600.
------------------------------------------------------------

Citation 44 [PARENTHETICAL]:
Line: 427
Citation: (Carlson 2015)

Context:
For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now prohibitively expensive.

Together, these two mechanisms animate Quine's static web. Pragmatic pushback provides the external discipline, and the entrenchment of low-brittleness principles explains how the web's resilient core is systematically constructed over time (Carlson 2015). This transforms the web from a logical snapshot into an evolving reef chart. In doing so, it resolves a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our three-level framework shows these are not contradictory but two necessary components of a naturalistic epistemology. Core principles achieve Justified Truth (Level 2) through this process of systematic, externally-validated selection, while the Apex Network (Level 3) functions as the regulative structure toward which our theories converge.

This three-level truth framework describes the justificatory status of claims at a given moment. Over historical time, pragmatic filtering produces a discernible two-zone structure in our evolving knowledge systems.

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 45 [PARENTHETICAL]:
Line: 427
Citation: (Tauriainen 2017)

Context:
For example, Conservation of Energy became entrenched after proving indispensable across domains, its revision now prohibitively expensive.

Together, these two mechanisms animate Quine's static web. Pragmatic pushback provides the external discipline, and the entrenchment of low-brittleness principles explains how the web's resilient core is systematically constructed over time (Carlson 2015). This transforms the web from a logical snapshot into an evolving reef chart. In doing so, it resolves a documented tension in Quine's thought between truth as immanent to our best theory and truth as a transcendent regulative ideal (Tauriainen 2017). Our three-level framework shows these are not contradictory but two necessary components of a naturalistic epistemology. Core principles achieve Justified Truth (Level 2) through this process of systematic, externally-validated selection, while the Apex Network (Level 3) functions as the regulative structure toward which our theories converge.

This three-level truth framework describes the justificatory status of claims at a given moment. Over historical time, pragmatic filtering produces a discernible two-zone structure in our evolving knowledge systems.

Reference:
Tauriainen, Teemu. 2017. "Quine's Naturalistic Conception of Truth." Master's thesis, University of Jyväskylä, Department of Social Sciences and Philosophy. https://urn.fi/URN:NBN:fi:jyu-201705312584.
------------------------------------------------------------

Citation 46 [PARENTHETICAL]:
Line: 436
Citation: (Price 1992)

Context:
The historical process of pragmatic filtering gives our evolving Consensus Networks a discernible structure, which can be understood as having two distinct epistemic zones. This distinction is not about the nature of reality itself, but describes the justificatory status of our claims at a given time.

* **The Convergent Core:** This represents the load-bearing foundations of our current knowledge. It comprises domains where the relentless pressure of pragmatic selection has eliminated all known rival formulations, leaving a single, or functionally identical, set of low-brittleness principles. Principles reside in this core, such as the laws of thermodynamics or the germ theory of disease, not because they are dogmatically held or self-evident but because all tested alternatives have been relegated to the Negative Canon after generating catastrophically high systemic costs. While no claim is immune to revision in principle, the principles in the Convergent Core are functionally unrevisable in practice, as doing so would require dismantling the most successful and resilient knowledge structures we have ever built. A claim from this core achieves the highest degree of justification we can assign, approaching our standard for Objective Truth (Level 1).
* **The Pluralist Frontier:** This describes the domains of active research where our current evidence is insufficient to decide between multiple, competing, and viable reconstructions of the landscape of viability. Here, rival systems (e.g., different interpretations of quantum mechanics or competing models of consciousness) may coexist, each with a demonstrably low and stable degree of brittleness. It is crucial to distinguish this constrained, evidence-based pluralism from relativism. The frontier is not an "anything goes" zone but a highly restricted space strictly bounded on all sides by the Negative Canon. A system based on phlogiston is not a "viable contender" on the frontier of chemistry but a demonstrably failed research program. This pluralism is therefore a sign of epistemic underdetermination: a feature of our map's current limitations, not reality's supposed indifference. This position resonates with pragmatist accounts of functional pluralism (Price 1992), which treat different conceptual frameworks as tools whose legitimacy is determined by their utility within a specific practice. Within this frontier, the core claims of each viable competing system can be granted the status of Justified Truth (Level 2). This is also the zone where non-epistemic factors, such as institutional power or contingent path dependencies, can play their most significant role, sometimes artificially constraining the range of options explored or creating temporary monopolies on what is considered justified.

### 4.5 Illustrative Cases of Convergence and Brittleness

Reference:
Price, Huw. 1992. "Metaphysical Pluralism." *Journal of Philosophy* 89(8): 387–409. https://doi.org/10.2307/2940741.
------------------------------------------------------------

Citation 47 [IN-PROSE]:
Line: 440
Citation: Kuhn (1962)

Context:
### 4.5 Illustrative Cases of Convergence and Brittleness

The transition from Newtonian to relativistic physics offers a canonical example of this framework's diagnostic application. After centuries of viability, the Newtonian system began to accumulate significant systemic costs in the late 19th century. These manifested as first-order predictive failures, such as its inability to account for the perihelion of Mercury, and as rising conceptual debt in the form of ad-hoc modifications like the Lorentz-FitzGerald contraction hypothesis. This accumulating brittleness created what Kuhn (1962) termed a "crisis" state preceding paradigm shifts. The Einsteinian system proved a more resilient solution, reducing this conceptual debt and substantially lowering the systemic costs of inquiry in physics.

A more contemporary case can be found in the recent history of artificial intelligence, which illustrates how a brittleness assessment might function in real time. The periodic "AI winters" can be understood as the collapse of high-brittleness paradigms, such as symbolic AI, which suffered from a high rate of ad-hoc modification when faced with novel challenges. While the subsequent deep learning paradigm proved a low-brittleness solution for many specific tasks, it may now be showing signs of rising systemic costs. These can be described conceptually as, for example, potentially unsustainable escalations in computational and energy resources for marginal performance gains, or an accelerating research focus on auxiliary, post-hoc modifications rather than on foundational architectural advances. This situation illustrates the Pluralist Frontier in action, as rival architectures might now be seen as competing to become the next low-brittleness solution.

Reference:
Kuhn, Thomas S. 1996. *The Structure of Scientific Revolutions*. 3rd ed. Chicago: University of Chicago Press (originally 1962). ISBN 978-0226458083.
------------------------------------------------------------

Citation 48 [PARENTHETICAL]:
Line: 446
Citation: (Wright 1932)

Context:
### 4.6 Navigating the Landscape: Fitness Traps, Path Dependence, and the Role of Power

An evolutionary model of knowledge must account for the complexities of history, not just an idealized linear progress. The landscape of viability is not smooth: knowledge systems can become entrenched in suboptimal but locally stable states, which we term "fitness traps" (Wright 1932). This section clarifies how the framework incorporates factors like path dependence and institutional power not as external exceptions but as core variables that explain these historical dynamics.

The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

Reference:
Wright, Sewall. 1932. "The Roles of Mutation, Inbreeding, Crossbreeding and Selection in Evolution." *Proceedings of the Sixth International Congress of Genetics* 1: 356–66.
------------------------------------------------------------

Citation 49 [PARENTHETICAL]:
Line: 450
Citation: (Acemoglu and Robinson 2012)

Context:
The model's claim is not deterministic prediction but probabilistic analysis: beneath the surface-level contingency historians rightly emphasize, underlying structural pressures create statistical tendencies over long timescales. A system accumulating brittleness is not fated to collapse on a specific date but becomes progressively more vulnerable to contingent shocks. The model thus complements historical explanation by offering tools to understand why some systems prove more resilient than others.

A system can become locked into a high-brittleness fitness trap by coercive institutions or other path-dependent factors. A slave economy, for instance, is a classic example. While objectively brittle in the long run, it creates institutional structures that make escaping the trap prohibitively costly in the short term (Acemoglu and Robinson 2012). The framework's key insight is that the exercise of power does not negate a system's brittleness; rather, the costs of maintaining that power become a primary indicator of it. This power manifests in two interrelated ways. First is its defensive role: the immense coercive overheads required to suppress dissent and manage internal friction are a direct measure of the energy a system must expend to resist the structural pressures pushing it toward collapse.

Second, power plays a constitutive role by actively shaping the epistemic landscape itself. Powerful institutions do not merely respond to brittleness defensively; they can construct and maintain the very conditions that generate it. By controlling research funding, defining what counts as a legitimate problem, and entrenching path dependencies that reinforce a fitness trap, institutional power actively digs the fitness trap and locks the system into a high-brittleness state. This pattern of epistemic capture appears across domains: from tobacco companies suppressing health research to colonial knowledge systems that extracted Indigenous insights while denying reciprocal engagement, thereby masking brittleness through institutional dominance. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. To detect such hidden brittleness, we can augment C(t) with sub-metrics for innovation stagnation, tracking lags in novel applications or cross-domain extensions relative to comparable systems as proxies for suppressed adaptive capacity. Concretely, innovation lag can be operationalized as: **I(t) = (Novel Applications per Unit Time) / (Defensive Publications per Unit Time)**. When I(t) declines while C(t) remains high, this signals power-induced rigidity masking underlying brittleness. For example, Lysenkoist biology in the Soviet Union showed I(t) approaching zero (no cross-domain applications) while defensive publications proliferated. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

Reference:
Acemoglu, Daron, and James A. Robinson. 2012. *Why Nations Fail: The Origins of Power, Prosperity, and Poverty*. New York: Crown Business. ISBN 978-0307719225.
------------------------------------------------------------

Citation 50 [IN-PROSE]:
Line: 454
Citation: Turchin (2003)

Context:
Second, power plays a constitutive role by actively shaping the epistemic landscape itself. Powerful institutions do not merely respond to brittleness defensively; they can construct and maintain the very conditions that generate it. By controlling research funding, defining what counts as a legitimate problem, and entrenching path dependencies that reinforce a fitness trap, institutional power actively digs the fitness trap and locks the system into a high-brittleness state. This pattern of epistemic capture appears across domains: from tobacco companies suppressing health research to colonial knowledge systems that extracted Indigenous insights while denying reciprocal engagement, thereby masking brittleness through institutional dominance. While this can create a temporary monopoly on justification, the framework can still diagnose the system's underlying brittleness. The costs of this constitutive power often manifest as a lack of adaptability, suppressed innovation, and a growing inability to solve novel problems that fall outside the officially sanctioned domain. To detect such hidden brittleness, we can augment C(t) with sub-metrics for innovation stagnation, tracking lags in novel applications or cross-domain extensions relative to comparable systems as proxies for suppressed adaptive capacity. Concretely, innovation lag can be operationalized as: **I(t) = (Novel Applications per Unit Time) / (Defensive Publications per Unit Time)**. When I(t) declines while C(t) remains high, this signals power-induced rigidity masking underlying brittleness. For example, Lysenkoist biology in the Soviet Union showed I(t) approaching zero (no cross-domain applications) while defensive publications proliferated. Over historical time, even the most entrenched systems face novel shocks, where the hidden costs of their power-induced rigidity are typically revealed.

The severity of a fitness trap can be metricized, providing an empirical check on these dynamics. Drawing on cliodynamic analysis, Turchin (2003) has shown that the ratio of defensive coercive overheads to a state’s productive capacity can serve as a powerful indicator of rising systemic fragility. For instance, historical polities where such overheads consumed over 30% of state resources for a sustained period exhibited a significantly higher probability of fragmentation when faced with an external shock. This provides a concrete method for diagnosing the depth of a fitness trap: by tracking the measurable, defensive costs a system must pay to enforce its power-induced constraints on inquiry and social organization.

Finally, it is necessary to distinguish this high-brittleness fitness trap from a different state: low-brittleness stagnation. A system can achieve a locally stable, low-cost equilibrium that is highly resilient to existing shocks but lacks the mechanisms for generating novel solutions. A traditional craft perfected for a stable environment but unable to adapt to a new material, or a scientific paradigm efficient at solving internal puzzles but resistant to revolutionary change, exemplifies this state. While not actively accumulating systemic costs, such a system is vulnerable to a different kind of failure: obsolescence in the face of a faster-adapting competitor. Diagnosing this condition requires not only a static assessment of current brittleness but also an analysis of the system's rate of adaptive innovation. True long-term viability therefore requires a balance between low-cost stability and adaptive capacity. This evolutionary perspective completes our reef chart, not as a finished map, but as an ongoing process of hazard detection and channel discovery.

Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press. ISBN 978-0691116693.
------------------------------------------------------------

Citation 51 [PARENTHETICAL]:
Line: 520
Citation: (Russell 1903)

Context:
- Appeared to exhibit low brittleness across all indicators
- Provided an elegant foundation for mathematics

**Russell's Paradox (Russell 1903):**
- Revealed infinite brittleness: the theory could derive a direct contradiction
- Considered the set R = {x | x ∉ x}. Is R ∈ R? Both yes and no follow from the axioms
- All inference paralyzed (if both A and ¬A are derivable, the principle of explosion allows derivation of anything)

Reference:
Russell, Bertrand. 1903. *The Principles of Mathematics*. Cambridge: Cambridge University Press. ISBN 978-1430476030.
------------------------------------------------------------

Citation 52 [PARENTHETICAL]:
Line: 558
Citation: (Harding 1991)

Context:
### 5.3 Power, Suppression, and the Hard Core

Engaging with insights from feminist epistemology (Harding 1991), we can see that even mathematics is not immune to power dynamics that generate brittleness. When a dominant mathematical community uses institutional power to suppress alternative approaches, this incurs measurable Coercive Overheads (C(t)):

**Mechanisms of Mathematical Suppression:**
- Career punishment for heterodox approaches to foundations or proof methods

Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press. ISBN 978-0801497469.
------------------------------------------------------------

Citation 53 [PARENTHETICAL]:
Line: 589
Citation: (March 1978)

Context:
1. Revising logic requires using logic to assess the revision
2. This creates infinite regress or circularity
3. Therefore logic exhibits infinite brittleness if removed
4. Systems under bounded rationality (March 1978) must treat such maximal-cost revisions as core

**This is pragmatic necessity, not a priori truth:**
- Logic could theoretically be revised if we encountered genuine pragmatic pressure sufficient to justify the cost

Reference:
March, James G. 1978. "Bounded Rationality, Ambiguity, and the Engineering of Choice." *The Bell Journal of Economics* 9, no. 2: 587–608. https://doi.org/10.2307/3003600.
------------------------------------------------------------

Citation 54 [IN-PROSE]:
Line: 613
Citation: Carlson (2015)

Context:
### 6.1 A Grounded Coherentism and a Naturalized Structural Realism

While internalist coherentists like Carlson (2015) have successfully shown that the web must have a functionally indispensable core, they lack resources to explain why that core is forged by external discipline. Systemic Externalism provides this missing causal engine, grounding Carlson's internal structure in an externalist history of pragmatic selection. Justification requires coherence plus network reliability via low brittleness. Unlike Zollman's (2007, 2013) static network models and Rosenstock et al. (2017), EPC examines evolving networks under pushback, extending ECHO's harmony principle with external brittleness filters Thagard's internalism lacks.

#### 6.1.1 A Naturalistic Engine for Structural Realism

Reference:
Carlson, Matthew. 2015. "Logic and the Structure of the Web of Belief." *Journal for the History of Analytical Philosophy* 3(5): 1–27. https://doi.org/10.15173/jhap.v3i5.28.
------------------------------------------------------------

Citation 55 [IN-PROSE]:
Line: 613
Citation: Rosenstock et al. (2017)

Context:
### 6.1 A Grounded Coherentism and a Naturalized Structural Realism

While internalist coherentists like Carlson (2015) have successfully shown that the web must have a functionally indispensable core, they lack resources to explain why that core is forged by external discipline. Systemic Externalism provides this missing causal engine, grounding Carlson's internal structure in an externalist history of pragmatic selection. Justification requires coherence plus network reliability via low brittleness. Unlike Zollman's (2007, 2013) static network models and Rosenstock et al. (2017), EPC examines evolving networks under pushback, extending ECHO's harmony principle with external brittleness filters Thagard's internalism lacks.

#### 6.1.1 A Naturalistic Engine for Structural Realism

Reference:
Rosenstock, Sarita, Cailin O'Connor, and Justin Bruner. 2017. "In Epistemic Networks, Is Less Really More?" *Philosophy of Science* 84(2): 234–52. https://doi.org/10.1086/690717.
------------------------------------------------------------

Citation 56 [PARENTHETICAL]:
Line: 617
Citation: (Worrall 1989)

Context:
#### 6.1.1 A Naturalistic Engine for Structural Realism

The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman and Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

Reference:
Worrall, John. 1989. "Structural Realism: The Best of Both Worlds?" *Dialectica* 43(1–2): 99–124. https://doi.org/10.1111/j.1746-8361.1989.tb00933.x.
------------------------------------------------------------

Citation 57 [PARENTHETICAL]:
Line: 619
Citation: (Ladyman and Ross 2007)

Context:
The Apex Network aligns with structural realism (Worrall 1989), providing its missing naturalistic engine. It explains convergence on objective structures via pragmatic filtering: brittle theories fail systematically, low-brittleness ones survive. The historical record shows systematic elimination of high-brittleness systems. The convergence toward low-brittleness structures, documented in the Negative Canon, provides positive inductive grounds for realism about the objective viability landscape our theories progressively map.

This provides an evolutionary, pragmatic engine for Ontic Structural Realism (Ladyman and Ross 2007). While OSR posits that the world is fundamentally structural, our framework explains how scientific practices are forced to converge on these objective structures through pragmatic filtering. The Apex Network is the complete set of viable relational structures, an emergent fact about our world's constraint topology, discovered through pragmatic selection.

#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press. ISBN 978-0199276196.
------------------------------------------------------------

Citation 58 [PARENTHETICAL]:
Line: 623
Citation: (Goldman 1979)

Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

Reference:
Goldman, Alvin I. 1979. "What Is Justified Belief?" In *Justification and Knowledge: New Studies in Epistemology*, edited by George S. Pappas, 1–23. Dordrecht: D. Reidel. https://doi.org/10.1007/978-94-009-9493-5_1.
------------------------------------------------------------

Citation 59 [PARENTHETICAL]:
Line: 623
Citation: (Zagzebski 1996)

Context:
#### 6.1.2 Distinguishing Systemic Externalism from Other Externalisms

Systemic Externalism contrasts with Process Reliabilism (Goldman 1979) and Virtue Epistemology (Zagzebski 1996). Process Reliabilism locates justification in the reliability of individual cognitive processes; Systemic Externalism shifts focus to the demonstrated historical viability of the public knowledge system that certifies the claim. Virtue Epistemology grounds justification in individual intellectual virtues; Systemic Externalism attributes resilience and adaptability to the collective system. Systemic Externalism thus offers macro-level externalism, complementing these micro-level approaches.

### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

Reference:
Zagzebski, Linda Trinkaus. 1996. *Virtues of the Mind: An Inquiry into the Nature of Virtue and the Ethical Foundations of Knowledge*. Cambridge: Cambridge University Press. ISBN 978-0521570602. https://doi.org/10.1017/CBO9780511582233.
------------------------------------------------------------

Citation 60 [PARENTHETICAL]:
Line: 627
Citation: (Baggio and Parravicini 2019)

Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Baggio, Guido, and Andrea Parravicini. 2019. "Introduction to Pragmatism and Theories of Emergence." *European Journal of Pragmatism and American Philosophy* XI-2. https://doi.org/10.4000/ejpap.1611.
------------------------------------------------------------

Citation 61 [PARENTHETICAL]:
Line: 627
Citation: (Putnam 2002)

Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Putnam, Hilary. 2002. *The Collapse of the Fact/Value Dichotomy and Other Essays*. Cambridge, MA: Harvard University Press. ISBN 978-0674013803.
------------------------------------------------------------

Citation 62 [PARENTHETICAL]:
Line: 627
Citation: (Lynch 2009)

Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Lynch, Michael P. 2009. *Truth as One and Many*. Oxford: Clarendon Press. ISBN 978-0199218738.
------------------------------------------------------------

Citation 63 [IN-PROSE]:
Line: 627
Citation: Rorty (1979)

Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Rorty, Richard. 1979. *Philosophy and the Mirror of Nature*. Princeton, NJ: Princeton University Press. ISBN 978-0691020167.
------------------------------------------------------------

Citation 64 [IN-PROSE]:
Line: 627
Citation: Brandom (1994)

Context:
### 6.2 A Realist Corrective to Neopragmatism and Social Epistemology

The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

Reference:
Brandom, Robert B. 1994. *Making It Explicit: Reasoning, Representing, and Discursive Commitment*. Cambridge, MA: Harvard University Press. ISBN 978-0674543195.
------------------------------------------------------------

Citation 65 [IN-PROSE]:
Line: 629
Citation: El-Hani and Pihlström (2002)

Context:
The framework developed here retains pragmatism's anti-foundationalist spirit and focus on inquiry as a social, problem-solving practice. Its core ambition aligns with the foundational project of classical pragmatism: to articulate a non-reductive naturalism that can explain the emergence of genuine novelty in the world (Baggio and Parravicini 2019). However, our model offers a crucial corrective to neopragmatist approaches that are vulnerable to the charge of conflating epistemic values with mere practical utility (Putnam 2002; Lynch 2009) or reducing objectivity to social consensus. Thinkers like Rorty (1979) and Brandom (1994), in their sophisticated accounts of justification as a linguistic or social practice, lack a robust, non-discursive external constraint. This leaves them with inadequate resources for handling cases where entire communities, through well-managed discourse, converge on unviable beliefs.

Our framework provides this missing external constraint through its analysis of systemic failure. The collapse of Lysenkoist biology in the Soviet Union, for instance, was not due to a breakdown in its internal "game of giving and asking for reasons"—indeed, that discourse was brutally enforced. Its failure was a matter of catastrophic first-order costs that no amount of conversational management could prevent. This focus on pragmatic consequence as a real, external filter allows us to distinguish our position from other forms of "pragmatic realism." El-Hani and Pihlström (2002), for example, resolve the emergentist dilemma by arguing that emergent properties "gain their ontological status from the practice-laden ontological commitments we make." While we agree that justification is tied to practice, our model grounds this process in a more robustly externalist manner. Pragmatic viability is not the source of objectivity; it is the primary empirical indicator of a system's alignment with the mind-independent, emergent structure of the Apex Network.

This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Reference:
El-Hani, Charbel Niño, and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3(2): article 3. https://doi.org/10.5840/eip2002325.
------------------------------------------------------------

Citation 66 [PARENTHETICAL]:
Line: 633
Citation: (Goldman 1999)

Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

#### Collective Calibration

Reference:
Goldman, Alvin I. 1999. *Knowledge in a Social World*. Oxford: Oxford University Press. ISBN 978-0198238201.
------------------------------------------------------------

Citation 67 [PARENTHETICAL]:
Line: 633
Citation: (Longino 2002)

Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

#### Collective Calibration

Reference:
Longino, Helen E. 2002. *The Fate of Knowledge*. Princeton, NJ: Princeton University Press. ISBN 978-0691088761.
------------------------------------------------------------

Citation 68 [PARENTHETICAL]:
Line: 633
Citation: (Harding 1991)

Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

#### Collective Calibration

Reference:
Harding, Sandra. 1991. *Whose Science? Whose Knowledge? Thinking from Women's Lives*. Ithaca, NY: Cornell University Press. ISBN 978-0801497469.
------------------------------------------------------------

Citation 69 [PARENTHETICAL]:
Line: 633
Citation: (Lugones 2003)

Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

#### Collective Calibration

Reference:
Lugones, María. 2003. *Pilgrimages/Peregrinajes: Theorizing Coalition against Multiple Oppressions*. Lanham, MD: Rowman & Littlefield. ISBN 978-0742514591.
------------------------------------------------------------

Citation 70 [PARENTHETICAL]:
Line: 633
Citation: (Sims 2024)

Context:
This leads to a key reframing of the relationship between agreement and truth. Genuine solidarity is not an alternative to objectivity but an emergent property of low-brittleness systems that have successfully adapted to pragmatic constraints. The practical project of cultivating viable knowledge systems is therefore the most secure path to enduring agreement. This stands in sharp contrast to any attempt to define truth as a stable consensus within a closed system, a procedure that our framework would diagnose as a potential coherence trap lacking the necessary externalist check of real-world systemic costs.

Similarly, our framework provides an evolutionary grounding for the core insights of **social epistemology** (Goldman 1999; Longino 2002). Social epistemic procedures like peer review and institutionalized criticism are not justified a priori; they persist because they are evolved adaptive strategies that demonstrably reduce systemic brittleness by helping networks detect errors and pay down conceptual debt. This provides the externalist check that purely procedural models can lack. It also offers an empirical grounding for the central insight of standpoint theory (Harding 1991; Lugones 2003), naturalizing the idea that marginalized perspectives can be a privileged source of data about a system's hidden costs. In our model, marginalized perspectives are not privileged due to a metaphysical claim about identity, but because they often function as the most sensitive detectors of a system's First-Order Costs and hidden Coercive Overheads (C(t)). A system that appears stable to its beneficiaries may be generating immense, unacknowledged costs for those at its margins. Suppressing these perspectives is therefore not just a moral failure, but a critical epistemic failure that allows brittleness to accumulate undetected. This view of collective knowledge as an emergent, adaptive process finds resonance in contemporary work on dynamic holism (Sims 2024).

#### Collective Calibration

Reference:
Sims, Matthew. 2024. "The Principle of Dynamic Holism: Guiding Methodology for Investigating Cognition in Nonneuronal Organisms." *Philosophy of Science* 91(2): 430–48. https://doi.org/10.1017/psa.2023.104.
------------------------------------------------------------

Citation 71 [PARENTHETICAL]:
Line: 637
Citation: (O'Connor and Weatherall 2019)

Context:
#### Collective Calibration

Empirical models of social epistemic networks (O'Connor and Weatherall 2019) suggest that objectivity is a function of communication topology. EPC operationalizes this insight: calibration efficiency inversely correlates with brittleness. The more diverse the error signals integrated (Longino 1990; Anderson 1996), the more stable the Apex Network.

### 6.3 Distinguishing from Lakatos and Laudan

Reference:
O'Connor, Cailin, and James Owen Weatherall. 2019. *The Misinformation Age: How False Beliefs Spread*. New Haven, CT: Yale University Press. ISBN 978-0300234015.
------------------------------------------------------------

Citation 72 [PARENTHETICAL]:
Line: 637
Citation: (Longino 1990)

Context:
#### Collective Calibration

Empirical models of social epistemic networks (O'Connor and Weatherall 2019) suggest that objectivity is a function of communication topology. EPC operationalizes this insight: calibration efficiency inversely correlates with brittleness. The more diverse the error signals integrated (Longino 1990; Anderson 1996), the more stable the Apex Network.

### 6.3 Distinguishing from Lakatos and Laudan

Reference:
Longino, Helen E. 1990. *Science as Social Knowledge: Values and Objectivity in Scientific Inquiry*. Princeton, NJ: Princeton University Press. ISBN 978-0691020518.
------------------------------------------------------------

Citation 73 [PARENTHETICAL]:
Line: 637
Citation: (Anderson 1996)

Context:
#### Collective Calibration

Empirical models of social epistemic networks (O'Connor and Weatherall 2019) suggest that objectivity is a function of communication topology. EPC operationalizes this insight: calibration efficiency inversely correlates with brittleness. The more diverse the error signals integrated (Longino 1990; Anderson 1996), the more stable the Apex Network.

### 6.3 Distinguishing from Lakatos and Laudan

Reference:
Anderson, Elizabeth. 1996. "Knowledge, Human Interests, and Objectivity in Feminist Epistemology." *Philosophical Topics* 23(2): 27–58. https://doi.org/10.5840/philtopics199623214.
------------------------------------------------------------

Citation 74 [IN-PROSE]:
Line: 641
Citation: Lakatos (1970)

Context:
### 6.3 Distinguishing from Lakatos and Laudan

While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

Reference:
Lakatos, Imre. 1970. "Falsification and the Methodology of Scientific Research Programmes." In *Criticism and the Growth of Knowledge*, edited by Imre Lakatos and Alan Musgrave, 91–196. Cambridge: Cambridge University Press.
------------------------------------------------------------

Citation 75 [IN-PROSE]:
Line: 641
Citation: Laudan (1977)

Context:
### 6.3 Distinguishing from Lakatos and Laudan

While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.

Reference:
Laudan, Larry. 1977. *Progress and Its Problems: Towards a Theory of Scientific Growth*. Berkeley: University of California Press. ISBN 978-0520037212.
------------------------------------------------------------

Citation 76 [PARENTHETICAL]:
Line: 643
Citation: (Pritchard 2016)

Context:
While our framework shares a historical-diagnostic ambition with Lakatos (1970) and Laudan (1977), it differs fundamentally: they provide retrospective descriptions of scientific change; we offer a forward-looking causal engine via quantifiable brittleness. Brittleness measures accumulated costs causing degeneration, serving as a real-time diagnostic of structural health, not merely historical output.

Similarly, while Laudan's model evaluates a theory based on the number and importance of the empirical problems it solves, our approach is subtly different. Systemic brittleness is a forward-looking measure of epistemic risk and resilience (Pritchard 2016). A system could have a high problem-solving score in Laudan's sense while simultaneously accumulating hidden systemic costs (like massive computational overheads or conceptual debt) that make it profoundly vulnerable to future shocks. Our framework is thus less a retrospective accounting of solved puzzles and more a real-time assessment of a system's long-term viability and adaptive efficiency.


### 6.4 Plantinga's Challenge: Does Evolution Select for Truth or Mere Survival?

Reference:
Pritchard, Duncan. 2016. "Epistemic Risk." *Journal of Philosophy* 113(11): 550–571. https://doi.org/10.5840/jphil20161131137.
------------------------------------------------------------

Citation 77 [PARENTHETICAL]:
Line: 648
Citation: (Plantinga 1993)

Context:
### 6.4 Plantinga's Challenge: Does Evolution Select for Truth or Mere Survival?

Alvin Plantinga's Evolutionary Argument Against Naturalism (EAAN) poses a formidable challenge to any naturalistic epistemology: if our cognitive faculties are products of natural selection, and natural selection optimizes for reproductive success rather than true belief, then we have no reason to trust that our faculties reliably produce true beliefs (Plantinga 1993, 2011). Evolution could equip us with systematically false but adaptive beliefs—useful fictions that enhance survival without tracking reality. If naturalism is true, the very faculties we use to conclude naturalism is true are unreliable, rendering naturalism self-defeating.

Our framework provides a novel response by collapsing Plantinga's proposed gap between adaptive success and truth-tracking. We argue that in domains where systematic misrepresentation generates costs, survival pressure and truth-tracking converge necessarily. This is not because evolution "cares about" truth, but because reality's constraint structure makes persistent falsehood unsustainable.

Reference:
Plantinga, Alvin. 1993. *Warrant and Proper Function*. New York: Oxford University Press. ISBN 978-0195078640.
------------------------------------------------------------

Citation 78 [IN-PROSE]:
Line: 706
Citation: Progress (1993)

Context:
**Rescher's Systematicity (1973, 2001):** Defines truth as praxis-tested systematicity (completeness, consistency, functional efficacy) but lacks quantification. **Advance:** SBI(t) operationalizes—P(t) = consistency metric, R(t) = completeness breadth, enabling falsifiable predictions (brittleness-collapse correlations).

**Kitcher's Evolutionary Progress (1993):** Models science as credit-driven selection with division of labor across 'significant problems.' **Advance:** Negative Canon provides failure engine, brittleness quantifies problem significance via coercive costs (C(t)), diagnosing degenerating programs without reducing to cynical credit-seeking.

**Sims' Dynamic Holism (2024):** Ecological constraints drive diachronic cognitive revision in nonneuronal organisms. EPC parallels this at macro-cultural scale: pragmatic pushback = ecological variables. **Distinction:** Sims focuses on individual adaptation; EPC on intergenerational knowledge systems. Both emphasize context-sensitive, constraint-driven evolution; EPC adds synchronic diagnostics to Sims' diachronic methodology.

Reference: NOT FOUND for 'Progress 1993'
------------------------------------------------------------

Citation 79 [IN-PROSE]:
Line: 708
Citation: Holism (2024)

Context:
**Kitcher's Evolutionary Progress (1993):** Models science as credit-driven selection with division of labor across 'significant problems.' **Advance:** Negative Canon provides failure engine, brittleness quantifies problem significance via coercive costs (C(t)), diagnosing degenerating programs without reducing to cynical credit-seeking.

**Sims' Dynamic Holism (2024):** Ecological constraints drive diachronic cognitive revision in nonneuronal organisms. EPC parallels this at macro-cultural scale: pragmatic pushback = ecological variables. **Distinction:** Sims focuses on individual adaptation; EPC on intergenerational knowledge systems. Both emphasize context-sensitive, constraint-driven evolution; EPC adds synchronic diagnostics to Sims' diachronic methodology.

These precursors provide micro-coherence (Thagard), meso-topology (Zollman), normative criteria (Rescher), macro-dynamics (Kitcher), and biological analogy (Sims). EPC unifies them through falsifiable brittleness assessment grounded in historical failure data.

Reference: NOT FOUND for 'Holism 2024'
------------------------------------------------------------

Citation 80 [PARENTHETICAL]:
Line: 716
Citation: (Staffel 2020)

Context:
As a macro-epistemology explaining the long-term viability of public knowledge systems, this framework does not primarily solve micro-epistemological problems like Gettier cases. Instead, it bridges the two levels through the concept of higher-order evidence: the diagnosed health of a public system provides a powerful defeater or corroborator for an individual's beliefs derived from that system.

The diagnosed brittleness of a knowledge system provides higher-order evidence that determines rational priors. Following Kelly (2005) on disagreement, when an agent receives a claim, they must condition their belief not only on the first-order evidence but also on the source's reliability (Staffel 2020). Let S be a high-brittleness network, like a denialist documentary. Its diagnosed non-viability acts as a powerful higher-order defeater. Therefore, even if S presents seemingly compelling first-order evidence E, a rational agent's posterior confidence in the claim properly remains low. Conversely, a low-brittleness network like the IPCC earns a high prior through demonstrated resilience. To doubt its claims without new evidence of rising brittleness is to doubt the entire adaptive project of science itself. This provides a rational, non-deferential basis for trust: justification flows from systemic health, grounding micro-level belief in macro-level viability.

### 7.1 From Hindsight to Foresight: Calibrating the Diagnostics

Reference:
Staffel, Julia. 2020. "Reasons Fundamentalism and Rational Uncertainty – Comments on Lord, The Importance of Being Rational." *Philosophy and Phenomenological Research* 100, no. 2: 463–468. https://doi.org/10.1111/phpr.12675. ISSN 0031-8205.
------------------------------------------------------------

Citation 81 [IN-PROSE]:
Line: 716
Citation: Kelly (2005)

Context:
As a macro-epistemology explaining the long-term viability of public knowledge systems, this framework does not primarily solve micro-epistemological problems like Gettier cases. Instead, it bridges the two levels through the concept of higher-order evidence: the diagnosed health of a public system provides a powerful defeater or corroborator for an individual's beliefs derived from that system.

The diagnosed brittleness of a knowledge system provides higher-order evidence that determines rational priors. Following Kelly (2005) on disagreement, when an agent receives a claim, they must condition their belief not only on the first-order evidence but also on the source's reliability (Staffel 2020). Let S be a high-brittleness network, like a denialist documentary. Its diagnosed non-viability acts as a powerful higher-order defeater. Therefore, even if S presents seemingly compelling first-order evidence E, a rational agent's posterior confidence in the claim properly remains low. Conversely, a low-brittleness network like the IPCC earns a high prior through demonstrated resilience. To doubt its claims without new evidence of rising brittleness is to doubt the entire adaptive project of science itself. This provides a rational, non-deferential basis for trust: justification flows from systemic health, grounding micro-level belief in macro-level viability.

### 7.1 From Hindsight to Foresight: Calibrating the Diagnostics

Reference:
Kelly, Thomas. 2005. "The Epistemic Significance of Disagreement." In *Oxford Studies in Epistemology*, vol. 1, edited by Tamar Szabó Gendler and John Hawthorne, 167–96. Oxford: Oxford University Press.
------------------------------------------------------------

Citation 82 [PARENTHETICAL]:
Line: 724
Citation: (Hodge 1992)

Context:
### 7.2 A Falsifiable Research Program

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.² The specific metrics and dynamic equations underlying this research program are detailed in the Mathematical Appendix.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Roda et al. 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

Reference:
Hodge, A. Trevor. 1992. *Roman Aqueducts & Water Supply*. London: Duckworth. ISBN 978-0715631713.
------------------------------------------------------------

Citation 83 [PARENTHETICAL]:
Line: 724
Citation: (Turchin 2003)

Context:
### 7.2 A Falsifiable Research Program

The framework grounds a concrete empirical research program with a falsifiable core hypothesis: *networks with high or rising measured brittleness carry statistically higher collapse probability when facing comparable external shocks.* Historical data on collapsed systems, such as Roman aqueduct failures due to brittleness in hydraulic engineering (Hodge 1992; Turchin 2003), support this link.² The specific metrics and dynamic equations underlying this research program are detailed in the Mathematical Appendix.

**Methodology**: (1) Operationalize brittleness through quantifiable proxies (security/R&D budget ratios, auxiliary hypothesis rates in literature). (2) Conduct comparative historical analysis using databases like Seshat (a database of historical societies) to compare outcomes across systems with different pre-existing brittleness facing similar shocks, controlling for contingent events. The precise methodology for this research program, including protocols for operationalizing P(t) and C(t) with inter-rater reliability checks, is detailed in Appendix B. A pilot study computed brittleness scores for competing COVID-19 models (2020–2022): complex epidemiological models with high M(t) (e.g., parameter-heavy SEIR variants) showed rising brittleness through predictive failures (e.g., overestimating herd immunity timelines), while simpler models with lower M(t) maintained better accuracy (Roda et al. 2020). This demonstrates predictive utility, with high-brittleness models requiring more revisions.

Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press. ISBN 978-0691116693.
------------------------------------------------------------

Citation 84 [PARENTHETICAL]:
Line: 792
Citation: (Turchin 2003)

Context:
**Why We Accept This:** Intellectual honesty. The framework is incomplete—it maps pragmatic viability, not all moral dimensions. If such a system existed (we doubt it does—internalization itself has costs), it would fall in Pluralist Frontier, not Negative Canon.

**Empirical Bet:** We predict that such systems are not merely repugnant but are, in fact, inherently brittle. Critics might point to the apparent long-term stability of systems like the Ottoman Empire's devşirme system or historical caste systems in India as counterexamples. However, our framework predicts, and historical analysis confirms, that such systems exhibit high underlying brittleness masked by coercive power. Their apparent stability was purchased at the cost of massive **Coercive Overheads (C(t))**—such as the resources spent on enforcing purity laws, suppressing revolts, and managing internal dissent—and they consistently demonstrated innovation lags and fragility in the face of external shocks, confirming their position on the landscape of non-viability (cf. Acemoglu & Robinson 2012; Turchin 2003). True internalization without coercion is rare and resource-intensive, while oppression reliably generates hidden costs that emerge under stress.

**But:** If empirics proved otherwise, we would acknowledge the framework's incompleteness rather than deny evidence.

Reference:
Turchin, Peter. 2003. *Historical Dynamics: Why States Rise and Fall*. Princeton, NJ: Princeton University Press. ISBN 978-0691116693.
------------------------------------------------------------

Citation 85 [PARENTHETICAL]:
Line: 846
Citation: (Godfrey-Smith 2003)

Context:
As a practical tool for epistemic risk management, Emergent Pragmatic Coherentism provides a structured methodology for policymakers, institutional designers, and the public to diagnose the health of our most critical knowledge-producing systems—from climate models to economic paradigms to public health infrastructure—before their hidden brittleness leads to catastrophic failure. The brittleness toolkit enables real-time assessment of structural health, allowing us to identify degenerating research programs and power-masked fragility before crises force recognition. This positions the work not just as a solution to an old philosophical puzzle, but as a practical and urgent research program for the 21st century, empowering democratic publics to hold their knowledge-generating systems accountable.

EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

Reference:
Godfrey-Smith, Peter. 2003. *Theory and Reality: An Introduction to the Philosophy of Science*. Chicago: University of Chicago Press.
------------------------------------------------------------

Citation 86 [POSSESSIVE]:
Line: 846
Citation: Boyd's (1973)

Context:
As a practical tool for epistemic risk management, Emergent Pragmatic Coherentism provides a structured methodology for policymakers, institutional designers, and the public to diagnose the health of our most critical knowledge-producing systems—from climate models to economic paradigms to public health infrastructure—before their hidden brittleness leads to catastrophic failure. The brittleness toolkit enables real-time assessment of structural health, allowing us to identify degenerating research programs and power-masked fragility before crises force recognition. This positions the work not just as a solution to an old philosophical puzzle, but as a practical and urgent research program for the 21st century, empowering democratic publics to hold their knowledge-generating systems accountable.

EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

Reference:
Boyd, Richard. 1973. "Realism, Underdetermination, and a Causal Theory of Evidence." *Noûs* 7(1): 1–12. https://doi.org/10.2307/2214367.
------------------------------------------------------------

Citation 87 [POSSESSIVE]:
Line: 846
Citation: Kitcher's (1993)

Context:
As a practical tool for epistemic risk management, Emergent Pragmatic Coherentism provides a structured methodology for policymakers, institutional designers, and the public to diagnose the health of our most critical knowledge-producing systems—from climate models to economic paradigms to public health infrastructure—before their hidden brittleness leads to catastrophic failure. The brittleness toolkit enables real-time assessment of structural health, allowing us to identify degenerating research programs and power-masked fragility before crises force recognition. This positions the work not just as a solution to an old philosophical puzzle, but as a practical and urgent research program for the 21st century, empowering democratic publics to hold their knowledge-generating systems accountable.

EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

Reference:
Kitcher, Philip. 1993. *The Advancement of Science: Science without Legend, Objectivity without Illusions*. New York: Oxford University Press. ISBN 978-0195046281.
------------------------------------------------------------

Citation 88 [POSSESSIVE]:
Line: 846
Citation: Ladyman and Ross's (2007)

Context:
As a practical tool for epistemic risk management, Emergent Pragmatic Coherentism provides a structured methodology for policymakers, institutional designers, and the public to diagnose the health of our most critical knowledge-producing systems—from climate models to economic paradigms to public health infrastructure—before their hidden brittleness leads to catastrophic failure. The brittleness toolkit enables real-time assessment of structural health, allowing us to identify degenerating research programs and power-masked fragility before crises force recognition. This positions the work not just as a solution to an old philosophical puzzle, but as a practical and urgent research program for the 21st century, empowering democratic publics to hold their knowledge-generating systems accountable.

EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

Reference:
Ladyman, James, and Don Ross. 2007. *Every Thing Must Go: Metaphysics Naturalized*. Oxford: Oxford University Press. ISBN 978-0199276196.
------------------------------------------------------------

Citation 89 [PARENTHETICAL]:
Line: 848
Citation: (Henrich 2015)

Context:
EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

The approach calls for epistemic humility, trading the ambition of a God's-eye view for the practical wisdom of a mariner. The payoff is not a final map of truth, but a continuously improving reef chart—a chart built from the architecture of failure, allowing us to more safely navigate the channels of viable knowledge.

Reference:
Henrich, Joseph. 2015. *The Secret of Our Success: How Culture Is Driving Human Evolution, Domesticating Our Species, and Making Us Smarter*. Princeton, NJ: Princeton University Press. ISBN 978-0691178431.
------------------------------------------------------------

Citation 90 [PARENTHETICAL]:
Line: 848
Citation: (Strevens 2020)

Context:
EPC shares the realist's conviction that convergence reflects constraint, not convention. Like Boyd's (1973) realism and Kitcher's (1993) naturalistic objectivity, it treats truth as an emergent structural attractor—an Apex Network stabilized through evolutionary filtration. Anchored in Ladyman and Ross's (2007) structural ontology, it situates realism within evolutionary epistemology (Godfrey-Smith 2003), where selective success reveals mind-independent structures.

EPC also offers predictive value for empirical domains: science policy diagnostics via evidence-based frameworks (Cartwright and Hardie 2012), cultural evolution modeling (Henrich 2015), and epistemic institutions as cases of engineered low-brittleness inquiry (Strevens 2020).

The approach calls for epistemic humility, trading the ambition of a God's-eye view for the practical wisdom of a mariner. The payoff is not a final map of truth, but a continuously improving reef chart—a chart built from the architecture of failure, allowing us to more safely navigate the channels of viable knowledge.

Reference:
Strevens, Michael. 2020. *The Knowledge Machine: How Irrationality Created Modern Science*. New York: Liveright Publishing. ISBN 1631491377, 9781631491375.
------------------------------------------------------------

Citation 91 [PARENTHETICAL]:
Line: 858
Citation: (Ingthorsson 2013)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
Ingthorsson, Rögnvaldur D. 2013. "Properties: Qualities, Powers, or Both?" *Dialectica* 67, no. 1: 55–80. https://doi.org/10.1111/1746-8361.12011.
------------------------------------------------------------

Citation 92 [PARENTHETICAL]:
Line: 858
Citation: (El-Hani and Pihlström 2002)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
El-Hani, Charbel Niño, and Sami Pihlström. 2002. "Emergence Theories and Pragmatic Realism." *Essays in Philosophy* 3(2): article 3. https://doi.org/10.5840/eip2002325.
------------------------------------------------------------

Citation 93 [PARENTHETICAL]:
Line: 858
Citation: (Rottschaefer 2012)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
Rottschaefer, William A. 2012. "The Moral Realism of Pragmatic Naturalism." *Analyse & Kritik* 34(1): 141–156. https://doi.org/10.1515/auk-2012-0110.
------------------------------------------------------------

Citation 94 [POSSESSIVE]:
Line: 858
Citation: Baysan's (2025)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
Baysan, Umut. 2025. "Emergent Moral Non-naturalism." *Philosophy and Phenomenological Research* 110(1): 1–20. https://doi.org/10.1111/phpr.70057.
------------------------------------------------------------

Citation 95 [IN-PROSE]:
Line: 858
Citation: Bennett-Hunter (2015)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
Bennett-Hunter, Guy. 2015. *Ineffability and Religious Experience*. London: Routledge (originally Pickering & Chatto). ISBN 978-1848934719.
------------------------------------------------------------

Citation 96 [IN-PROSE]:
Line: 858
Citation: Peter (2024)

Context:
The framework for assessing brittleness is general, but its application reveals two primary modalities of failure, corresponding to the different kinds of pragmatic constraints a network can violate. This distinction clarifies how our approach unifies descriptive and normative inquiry under a single explanatory mechanism.

* **Normative Brittleness:** This is the modality of failure resulting from a misalignment with the emergent normative structure of the world. It is found in socio-political and ethical networks whose primary function is to organize cooperative human action. The specific mechanism for this failure can be precisely articulated through a theory of emergent moral properties. Drawing on Baysan's (2025) account of emergent moral non-naturalism, we can understand objective moral properties as conferring noncausal powers (Ingthorsson 2013). While a causal power manifests as a physical change, a noncausal power manifests as the obtaining of a normative fact. For example, the property of being unjust confers on an institution the noncausal power to justify resentment and require condemnation. A network's alignment with this structure is not optional. A society predicated on slavery, for instance, exhibits profound normative brittleness because it must expend immense real-world energy to counteract these noncausal powers. The immense coercive overheads required to maintain the institution are the direct, measurable, and causal signature of a system struggling to suppress the real normative fact that its core practices justify resistance. This account requires that we move beyond what Bennett-Hunter (2015) calls the 'causalist assumption'—the dictum that 'to be real is to have causal power'—and recognize that a causal vocabulary may not be appropriate for every explanatory domain (El-Hani and Pihlström 2002). This macro-level diagnosis finds a plausible correlate in agent-level moral psychology, where moral intuitions can be understood as evolved detectors for such "response-invoking" features of our environment (Rottschaefer 2012) that signal potential systemic costs if ignored, a direct experience of what Peter (2024) calls the "demands of fittingness."

## Appendix B: Operationalizing Brittleness Metrics—A Worked Example

Reference:
Peter, Fabienne. 2024. "Moral Affordances and the Demands of Fittingness." *Philosophical Psychology* 37(7): 1948–70. https://doi.org/10.1080/09515089.2023.2236120.
------------------------------------------------------------

