--- Page 1 ---

“ iusto . 
BRITISH JOURNAL 
FOR susie 
_ PHILOSOPHY OF 
- $CIENCE VOLUME 60 * NUMBER | * MARCH 2009 
C 
ARTICLES o 
pat DW rem DeLeon 
U. E. Stegmann 
The Crux of Crucial Experiments: Duhem’s- peer Fi 
and Inference to the Best Explanation 
NY ALG 
eters Tenet Explaining How Biological Mechanisms C vt od 
B. Calcott wee S 
i eretniotm Gescaleceparinem@koiiateprie (et mam 
; CT : 
When Empirical Success Implies Theoretical Reference: 
A Structural Correspondence Theorem 
CRN G/T pt 
Nonna MOL} s Cet) ate 
P.M. Ainsworth eee 
aan and the Mystery of the Missing Physics | 
er AL 
What Are the New Implications of Chaos for ee 
' C. Werndl 
es ‘i Be. 
HASOK CHANG /m Para Temperature: pee eer 
and Rarer ia UR aNY 
CITAS 
MARKUS SCHRENK The Metaphysics of Ceteris Paribus Laws. 
me Cala 
MICHELA MASSIMI Pauli's Exclusion Principle: 
The Origin and Validation of a Scientific Principle 
she arr) 
Published for the British Society for the Philosophy of Noten 
* UY oi 
EN KEE} A 
fo) (76) ~1B i o\6)-\\) Nae OXFORD UNIVERSITY PRESS

--- Page 2 ---

BRITISH SOCIETY FOR THE PHILOSOPHY OF SCIE 
COMMITTEE 2008-2009 
President Professor Harvey Brown 
President Elect Professor John Worrall 
Hon Secretary Dr Rachel Cooper 
Hon Treasurer Dr Alice Drewery 
Members Dr Tim Lewens, Dr Michela Massimi, 
Dr Chris Timpson, Dr Roman Frigg 
The British Journal for the Philosophy of Science is published for the Society by Oxford University 
Press. The Committee of the Society is responsible for the management of the Journal 
The Society holds regular meetings during the academic year, at which papers are read and 
discussed, in the Centre for Philosophy of Natural and Social Science, Lakatos Building, London 
School of Economics, Houghton Street, London WC2. These are open to members and the general 
public. The Society also holds an annual conference and occasional one-day conferences at different 
venues. 
Membership is open to anybody on payment of the annual subscription. Members can subscribe 
to the Journal at a reduced rate. Members also receive the programme of monthly talks and are 
entitled to vote at the general meetings of the Society. The annual subscription to the Society is £7 
for the UK and Europe, $12 for the USA and elsewhere. Student subscriptions are £1 ($2 outside 
Europe). Those wishing to join should write to the Membership Secretary, British Journal for the 
Philosophy of Science, Oxford Journals, Oxford University Press, Great Clarendon Street, Oxford 
OX2 6DP, UK. 
The past Presidents of the Society comprise an editorial board for the Journal. They are 
J. Butterfield, S. French, D. Gillies, B. Gower, R. Harré, M.B. Hesse, C. Howson, C.W. Kilmister, 
J.R. Lucas, D.H. Mellor, D.J. O°Connor, D.C. Papineau, M.L.G. Redhead 
Further information about the Society and its activities can be obtained at the Society’s web pages 
at www.thebsps.org 
COPYRIGHT 
© British Society for the Philosophy of Science 2009 
All rights reserved; no part of this publication may be reproduced, stored in a retrieval system, or 
transmitted in any form or by any means, electronic, mechanical, photocopying, recording, or other- 
wise without either the prior written permission of the Publishers, or a licence permitting restricted 
copying issued in the UK by the Copyright Licensing Agency Ltd, 90 Tottenham Court Road, London 
WIP 9HE, or in the USA by the Copyright Clearance Center, 222 Rosewood Drive, Danvers, 
Massachusetts 01923, USA 
It is a condition of publication in the Journal that authors assign copyright to British Society for 
the Philosophy of Science. Authors who are unable to assign copyright (eg UK Government employ- 
ees) should request a publication licence from OUP. This ensures that requests from third parties to 
reproduce articles are handled efficiently and consistently and will also allow the article to be as 
widely disseminated as possible. In assigning copyright, authors may use their own material in other 
publications provided that the Journal is acknowledged as the original place of publication, and 
Oxford University Press (on behalf of the Society) is notified in writing and in advance. For infor- 
mation on how to request permissions to reproduce articles/information from this journal, please visit 
www.oxfordjournals.org/permissions 
BOOKS FOR REVIEW 
Books for review should be sent to: The British Journal for the Philosophy of Science, Department 
of Philosophy, Wniversity of Bristol, 9 Woodland Road, Bristol BS8 1 TB. All correspondence concern- 
ing reviews, other than books for review themselves, should be addressed to the Assistant Editor at the 
above address, or by e-mail. 
WEB PAGES 
Further information regarding the Journal can be obtained from the Oxford University Press Journals 
home page www.bjps.oxfordjournals.org

--- Page 3 ---

The British Journal for the 
Philosophy of Science 
Volume 60 e Number 1 e March 2009 
Editors 
ALEXANDER BIRD and JAMES LADYMAN 
Assistant Editor 
STUART PRESNELL 
Editorial Advisory Panel 
JEREMY BUTTERFIELD, NANCY CARTWRIGHT 
DOROTHY EDGINGTON, MARCUS GIAQUINTO 
BARRY GOWER, KEITH HOSSACK 
COLIN HOWSON, MOSHE MACHOVER 
JOHN MILTON, MICHAEL REDHEAD 
PETER URBACH, JOHN WORRALL 
ELIE ZAHAR 
Published for the 
British Society for the Philosophy of Science 
by Oxford University Press

--- Page 4 ---

Mixed Sources Product group from well-managed 
forests and other controlled sources 
www.fsc.org Cert no. TT-COC-002769 
© 1996 Forest Stewardship Council

--- Page 5 ---

Contents 
Articles 
DNA, Inference, and Information 
U. E. Stegmann 
The Crux of Crucial Experiments: Duhem’s Problems and 
Inference to the Best Explanation 
M. Weber 
Lineage Explanations: Explaining How Biological Mechanisms 
Change 
B. Calcott 
Focused Correlation and Confirmation 
G. Wheeler 
When Empirical Success Implies Theoretical Reference: 
A Structural Correspondence Theorem 
G. Schurz 
Newman’s Objection 
P. M. Ainsworth 
Determinism and the Mystery of the Missing Physics 
M. Wilson 
What Are the New Implications of Chaos for Unpredictability? 
C. Werndl 
Reviews 
HASOK CHANG Inventing Temperature: Measurement and 
Scientific Progress 
D. Gillies 
MARKUS SCHRENK The Metaphysics of Ceteris Paribus 
Laws 
A. Reutlinger

--- Page 6 ---

MICHELA MASSIMI Pauli’s Exclusion Principle: The Origin 
and Validation of a Scientific Principle 
H. Kragh 
The British Journal for the Philosophy of Science is covered by the 
following abstracting/indexing services: 
Linguistics and Language Behaviour Abstracts; Institute for 
Scientific Information: Social Science Citation Index, Science 
Citation Index, Arts and Humanities Citation Index, Scisearch, 
Research Alert, Social Scisearch, Current Contents/Life Sciences, 
Current Contents/Arts & Humanities; Periodicals Contents Index; 
The Philosophers Index; Science Culture (Centre National de 
la Recherche Scientifique); Sociological Abstracts/Social Services 
Abstracts; Zentralblatt MATH

--- Page 7 ---

Brit. J. Phil. Sci. 60 (2009), 1-17 
DNA, Inference, and Information 
Ulrich E. Stegmann 
ABSTRACT 
This paper assesses Sarkar’s ({[2003]) deflationary account of genetic information. On 
Sarkar’s account, genes carry information about proteins because protein synthesis 
exemplifies what Sarkar calls a ‘formal information system’. Furthermore, genes are 
informationally privileged over non-genetic factors of development because only genes 
enter into arbitrary relations to their products (in virtue of the alleged arbitrariness of the 
genetic code). I argue that the deflationary theory does not capture four essential features 
of the ordinary concept of genetic information: intentionality, exclusiveness, asymmetry, 
and causal relevance. It is therefore further removed from what is customarily meant 
by genetic information than Sarkar admits. Moreover, I argue that it is questionable 
whether the account succeeds in demonstrating that information is theoretically useful 
in molecular genetics. 
Introduction 
Sarkar’s Information System 
The Pre-theoretic Features of Genetic Information 
3.1. Intentionality 
3.2 Exclusiveness 
3.3. Asymmetry 
3.4 Causal relevance 
Theoretical Usefulness 
Conclusion 
1 Introduction 
Genetic information is generally regarded as being ‘intentional’ and exclusive 
(e.g. Godfrey-Smith [1999]; Sterelny and Griffiths [1999]; Griffiths [2001]). In- 
tentionality in this context is the idea that the information carried by genes is 
of a sort similar to that carried by things like street maps or beliefs. Maps and 
beliefs can represent something as being a certain way it is not. Such ‘semantic’ 
or ‘intentional information’ is often contrasted with natural information, which 
cannot be false. By exclusiveness I mean the view that the information carried 
by genes is only carried by genes, or nearly so. Information makes genes special 
The Author (2009). Published by Oxford University Press on behalf of British Society for the Philosophy of Science. All rights reserved 
doi:10.1093/bjps/axn041 For permissions, please email: journals. permissions@oxfordjournals org 
Advance Access published on January 8, 2009

--- Page 8 ---

2 Ulrich E. Stegmann 
and separates them from non-genetic factors of development. Intentionality 
and exclusiveness are among the essential ingredients of the customary notion 
of genetic information. They should be reconstructed by any account purport- 
ing to defend this notion as scientifically respectable. 
Yet, the two ingredients have proven difficult to reconcile. Teleosemantic 
theories provide accounts of intentionality based on evolutionary function 
(Sterelny et al. [1996]; Maynard Smith [2000]). A trait can have a function 
while failing to satisfy it. By assimilating information to function, teleosemantic 
theories hope to make sense of the idea that a gene can carry information while 
failing to properly express it. Though prominent, teleosemantic theories do 
not deliver exclusiveness: they scatter semantic information generously across 
both genetic and non-genetic factors of development (Godfrey-Smith [1999]; 
Griffiths [2001]).! One response to this difficulty is to reject the notion of 
genetic information altogether (e.g. Weber [2005]). Another is to continue the 
quest for reconciling intentionality with exclusiveness, e.g. by modifying the 
teleosemantic approach (Shea [2007]) or by developing alternative accounts 
of intentionality, such as those provided by the developmental role theory 
(Godfrey-Smith [2000]) and the instructional theory of genetic information 
(Stegmann [2005]). The third response is to abandon one of these components 
in order to make genetic information a defensible concept. For instance, by 
surrendering exclusiveness in favour of intentionality, development becomes 
a process guided by semantic information, residing in both genes and a great 
variety of non-genetic factors. This option is in line with the parity thesis 
(Griffiths [2001]). According to the parity thesis, both genetic and non-genetic 
factors carry information on any legitimate theory of that notion. So, if a 
defensible notion of semantic information were available, both genetic and 
non-genetic factors would have it, and this fact would rule out exclusiveness. 
The second option is to surrender intentionality in favour of exclusiveness. 
This move is defended by Sarkar ({2003]), who rejects any demand that theo- 
ries of genetic information need to involve semantic information in order to 
be adequate.- He calls his account ‘deflationary’ for giving up on intention- 
ality. Roughly, the deflationary account aims to capture our general sense of 
information by defining a ‘formal information system’. This system is then 
found to be (sometimes) instantiated by the mechanisms of protein synthe- 
sis. Hence, genes are said to carry information in a perfectly legitimate sense. 
Furthermore, Sarkar accepts that many non-genetic factors of development 
' As Wheeler ([{2003]) pointed out, teleosemantic theories also suffer from the opposite difficulty 
in being too restrictive; they exclude genes that have not been selected for, such as hitchhiking 
genes. 
The account is foreshadowed in (Sarkar [2000]). Furthermore, (Sarkar [2005]) supplements 
(Sarkar [2003]) with a section on the methodological challenges for demonstrating that genetic 
codes other than the standard code were evolutionarily possible.

--- Page 9 ---

DNA, Inference, and Information 3 
frequently carry the same sort of information, but he maintains that genes are 
nevertheless informationally privileged because the genetic code is arbitrary, 
whereas the non-genetic factors do not enter into arbitrary relations. And, ac- 
cording to Sarkar, arbitrariness together with our general notion of information 
yields ‘semiotic’ information, which is unique to genes. 
This paper assesses Sarkar’s defence of genetic information. I first expound 
the deflationary account, arguing that both ‘specific’ and ‘semiotic’ information 
are versions of what is known as epistemic information (Fodor [1990]); roughly, 
A carries information for B if we can learn from A something about B. The 
following sections then explore the extent to which the deflationary account 
captures the ordinary notion of genetic information. While Sarkar may not 
intend his account to do justice to that notion, he does assert that it happens 
to capture a good portion of it. Yet I aim to show that the deflationary account 
vastly departs from the customary notion, and to a greater extent than Sarkar 
maintains. The final section assesses the theory by its explicitly stated goal, 
which is to show that the concept of information is useful in molecular genetics 
(Sarkar [2003], p. 261). On this separate score, I believe that the theory does 
better, although its usefulness still turns out to be rather modest. 
2 Sarkar’s Information System 
Sarkar’s ‘formal information system’ is defined as consisting of two sets, A and 
B, and a relation connecting A-elements with B-elements. The relation also 
connects the sets as a consequence of connecting their elements. The relation is 
called ‘information relation’ and denoted by ‘v’. It will become apparent below 
why Sarkar regards this relation as informational. For the moment, it should be 
stressed that no constraints are placed on the obtaining of this relation between 
an A- and a B-element other than that they relate to one another. For example, 
they do not need to cause one another. 
Formal information systems are characterised by two additional features. 
First, every B-element is related to at least one A-element (but not necessarily 
vice versa). Second, each set may contain ‘equivalence classes’, which con- 
sist of ‘informationally equivalent’ A- or B-elements, respectively. Sarkar does 
not define informational equivalence. Instead he gives as an example of an 
equivalence class the set of DNA triplets that specify one kind of amino acid 
(p. 267).° So if set A consists of DNA triplets and set B amino acids, then the 
informationally equivalent DNA triplets are those that relate to one and the 
same amino acid. This example suggests that A-elements are informationally 
equivalent if they bear an information relation to one and the same B-element 
(cf. footnote 5). 
Throughout, page numbers refer to (Sarkar (2003]) unless noted otherwise.

--- Page 10 ---

Ulrich E. Stegmann 
Two conditions guarantee increasing degrees of specificity. 
1. ‘Differential specificity: suppose that a and a’ belong to different 
equivalence classes of A. Then, if i(a, b) and «(a’, b’) hold, then b 
and b’ must be different elements of B.’ 
‘Reverse differential specificity: if «(a, b) and ua’, b’) hold, and b 
and b’ are different elements of B, then a and a’ belong to different 
equivalence classes in A.’ (p. 267) 
According to Sarkar, both conditions hold with respect to protein synthesis 
in prokaryotes, but only the first holds in eukaryotes (p. 270). In order to un- 
derstand these claims, it is useful to assess the extent to which these conditions 
are satisfied in certain set theoretic relations. For it then becomes apparent 
that prokaryotes and eukaryotes differ because they instantiate different set 
theoretic relations and because only some of these relations satisfy both the 
specificity conditions. 
Four classes of set theoretic relations are important here: one-to-one, many- 
to-one, one-to-many, and many-to-many relations. I take it that all four rela- 
tions satisfy differential specificity. This is because differential specificity ap- 
pears to be a consequence of considering non-equivalent A-elements (although 
Sarkar does not put it this way). Pick any two A-elements, a and a’, and the 
requirement that they belong to different equivalence classes ensures that they 
relate to distinct B-elements. After all, for a and a’ to be non-equivalent is 
for them to relate to distinct B-elements, rather than to the same one. This 
consequence holds in all four classes. Reverse differential specificity, however, 
holds only in one-to-one, many-to-one, and one-to-many relations.* In many- 
to-many relations, the B-elements may be related to informationally equivalent 
A-elements, in which case reverse differential specificity does not obtain. Sup- 
pose a relates to two B-elements, b and b’, and there is a second A-element, a’, 
which relates to b’. If we now pick the two distinct elements 5 and b’, then the 
corresponding A-elements, a and a’, will belong to the same equivalence class 
because they both specify 5’.° 
4 Whether reverse differential specificity holds in one-to-many relations depends on whether a and 
a ought to be distinct. If a and a’ must be distinct, then any two distinct B-elements will relate to 
informationally non-equivalent A-elements and, therefore, reverse differential specificity holds. 
On the other hand, if they need not be distinct, then the two B-elements may be related to the 
same A-element. Consequently, a and a’ would not belong to different equivalence classes and 
reverse differential specificity would therefore not apply. 
One referee suggested that this example would work only if a’ also related to 5 (in addition to 5’), 
such that both a and a’ related to exactly the samme B-elements, on the grounds that the latter is 
required for a and a’ to belong to the same equivalence class. However, the sort of equivalence 
Sarkar needs in order to exclude reverse differential specificity in eukaryotes is not available on 
this strong version of equivalence, though it is on the weak one adopted here (if two A-elements 
relate to at least one common B-element, then they belong to the same equivalence class). 
Consider again the original example of a many-to-many relation (a relates to both b and b’, but 
da’ relates only to 5’). On the strong version, a and a’ are non-equivalent and the corresponding

--- Page 11 ---

DNA, Inference, and Information 5 
Now, the molecular genetic mechanisms of protein synthesis instantiate some 
of the four set theoretic relations. The DNA triplets in prokaryotes specify 
amino acids in a many-to-one mode, because several codons may specify one 
amino acid (degeneracy of the genetic code). Since the relation is many-to-one, 
both conditions (1) and (2) obtain in prokaryotic protein synthesis. By contrast, 
eukaryotes only satisfy condition (1). Their codons and amino acids not only 
relate many-to-one as in prokaryotes (due to the degeneracy of the code), 
but the relation is also one-to-many. For example, RNA editing may alter an 
RNA codon transcribed from a DNA triplet such that one kind of DNA triplet 
may come to specify several kinds of amino acids. As a consequence, eukaryotic 
codons relate to amino acids in a many-to-many mode, which precludes reverse 
differential specificity but satisfies differential specificity. 
Sarkar introduces two further conditions, which together are intended to 
capture the idea that A-elements are arbitrarily related to B-elements. The 
first condition is ‘medium independence’ (‘Al’, p. 268), according to which 
there is ‘no preferred representation’ of an information system. The absence 
of a preferred representation seems to consist in all implementations being 
‘epistemologically on a par with each other’ (p. 268). I understand this condition 
as follows (for an alternative reading see Section 3.2). If there are two physically 
different yet isomorphic implementations of an information system, then we 
can learn from the A-elements of implementation | as much about B as we 
can learn from the A-elements of implementation 2. This interpretation fits 
Sarkar’s example for medium independence, i.e. a physical piece of DNA that 
is isomorphic to a string of symbols on paper. For we can learn the same things 
from either implementation, i.e. we can derive the same amino acid sequence. 
The second condition for arbitrariness is ‘template assignment freedom’ (‘A2’, 
p. 268). This condition requires that various alternative assignments between 
A- and B-elements are evolutionarily possible. Again, this condition is arguably 
satisfied by the genetic code, because the current assignment of codons to amino 
acids is one of several evolutionarily possible outcomes.° 
b and b’ are distinct, which satisfies differential specificity. Furthermore, the distinct 6 and 6 
pick out the non-equivalent a and a’, thus satisfying reverse differential specificity. The strong 
version of equivalence therefore restores reverse differential specificity even in some many-to- 
many cases. Of course, the weak version has the counter-intuitive consequence that two 4- 
elements sharing one B-element are informationally equivalent even if they relate to otherwise 
vastly different corresponding sets of B-elements. 
Sarkar ((2003]) argues that the evolutionary possibility of alternative assignments is supported by 
the frozen accident hypothesis (Crick [1968]). Sarkar ({2005], p. 275) elaborates on this point as 
follows. There may be several local optima for energy efficient codon—amino acid assignments, one 
of which is occupied by the standard genetic code. The one occupied was the optimum accessed 
first in evolution. But this fact is accidental; a different optimum might have been accessed first. 
Note that this sense of accidental, the possibility of alternative ‘solutions’ to a given selection 
regime, is not the sense of accidental used in Crick’s hypothesis. For Crick, the genetic code is 
accidental because the initial stages of code evolution, which lead to the first assignments, were 
not driven by selection at all (cf. Section 3.2)

--- Page 12 ---

Ulrich E. Stegmann 
The point of these various conditions is, of course, that on Sarkar’s view they 
justify describing the relation between sets A and B in informational terms. The 
idea is that A carries some degree of information for B depending on which 
of the conditions is satisfied. By definition, if (1) obtains, A carries ‘specific 
information’ for B; if both (1) and (2) obtain, then ‘A alone carries specific 
information for B’ (p. 267); and if (1), (Al) and (A2) obtain, then A contains 
‘semiotic information’ for B (p. 270) or, in other words, A ‘encodes’ B (p. 269). 
As it turns out, genetic and non-genetic factors differ with respect to which 
conditions they satisfy. The genetic code satisfies the arbitrariness conditions 
in both prokaryotes and eukaryotes. It also satisfies conditions (1) and (2) 
in prokaryotes and (1) in eukaryotes. Hence the overall! conclusion that ‘DNA 
encodes proteins; for prokaryotes, DNA alone encodes proteins’ (p. 269). While 
not excluding the possibility that non-genetic factors of development could in 
principle satisfy all conditions, Sarkar asserts that, as a matter of fact, they 
at most carry some degree of specific, but not semiotic, information: ‘So far, 
there is no evidence that any of them satisfy (Al) and (A2)’ (p. 270). Genes 
are therefore privileged over all other developmental factors in being the only 
carriers of semiotic information. The exclusiveness of genetic information is 
thus restored. 
Before discussing various difficulties, one step in Sarkar’s argument still needs 
to be addressed. The argument moves from certain conditions to information, 
but what licenses this move? All Sarkar says about why his formal information 
system is an information system is this: ‘The justification for these conditions 
[1 and 2] is that they capture what is customarily meant by information in any 
context: for instance, they capture the sense in which the present positions and 
momenta of the planets carry information about their future positions’ (p. 267). 
We are not told more about this customary sense. But presumably the sense 
in which the present positions and momenta of the planets carry information 
about their future positions is that knowledge of the present positions allows 
us to infer, together with accepted theories, the future positions. On this notion 
of information, A carries information for B if one can learn about B from A. 
Sarkar’s notion of information therefore appears to be a species of natural 
information, i.e. an ‘epistemic access theory’ of indication (Fodor [1990]).’ Pre- 
sumably therefore, Sarkar regards : as an informational relation because, given 
the above conditions, it licenses inferences and, hence, grounds an epistemic 
sense of information. 
In Fodor’s words, ‘R represents S if you can find out about S from R’ ({1990], p. 34). This is the 
epistemic, as opposed to the causal, theory of indication. Fodor uses the term ‘representation’ 
instead of ‘indication’.

--- Page 13 ---

DNA, Inference, and Information 7 
It is tempting to think ttiat the notions of semiotic and specific information 
stand for different sorts of information, not least because Sarkar explicitly dis- 
tinguishes semiotic from semantic information and also from the information 
of communication theory (pp. 262 and 264, respectively). It may seem as if, on 
the deflationary account, the privilege of genes is a privilege to an exclusive 
sort of information. But this is not what is shown. The (intended) difference 
between genetic and non-genetic factors is that genes are arbitrarily related to 
some developmental outcomes, whereas non-genetic factors are not. Yet this 
fact makes no difference to the kind of information carried by genetic and 
non-genetic factors, respectively. The information is epistemic in either case. 
Semiotic information is just the epistemic information of arbitrary systems. 
There is no reason why the epistemic information within the class of arbitrary 
systems should be somehow different from the epistemic information carried 
by non-arbitrary systems. 
Let us now see whether the deflationary theory captures the notion of genetic 
information as customarily understood. 
3 The Pre-theoretic Features of Genetic Information 
3.1 Intentionality 
The most obvious worry with the deflationary account is the very idea of aban- 
doning intentionality. For, since the customary notion of genetic information 
appears to be semantic, any adequate theory of this notion needs to account 
for intentionality. Sarkar briefly considers this worry, and I take the following 
quote to be the essence of his remarks: 
There is no reason to suppose that any concept of biological information 
must be ‘semantic’ in the sense that philosophers use that term. Biological 
interactions, at this level, are[. . .] not about meaning, intentionality, and the 
like; any demand that such notions be explicated in an account of biological 
information is no more than a signifier for a philosophical agenda inherited 
from manifestly nonbiological contexts, in particular from the philosophy 
of language and mind. It only raises spurious problems for the philosophy 
of biology. (p. 262) 
This remark may appear to beg the question. If biological interactions indeed 
lacked semantic properties, then theoretical accounts could safely ignore them. 
But whether they do has not been decided one way or another. This issue is a 
crucial part of what is at stake in the debate. 
However, the move to abandon intentionality should not be dismissed 
too quickly. Perhaps the biologists’ usual understanding does not involve

--- Page 14 ---

8 Ulrich E. Stegmann 
intentionality after all.® If it is not a feature of the customary notion, theories 
aiming to explicate this notion will not need to capture it.” But Sarkar offers no 
argument to the effect that intentionality is not such a feature. Although biol- 
ogists like Richard Lewontin ({2001]) endorse a deflationary understanding, '° 
it is not in line with the way in which genetic information is generally used. For 
instance, one of the dominant pre-theoretic notions of genetic information, the 
one featuring in the ‘central dogma’, is ‘hereditary information required for 
sequentialization’ (Crick [1958], p. 144), i.e. information required for arranging 
amino acids (and nucleic acids) into specific sequences during polymerisation. 
If taken literally, information about how to arrange amino acids can be imple- 
mented correctly or incorrectly, just as information about how to bake a cake. 
And since even the most frequent way of implementing a baking recipe may 
be incorrect, correctness on the literal reading is not a matter of producing the 
most frequent result. This in turn motivates the view that customary notions 
of genetic information involve a strong form of error, which is evaluative and 
non-statistical. 
3.2 Exclusiveness 
In contrast to intentionality, Sarkar takes his account as preserving and ac- 
counting for exclusiveness, because no non-genetic factor of development sat- 
isfies the arbitrariness conditions. Semiotic information is therefore carried 
only by genes (p. 270). In this section, I argue that the deflationary account 
fails in so privileging genes. As the arbitrariness conditions are formulated, 
they are satisfied by a range of non-genetic factors for development and even 
by biological entities unrelated to development. 
First, consider the role of auxin in early angiosperm embryogenesis. Auxin is 
a ‘plant hormone’ that establishes the apical—basal axis of the angiosperm em- 
bryo and is thus a non-genetic and non-environmental factor of development. 
Angiosperm zygotes divide into the apical cell, which develops into the proem- 
bryo, and the basal cell, which develops into the suspensor. Auxin accumulates 
in the apical cell and its presence is crucial for specifying the apical cell’s fate, 
i.e. for its becoming the founder cell of the proembryo (Friml et al. [2003]). At 
this early stage of development, auxin therefore relates one-to-one to cell fate, 
This is indeed Sarkar’s claim (personal communication). 
It should be noted that a semantic theory of genetic information could be appropriate even if the 
pre-theoretic notion was not semantic. A semantic theory might be needed to account for the 
biological facts or it might be shown to be explanatory. But then it would not be an explication 
of the pre-theoretic notion. 
Lewontin ((2001]) writes in his review of Lily Kay’s 2000 monograph Who Wrote the Book of 
Life?: ‘[To] say that DNA contains determinative information about amino acid sequences is 
simply to say that a knowledge of the DNA sequence is sufficient to provide knowledge of the 
amino acid sequence but not vice versa’; he suggests, moreover, that any other understanding of 
information would be metaphoric and misleading.

--- Page 15 ---

DNA, Inference, and Information 9 
i.e. auxin relates to becoming the proembryo, whereas its absence relates to 
becoming the suspensor. Since both differential and reverse differential speci- 
ficity hold for one-to-one relations, auxin concentration alone carries specific 
information for cell fate. This conclusion is unproblematic for Sarkar, because 
he explicitly accepts that non-genetic factors may carry specific information for 
development (p. 270). 
However, auxin concentrations also satisfy the arbitrariness conditions. 
First, the physical auxin-cell fate system is epistemically on a par with its 
isomorphic representation on a piece of paper insofar as we can infer the 
same conclusions concerning cell fate from either. The system therefore ex- 
hibits medium independence. It also exhibits template assignment freedom.!! 
For although auxin induces the transcription of auxin-dependent genes in an- 
giosperms today (Jenik and Barton [2005]), it seems evolutionarily possible that 
auxin could act as a transcriptional repressor (a protein might evolve specificity 
to auxin, the binding of which might prevent formation of the transcription 
complex). If so, the auxin-cell fate assignment would be reversed (everything 
else being equal): high auxin concentrations would repress transcription and 
therefore specify suspensor fate, whereas low concentrations would induce tran- 
scription and specify proembryo fate. Hence, auxin concentration alone carries 
semiotic information for cell fate. 
The second example concerns environmental factors for development. Many 
aquatic plants respond flexibly to varying environmental conditions by develop- 
ing differently shaped leaves (plastic heterophylly). One well-studied species is 
Proserpinaca palustris, which inhabits shallow, seasonally flooded depressions 
of wetland floors (Wells and Pigliucci [2000}). During winter, the submerged 
shoots produce filamentous leaves. In spring, vertical shoots rise through the 
water column and increasingly produce entire (lanceolate) leaves. Submergence 
is but one of several factors determining whether a given plant individuai will 
develop filamentous or entire leaves. Short daylengths result in filamentous 
leaves irrespective of whether the shoot apex was submerged or not. Long 
daylengths and air exposure generate entire leaves, as do long daylengths in 
combination with submergence and high temperatures. The relation between 
environmental factors and leaf shape is therefore many-to-one and the factors 
alone carry specific information for leaf shape. 
The environment-—leaf system also exhibits arbitrariness. From physical in- 
stances of environmental factors, we can learn as much about leaf shape as 
we can learn from an isomorphic system on a piece of paper (medium inde- 
pendence). Furthermore, given adequate selection pressures, it seems possible 
'! Of course, auxin concentrations are not templates for cell fate. But Sarkar’s condition of template 
assignment freedom only requires that two sets of entities, which need not include templates, can 
be evolutionarily reassigned to one another.

--- Page 16 ---

10 Ulrich E. Stegmann 
that the environmental variables could generate the reverse leaf shapes in 
P. palustris (e.g. short daylengths specifying entire instead of filamentous 
leaves), thereby satisfying template assignment freedom. For instance, a rever- 
sal of the environment-—leaf assignment should be expected under sufficiently 
strong selection for a low relative surface area in submerged, rather than aerial, 
leaves (and vice versa). There is no obvious constraint on aquatic plants that 
their submerged leaves must be filamentous and their aerial leaves entire. Since 
the environment-leaf system exhibits an arbitrary many-to-one relation, we 
should conclude on Sarkar’s account that the environment alone carries semi- 
otic information for leaf shape. 
Finally, semiotic information is found in biological systems unrelated to 
development. Many animal taxa are vertically stratified in tropical rainforests. 
Birds in Costa Rican lowland forests provide one example. Whereas ground 
doves and wrens inhabit the forest floor, and hummingbirds and flycatchers the 
understorey, the canopy top is inhabited by yet other bird taxa, notably toucans 
and parrots (Bourliére [1983]). Butterflies, too, can be vertically stratified. In a 
Bornean rainforest, some species inhabit the canopy, whereas others live in the 
understorey (Schulze et al. [2001]). Thus, forest layers (or height intervals) relate 
one-to-many to their animal inhabitants, thereby carrying specific information 
for them. In addition, assignments of forest heights to animals appear to be 
arbitrary in Sarkar’s sense. First, the physical system is epistemically on a 
par with an isomorphic instantiation of that system on a piece of paper; in 
either case we can learn from forest height about the inhabitants. Second, the 
forest layer—animal system exhibits template assignment freedom. True, it is 
unlikely that, say, highly specialised fruit-feeders like toucans would evolve 
into ground-dwelling anteaters, or wrens into large-winged birds that roam 
the tree tops. But even such dramatic reversals can hardly be excluded as 
evolutionarily impossible. So again there is semiotic information. The forest 
layer—animal system manifests an arbitrary one-to-many relation and we are 
therefore committed on the deflationary account to saying that forest layers 
encode their animal inhabitants. 
Let me consider two responses to these examples. First, one might object 
to interpreting the condition of medium independence in terms of epistemic 
parity; representational symmetry might seem the better interpretation. Take 
Sarkar’s example of a system that does nor satisfy medium independence: the 
present, physical states of the planets (p. 268). He says that an isomorphic 
paper implementation of the planets’ states represents the physical planets in 
a way in which the physical planets do not represent the paper implementa- 
tion. There is a representational asymmetry between the two implementations 
of this information system. Now, the example nonetheless satisfies epistemic 
parity; we can learn from either implementation equally well about the planet’s 
future positions. Since epistemic parity occurs in an example that lacks medium

--- Page 17 ---

DNA, Inference, and Information 11 
independence, it cannot be equivalent to medium independence. So, medium 
independence is better interpreted as representational symmetry between two 
isomorphic implementations of an information system. 
It is correct that this example fits ill with the epistemic parity interpreta- 
tion of medium independence. However, if medium independence is taken to 
mean representational symmetry, we encounter a consequence that appears to 
be worse: the genetic code turns out to be non-arbitrary on the deflationary 
account, because it is not representationally symmetric. A paper implementa- 
tion of the physical relations between codons and amino acids represents those 
relations in a way in which the physical relations do not represent the paper 
implementation. It is not clear why a paper implementation should be represen- 
tational with regard to planets but somehow less so with regard to the genetic 
code. In order to avoid this difficulty, I interpreted medium independence as 
epistemic parity. 
Here is a second response to these examples. Why not introduce a more 
demanding notion of arbitrariness and of template assignment freedom in par- 
ticular? For instance, one might suggest that assignment freedom is a matter 
of randomness, rather than of evolutionary contingencies. The thought may be 
this. According to Crick’s ({1968]) frozen accident hypothesis of code evolution, 
the assignments during the incipient stages of code evolution were entirely ac- 
cidental. The basis of assignment freedom is therefore the randomness of these 
early assignments, not the possibility that different selection regimes would 
have generated different assignments, or that a given selection regime could 
have led to various alternative assignments (cf. footnote 6). Since my examples 
exhibit assignment freedom only in the sense of evolutionary contingency, they 
are not genuine counter-examples if assignment freedom is construed as ran- 
domness. The difficulty with this move is that the frozen accident hypothesis 
is poorly supported and probably false (Knight et al. [1999]). So if assignment 
freedom were to be construed as randomness, not even the genetic code would 
exhibit assignment freedom and, hence, arbitrariness. While there are other no- 
tions of arbitrariness (Stegmann [2004]) that may be more amenable to Sarkar’s 
purposes, I shall not pursue this issue further here. 
It seems fair to conclude that the conditions of arbitrariness, as proposed 
by Sarkar, endow many non-genetic factors with semiotic information for de- 
velopment. Contrary to his assertion, there is ample evidence that non-genetic 
factors satisfy his arbitrariness conditions. 
3.3 Asymmetry 
There is an additional worry about the deflationary account, concerning 
the alleged direction of genetic information. On the pre-theoretic notion, 
DNA carries information for proteins (and perhaps for other developmental

--- Page 18 ---

12 Ulrich E. Stegmann 
outcomes), but not vice versa. It seems to me that the specificity and arbitrari- 
ness conditions do not deliver this asymmetry. 
Let set A consist of prokaryotic amino acids and set B the correspond- 
ing DNA triplets. Nothing about the formal information system blocks this 
exchange of elements between A and B. The amino acids are then related 
one-to-many to their DNA triplets and the system therefore exhibits differ- 
ential specificity: pick any two amino acids belonging to different equivalence 
classes and the corresponding triplets will be distinct. Proteins therefore contain 
‘specific information’ for DNA. What is more, proteins encode tiseir specific in- 
formation for DNA. This is because the prokaryotic genetic code satisfies both 
conditions for arbitrariness. First, the evolutionary possibility of reassigning 
DNA triplets to different amino acids (which Sarkar accepts) implies the re- 
verse, i.e. that amino acids could be reassigned to different triplets (template 
assignment freedom). Second, we can learn from physical amino acid sequences 
as much as we can learn from isomorphic symbols on paper (medium indepen- 
dence). Hence, prokaryotic proteins encode specific information for DNA just 
as DNA encodes specific information for proteins. 
Three responses might be offered. First, one might deny that asymmetry 
fails. If genetic information is identified with the epistemic information that is 
carried by genes, as opposed to the epistemic information carried by proteins, 
then genetic information is asymmetric after all. For the epistemic information 
gained from nucleic acids has its source in nucleic acids and is about, say, 
proteins, whereas the information in the opposite direction (from proteins 
to nucleic acids) is, by definition, not genetic information. This response is 
beside the point, however, because the worry is that proteins carry the same 
kind of information about DNA as DNA carries about proteins, i.e. semiotic 
information. Hence, DNA is not informationally privileged over proteins in 
the sense that it carries information of a sort which proteins do not. 
Second, one might place restrictions on the information relation such that 
the desired asymmetry is restored. For example, A-elements may be required 
to be the causes of B-elements or to causally contribute to their obtaining. 
Since in protein synthesis DNA causally contributes to producing polypeptides, 
but not the other way around, DNA carries information about polypeptides, 
but not vice versa. But this suggestion is unsatisfactory. Apart from being ad 
hoc, informational relations do not in general depend on the carrier causing 
whatever the information is about. As testified by the famous trails of quails 
(Dretske [1988]), it is often the effects that carry information about their causes. 
Also, we can in fact gain some knowledge about prokaryotic DNA templates 
from the polypeptides they determine. Using the genetic code we can infer for 
any amino acid either a specific codon or a set of codons. As far as epistemic 
information is concerned, we do not want to exclude proteins from having it. A 
dilemma ensues: the information relation either remains unrestricted, in which

--- Page 19 ---

DNA, Inference, and Information 13 
case genetic information is symmetric, or it is restricted, in which case proteins 
are denied the sort of information they uncontroversially carry. 
Finally, one may object that when the A-elements are amino acids, then 
there are more B-elements per A-elements as compared to the case when the 
A-elements are DNA triplets. That is, the one-to-many component is stronger 
in the first instance (this is because it is due to degeneracy rather than RNA 
editing, and the former generates relatively more B-elements per A-element). 
Consequently, proteins carry less semiotic information for DNA than DNA 
carries for proteins, and this may be seen as a reason to drop information 
talk with respect to proteins altogether. As Sarkar observes, heterogeneity 
can compromise the utility of informational interpretations (p. 270). However, 
heterogeneity and utility are matters of degree. Even if proteins carry very little 
information, it still is semiotic information. 
Ultimately, the deflationary account cannot block the flow of information 
from proteins back to DNA. It is likely that this is an unwanted result by the 
theory’s own lights. As Sarkar observes repeatedly, on his account DNA carries 
semiotic information for proteins. And while he does not explicitly say that his 
account excludes the reverse, this appears to be the understanding. 
3.4 Causal relevance 
Finally, it is difficult to see how epistemic information plays the alleged causal 
role within organisms. On the customary understanding, genetic information is 
causally effective; it helps to bring about certain developmental outcomes. For 
instance, it can guide protein synthesis by determining the linear arrangement 
of amino acids. But given that our inferences are not even components of 
biological systems, how could they play the relevant causal role? 
One might be tempted to restore causal relevance for the deflationary ac- 
count as follows. For A to carry information for B is for A to stand in a relation 
to B that allows us to draw from A inferences about B. What is constitutive 
of epistemic information is not our ability to make inferences, but rather a 
certain relation between A and B. Whether this relation obtains is an empirical 
matter and independent of drawing inferences. Furthermore, these relations 
are exemplified by causally relevant entities in the case of protein synthesis. The 
many-to-one relation between prokaryotic codons and amino acids is instan- 
tiated by a set of causal processes, which include stereochemical interactions, 
and so on. So, since the informational relations are realised by causal processes, 
the former may appear to be causally relevant. 
But in the case of prokaryotic codons, the many-to-one relation is still distinct 
from the causal relation between codons and amino acids. On Sarkar’s account, 
the information relation is not identified with a causal relation, though it may be 
exemplified by items that also exemplify causal relations. There is therefore no

--- Page 20 ---

14 Ulrich E. Stegmann 
sense in which codons causally contribute to producing amino acid sequences 
in virtue of the fact that they instantiate an informational relation. Consider, by 
way of contrast, a version of the teleosemantic theory (Sterelny et al. [1996]). 
It has the potential to make information causally relevant because carrying 
information for, say, peptide sequences is identified with having the function 
to cause peptide sequences. To the extent it is reasonable to say that codons 
specify peptide sequences in virtue of the codons’ function to cause certain 
amino acids to be added to the peptide chain, it is reasonable to say that they 
specify amino acid sequences in virtue of carrying information for them. 
4 Theoretical Usefulness 
The lesson from the previous sections is that Sarkar’s proposal fails to capture 
four essential components of the pre-theoretic notion of genetic information, 
i.e. intentionality, exclusiveness, asymmetry, and causal relevance. On this score, 
then, it is inadequate. But this entails nothing with respect to the proposal’s 
value as a substantive theory of genetic information. By a substantive theory!” 
I mean one which asserts that genes carry something appropriately called ‘in- 
formation’, and which provides an account of this information. I take it that 
Sarkar wants to provide a substantive theory in this sense, because he writes: 
‘Either informational talk should be abandoned altogether or an attempt must 
be made to provide a formal explication of “information” that shows that it can 
be used consistently in this context and, moreover, is useful.’ And he goes on to 
say that he provides ‘a sketch of one such attempted explication’ (p. 261). Such 
a theory need not capture the customary sense because this sense may overlap 
little with any defensible kind of information. Perhaps, the pre-theoretic idea 
that information is intentional is untenable; indeed, it may turn out that none 
of the four features can be salvaged from the customary notion. 
Let us see then whether the deflationary account fares better as a substantive 
theory. Does it show that attributing information to genes is useful? Sarkar does 
not qualify usefulness or explain explicitly how his account is useful. But we can 
get a grip on this issue by construing usefulness as theoretical usefulness and, 
in particular, as predictive and explanatory power. This construal is reasonable 
for a theory and it is the one Sarkar adopted in earlier papers ([{1996], [2000)). 
First consider predictive power. The many-to-one relation between prokary- 
otic DNA and amino acids enables us to predict amino acid sequences from 
DNA sequences. In eukaryotes, the complexities of post-transcriptional and 
post-translational modifications result in the many-to-many specifying prop- 
erties of the DNA template, and this strongly restricts the extent to which 
'2 Not to be confused with Sarkar’s ({2000]) ‘substantive’ role of theories or concepts, which they 
have when they ‘explicitly occur [.. .]’ in a scientific entity.

--- Page 21 ---

DNA, Inference, and Information 15 
predictions are feasible. So the informational relation, :, provides some predic- 
tive power, although it is effectively restricted to prokaryotes. 
A similar picture emerges with respect to explanatory power. The sequence 
of prokaryotic amino acids is causally determined, and explained, by the DNA 
template sequence in conjunction with the many-to-one specifying properties of 
prokaryotic codons. Since the many-to-one relation is an instance of Sarkar’s 
informational relation, 1, this relation is explanatory for prokaryotic protein 
synthesis. In eukaryotes, however, this relation is many-to-many. This very fact 
limits the extent to which : causally determines (together with a template) a 
given amino acid sequence. In eukaryotes, the informational relation therefore 
only plays a limited role in explaining amino acid sequences. 
Thus, the informational relation 1 is to some extent predictive and explana- 
tory. But it is questionable whether this translates into predictive and explana- 
tory power for the concept of information. One reason is that the modest extent 
to which the relation is predictive and explanatory may be regarded as being 
insufficient for licensing information talk. This was Sarkar’s earlier position 
([1996]): he argued that the customary notion of genetic information as (DNA) 
sequence is of limited predictive and explanatory usefulness, for the reasons 
outlined above, and should therefore be abandoned.'* Yet his new account is 
explanatory and predictive to the same extent, and for the same reasons. 
More importantly, on an epistemic access theory of information, our ability 
to draw inferences from A about B is essential for what it is for A to carry 
information about B. Without anyone having this ability, A would not be infor- 
mational. Carrying information does not reduce to whatever enables or justifies 
our inferences, i.e. « according to the deflationary account. The explanatory 
power of « and that of A’s carrying information about B may therefore come 
apart; epistemic information may do no explanatory work at all even though 
t, one of its components, does. Indeed, this seems to be the case with respect 
to explaining amino acid sequences. The explanation for why such-and-such 
amino acid sequence arose is not that we were able to infer it from its template 
sequence (epistemic information). It is rather that there was such-and-such a 
template sequence with many-to-one specifying properties. In other words, t’s 
explanatory value does not render explanatory the attribution of (epistemic) 
information. 
5 Conclusion 
The deflationary account promises to show that the concept of information 
has a legitimate theoretical role to play in molecular genetics. I doubt that 
'3 Sarkar emphasises that in his 1996 paper, he did not advocate an outright, but rather a conditional, 
rejection of genetic information: it is illegitimate if we cannot provide a technical explication 
different from the usual one in terms of coding (personal communication; cf. [2003], p. 261).

--- Page 22 ---

16 Ulrich E. Stegmann 
the account redeems its promise. It turns genetic information into a species 
of epistemic information and it secures only a limited degree of usefulness 
for the notion of information, if any at all. Moreover, the notion of genetic 
information we end up with is far removed from ordinary usage. In deflating 
genetic information we lose sight of what appear to be four essential features of 
the pre-theoretic notion: intentionality, exclusiveness, asymmetry, and causal 
relevance. This in itself would be unproblematic if indeed they could not be 
salvaged. But just which features can be salvaged from the ordinary notion of 
genetic information has not been settled. 
Acknowledgements 
My thanks to David Papineau, Sahotra Sarkar and two anonymous referees 
for comments on earlier versions of this paper. Funding was provided by the 
British Society for the Philosophy of Science (doctoral fellowship) and the 
British Academy (postdoctoral fellowship). 
Department of Philosophy 
King’s College London 
Strand, London WC2R 2LS 
urich.stegmann@kcl.ac.uk 
References 
Bourliére, F. [1983]: ‘Animal Species Diversity in Tropical Forests’, in F. B. Golley 
(ed.), Tropical Rain Forest Ecosystems, Structure and Function, Amsterdam: Elsevier, 
pp. 77-92. 
Crick, F. H. [1958]: ‘On Protein Synthesis’, Symposia of the Society for Experimental 
Biology, 12, pp. 138-67. 
Crick, F. H. [1968]: ‘The Origin of the Genetic Code’, Journal of Molecular Biology, 38, 
pp. 367-79. 
Dretske, F. [1988]: Explaining Behavior: Reasons in a World of Causes, Cambridge, MA: 
MIT Press. 
Fodor, J. [1990]: ‘Semantics, Wisconsin Style’, in J. Fodor (ed.), A Theory of Content and 
Other Essays, Cambridge, MA: MIT Press, pp. 31-49. 
Friml, J., Vieten, A., Sauer, M., Weijers, D., Schwarz, H., Hamann, T., Offringa, R. and 
Jiirgens, G. [2003]: ‘Efflux-Dependent Auxin Gradients Establish the Apical-Basal 
Axis of Arabidopsis’, Nature, 426, pp. 147-53. 
Godfrey-Smith, P. [1999]: “Genes and Codes: Lessons from the Philosophy of Mind?’, 
in V. G. Hardcastle (ed.), Where Biology Meets Psychology: Philosophical Essays, 
Cambridge, MA: MIT Press, pp. 305-31. 
Godfrey-Smith, P. [2000]: ‘On the Theoretical Role of “Genetic Coding”’, Philosophy of 
Science, 67, pp. 26-44. 
Griffiths, P. E. [2001]: ‘Genetic Information: A Metaphor in Search of a Theory’, Phi- 
losophy of Science, 68, pp. 394-412.

--- Page 23 ---

DNA, Inference, and Information 17 
Jenik, P. D. and Barton, M. K. [2005]: ‘Surge and Destroy: The Role of Auxin in Plant 
Embryogenesis’, Development, 132 (16), pp. 3577-85. 
Knight, R. D., Freeland, S. J. and Landweber, L. F. [1999]: ‘Selection, History and 
Chemistry: The Three Faces of the Genetic Code’, Trends in Biochemical Sciences, 
24, pp. 241-7. 
Lewontin, R. C. [2001]: ‘In the Beginning Was the Word’, Science, 291, pp. 1263-4. 
Maynard Smith, J. [2000]: ‘The Concept of Information in Biology’, Philosophy of 
Science, 67, pp. 177-94. 
Sarkar, S. [1996]: ‘Biological Information: A Skeptical Look at Some Central Dogmas 
of Molecular Biology’, in S. Sarkar (ed.), The Philosophy and History of Molecular 
Biology: New Perspectives, Dordrecht: Kluwer, pp. 187-231. 
Sarkar, S. [2000]: ‘Information in Genetics and Developmental Biology: Comments on 
Maynard Smith’, Philosophy of Science, 67, pp. 208-13 
Sarkar, S. [2003]: ‘Genes Encode Information for Phenotypic Traits’, in C. Hitchcock 
(ed.), Contemporary Debates in Philosophy of Science, London: Blackwell, pp. 259-72. 
Sarkar, S. [2005]: Molecular Models of Life: Philosophical Papers on Molecular Biology, 
Cambridge, MA: MIT Press. 
Schulze, C. H., Linsenmair, K. E. and Fiedler, K. [2001]: ‘Understorey Versus Canopy: 
Patterns of Vertical Stratification and Diversity among Lepidoptera in a Bornean 
Rain Forest’, Plant Ecology, 153, pp. 133-52. 
Shea, N. [2007]: ‘Representation in the Genome and in Other Inheritance Systems’, 
Biology and Philosophy, 22, pp. 313-31. 
Stegmann, U. E. [2004]: ‘The Arbitrariness of the Genetic Code’, Biology and Philosophy, 
19, pp. 205-22. 
Stegmann, U. E. [2005]: ‘Genetic Information as Instructional Content’, Philosophy of 
Science, 72, pp. 425-43. 
Sterelny, K. and Griffiths, P. E. [1999]: Sex and Death: An Introduction to Philosophy of 
Biology, Chicago: University of Chicago Press. 
Sterelny, K., Smith, K. and Dickison, M. [1996]: “The Extended Replicator’, Biology and 
Philosophy, 11, pp. 377-403. 
Weber, M. [2005]: Philosophy of Experimental Biology, Cambridge: Cambridge Univer- 
sity Press. 
Wells, C. L. and Pigliucci, M. [2000}: ‘Adaptive Phenotypic Plasticity: The Case of Het- 
erophylly in Aquatic Plants’, Perspectives in Plant Ecology, Evolution and Systematics, 
3 (1), pp. 1-18. 
Wheeler, M. [2003]: ‘Do Genes Code for Traits?’, in A. Rojszczak, J. Cachro and G. 
Kurczewski (eds), Philosophical Dimensions of Logic and Science: Selected Contributed 
Papers from the 11th International Congress of Logic, Methodology, and Philosophy of 
Science, Dordrecht: Kluwer, pp. 151-64.

--- Page 25 ---

Brit. J. Phil. Sci. 60 (2009), 19-49 
The Crux of Crucial Experiments: 
Duhem’s Problems and Inference 
to the Best Explanation 
Marcel Weber 
ABSTRACT 
Going back at least to Duhem, there is a tradition of thinking that crucial experiments 
are impossible in science. I analyse Duhem’s arguments and show that they are based 
on the excessively strong assumption that only deductive reasoning is permissible in 
experimental science. This opens the possibility that some principle of inductive inference 
could provide a sufficient reason for preferring one among a group of hypotheses on 
the basis of an appropriately controlled experiment. To be sure, there are analogues 
to Duhem’s problems that pertain to inductive inference. Using a famous experiment 
from the history of molecular biology as an example, I show that an experimentalist 
version of inference to the best explanation (IBE) does a better job in handling these 
problems than other accounts of scientific inference. Furthermore, I introduce a concept 
of experimental mechanism and show that it can guide inferences from data within an 
IBE-based framework for induction. 
Introduction 
Duhem on the Logic of Crucial Experiments 
‘The Most Beautiful Experiment in Biology’ 
Why Not Simple Elimination? 
Severe Testing 
An Experimentalist Version of IBE 
6.1 Physiological and experimental mechanisms 
6.2 Explaining the data 
6.3 IBE and the problem of untested auxiliaries 
6.4 IBE-turtles all the way down 
Van Fraassen’s ‘Bad Lot’ Argument 
IBE and Bayesianism 
Conclusions 
© The Author (2008). Published by Oxford University Press on behalf of British Society for the Philosophy of Science I nghts reserved 
doi: 10.1093/bjps/axn040 ail ions@oxfc urnals org 
Advance Access published on November 7, 2008

--- Page 26 ---

Marcel Weber 
1 Introduction 
Some of the major discoveries in the history of molecular biology are associated 
with an alleged ‘crucial experiment’ that is thought to have provided decisive 
evidence for one among a group of hypotheses. A well-known example is the 
Hershey—Chase experiment (1952), which showed that viral DNA, not protein, 
enters a bacterial cell to reprogram it to make virus particles. Another example 
is the ‘PaJaMo’ experiment (1958), which showed that a certain bacterial gene 
produces a substance that represses the activity of other genes. In both cases, 
there were two major hypotheses that could explain the facts known before- 
hand: Either viral protein or viral DNA contains the information for making 
new virus particles (Hershey—Chase). Similarly, either ‘generalized induction’ 
(in the molecular biological, not logical sense!) or suppression of a repressor 
(the ‘double bluff theory of Leo Szilard) was thought to be responsible for 
the regulation of sugar metabolism in bacteria (PaJaMo). Examples such as 
these abound in experimental biology (see Weber [2005], Chapters 3-5). In 
many cases, a single experiment seems to have enabled a choice between the 
competing hypotheses at hand, thus strongly resembling Bacon’s ‘instances of 
the fingerpost’ or Newton’s ‘experimentum crucis’. 
Philosophers of science, of course, have been less than enthusiastic about the 
possibility of crucial experiments.' Following Duhem ({1954]), many seem to 
think that a single experiment, as a matter of principle, is not able to choose 
among a group of hypotheses. However, as I will show, Duhem made extremely 
strong assumptions concerning the kind of inferences that are to be permitted. 
Namely, he allowed only deductive inferences to be used. In this paper, I will 
show that when crucial experiments are construed along the lines of induc- 
tive (ampliative) inference, Duhem’s arguments become less persuasive. Even 
though there are analogues to Duhem’s problems in the realm of inductive in- 
ference, these are solvable within the framework of a theory of induction based 
on inference to the best explanation. 
I want to demonstrate the possibility of crucial experiments on a concrete 
historical example from molecular biology, namely the Meselson-Stahl experi- 
ment done in 1957. Even though there is an extremely detailed historical study 
of this experiment available (Holmes [2001)), it has to my knowledge never been 
subjected to a thorough methodological analysis.* ‘The most beautiful experi- 
ment in biology,’ as it has been called, is widely thought to have demonstrated 
semi-conservative replication of DNA as predicted by Watson and Crick in 
An exception is Franklin ({2007)). 
Brief methodological discussions of the case can be found in (Franklin [2007], Section E.2) and 
in (Roush [2005], pp. 14-6). Franklin uses it to highlight the importance of intervention and 
experimental control. Roush uses the episode to illustrate her tracking account of evidence.

--- Page 27 ---

Duhem’s Problems and IBE 21 
1953. But it remains to be shown that this experiment was actually decisive 
from a methodological point of view. 
In Section 2, I will discuss Duh«m’s infamous arguments against crucial 
experiments. Section 3 provides a brief account of the Meselson—Stahl experi- 
ment and some of the theoretical controversies that preceded it. In Section 4, I 
show that the evidential import of this experiment cannot be accounted for by 
a simple elimination scheme. In Section 5, I argue that the experiment cannot 
be viewed as a severe test as prescribed by the error-statistical approach to 
scientific inference. In Section 6, I propose an experimentalist version of infer- 
ence to the best explanation (IBE) and show that it provides the most adequate 
reconstruction of the experiment as providing strong evidence for the semi- 
conservative hypothesis. My account is based on the idea that explanations in 
experimental biology often involve more or less detailed descriptions of mech- 
anisms, which is substantiated by much recent scholarship (e.g., Machamer, 
Darden, and Craver [2000]; Bechtel [2005]; Darden [2006]; Craver [2007]). I 
add to these accounts the concept of an experimental mechanism and analyse 
the role of such mechanisms in making inferences from data (Sections 6.1 and 
6.2). 
The main reason why IBE provides the best account of the Meselson-Stahl 
experiment, I will argue, is that it does a better job in dealing with the inductive 
analogues of Duhem’s problems. One is the problem of untested auxiliaries, 
treated in Sections 6.3 and 6.4, while the other is very similar to van Fraassen’s 
‘bad lot’ objection to IBE, discussed in Section 7. In Section 8, I briefly discuss 
the relationship of my IBE-based account to Bayesian confirmation theory. 
2 Duhem on the Logic of Crucial Experiments 
Duhem characterized crucial experiments as follows: 
Do you wish to obtain from a group of phenomena a theoretically certain 
and indisputable explanation? Enumerate all the hypotheses that can be 
made to account for this group of phenomena; then, by experimental con- 
tradiction eliminate all except one; the latter will no longer be a hypothesis, 
but will become a certainty (Duhem [1954], p. 188). 
This passage strongly suggests that Duhem thought of crucial experiments in 
erms of eliminative induction, in other words, in terms of the following logical 
scheme”: 
H, Vv A 
a ty 
I reconstruct the logical scheme for two hypotheses. It is obvious how it could be expanded for 
more than two.

--- Page 28 ---

Marcel Weber 
H> >-e 
e 
From (3), (4): ~H>2 [by modus tollens] 
6. From (1), (5): A [by disjunctive syllogism] 
Such a train of inference faces two major problems according to Duhem. 
The first problem is the one that is today known as ‘Duhem’s problem’. This 
is the problem that auxiliary assumptions are needed to secure the deductive 
relation between hypothesis and evidence. Therefore, (5) will never involve a 
hypothesis alone; it will always be a conjunction of hypotheses that can be said 
to be falsified. Famously: 
The only thing the experiment teaches us is that among the propositions 
used to predict the phenomenon and to establish whether it would be 
produced, there is at least one error; but where this error lies is just what it 
does not tell us (ibid., p. 185). 
But if the falsity of one of the hypotheses at issue cannot be asserted, the 
inference (6) does not go through. As if this weren’t enough, Duhem identifies 
a second problem: 
Between two contradictory theorems of geometry there is no room for 
a third judgment; if one is false, the other is necessarily true. Do two 
hypotheses in physics ever constitute such a strict dilemma? Shall we ever 
dare to assert that no other hypothesis is imaginable? Light may be a swarm 
of projectiles, or it may a vibratory motion whose waves are propagated in 
a medium; is it forbidden to be anything else at all? (ibid., p. 190). 
The answer to the latter, rather rhetorical question is clear: Unlike math- 
ematicians, physicists can never have grounds for assuming that they have 
exhausted the space of possible truths. In other words, there can be no warrant 
for a premise such as (1) in the scheme above. 
Given what he sets out to prove, Duhem’s arguments are impeccable. But 
note that Duhem is clearly thinking in terms of deductive inference. What he 
proves is that experiments conjoined with deductive logic, together, are unable 
to bring about a decision for one among a group of hypotheses. Of course, 
he is absolutely right about that. However, Duhem’s arguments do not touch 
the possibility of inductive or ampliative inference enabling such a choice.* An 
4 Ofcourse, the possibility of inductive inferences is not something that Duhem simply overlooked; 
he provided elaborate arguments against inductivism (mostly using Newtonian mechanics as an 
example). I lack the space to discuss these here.

--- Page 29 ---

Duhem’s Problems and IBE 23 
ampliative inference rule might very well be able to mark one hypothesis as the 
preferable one.° 
This proposal raises the question if such a procedure does not run into 
similar difficulties. It would seem that Duhem’s first problem concerns only 
the possibility of refuting hypotheses. On the account that I shall give, crucial 
experiments do not refute the alternatives. Instead, they positively select one 
of the hypotheses as best supported by the evidence. Therefore, Duhem’s first 
problem in its classic form seems to be irrelevant. Even so, it is clearly the 
case that a crucial experiment relies on auxiliary assumptions. If these are false, 
there can be no evidential support. False auxiliaries could mask the truth or the 
falsity of a hypothesis under test. While it might be possible to independently 
test some auxiliaries (see Section 6.4), it is never possible to test all of them. 
The reason is that each attempted test of an auxiliary assumption will require 
further assumptions, and so on. If we require that all auxiliaries be tested, 
there will never be any conclusive evidential support from an experiment. This 
is the analogue of Duhem’s first problem that arises within my framework. I 
shall refer to it as the ‘problem of untested auxiliaries’. In Section 6.3, I will 
show how the problem can be solved within an IBE-based framework. 
As for Duhem’s second problem, it is no less relevant for inductive inference 
as it is for Duhem’s deductive inference scheme, at least if inductive inference is 
to be truth-tropic. When scientists hold a hypothesis to be true on the grounds 
that it is the one from a group that is best supported by the evidence, they 
must have grounds for claiming that there are no better ones that they have 
not considered. If no such grounds can be had, then an inductive inference 
regime runs into exactly the same problem as Duhem’s eliminative scheme. 
This, put into the context of a specific inductive principle, namely IBE, is Bas 
van Fraassen’s ‘bad lot’-argument.° I shall deal with it in Section 7. 
As this discussion reveals, we can expect an inductive selection regime for 
hypotheses to run into an analogue of Duhem’s first problem, and another 
problem that is basically Duhem’s second. I shall address these problems in 
due course. But right now, it is time to introduce my historical example. 
3 ‘The Most Beautiful Experiment in Biology’ 
As is well known, James D. Watson and Francis H. C. Crick closed their 
landmark paper on the structure of DNA with the short and crisp remark ‘It 
A similar claim can be found in (Laudan [1990]). Duhem-type problems (and their Quinean 
relatives) are often discussed under the rubric of ‘underdetermination of theory by the evidence.’ 
However, this expression is ambiguous, as Laudan shows. This is why I prefer to develop the 
problem in terms of a reply to Duhem’s classic arguments. 
Van Fraassen discussed this problem specifically as a part of his argument against IBE 
Stanford ({2006]) offers a more general and systematic discussion of what he calls ‘the prob- 
lem of unconceived alternatives’ and its epistemological ramifications

--- Page 30 ---

24 Marcel Weber 
has not escaped our notice that the specific base pairing we have postulated 
immediately suggests a possible copying mechanism for the genetic material’ 
(Watson and Crick [1953]). It is fairly obvious what Watson and Crick had 
in mind: Because of the complementarity of the base sequences of the two 
nucleotide chains in the double helix, a DNA molecule could be copied by first 
separating the two strands, and then synthesizing two new strands using the two 
old strands as templates. On this scheme, each newly synthesized DNA molecule 
will contain one strand that was already present in the parental molecule, and 
one newly made strand. This scheme is called ‘semi-conservative replication.’ 
However, as plausible as this scheme might seem, sceptics were quick to notice 
some theoretical difficulties. Here is the greatest of them all, Max Delbrick: 
I am willing to bet that the complementarity idea is correct, on the basis of 
the base analysis data and because of the implication regarding replication. 
Further, I am willing to bet that the plectonemic coiling of the chains in 
your structure is radically wrong, because (1) the difficulties of untangling 
the chains do seem, after all, insuperable to me. (2) The X-ray data suggest 
only coiling but not specifically your kind of coiling (Delbriick to Watson, 
12 May 1953, quoted from Holmes [2001], pp. 21-2). 
The term ‘plectonemic’ referred to the topological property that, according 
to Watson and Crick, two DNA strands are twisted about each other so that 
they cannot be separated without uncoiling. The ‘base analysis data’ refer 
to the work of Erwin Chargaff, who had shown previously that the building 
blocks of DNA occur in certain fixed ratios. Delbrick is also pointing out 
that the double helix was, at the time when Watson and Crick proposed it, 
strongly underdetermined by the available X-ray diffraction data (i.e., other 
coiled structures would have been consistent with these data). 
But Delbriick not only expressed scepticism about the specific kind of coil- 
ing. His point (1) also called into question the whole idea of a semi-conservative 
replication mechanism as suggested by Watson and Crick. The problem was 
that, given the plectonemic topology of the double helix, untangling the two 
strands requires the breaking and rejoining of the sugar-phosphate backbone 
of the molecule. Given the fast rate by which DNA replicates, especially in 
rapidly dividing bacterial cells, the molecule would have to rotate at mind- 
boggling velocities.’ This was also known as the ‘problem of untwiddling’. For 
a while, it was a major source of scepticism about Watson and Crick’s extremely 
elegant solution. While the structure itself became rapidly accepted thanks to 
Today, it is known that this is actually what happens. There is a whole class of enzymes called 
topoisomerases that control the coiling of the DNA molecule. These enzymes can catalyze 
extremely fast breaking and re-joining of the sugar—-phosphate backbone of DNA. Some enzymes 
can even introduce rotational strain into the molecule under the expenditure of metabolic energy.

--- Page 31 ---

Duhem’s Problems and IBE 
t 1 ‘ 
\ ? 
a sé ‘ 
q 
Figure 1. Delbriick’s dispersive mechanism (Delbriick [1954], p. 786). The solid 
lines represent parental DNA strands, the dotted lines newly synthesized material. 
Reprinted with publisher’s permission. 
the available of improved X-ray data, the semi-conservative replication mech- 
anism continued to be doubtful for the years to come. 
In the years following Watson and Crick’s announcement, two alternative 
replication mechanisms were proposed. Delbriick ({1954]) devised a scheme un- 
der which each newly synthesized DNA molecule contains bits of the parental 
molecule that are interspersed with newly synthesized material (Figure 1). This 
became known as the dispersive mechanism 
Gunther Stent ({1958]) proposed that the whole double-stranded DNA 
molecule could serve as the template for synthesizing a copy (Figure 2). This 
would not require any untwisting of the parental molecule. 
According to this mechanism, which was called the conservative mechanism, 
the parental molecule emerges unchanged from the replication process while the 
newly synthesized molecules contain only new material. The three mechanisms 
differ with respect to the distribution of parental and newly synthesized material 
that end up in the daughter molecules. Thus, in the mid-1950s there were 
three different hypotheses concerning the distribution of parental and newly 
synthesized nucleic acid chains. 
Now enter two young experimentalists, Matthew Meselson and Frank Stahl, 
working at the California Institute of Technology in Pasadena. Using a power- 
ful analytic ultracentrifuge, they performed a remarkable experiment in 1957.° 
8 See (Holmes [2001]) for an extremely detailed account. As usual, this experiment was preceded 
by a long and painstaking series of failures and cul-de-sacs. Holmes, who had the complete lab

--- Page 32 ---

Marcel Weber 
Figure 2. Stent’s conservative hypothesis (Stent [1958], p. 137), showing anew DNA 
strand being synthesized in the major groove of the parental double helix. Reprinted 
with publisher’s permission. 
Meselson and Stahl grew E. coli bacteria in the presence of a heavy isotope of 
nitrogen, nitrogen-15. Ordinarily, DNA contains the most common isotope of 
nitrogen, which is nitrogen-14. But when grown in the presence of nitrogen-15, 
the bacteria incorporate the heavy nitrogen into their DNA. Now, DNA that 
contains the ordinary, light nitrogen atoms and the DNA containing heavy 
nitrogen can be distinguished by their weight. Of course, DNA does not occur 
in large enough quantities to be weighed by an ordinary balance. But Meselson 
and Stahl developed a highly precise instrument for determining the weight of 
DNA. They first dissolved the bacterial cells in a strong detergent. Then they 
placed the extract in a very dense solution of the salt CsCl. When a CsCl solu- 
tion is centrifuged at very high speed in an ultracentrifuge for many hours, it will 
form a density gradient after a while. At equilibrium, the DNA molecules will 
float in that region of the gradient that corresponds to their own density. They 
form a band that can be observed with the help of UV light. Thus, the weight of 
the DNA molecules can be measured by determining the position of the band. 
The experiment that Meselson and Stahl now did was to transfer the bacteria 
from a medium containing heavy nitrogen to a medium containing light nitro- 
gen and allowing the bacteria to multiply further. At regular time intervals 
records available and conducted extensive interviews with the two scientists, traces the progress 
of Meselson’s and Stahl’s work on a day-to-day basis.

--- Page 33 ---

Duhem’s Problems and IBE 
Figure 3. UV absorption photographs (left) and densitometric scans (right) of the 
ultracentrifuge cell (Meselson and Stahl [1958], p. 675). The bands show where 
the DNA floats in the CsCl density gradient. What is particularly important about 
these data is that the band of intermediate density was located exactly in between 
the heavy and light bands. As both theoretical calculations and measurements 
showed, the density gradient was very nearly linear in the range where the DNA 
was floating (see Section 6.4). This allowed the inference that the intermediate band 
contained molecules that were composed of heavy and light nitrogen exactly ina 1:1 
ratio, as predicted by the semi-conservative hypothesis. Reprinted with publisher’s 
permission. 
after the transfer, they took samples and placed them in the ultracentrifuge. 
What they observed is that after one generation, a band of intermediate density 
appeared. After another generation, the intermediate band was still present, 
but a new band that corresponded to light DNA appeared (Figure 3). An ob- 
vious interpretation of this pattern was that the band of intermediate density 
consisted of hybrid molecules composed of one heavy and one light strand 
(see Figures 4 and 5). Such a hybrid could obviously have been produced by 
the semi-conservative scheme, according to which each newly produced double 
helix preserves one strand from the parental molecule. In contrast, the conser- 
vative mechanism should not produce a band of intermediate density (but see 
Section 4). According to the dispersive mechanism, the result would look the 
same after one generation, but the band should shift further after subsequent 
rounds of replication, as the molecules would lose the heavy nitrogen bit by bit 
over the generations. But there were no such shifts. 
While it might seem obvious that these data supported the semi-conservative 
scheme best, there are methodological complications that I will discuss in the 
following sections. At any rate, the impact of this experiment on the scientific

--- Page 34 ---

Marcel Weber 
BA AAA ORIGINAL Le Y44 
PARENT 
MOLECULE 
FIRST 
GENERATION 
DAUGHTER 
MOLECULES 
SECOND 
GENERATION | 
DAUGHTER f 
MOLECULES F 
EZ 
Figure 4. What the data showed according to Meselson and Stahl ({1958], p. 
677). The shaded areas represent heavy nitrogen ('°N). Reprinted with publisher’s 
permission. 
ORIGINAL 
PARENT 
MOLECULE 
FIRST 
GENERATION 
DAUGHTER 
MOLECULES 
SECOND 
GENERATION 
DAUGHTER 
MOLECULES 
Figure 5. The most obvious interpretation of why the nitrogen was distributed as 
shown in Figure 4 (Meselson and Stahl [1958], p. 678). Reprinted with publisher’s 
permission. 
community at that time was considerable. Almost everyone agreed that the 
Meselson-Stahl experiment beautifully demonstrates semi-conservative repli- 
cation. The only exception known to me is Max Delbrick, but his role in the 
closely knit molecular biology of that time seems to have been that of advocatus 
diaboli.

--- Page 35 ---

Duhem’s Problems and IBE 29 
In the following sections, I shall provide a methodological analysis of this 
experiment and its evidential support for Watson and Crick’s semi-conservative 
mechanism. It will turn out that the case is much more complex than it might 
seem at first sight. 
4 Why Not Simple Elimination? 
The question that I shall address first is why we cannot simply say that the semi- 
conservative scheme was the only one to survive the test to which Meselson 
and Stahl subjected it, while the alternative schemes were falsified and therefore 
eliminated. After all, the semi-conservative model predicted the outcome for 
this experiment correctly, while the two alternatives did not. The first thing to 
note is that this would amount to an eliminative induction, which is exactly the 
kind of reasoning that is not possible according to Duhem (see Section 2). If 
we construe the experimental reasoning like this, both of Duhem’s objections 
can be raised. Here, i shall concentrate on Duhem’s first problem. 
In a Duhemian frame of mind, it could be argued that the dispersive and 
conservative hypotheses could still have been true because one or several auxil- 
lary assumptions might have been false. For example, it could be that Meselson 
and Stahl were wrong about the molecular units that they resolved in their 
ultracentrifuge. Technically, what the centrifuge data showed is merely that 
there are three colloidal substances of different density. It does not show that 
these substances were simple DNA duplexes. In other words, the identification 
of the pattern shown in Figure 4 with the molecular entities shown in Figure 5 
was a theoretical interpretation of the data. 
This interpretation could have been false. The problem is especially acute 
because it is known today that Meselson and Stahl were mistaken about the 
length of the molecules they saw floating in their gradients. The hypoder- 
mic needles that Meselson and Stahl used to load the DNA onto the gradi- 
ent must have mechanically sheared the DNA molecules into much smaller 
pieces—unbeknownst to these scientists in 1957.? This did not alter the result 
because the CsCl-gradient technique separates DNA molecules according to 
density, not length. But this does show that there were severely mistaken as- 
sumptions about the experimental system. Meselson and Stahl were lucky that 
their mistake concerning length was not relevant. But what guaranteed that all 
the salient auxiliary assumptions were correct? 
As these historical facts make clear, there would have been ample reason 
for defenders of the alternative hypotheses to blame Meselson’s and Stahl’s 
auxiliary assumptions rather than their preferred model. In fact, Meselson and 
Stahl, on their part, were quite cautious in stating their conclusions: 
9 (Hanawalt [2004]). Thanks to Beatrix Rubin for bringing this paper to my attention.

--- Page 36 ---

Marcel Weber 
The structure for DNA proposed by Watson and Crick brought forward a 
number of proposals as to how such a molecule might replicate [the semi- 
conservative, dispersive and conservative mechanisms] These proposals 
make specific predictions concerning the distribution of parental atoms 
among progeny molecules. The results presented here give a detailed answer 
to the question of this distribution and simultaneously direct our attention 
to other problems whose solution must be the next step in progress toward 
a complete understanding of the molecular basis of DNA duplication. 
What are the molecular structures of the subunits of E. coli DNA which 
are passed on intact to each daughter molecule? What is the relationship of 
these subunits to each other in a DNA molecule? What is the mechanism 
of the synthesis and dissociation of the subunits in vivo? (Meselson and 
Stahl [1958], p. 681). 
As this passage makes clear, Meselson and Stahl did not even draw the 
inference from their data to the semi-conservative mechanism, at least not in 
their official publication. Curiously, the questions they raise toward the end 
of this passage are precisely those that their experiment is supposed to have 
answered. In print, Meselson and Stahl did obviously not want to go beyond 
what their data said. 
sent J. D. Watson a little poem!': 
'© However, unofficially they showed less caution. Meselson 
Now !°N by heavy trickery/Ends the sway of Watson-Crickery./But now 
we have WC with a mighty vengeance ... or else a diabolical camouflage. 
This line strongly suggests that Meselson did think that the experiment sup- 
ported Watson’s and Crick’s replication scheme, even though he knew that they 
had not really established the nature of the molecular units that they resolved in 
their ultracentrifuge. As Holmes ({2001], p. 329) reports, ‘Meselson and Stahl 
were personally convinced that the experiment had proven the position that 
'0 While the conclusive refutation of Stent’s model would have required more certainty about the 
nature of the molecular units resolved by the centrifuge, Meselson and Stahl were at least confident 
enough that their results ruled out Delbriick’s dispersive mechanism ([{1958], p. 681). However, 
they did not justify this claim on the grounds that the intermediate band did not shift any further 
after subsequent rounds of replication (as most textbooks have ii). Rather, they reported an 
additional experiment with heat-denatured E. coli-DNA as speaking against Delbriick. When 
heated enough, the two strands of DNA dissociate. Meselson and Stahl denatured heavy, hybrid, 
and light molecules in this way and analyzed them in the ultracentrifuge. The hybrid molecule 
produced bands of the same density as a mixture of heavy and light DNA. Furthermore, the 
molecular weight of the molecules was estimated to be reduced by half by denaturing. This 
suggested that the DNA strands themselves were not broken and re-annealed during replication 
(as Delbriick’s mechanism required). However, Meselson and Stahl were worried because their 
E. coli DNA dissociated at a temperature at which salmon sperm DNA did not, which led them 
to wonder if E. coli-DNA might be ‘a more complex structure’ than salmon sperm DNA. (To 
my knowledge, this could simply be due to a greater GC content in salmon sperm DNA.) At 
any rate, this only shows again that there were serious doubts as to what the exact nature of the 
molecular units was that the density gradient resolved. I will show below that this did not affect 
the evidential import as regards the Watson—Crick scheme (Section 6.2). 
'! Meselson to Watson, 8 November 1957. Quoted from (Holmes [2001], pp. 327-8).

--- Page 37 ---

Duhem’s Problems and IBE 31 
Watson and Crick had taken in 1953 to be right.’ As they had been quite scep- 
tical of the Watson—Crick hypothesis initially, the experiment seems io have 
had an impact on their degrees of belief. 
Of course, the scientists’ own methodological judgments cannot be our ulti- 
mate standard when engaging in normative philosophy of science. Nevertheless, 
we should not dismiss these judgments lightly. I take part of my task to be the 
explication or rational reconstruction of the scientists’ own reasoning. Thus, 
I would like to show that there is a way of making sense of the apparent dis- 
crepancy between Meselson and Stahl’s official view that they expressed in 
the conclusion of their PNAS paper, according to which the experiment only 
showed the equal distribution of parental nitrogen and not semi-conservative 
replication, and their unofficial view, which granted their own experiment much 
more discriminatory power. 
The reasons might have to do with the control of inductive risk. In print, 
scientists will take as little inductive risk as possible in order to minimize the 
chance of being on the record for being wrong. But on other occasions they 
might be willing to take more inductive risk and go a little further beyond what 
their data say, especially when this allows them to carry on with their research. 
The alternative replication models (or at least the conservative model; see 
footnote 10) could not be refuted then because there was an important untested 
auxiliary assumption in the interpretation of the data: that the molecular units 
represented as bands were single DNA molecules (and not, for example, end- 
to-end associations of heavy parental duplexes with newly synthesized, light 
double strands). The evidence for semi-conservative replication was only as 
good as this assumption. This is the inductive analogue of Duhem’s first prob- 
lem mentioned in Section 2, the problem of untested auxiliaries. And this is also 
why the simple eliminative induction scheme fails to bring out the evidential 
import of this experiment. 
It should be clear by now that, if we want to be able to explicate Meselson’s 
and Stahl’s view that the experiment in its original form was strong evidence for 
the Watson—Crick mechanism, we must grapple with the problem of untested 
auxiliaries. Before I present my own IBE-based solution, I shall examine how 
another theory of scientific evidence handles this problem. 
5 Severe Testing 
Perhaps Meselson’s hint with the ‘diabolical camouflage’ (see the above citation) 
is revealing. It indicates that Meselson thought it unlikely in the extreme that 
their experiment would have turned out the way it did had the semi-conservative 
hypothesis been false. This suggests yet another construal of the case: It could 
be argued that what Meselson and Stahl actually provided was a severe test in 
the sense of Mayo’s ([{1996]) error-statistical theory of scientific reasoning.

--- Page 38 ---

32 Marcel Weber 
A severe test in this sense is a test with a low error probability, in other 
words, a low probability that the hypothesis passes a test in spite of being false. 
The term ‘error probability’ is originally a technical term from the Neyman— 
Pearson statistical method. However, Mayo ({1996]) argues that there is an 
informal analogue to such a test that also applies in non-statistical contexts. 
The centrepiece of this informal approach is what Mayo calls ‘arguments from 
error’. On her view, a hypothesis receives inductive support from some data 
to the extent in which it fits these data and some investigative procedure that 
was likely to detect an erroneous fit (i.e., one where the hypothesis under test is 
false) did not find an error. 
A first problem with such a construal is to say what justifies the judgment 
that some test procedure was likely to find an error, had there been one. Specifi- 
cally, how could Meselson’s judgment be justified that it was unlikely that DNA 
would behave as it did, had the semi-conservative scheme been false? I cannot 
think of a better answer than just saying that it would be a strange coincidence 
if Meselson’s and Stahl’s experiment behaved as if a semi-conservative mecha- 
nism was at work while, in fact, there was some other physiological mechanism 
at work. But this is just another way of expressing the intuition that this was 
unlikely; it does not really give a justification for it. Therefore, I think a recon- 
struction of the case as a severe test with an argument from error is not really 
helpful (unlike in cases where a formal Neyman-—Pearson statistical test can be 
done). 
But there is a second problem, and this is that there are major potential 
errors in the original experiment that Meselson and Stahl were not able to rule 
out in 1957. As already mentioned (Section 4), what the experiment showed 
primarily was the symmetrical distribution of heavy nitrogen in replication, 
not that the bands corresponded to single DNA duplexes. It was technically 
possible that the intermediate band represents an end-to-end association of 
parental DNA duplexes with newly synthesized duplexes rather than hybrid 
molecules composed of a light and a heavy strand (this would make the results 
compatible with the conservative hypothesis). This interpretation was ruled 
out about five years later, when Meselson’s student Ron Rolfe showed that the 
DNA could be broken into smaller fragments by sonication without affecting 
its density (Hanawalt [2004)). 
According to the error-statistical approach, an experimental inquiry only 
supports a hypothesis to the extent in which it rules out possible errors in the 
data interpretation. Meselson and Stahl were unable to rule out some quite 
severe errors; therefore, this approach does not allow us to say that they had 
good evidence for the Watson—Crick hypothesis. Of course, a follower of the 
severe testing approach could just shrug her shoulders at this point and say: 
Well, in that case there really was no decisive evidence coming out of the 
Meselson-Stahl experiment, at least until Rolfe’s results were in.

--- Page 39 ---

Duhem’s Problems and IBE 33 
The problem with this way of arguing is not merely that it does not reflect 
how many scientists thought about the experiment, including Meselson and 
Stahl themselves. The real problem is that it will never allow us to say that the 
experiment supported the Watson—Crick hypothesis; even once some additional 
tests had been done. Consider Rolfe’s experiment, mentioned above. It might 
be viewed as ruling out the error of misidentifying the molecular units that 
worried Meselson and Stahl so much. However, it is always possible to come 
up with an interpretation of Rolfe’s data that make them compatible with one of 
the alternative hypotheses. Perhaps there were covalent cross-links between the 
parental and newly synthesized duplexes that were resistant to the sonication 
treatment. So Rolfe’s data were no severe test before these other possible errors 
were ruled out, and so on. When is the point reached where the data speak of 
the truth of a hypothesis? There are always possible errors, so the jury is never 
really in. For this reason, it seems to me that the error-statistical approach, far 
from solving Duhem’s problem, makes it run amok. 
The last two sections, I hope, have made it clear that we need to think about 
the Meselson-Stahl experiment in altogether different terms if we want to show 
that it was, in fact, strong evidence for the semi-conservative hypothesis. 
6 An Experimentalist Version of IBE 
I suggest that the Meselson—Stahl experiment selects the semi-conservative 
hypothesis by an inference to the best explanation (IBE).'* In order to make 
this thesis good, I first need to elaborate on the relevant concept of scientific 
explanation. For the purposes of this paper, I shall adopt a mechanistic account 
of explanation. According to such an account, to explain a phenomenon means 
to describe a mechanism that produces this phenomenon. A highly influential 
account of the relevant concept of mechanism has been given by Machamer, 
Darden, and Craver ({2000]), who define mechanisms as ‘entities and activities 
organized such that they are productive of regular changes from start or set- 
up conditions to finish or termination conditions’. A considerable body of 
scholarship exists now that shows how much experimental research in biology 
is organized around mechanisms in this sense (e.g., Bechtel [2005]; Darden 
!2 See (Lipton [2004]) for a deep book-length philosophical study of IBE. Lipton’s main example 
is Semmelweis’s discovery of the cause of childbed fever. This is a case where IBE was used to 
pinpoint a causal factor (‘cadaveric matter’) that accounted for variations in the incidence of 
an infectious disease. Even though there was some controlled experimentation involved in this 
example, the case differs considerably from my example of molecular biology. One difference is 
that my example involves not merely the identification of a causal factor, but also the elucidation 
of a specific mechanism. Another difference is the use of a sophisticated measurement device. 
Problems such as Duhem’s first and its inductive analogue (not discussed by Lipton) are more 
pressing here.

--- Page 40 ---

34 Marcel Weber 
[2006]; Craver [2007]).'> To my knowledge, no one has yet shown how such an 
account of explanation could support an IBE-based account of induction.'4 
In order to do this, a new distinction is needed: I shall distinguish physiolog- 
ical from experimental mechanisms. 
6.1 Physiological and experimental mechanisms 
Physiological mechanisms are mechanisms that operate in a living cell. This 
kind of mechanism has received much attention lately. I would like to introduce 
a new type of mechanism: experimental mechanisms. In doing so, I shall leave 
the meaning of the term ‘mechanism’ itself the same, but allow the entities and 
activities as well as the changes, set-up and finish conditions to include parts of 
the experimental system (in the sense of Rheinberger [1997]). In other words, the 
artificially prepared materials such as the heavy nitrogen source as well as the 
characteristic manipulations and measurement devices used in the experiment 
also qualify as parts of a mechanism—the experimental mechanism. While 
physiological mechanisms occur in nature, experimental mechanisms require 
interventions. 
In order to motivate this move a little, note that it makes perfect sense to 
speak of the mechanism that produced the UV absorption bands in Meselson 
and Stahl’s experimental setup. This mechanism includes the heavy nitrogen 
added to the growth medium, as well as the transfer of the growing bacteria 
into a medium containing light nitrogen. Furthermore, the mechanism in- 
cludes the mechanical devices used to grind up the cells, extract the DNA, and 
transfer them onto the CsCl gradient (which, needless to say, is also part of the 
mechanism). 
What is also important is that the physiological mechanism—.e., the mech- 
anism of DNA replication in this case—was somehow embedded in the ex- 
perimental mechanism. In other words, it was responsible for some of the 
regular changes that constituted the experimental mechanism. Mechanisms 
often form hierarchical structures where particular entities and activities can 
be themselves decomposed into lower-level mechanisms (Craver and Darden 
[2001]}). The lower-level mechanisms may be responsible for some of the activ- 
ities that feature in higher-level mechanisms. But such a hierarchical organiza- 
tion is not necessary. Mechanisms may be related by one mechanism providing 
'3 The term ‘mechanism’ is sometimes used in a double sense in this literature, sometimes ontological 
and sometimes epistemic. In my view, the latter use should be understood as shorthand for 
‘description of a mechanism’ or ‘model of a mechanism’ and the context should normally make 
it clear as to which of the two senses is relevant. 
Lipton ([2004], p. 122) cites ‘mechanism’ as an ‘explanatory virtue’ (along with precision, scope, 
simplicity, fertility or fruitfulness, and fit with background beliefs), but the examples he discusses 
involve mostly just the identification of causal factors rather than the elucidation of elaborate 
mechanisms.

--- Page 41 ---

Duhem’s Problems and IBE 35 
the substrate that another mechanism operates on. Biochemical pathways are 
a nice example for this. Thus, mechanisms may be horizontally linked. Such 
horizontal links exist in our present example: the heavy nitrogen is an entity 
of the experimental mechanism, and it is a substrate on which the physiolog- 
ical mechanism can act if it is provided instead of the usual substrate (i.e., 
light nitrogen). This entity has the same activity (or almost the same) within 
the physiological mechanism, but a different activity within the experimental 
mechanism. Finally, an important way for mechanisms to be embedded is by 
the physiological mechanism being a stage of the experimental mechanism. In 
other words, a product of the physiological mechanism (here: DNA) is further 
processed by the experimental mechanism. 
We do not have to require that the embedded mechanism and the physiolog- 
ical mechanism that is under study are exactly the same.'> It might be enough 
if something similar to the physiological mechanism is embedded, provided 
that the embedded mechanism shares the salient nomological properties of the 
physiological mechanism. In experimental biology, mechanisms often come in 
families that may bear more or less resemblance to some prototype (Schaffner 
[1993], pp. 97-8). However, in this case, I think we can really say that the physio- 
logical mechanism itself was embedded. After all, we are dealing with an in vivo 
experiment. In other words, the experiment was done with living bacteria—at 
least before their DNA was extracted. 
Of course, it must be assumed that the experimental mechanism is well 
understood by the experimenters. Generally, experimental mechanisms may 
be expected to be more accessible epistemically because, unlike physiological 
mechanisms, they were at least in part designed by the experimenter. But some 
of the assumptions made about experimental mechanisms may also be subject 
to independent test (see Section 6.4). 
Why this extension of the notion of mechanism? What I would like to suggest 
is that the experimental mechanism is part of the explanation for the actual 
data patterns that Meselson and Stahl saw in their experiment (as shown in 
Figure 3). Further, I want to claim that this explanation is better than the two 
alternative explanations that involve the dispersive or conservative replication 
mechanism instead of the semi-conservative one. The experimental mechanism 
in combination with the semi-conservative physiological mechanism is the best 
explanation for the banding patterns obtained by Meselson and Stahl, at least in 
the group of experimental mechanisms that involve either the semi-conservative, 
the dispersive or the conservative mechanism and are otherwise identical. | 
will argue now that this explanatory relation is constitutive for the inductive 
support that the Meselson—Stahl experiment bestowed on the semi-conservative 
hypothesis. 
'5 | owe this point to an anonymous referee.

--- Page 42 ---

Marcel Weber 
6.2 Explaining the data 
In order to make this claim good, I need to be more specific what exactly IBE 
is. Lipton ((2004], pp. 58-9) has made a strong case for construing IBE as 
inference to the loveliest potential explanation. The relevant potential explana- 
tions are explanations that, if they were true, would explain the premises of 
the inference. (Actual explanations are actually true). The /oveliest explanation 
is the one that, if true, would be most explanatory. Lipton also characterises 
the loveliest explanation as the one that provides ‘the most understanding.’ 
This should not be taken to imply that explanatoriness is a purely subjective 
or psychological matter, as it were, ‘in the eye of the beholder.’ Whether or 
not a set of propositions are explanatory with regard to some other set is a 
matter of their conforming to certain norms such as those of the mechanistic 
approach taken here. Thus, I do not mean ‘loveliness’, ‘explanatory’ and ‘un- 
derstand’ in a psychological sense (i.e., as some subjective state of mind), but 
in a sense that is bound to strict normative standards as to what qualifies as an 
explanation. According to the mechanistic account that I adopt, to understand 
a phenomenon is a cognitive state characterized by an acquaintance with the 
entities and activities as well as certain patterns of counterfactual dependence 
(see Woodward [2002]) involved in producing the explanandum phenomenon, 
in particular insofar as it instantiates regularities. 
The loveliest explanation should be conceptually distinguished from the 
likeliest explanation, which is the one that is most likely to be true. While likely 
explanations are what scientists strive for, to suggest that they infer explanations 
on the basis of their likeliness would be ‘like a dessert recipe that says start with 
a souffle’ (Lipton [2004], p. 59). How likely an explanation is to be true is 
what we don’t know when we are drawing an inductive inference; what we can 
know is only how explanatory or lovely an explanation would be if it were true. 
On Lipton’s account, IBE is based on the idea that explanatory loveliness is a 
guide or a reliable indicator to likeliness. It is often enough the case that some 
complex state of affairs or some contrast (Lipton [2004], pp. 33-6) has some 
specific cause rather than another. A footprint in the wilderness that is exactly 
shaped like a bear paw is more likely to have been caused by a passing bear 
than, say, by surface air turbulence. That’s why it is recommended.to infer the 
presence of a bear rather than some unusual turbulence when seeing such a 
structure in the wild. The passing bear is the loveliest explanation for the shape 
of the prints. Of course, the prints might have been caused by someone who 
was trying to pull a prank, but nobody claims that IBE is infallible. 
The next step must be to specify what the relevant explanandum is, that is, 
the premise of the IBE. In contrast to other accounts, I suggest that the ex- 
planandum is provided by the data rather than the phenomena (in the sense 
of Bogen and Woodward [1988]). Applied to the present case, this means that

--- Page 43 ---

Duhem’s Problems and IBE 37 
the semi-conservative hypothesis was the loveliest potential explanation of the 
data that Meselson and Stahl obtained, i.e., the banding pattern that they ob- 
served (see Figure 3). According to the mechanistic account of explanation that 
I adopt for the purposes of this analysis, a lovely explanation is a description of 
a mechanism, in other words, an arrangement of interlocking causal processes 
that together produce the explanandum facts. 
It is central to my account that—unlike in Lipton—the explanandum that 
serves as a premise for the IBE are data, not phenomena. According to Wood- 
ward ({2000], p. S163), phenomena are ‘stable repeatable effects or processes 
that are potential objects of prediction and systematic explanation by general 
theories and which can serve as evidence for such theories’, while data are 
‘public records (...) produced by measurement and experiment, that serve as 
evidence for the existence of phenomena.’ Phenomena and data are not re- 
lated by relations of entailment, but by a hodgepodge of factual, empirical 
relations. '® 
In the realm of experimental biology, we need to refine these characteriza- 
tions a little, because there are hardly any general theories. Instead, there are 
descriptions of mechanisms that serve the explanatory role of theories. Thus, 
for the case in hand, the phenomenon is the copying of DNA while the data are 
the banding patterns observed by Meselson and Stahl. Phenomena and data 
are causally connected through the experimental mechanism (see Section 6.1). 
In our current example, the phenomenon—DNA copying—is potentially ex- 
plained by all three replication mechanisms. Give or take some theoretical 
difficulties such as the ‘problem of untwiddling’, the three proposed mech- 
anism schemes are about equally successful in explaining the phenomenon. 
Thus, the experimental IBE is not helping at this level. Where the experimental 
IBE argument comes into the picture is in that the experimental mechanism 
that contains the semi-conservative scheme provides the best explanation for 
the experimental data. The semi-conservative scheme augmented with the de- 
tails of the experimental setup describes a collection of causal processes that 
can produce the UV absorption bands as they were observed by Meselson and 
Stahl, given their background knowledge about equilibrium sedimentation (see 
Section 6.4) and the biochemistry of nucleic acids. The alternative schemes in 
combination with the details of the experimental setup describe causal pro- 
cesses that would produce different banding patterns, unless they are fitted 
with additional assumptions that are not part of the physiological mechanism 
(see Section 6.3). This is why the semi-conservative scheme, together with the 
'6 There is much that I agree with in Woodward’s account concerning the relationship between 
data‘and phenomena. The only amendment that I would suggest is that inferences from data are 
sometimes guided by considerations of what mechanism(s) would explain specific data outcomes. 
This can help scientists to determine how reliably the data track competing claims about what 
causes the phenomenon in question.

--- Page 44 ---

38 Marcel Weber 
details of the experimental mechanism, is the best explanation of the data and, 
ultimately, the reason why this experiment provided strong support for the 
semi-conservative hypothesis. !’ 
I will show next that the account just outlined succeeds where the other 
approaches discussed in Sections 4-5 failed, namely, in solving the problem of 
untested auxiliaries. 
6.3 IBE and the problem of untested auxiliaries 
Meselson’s and Stahl’s data seemed to fit the semi-conservative hypothesis best, 
but, as we have seen, this judgment relied on the truth of auxiliary assumptions, 
in particular that the bands corresponded to simple DNA double strands (see 
Figures 4 and 5). As this assumption (together with some others) was not yet 
testable in 1957, how can we nonetheless justify the claim that the experiment 
spoke of the truth of the Watson—Crick hypothesis of replication, a judgment 
shared by many scientists at that time? I will argue now that IBE can be used 
to justify such a claim. 
What needs to be shown is that the Meselson-Stahl experiment supported 
the semi-conservative hypothesis by its own wits, that is, without the help of 
additional tests that ruled out possible errors in the interpretation of the data 
(except the calibration of the instrument; see Section 6.4). I suggest that this is 
the case because the semi-conservative mechanism, in combination with what 
I have called the experimental mechanism, was sufficient to explain the data 
by its own wits. In contrast, the alternative mechanisms would require add- 
on mechanisms or ‘epicycles’ in order to explain the Meselson-Stahl data. 
It would be utterly mysterious if, for example, the conservative mechanism 
would produce end-to-end associations of heavy and light DNA molecules, 
which could give rise to the pattern of bands that was actually observed (see 
Figure 2). With the semi-conservative mechanism, in contrast, it is absolutely 
clear why it is likely to produce this banding pattern; nothing is left mysterious. 
This is exactly the kind of difference that IBE is sensitive to. 
'7 An anonymous referee asked why it is not enough to say that the semi-conservative hypothesis 
predicted the outcome observed, and that it was supported by this outcome for this reason. To 
answer this question, it must first be noted that ‘predict’ is ambiguous in this context; it can mean 
either ‘imply’, or it can refer to what is known as ‘novel prediction.’ In either sense, prediction 
does not capture the methodological import of Meselson’s and Stahl’s data. The hypothesis did 
not entail the data (as the hypothetico-deductive account of confirmation would have it); rather, 
hypothesis and data were connected by factual, empirical relations (see Woodward [2000]). As for 
the second sense of ‘predict,’ we may have an example of a novel prediction here, although ‘novel’ 
can also mean different things. In any case, novelty cannot be the reason why the experiment 
supported the Watson—Crick hypothesis either, for it would have supported it no less if the latter 
hypothesis had been formulated after the experiment had been performed. In fact, the evidential 
support would have been just the same even if the hypothesis had been deliberately designed to 
fit the Meselson-Stahl data. This is why I think that the methodologically salient relation here 
is that the hypothesis, combined with the experimental mechanism, explained the data (see also 
Achinstein [2001]). Prediction is too heterogeneous a category from a methodological perspective.

--- Page 45 ---

Duhem’s Problems and IBE 39 
This explanatory sufficiency, I suggest, is the methodologically relevant dif- 
ference between the semi-conservative and the alternative schemes and the 
reason why the experiment supported the former but not the latter. Thus, the 
point is not that the semi-conservative mechanism was simpler or required 
fewer assumptions than the alternatives; what is crucial is that it was able to 
explain the data pattern at all. 
The same point can be put as follows. It is as if the auxiliary assumptions were 
hitching a free inferential ride on the experimental mechanism, powered by the 
latter’s explanatory force. Because the hypothesis under test is augmented by 
certain auxiliary assumptions in explaining the data, the IBE supports these 
assumptions along with the hypothesis. In order to elaborate on this idea, I 
shall make use of Norton’s ({1993]) notion of relocating inductive risk. Norton 
discusses a case from the history of quantum physics where inductive risk was 
relocated from rules to premises such that the evidence uniquely determined 
a theoretical claim. In a somewhat similar way, I suggest, we can think of the 
inference from the Meselson-Stahl data to the semi-conservative hypothesis as 
dividing the inductive risk equally between the hypothesis itself and some of 
the auxiliary assumptions needed to connect the former to the data, in par- 
ticular the assumption that the bands represent single DNA duplexes. Instead 
of requiring that this auxiliary assumption be secured before or independently 
of the inference to the theoretical hypothesis in question, we can say that both 
are inferred in one fell swoop on the grounds that the combination of them—in 
form of the experimental mechanism—provides a sufficient causal-mechanical 
explanation of the data. In other words, the experimental mechanism is in- 
ferred from the data by IBE as a whole structure.'* Because it contains the 
semi-conservative mechanism as a substructure, the latter is inferred along, 
but so are the auxiliary assumptions (which were of little theoretical interest 
themselves). 
The reason why this solves the problem of untested auxiliaries is that there is 
no comparable inference to the alternative hypotheses. Even though there are 
ways of fiddling with the auxiliaries to make them consistent with the data, this 
will not provide a sufficient mechanistic explanation of them. This can best be 
seen in the case of Stent’s conservative hypothesis. As I already mentioned, this 
hypothesis can be made consistent with the Meselson-Stahl data by assuming 
that the DNA duplexes somehow stick together after replication, for example, 
as end-to-end covalent associations of newly synthesized and parental poly- 
phosphodeoxyribose nucleotide. Why can we not say that this assumption was 
also taken for a free ride by the conservative mechanism, in the same way in 
'8 There were also aspects of the mechanism that were not subject to this inference, for example, the 
Watson-Crick structure of the DNA double helix. Such theoretical assumptions are not involved 
in this sharing of inductive risk.

--- Page 46 ---

40 Marcel Weber 
which the semi-conservative scheme took the assumption that the bands were 
single duplexes along in explaining the data?!” 
The reason is that there is nothing in, say, the conservative hypothesis that 
says that the molecules should remain so associated (see Figure 2). The assump- 
tion that the bands represented simple DNA double helices is part of a lovely 
explanation of the data (in the mechanistic sense), while the assumption that 
the bands represent end-to-end associations of parental and newly synthesized 
double strands is not part of such a lovely explanation. It is not clear why the 
alternative replication schemes should produce such structures. Of course, this 
does not mean that the alternative explanations are ruled out with certainty. 
But, once again, nobody claims that IBE is an infallible inference rule. No 
inductive inference rule is infallible. Proponents of IBE do not claim that the 
loveliest explanation is always true; they only say that this is the case often 
enough for this rule to be epistemically useful. Our present case is certainly no 
counter-instance. 
It seems to me that only such an IBE-type of argument can make sense 
of the widely shared intuition (see Franklin [2007], Section E.2; Roush [2005], 
pp. 14-6) that the Meselson-Stahl data provided strong discriminatory evidence 
for the semi-conservative hypothesis. As I have shown, two other approaches 
to reconstructing the scientific reasoning behind ‘the most beautiful experi- 
ment in biology’ succumb to Duhem’s first problem, or its inductive analogue. 
The merits of a third approach to scientific inference in reconstructing this 
case—Bayesian confirmation theory—will be discussed in Section 8. But first, 
I want to demonstrate that an IBE-type argument can also be applied to those 
auxiliary assumptions that were actually tested. 
6.4 IBE-turtles all the way down 
Not all auxiliary assumptions used by Meselson and Stahl were free riders in 
the sense explained in the previous section. The main example of such a free 
rider was the identification of the bands with single DNA duplexes (Figures 4 
and 5). However, even if this is granted, Meselson’s and Stahl’s evidence could 
only have been as good as the correlation between the density of the DNA 
and the position of the bands. As we have seen, it was crucially important that 
the band of intermediate density was lying exactly between the heavy and light 
bands. But how good was Meselson and Stahl’s analytic technique to resolve 
molecules according to their density? Franklin ({2007], Section E.2) argues 
'9 This objection is due to an anonymous referee. Note that my notion of ‘inferential hitch-hiking’ 
is not a new methodological principle; it is merely a way of describing how IBE operates in a 
case like this, where the explanation inferred is not a single proposition or a systematic theory 
but a heterogeneous assemblage of theoretical and experimental assumptions (the experimental 
mechanism).

--- Page 47 ---

Duhem’s Problems and IBE 41 
that the mere fact that heavy and light DNA produced two clearly separated 
bands (before the actual experiment was done) provided some evidence that the 
technique was reliable. I agree, except that the linearity of the gradient was an 
important extra element in the argument that the intermediate contained '4N 
and !°N in equal amounts. Therefore, a good calibration of the instrument was 
of the essence for this experiment. For this reason, I want to briefly examine 
how this was done. 
The theory of ultracentrifugation had been worked out to a large extent by 
Theodor Svedberg in the 1920s. In his PhD thesis, Matthew Meselson extended 
the work of Svedberg to experiments with solutions of very high density, such 
as the CsCl-gradients that they were using. In those days, molecular biology 
was institutionally closely associated with physics and physical chemistry (im- 
pressively, Meselson’s thesis committee included Richard Feynman and Linus 
Pauling). Meselson investigated in particular the conditions under which a 
CsCl-gradient and the macromolecules that float in it would reach a point of 
equilibrium. At equilibrium, the centrifugal force and the buoyant force would 
balance each other, tending to keep the DNA at that point where its buoyant 
density equals that of the solution. But there is another force that tends to dis- 
place the DNA from this equilibrium: namely molecular diffusion or Brownian 
motion. Meselson was able to show theoretically that, at equilibrium, these op- 
posing forces would generate a Gaussian distribution of the molecules. Here is 
the relationship that Meselson derived: 
. ; =F — ro) 
Cex, (r) = Cex, (ro) exp ——— zo~- 
This equation describes the concentration of a charged polymer such as 
DNA in a linear density gradient. This is a Gaussian distribution with standard 
deviation a. Meselson also obtained the following expression for the standard 
deviation: 
> RT 
Mpx, i px,(dp/dr),,w*ro 
where Mpy, is the molecular weight of the polymer PX,,; wpy, is the partial spe- 
cific volume of the polymer PX,,; (dp/dr),, is the slope of the density gradient; 
w is the angular velocity; and r is the distance from the rotation axis. 
The width of the distribution therefore allowed the biologists to calculate 
the molecular weight of the bands. The physical reason for this is that lighter 
molecules diffuse more rapidly; therefore they will smear more strongly when 
they form a band. 
Meselson and Stahl checked these theoretical results against their experi- 
mental data, using DNA from bacteriophage T4 as a marker. The agreement 
was quite remarkable (see Figure 6).

--- Page 48 ---

Marcel Weber 
Scale: -—0.5mm—44 
| e Observed DNA 
| Distribution 
|— Calculated 
Gaussian 
DNA CONCENTRATION 
a 
| ” ~ 0 soccocente ee 
DISTANCE FROM ROTOR CENTER ——+ 
Figure 6. Agreement of the theoretical calculation with the measured DNA con- 
centration at equilibrium. The DNA used in this experiment was derived from 
bacteriophage T4. This figure appeared only in Meselson’s PhD thesis (Meselson 
[1957]), not in the 1958 publication. Reprinted with author’s permission. 
This clean result may be viewed as a test that the measuring device worked 
properly and that the gradient was almost perfectly linear over a certain range. 
Thus, distances from the centre of rotation translate directly into buoyant den- 
sities. This linear CsCl gradient was an important part of what I have called the 
experimental mechanism, which is the centrepiece of my IBE-based reconstruc- 
tion of the experiment. I would like to call the theory of how centrifugation 
produces a linear gradient from which the density of molecules that float in it 
can be read off directly the theory of the instrument. The final question to be 
discussed in this section is how this theory of the instrument was confirmed. 
My proposed answer to this last question is that the theory of the instrument 
was also supported by an IBE-type argument, and that explanation is best 
understood in the mechanistic sense. Here, the relevant experimental mecha- 
nism contains the DNA molecules, the caesium and chloride ions, as well as 
the water molecules. These entities interact by electrostatic forces and weak 
chemical bonds (hydrogen bonds). Further, this.experimental mechanism in- 
volves the centrifuge itself with its rotor and the cell containing the DNA/CsCl 
solution. Together with physical laws”? (Newton’s laws, Coulomb’s law, and the 
laws of thermodynamics), this mechanism explains why, under suitable condi- 
tions, DNA molecules will reach a sedimentation equilibrium, in which they: 
are distributed in accordance with a Gauss curve where the mean is a linear 
20 Some proponents of a mechanistic account of explanation have argued that laws are redundant; 
all the explanatory work they were once thought to do can be captured by activities (Machamer, 
Darden, and Craver [2000]). I have criticised this view in a previous work (Weber [2005], 
Chapter 2). Woodward ([2002]) gives an account of mechanisms based on his counterfactual 
account of causal regularities. These differences are of no relevance for the present discussion.

--- Page 49 ---

Duhem’s Problems and IBE 43 
function of density and the width an inversely linear function of molecular 
weight, which is what was actually observed. It is this explanatory relation 
that provided grounds for thinking that the analytic ultracentrifuge is a reliable 
instrument for determining the density of certain biopolymers. In other words, 
it’s [BE-turtles all the way down. 
7 Van Fraassen’s ‘Bad Lot’ Argument 
In the previous section, I have shown how the IBE approach combined with a 
mechanistic account of explanation solves the inductive analogue to Duhem’s 
first problem. But we still have Duhem’s second problem to cope with, which 
is the claim that scientists can never have rational grounds for believing that 
the set of available hypotheses includes one that is true. There is a more recent 
version of this argument that pertains directly to IBE, namely van Fraassen’s 
‘bad lot’ objection (van Fraassen [1989], p. 142ff.). According to this argument, 
IBE can perhaps rank a set of hypotheses with regard to their explanatory 
merits, but it cannot provide grounds for accepting one of them as true. For the 
best explanation could still be a very bad one; it affords no epistemic distinction 
to be the best of a bad lot. 
The most direct way of answering this challenge would be to show that the 
list of alternatives was, in fact, exhaustive. This is what Roush ([2005], p. 15) 
suggests in her brief discussion of the Meselson-Stahl case: 
It is hard to argue with the claim that all, some, or none of an original 
strand appears in a daughter molecule, and all, some or none exhaust the 
possibilities. The genius of the investigation, perhaps, was to have pitched 
the question at a level of description where this exhaustiveness could be 
achieved in a simple way. 
According to Roush, the level of description chosen in this case rules out 
that there are alternatives that have not been considered. If she were right, 
this would be a direct counterexample to van Fraassen’s and Duhem’s claims 
that there are always unconsidered alternatives (see Section 2). In a similar vein, 
John Norton ({1993]) has argued that there are theoretical claims in physics that 
are completely determined by a body of evidence. Might something like this 
work here as well? Unfortunately, I do not think so. Roush’s argument for the 
exhaustiveness of the three replication schemes is not successful; there are other 
conceivable schemes. For example, it is at least logically possible that the original 
molecule is degraded completely in the process; in other words, that both 
molecules are newly synthesized.*' There are other conceivable alternatives.~ 
7! | wish to thank Eric Oberheim for pointing this possibility out to me. 
22 One could also think of mechanisms that use some kind of intermediate (e.g., RNA or protein) for 
copying the DNA molecule. To use an analogy, in the early days of molecular biology, there were

--- Page 50 ---

44 Marcel Weber 
To be sure, such a mechanism is not supported by Meselson and Stahl’s data, 
but it is enough to cast doubt on the exhaustiveness of the list of alternatives. 
Even though Roush’s formulation (‘all, some or none of an original strand’) 
makes it look as if there were a complete disjunction involved, this is a result 
of an ambiguity in this way of describing the alternatives. We will thus have to 
settle with something weaker than a complete disjunction. 
At least Roush’s argument points us in the right direction. The three hy- 
potheses about DNA replication might not exhaust the space of all possible 
replication mechanisms, but they may be the only ones that satisfy certain 
mechanistic constraints. All three schemes of DNA replication had to incor- 
porate some quite stringent constraints. Most importantly, the schemes had 
to explain how DNA molecules with the same nucleotide sequence as an ex- 
isting molecule could be synthesized. Thus, explanatory considerations were 
already involved in the construction of the various hypotheses. This fits nicely 
with Lipton’s ({2004]) two-filter strategy, according to which the generation of 
a number of ‘live options’ of candidate hypotheses is followed by a selection 
of the ‘loveliest’ one and where explanatory considerations enter at all stages 
of the research process, 1.e., in both ‘filters.’ The main difference to my account 
is that I propose to base these explanatory considerations on a mechanistic 
account of explanation. 
This mechanism-based view puts very stringent constraints”* on what quali- 
fies as a live option. Suitable candidate hypotheses must incorporate a consider- 
able body of knowledge from organic chemistry and molecular biology. In my 
example, the double helix model was such a constraint. It incorporated a great 
body of knowledge from organic chemistry, the physical chemistry of colloids, 
and crystallography. Furthermore, it was already fairly clear at that time that 
the sequence of bases in DNA was biologically highly significant (see Crick 
[1958], who could already cite a considerable body of evidence that supported 
this idea). Therefore, the replication mechanism had to preserve the nucleotide 
sequence of DNA. The complementarity of base pairing provided a lovely ex- 
planation for how a mechanism of DNA synthesis could achieve this. Hence, it 
was set that either single- or double-stranded DNA had to serve as a template 
ideas around that proteins are assembled directly on the DNA molecule (Gamow’s ‘diamond’ 
hypothesis). Later, it was shown that protein synthesis requires RNA as an intermediate (Judson 
[1979], p. 252). The hypothesis that DNA replication might require an intermediate was, to my 
knowledge, never seriously entertained; but it cannot be ruled out a priori, which makes our case 
vulnerable to Duhem’s and van Fraassen’s arguments. 
A detailed list of such mechanistic constraints can be found in (Craver [2007], Chapter 3, Sec- 
tion 2). He distinguishes componency constraints (given by the stock of available entities), spatial 
constraints (pertaining to the possible spatial organization of mechanisms), temporal constraints 
(time courses and necessary sequences of events), and active constraints (given by invariant re- 
lationships between intervention variabies). I would add in particular functional constraints, i.e., 
considerations on what biological task the mechanism must perform (here: copying of the genetic 
material).

--- Page 51 ---

Duhem’s Problems and IBE 45 
for the (then still putative) DNA polymerase. Indeed, all the three major repli- 
cation mechanisms that were considered as live options during the mid-1950s 
incorporated this template idea. The great open questions were whether the 
template was single- or double stranded, and the extent to which the template 
was conserved in the process. 
Thus, background knowledge imposed a set of mechanistic constraints on 
the space of possible solutions to the replication problem. There was simply no 
alternative that could satisfy all these constraints and explain .he Meselson 
Stahl data by their own wits. Only the Watson—Crick model passed both of 
these IBE-filters—this, I suggest, is what made this experiment so compelling. 
Possibly, many alleged ‘crucial experiments’ in and out of biology owe their 
strength to this kind of logic. 
8 IBE and Bayesianism 
So far, I have argued that IBE does a better job in reconstructing ‘the most beau- 
tiful experiment in biology’ than eliminative induction and the error-statistical 
approach to scientific inference. In this last section, I want to show that the 
IBE approach is not in conflict with the current mainstream theory (or family 
of theories) of confirmation, which, of course, is Bayesianism. Okasha ({2000)) 
and Lipton ({2004], Chapter 7) have already shown that IBE need not be in 
conflict with Bayesian constraints on personal degrees of belief. That is, pro- 
ponents of IBE need in no way challenge the Bayesian’s credo that the only 
rational way of assigning probabilities to hypotheses that are subject to em- 
pirical confirmation is by conditionalizing on the evidence in accordance with 
Bayes’s theorem. But this does not make IBE superfluous. Far from it, IBE 
can provide a way of realizing the Bayesian formalism in concrete cases. The 
formalism as such makes no prescriptions as to how the prior probabilities 
and likelihoods ought to be set; it only says that once these have been set, the 
posterior probabilities are set as well, on pain of incoherence. This is quite a 
weak constraint. What IBE can do here is to provide some further constraints, 
for example, on the prior probabilities and likelihoods themselves. Sometimes, 
estimating how likely a hypothesis makes some piece of evidence, that is, esti- 
mating p(e|h), might involve considerations as to whether / is able to explain e. 
In our present idiom, this means that there must be a mechanism whereby the 
state of affairs described in e is produced and where this mechanism, or parts 
of it, are described by 4. Furthermore, explanatory considerations may be used 
to set prior probabilities, on which Bayesianism imposes no constraints. 
This way of reconciling IBE and Bayesianism is not new. What I would like to 
do, briefly, is to show how the present case study illuminates this reconciliation 
and thus adds credibility to it.

--- Page 52 ---

46 Marcel Weber 
The semi-conservative mechanism does not entail the Meselson-Stahl re- 
sults; as I have argued (Section 6.1) the two are connected by the factual 
assumptions that describe the experimental mechanism. Since this mechanism 
is plausible given the background knowledge (see Section 7), we could say that 
its description h bestows a high probability on e (the banding pattern observed). 
Since no comparable mechanism explains e given the other hypotheses, we can 
say that the latter make e less likely. Assuming that the hypotheses do not differ 
too much in prior probability, this already means that the semi-conservative 
hypotheses had a higher posterior probability after the experiment was done. 
Alternatively, it could be argued that explanatory considerations afford the 
semi-conservative scheme with the most favourable likelihood ratio (Roush 
{2005}). 
What is the advantage of supplementing the Bayesian account with explana- 
tory considerations in this manner? I suggest that, in addition to providing 
a way of assigning likelihoods, IBE can also illuminate Bayesian solutions 
to Duhem’s problem. Some Bayesians have argued that untested auxiliary 
assumptions could still have a prior probability that is sufficiently high to 
allow confirmation or disconfirmation of a hypothesis under test, thus tak- 
ing the sting out of Duhem’s problem (Dorling [1979]). The problem with 
this approach is that it implies that most experimental tests will only be con- 
clusive for some people (namely, those who give a high prior probability to 
all the auxiliaries) and not for others, which is undesirable. Here, IBE can 
help: Explanatory considerations such as those discussed in Section 6.3 can be 
used to set bounds for belief in the auxiliaries, thus rendering evidence more 
objective. 
9 Conclusions 
I have argued that an experimentalist version of IBE permits a reconstruc- 
tion of the Meselson-Stahl experiment according to which the latter provided 
decisive veridical evidence (Achinstein [2001]) for the semi-conservative hy- 
pothesis, while the two alternatives remained without such support. This is 
pretty close to what crucial experiments were always supposed to do, except 
that I am of course not claiming that such an experimental demonstration 
can reach the apodictic certainty of deduction (as Duhem required, see Sec- 
tion 2). In contrast, eliminative induction and the severe-testing approach fail 
to exhibit the evidential support of the experiment for the semi-conservative 
hypothesis. 
In contrast to Lipton’s ({2004]) account of IBE, I have used a mechanistie 
account of explanation. An advantage of such an account is that it does justice 
to actual explanations in molecular biology. Another advantage is that it makes 
explanation an objective relation between explanans and explanandum, which

--- Page 53 ---

Duhem’s Problems and IBE 47 
means that the evidential relation can also be objective. I have also introduced 
the notion of an experimental mechanism, which is like the physiological mech- 
anisms discussed by philosophers of biology and neuroscience, except that it 
includes experimental manipulations, instruments and artificially created en- 
tities and activities. Experimental mechanisms connect data and phenomena 
via causal processes. Physiological mechanisms are embedded in experimental 
mechanisms. This notion allows IBE to be extended to show how inferences 
can be drawn from experimental data such as the banding patterns observed in 
an analytic ultracentrifuge. 
Finally, I have shown that the two predicaments that Duhem identified for 
crucial experiments (though on the assumption that all inferences would have 
to be deductive) as well as van Fraassen’s well-known ‘bad lot’-objection to 
IBE can be solved in the IBE-based framework that I have used. As regards 
Duhem’s first problem, the experimentalist variant of IBE allows (fallible) infer- 
ences to hypotheses about mechanisms even if there are untested experimental 
assumptions.”* The crucial move is the recognition that an experimental mech- 
anism containing untested auxiliaries that is sufficient for explaining the data 
is better supported by these data than schemes that are not sufficient. As it 
were, the untested auxiliaries hitch a free inferential ride on the experimental 
mechanism. In addition to the free riders, there were experimental assumptions 
that were actually tested in this case, and these tests also involved IBE. 
Van Fraassen’s ‘bad lot’ problem (which I take to be basically Duhem’s sec- 
ond problem as applied to ampliative instead of deductive inference) can be 
handled by showing how an extensive body of background knowledge provided 
a host of stringent material constraints on the candidate hypotheses. 
Mechanistic-explanatory considerations are involved in the construction of 
such candidates as well as in the selection of the best one by a crucial experi- 
mental test. 
Finally, I have shown that this case study strengthens the view that propo- 
nents of IBE and Bayesians can be friends. 
Acknowledgements 
Versions of this paper were presented at the conferences ‘Confirmation, Induc- 
tion and Science’ (London School of Economics, March 2007) and “Generating 
Experimental Knowledge’ (University of Wuppertal, June 2007). I am grateful 
to the organisers and audiences of these conferences. Further, I wish to thank 
Daniel Sirtes for critically reading several drafts and suggesting the title, and 
4 Note that the justificatory status of a proposition does not affect its explanatory force, at least 
not with respect to potential explanations (where the explanans need not be true), which is the 
relevant sense of explanation in IBE.

--- Page 54 ---

48 Marcel Weber 
Matthew Meselson for giving me permission to reproduce a figure from his PhD 
thesis. This research was supported by the Swiss National Science Foundation. 
Science Studies Program and Department of Philosophy 
University of Basel 
Missionsstrasse 21, 4003, Basel 
Switzerland 
marcel.weber@unibas.ch 
References 
Achinstein, P. [2001]: The Book of Evidence, Oxford: Oxford University Press. 
Bechtel, W. [2005]: Discovering Cell Mechanisms: The Creation of Modern Cell Biology, 
Cambridge: Cambridge University Press. 
Bogen, J. and Woodward, J. [1988]: “Saving tne Phenomena’, Philosophical Review, 97, 
pp. 303-52. 
Craver, C. F. [2007]: Explaining the Brain. Mechanisms and the Mosaic Unity of Neuro- 
science, Oxford: Oxford University Press. 
Craver, C. F. and Darden, L. [2001]: ‘Discovering Mechanisms in Neurobiology: The 
Case of Spatial Memory’, in P. K. Machamer, R. Grush and P. McLaughlin (eds), 
Theory and Method in the Neurosciences, Pittsburgh: Pittsburgh University Press, 
pp. 112-37. 
Crick, F. H. C. [1958]: ‘On Protein Synthesis’, Symposia of the Society for Experimental 
Biology, 12, pp. 138-63. 
Darden, L. [2006]: Reasoning in Biological Discoveries: Essays on Mechanisms, Interfield 
Relations, and Anomaly Resolution, Cambridge: Cambridge University Press. 
Delbriick, M. [1954]: ‘On the Replication of Deoxyribonucleic Acid’, Proceedings of the 
National Academy of Sciences of the United States of America, 40, pp. 783-8. 
Dorling, J. [1979]: ‘Bayesian Personalism and Duhem’s Problem’, Studies in History and 
Philosophy of Science, 10, pp. 177-87. 
Duhem, P. [1954]: The Aim and Structure of Physical Theory, Princeton, NJ: Princeton 
University Press. 
Franklin, A. [2007]: ‘Experiment in Physics’, in E. N. Zalta (ed.), The Stanford Encyclo- 
pedia of Philosophy, <plato.stanford.edu/entries/physics-experiment/> 
Hanawalt, P. C. [2004]: ‘Density Matters: The Semiconservative Replication of DNA’, 
Proceedings of the National Academy of Science of the United States of America, 101, 
pp. 17889-94. 
Holmes, F. L. [2001]: Meselson, Stahl, and the Replication of DNA: A History of ‘The 
Most Beautiful Experiment in Biology’, New Haven, CT: Yale University Press. 
Judson, H. F. [1979]: The Eighth Day of Creation: Makers of the Revolution in Biology, 
London: Jonathan Cape. 
Laudan, L. [1990]: ‘Demystifying Underdetermination’, in C. Wade Savage (ed.), Scien- 
tific Theories, Minnesota Studies in the Philosophy of Science, Vol. XIV, Minneapolis, 
MN: University of Minnesota Press, pp. 267-97. 
Lipton, P. [2004]: Inference to the Best Explanation, 2nd edition, London: Routledge.

--- Page 55 ---

Duhem’s Problems and IBE 49 
Machamer, P., Darden, L. and Craver, C. F. [2000]: ‘Thinking about Mechanisms’, 
Philosophy of Science, 67, pp. 1-25. 
Mayo, D. G. [1996]: Error and the Growth of Experimental Knowledge, Chicago: Univer- 
sity of Chicago Press. 
Meselson, M. [1957]: Equilibrium Sedimentation of Macromolecules in Density Gradients 
with Application to the Study of Deoxyribonucleic Acid, PhD thesis, Pasadena, CA: 
California Institute of Technology. 
Meselson, M. and Stahl, F. W. [1958]: ‘The Replication of DNA in Escherichia coli’, 
Proceedings of the National Academy of Science of the United States of America, 44, 
pp. 671-82. 
Norton, J. D. [1993]: ‘The Determination of Theory by Evidence: The Case for Quantum 
Discontinuity, 1900-1915’, Synthese, 97, pp. 1-31. 
Okasha, S. [2000]: ‘Van Fraassen’s Critique of Inference to the Best Explanation’, Studies 
in History and Philosophy of Science, 31, pp. 691-710. 
Rheinberger, H.-J. [1997]: Toward a History of Epistemic Things: Synthesizing Proteins 
in the Test Tube, Stanford, CA: Stanford University Press. 
Roush, S. [2005]: Tracking Truth: Knowledge, Evidence, and Science, Oxford: Oxford 
University Press. 
Schaffner, K. F. [1993]: Discovery and Explanation in Biology and Medicine, Chicago: 
The University of Chicago Press. 
Stanford, P. K. [2006]: Exceeding Our Grasp: Science, History, and the Problem of Un- 
conceived Alternatives, Oxford: Oxford University Press. 
Stent, G. S. [1958]: ‘Mating in the Reproduction of Bacterial Viruses’, Advances in Virus 
Research, 5, pp. 95-149. 
van Fraassen, B. C. [1989]: Laws and Symmetry, Oxford: Clarendon. 
Watson, J. D. and Crick, F. H. C. [1953]: ‘Molecular Structure of Nucleic Acids. A 
Structure for Deoxyribose Nucleic Acid’, Nature, 171, pp. 737-8. 
Weber, M. [2005]: Philosophy of Experimental Biology, Cambridge: Cambridge Univer- 
sity Press. 
Woodward, J. [2000]: ‘Data, Phenomena, and Reliability’, Philosophy of Science ( Pro- 
ceedings ), 67, pp. S163-S179. 
Woodward, J. [2002]: ‘What Is a Mechanism? A Counterfactual Account’, Philosophy of 
Science ( Proceedings ), 69, pp. S366—S377.

--- Page 57 ---

Brit. J. Phil. Sci. 60 (2009), 51-78 
Lineage Explanations: Explaining 
How Biological Mechanisms 
Change 
Brett Calcott 
ABSTRACT 
This paper describes a pattern of explanation prevalent in the biological sciences that I 
call a ‘lineage explanation’. The aim of these explanations is to make plausible certain 
trajectories of change through phenotypic space. They do this by laying out a series 
of stages, where each stage shows how some mechanism worked, and the differences 
between each adjacent stage demonstrates how one mechanism, through minor modifi- 
cations, could be changed into another. These explanations are important, for though 
it is widely accepted that there is an ‘incremental constraint’ on evolutionary change, 
in an important class of cases it is difficult to see how to satisfy this constraint. I show 
that lineage explanations answer important questions about evolutionary change, but 
do so by demonstrating differences between individuals rather than invoking population 
processes, such as natural selection. 
Introduction 
Turning a ‘Scale’ into a ‘Plume’ 
Lineage Explanations in Biology 
3.1. The evolution of eyes 
3.2 The evolution of feathers 
The Two Dimensions of a Lineage Explanation 
4.1 The production dimension 
4.2 The continuity dimension 
4.3 The dual role of the parts 
Constraining the Explanations 
Operational and Generative Lineages 
Explaining Change Without Populations 
Conclusion 
The Author (2008). Published by Oxford University Press on behalf of British Society for the Philosophy of Science. All rights reserved 
doi:10.1093/bjps/axn047 For Permissions, please email: journals. permissions@oxfordjournals.org 
Advance Access published on December 10, 2008

--- Page 58 ---

Brett Calcott 
1 Introduction 
This paper describes a pattern of explanation prevalent in the biological sciences 
that I call a ‘lineage explanation’. These explanations often appear as a diagram 
resembling a cartoon strip. Each stage shows how some mechanism worked, 
and the differences between each adjacent stage demonstrates how one work- 
ing mechanism, through minor modifications, could be changed into another 
working mechanism. This series of mechanisms, as a whole, shows the plau- 
sibility of certain trajectories through phenotypic space. Demonstrating this 
plausibility is an important goal, for it is widely accepted within evolutionary 
biology that there is an ‘incremental constraint’ on evolutionary explanations 
of change. In an important class of cases, it is difficult to see how to satisfy this 
constraint. A distinctive virtue of lineage explanations is that they show how 
change can be incremental, even in these difficult cases. 
Lineage explanations are an answer to a distinctive kind of question about 
evolutionary change. The question ‘how did biological mechanism X change 
over time’ is ambiguous.' We could be asking about the process that drove 
change—a question to which natural selection, in part, provides an answer, 
or we could be asking a question about what particular modifications are 
responsible for the differences between organisms related by descent. Here, the 
process of natural selection has little to say. Instead, the explanation lies in the 
details of how the particular mechanism of interest worked, and how it changed 
over time. It is this second question that lineage explanations address. 
Lineage explanations are particularly important in evolutionary develop- 
mental biology (evo—devo), where they are used to explain the origins of evo- 
lutionary novelty. The distinctive nature of explanation in evo—devo has been 
the subject of some philosophical attention (Wagner [2000]; Amundson [2001]; 
Winther [2005]). The account that I give here divides the landscape up in a 
unique way. For I take the primary distinction to be one between population- 
level and individual-level explanations, rather than between adaptationist and 
developmental explanations. As I shall show, non-developmental, adaptation- 
ist explanations can a'so use this pattern of explanation, particularly when they 
attempt to explain the origins of complex adaptations. 
My approach in this paper will be to concentrate on outlining the general 
structure of these explanations. I provide some detailed examples, demonstrat- 
ing the core aspects of these explanations and some important variations on 
how they can be constructed. These explanations are complex, so exploring 
their structure shall take much of the paper. I shall have little space to examine 
in detail how these explanations relate to more general issues of explanation, 
' See Amundson ({2001], [2005]) for some excelleni discussion about the differing types of con- 
trastive questions one might ask in biology.

--- Page 59 ---

Lineage Explanations: How Biological Mechanisms Change 53 
but I shall make some important connections in the final section. A deeper 
exploration of the issues shall have to wait. 
I begin by outlining the core features of these explanations via a toy example 
in Section 2. In Section 3, I introduce two biological examples of lineage ex- 
planations. Section 4 explores the similarities between these examples, showing 
that they both possess the same core structure outlined in the toy example. 
Section 5 examines a number of different ways that these explanations might 
be constrained. In Section 6, I identify two variants of lineage explanations. I 
then look more closely at the type of explanation I have described in Section 7, 
and finish with a brief summary of these explanations. 
2 Turning a ‘Scale’ into a ‘Plume’ £ 
This section is all about a particular type of puzzle. There is nothing biological 
or evolutionary about this puzzle. I use it to show the core structural features 
of a lineage explanation first, without being distracted by the ‘gory details’ of 
biology. 
Here is one version of this type of puzzle. Given two words, the challenge is 
to transform one word into another by changing only a single letter at a time. 
Each of the intervening stages must also be a word. A valid word is any English 
word that isn’t a name or an abbreviation. Here, for example, is one way to get 
from the word ‘scale’ to the word ‘plume’. 
scale—scalp—scamp—stamp—stump—slump—plump—plume 
Beginning with ‘scale’, we change the last letter to ‘p’, forming ‘scalp’. We then 
change the fourth letter in ‘scalp’ to ‘m’, forming ‘scamp’. We proceed in the 
same manner, changing a single letter at a time, finally forming ‘plume’.- 
This is a very simple game, but it has a number of features I want to spell out. 
A solution to the puzzle describes a trajectory between two chosen words. This 
solution provided must meet two requirements. First, only a single letter may 
be changed at a time, so each adjacent word must share a significant number 
of letters in common. No big jumps are allowed. I'll call this a continuity 
requirement. Second, each intermediate stage must form an actual word. We 
cannot, for example, go from ‘scale’ to ‘slale’. For even though this change meets 
the continuity constraint (a single letter change), ‘slale’ is not a valid English 
word. I'll call this a production requirement. At each stage in the trajectory the 
letters must produce some output that meets a particular criterion; in this case, 
the letters must form a valid English word. 
For puzzle pedants: As far as I can work out, there is no trajectory between ‘scale’ and ‘plume’ 
with fewer stages than the example shown here, though there are other routes where the ‘e’ 
remains unchanged for the entire route.

--- Page 60 ---

54 Brett Calcott 
Fulfilling both of these requirements is achieved by manipulating the letters. 
The chosen letters and their organisation are what produce the word, and the 
difference between stages is brought about by changes in those same letters. We 
can say that the letters are simultaneously the components that produce some 
required output at each stage, and they are the grain of change that brings about 
the differences between each stage. This dual role played by the components 
due to the two requirements is a central feature of the puzzle. 
Note that we could modify the puzzle in various ways by constraining how 
these requirements are fulfilled. Rather than using the entire English lexicon, 
I could limit the set of valid words in some manner: only words that appear 
in Shakespeare’s ‘Hamlet’, for example. It is also possible to apply additional 
constraints between the stages (in addition to continuity). Perhaps a valid word 
must not only be drawn from a defined lexicon, but it must also sort alpha- 
betically after the word in the previous stage (in this case, getting from scale 
to plume would not be possible, scale/scalp/scamp is okay). This additional 
sorting constraint between the stages in is terms of the words produced at each 
stage. But we could also constrain the way letter changes are made in a more 
restrictive manner: the first change must be on the first letter, the second change 
on the second letter, and so on. Or we might have a more interesting idea of 
what constitutes a continuous change; maybe treating letter exchanges within 
the word as continuous too (an organisational change, rather than a replace- 
ment). Many of these kinds of changes will make the puzzle harder (maybe 
even impossible), for they reduce or constrain the number of possible trajecto- 
ries that meet the requirements. Nevertheless, the core of the puzzle remains the 
same: constructing a continuous trajectory of change in a set of components 
that produce some output. 
One last puzzle variant will emphasize an important point. Imagine a puzzle 
where, instead of letters, there are numerals and mathematical symbols. The 
components make an equation (‘6 x 4+ 1’, for example) and this equation 
produces a result (‘25’, in this case). As with the letters, there will be constraints 
on what combination of symbols produce a valid equation. Now let us add an 
additional constraint on the relation between the stages: they must all produce 
the same result. So, we begin with two equations that both produce the result 
‘9’, and now we must find a series of one-step symbol changes between these 
equations where each equation in the series likewise produces the result ‘9’. 
Here is the important point that this variant highlights: it is not simply 
changes in the output, but changes in how the output is produced that is central 
to the puzzle. For even if the series of outputs remains exactly the same, as it 
does in this previous example, there is clearly still a puzzle to solve. The question 
3 No, I don’t have an example of this. I suspect you might need a more liberal continuity rule to 
actually get some workable examples.

--- Page 61 ---

Lineage Explanations: How Biological Mechanisms Change 
“scale” “scalp” “scamp” “stamp” “stump” “slump” “plump” “plume” 
a a a 4 ’ a 4 
| | i 
—» iy —> 5 
i. U 
s ri wt ul 
a — P 
5 5 5 — P| 
Ly fF 
Production 
oan > 
P —o 
Continuity 
Figure 1. The two dimensions of a lineage explanation. Production runs vertically: 
each choice of letters and their organisation must make a word. Continuity runs 
horizontally: these same words change through a single replacement of the letters. 
of change in how output is produced is what is puzzling, despite variations in 
the demands that can be made on the series of outputs. Simple membership 
of a group at each stage might be sufficient, or directional demands between 
stages may be imposed (the sorting constraint given above), or the outputs may 
have to stay the same (as in the equation example). These different demands 
on the output constrain the puzzle in important ways. But the puzzle itself lies 
in showing how production changes in a continuous manner. 
These two requirements—production and continuity—form what I call the 
two dimensions of a lineage explanation (see Figure 1). The first dimension runs 
vertically through each stage of the trajectory, showing how the component let- 
ters meet the production requirement. The other dimension runs perpendicular, 
connecting each combination of letters via the continuity requirement. These 
two dimensions are what gives the letters this dual role as both the units of 
change and the components for constructing valid words. 
I’ll summarise the structure of this kind of puzzle in the following way. We 
begin by choosing two endpoints, and then show a series of stages that demon- 
strates how one could be changed into another. Each stage in the reconstruction 
must itself produce some output in virtue of the components that make it up, 
and each of the adjacent stages is related to one another by small continuous 
changes in those components. The dual requirements of puzzle mean that the 
components making up the puzzle must play two roles: both producing a result 
and providing the grain of change. These requirements may be constrained in 
different ways, by specifying particular conditions that must hold both within 
and between stages. Additional constraints can limit the number of possible 
trajectories in various ways. I'll return in more detail to both the requirements 
and the constraints on lineage explanations after presenting some biological 
examples. 
It is also important to note what is not part of the solution to the word 
puzzle. The puzzle does not specify any particular process for producing the

--- Page 62 ---

56 Brett Calcott 
trajectory—how you go about finding a possible route from one word to another 
is up to you. You can start at one end and work your way to the other, or you 
could start at both ends and try and find some overlapping middle point. These 
details do not figure in the solution. It is also quite likely that, in the pursuit 
of a trajectory you will explore many dead-ends—partial trajectories to words 
where you can go no further. But these discarded paths do not figure in the 
solution to the puzzle either. What matters is simply that you show a trajectory 
that satisfies the two requirements of continuity and production. Any trajectory 
containing intervening stages that meets these requirements is sufficient. 
We shall see that what is excluded from these solutions also has an analogue 
with the biological versions of lineage explanations. The population processes 
essential to driving the changes in lineages are excluded from the lineage ex- 
planations. This is not to say that they don’t occur, or are not important. But 
lineage explanations focus on a different, and difficult, question. What matters 
in these explanations is demonstrating how, given certain constraints, a partic- 
ular trajectory of change is plausible. What processes might be responsible for 
making these changes are omitted from the explanation. 
3 Lineage Explanations in Biology 
Biologists are often faced with puzzles resembling those in the previous section. 
They identify two organisms related by descent, yet these organisms differ in 
some aspect of behaviour, in the presence or absence of some feature, or perhaps 
in the different way that some common feature is produced. The puzzle is to 
figure out how something that works like this could have turned into something 
that works like that. 
The explanations given for these kinds of problems consist of two dimen- 
sions. These two dimensions are (unsurprisingly) somewhat more complex than 
the puzzles in the last section. Rather than the production dimension consist- 
ing of some simple composition where letters form words, the production at 
each stage is performed by some kind of mechanism. I shall return in more 
detail to the issue of mechanistic explanation in Section 4.1. For now, however, 
it will be sufficient to say that a mechanism consists of a set of components 
(individual parts, processes, and their organisation) that produces some regu- 
lar phenomenon (a behaviour, or a morphological form, for example).* It is 
in virtue of gradual changes to these components that we make plausible a 
continuous trajectory of change in a lineage of organisms. 
4 To avoid wordiness I will use the word ‘components’ to capture all the relevant details that are 
meant to make up a mechanism, such as entities, activities, and their organisation. Exactly what 
this list includes is still somewhat controversial (see Tabery [2004] for some discussion of this 
issue).

--- Page 63 ---

Lineage Explanations: How Biological Mechanisms Change 57 
Like the puzzle, there are some variants on how we might constrain the tra- 
jectory between the two endpoints. The output in each stage might be required 
to resemble a member of some series of fossils, or some related extant species. 
Directionality may be imposed on the series by appealing to the adaptive value 
of the phenomenon that mechanism produces. Or we might try to explain 
how the mechanisms underlying the development of an animal have changed 
over time whilst the adult form has remained largely the same—an example 
resembling the puzzle with the equation in the previous section. In each case, 
however, the puzzle to be solved is how gradual change could have occurred in 
the way a mechanism produces some phenomenon. 
In what follows, I give two detailed examples of lineage explanations. The 
first is Nilsson and Pelger’s account of eye evolution (Nilsson and Pelger [1994]). 
The second is Prum’s account of feather evolution (Prum [1999}).° 
3.1 The evolution of eyes 
Nilsson and Pelger construct a model describing a possible route from a simple 
light-sensitive patch to a focussed lens eye (Nilsson and Pelger [1994]). They 
base their reconstruction on an idealised morphological model that is general 
enough to capture the structural details present in the eyes of many different 
organisms. Each successive stage shows some simple change from the previous 
one, and in each successive stage the eye is capable of producing better and 
better images. This improvement plays an important part of the model as it 
provides an adaptive constraint on the series. Nilsson and Pelger provide an 
estimate of the time that these transformations would take, based on the relative 
size of these small changes in morphology (shape, size, etc.). They conclude that, 
even with pessimistic parameters, such an eye could evolve in a few hundred 
thousand years. 
The measure of an eye’s adaptive value in this sequence is based on the spatial 
resolution that each morphological variant can produce. This is simply the abil- 
ity for it to provide a sharp picture. According to Nilsson and Pelger, no matter 
what varied visual functions an eye might have in different organisms—such as 
movement tracking or pattern detection—spatial resolution is central to how 
well that eye can perform. 
Each stage in Nilsson and Pelger’s model has the same parts: a light-sensitive 
patch of cells sandwiched between a transparent layer and a dark pigmented 
layer. The parts undergo various modifications throughout the series from a 
For some other clear examples of the two dimensions of lineage explanations, see the diagrams in 
the following: (Truman and Riddiford [1999]; Peel and Akam [2003]; Suzuki and Nijhout {2006)}). 
For an example where the output of the mechanism stays the same, but the mechanism changes 
over time, see the discussion in Raff’s chapter ‘Building Similar Animals in Different Ways’ (Raff 
[1996]).

--- Page 64 ---

Brett Calcott 
E °F yes 
es -” 
> 
Production | C C i, il. iil. Iv. V. 
A A 
Continuity 
Figure 2. Five stages in eye evolution. Each small change increases the ability 
of the eye to produce a better image (as the ‘eye chart’ shows). As in the word 
puzzle, the two dimensions of the explanation are present. The parts, processes 
and their organisation in the eye explain this ability, and the small changes in the 
organisation of the components provide the continuity. Redrawn from (Nilsson and 
Pelger [1994]). 
simple light-sensitive spot to a camera-lens eye. The stages proceed as follows 
(see Figure 2).° 
(i) The three layers of cells begin as a flat light-sensitive spot. 
(ii) A depression forms in the centre of the light-sensitive and pigmented 
layer (together, forming the retina), and the transparent layer deepens 
to fill this depression. 
(iii) The opening towards the light then narrows. The eye now resembles a 
simple pin-hole camera. 
(iv) An increase in the refractive index in a portion of the transparent layer 
produces a graded index lens that begins to focus the incoming light.’ 
(v) The light aperture narrows further to form an iris, and moves to the 
centre of curvature to produce a tightly focussed image on the retina. 
The increase in spatial resolution over the first three stages relies on the nar- 
rowing of the aperture that permits light through and the increased curvature 
of the receptive surface. But the continued narrowing of the light aperture also 
lessens the total light that is received, producing a noisy image. At some point 
a trade-off is reached, where narrowing the aperture decreases the field of view 
© These stages are simplified from the original paper, but maintain the central changes. Nilsson 
and Pelger’s diagram contained eight stages. I have compressed this to five stages that capture the 
key events. 
Glass lenses have a constant refractive index. The transparent part of vertebrate eyes has a 
variable refractive index producing a graded index lens.

--- Page 65 ---

Lineage Explanations: How Biological Mechanisms Change 59 
for the receptors, but simultaneously increases the noisiness of the image. At 
this stage a lens is required to increase resolution which appears in stage (iv). 
Before this stage, the refractive index of the transparent layer of cells has been 
homogeneous. The remaining stages produce a graded index lens, and the shape 
of the receptive surface takes on a parabolic shape. 
Nilsson and Pelger show that these modifications can produce a roughly 
linear increase in spatial resolution. So there is no place where a large mor- 
phological change is required to produce a small adaptive payoff. Nilsson and 
Pelger emphasise the importance of small changes providing a smooth gradient 
on which selection can act. 
This reconstruction seems plausible as only a small change is needed in the 
morphology of the eye in each stage. But there is no guarantee that small 
changes in morphospace can be generated by small changes in the genotype. 
It could be the case that what appears as a small difference in morphology 
required some large change or reorganisation at the genotypic level. So this 
explanation makes an implicit bet: that there is genotypic variability available 
that maps smoothly to these small morphological changes. 
These assumptions are at least partially supported by providing other evi- 
dence. Eyes resembling each of these stages can also be found in living organ- 
isms. Moreover, a complete series of these eyes can be found within a single 
phylum (both the annelids and the molluscs). So there is at least some prima 
facie evidence that the requisite variability is available. The origin of the lens is 
also made more credible as the change in refractive index is caused by proteins 
that have other cellular functions. So they may have been co-opted rather than 
have arisen de novo. 
3.2 The evolution of feathers 
The complex structure of feathers is regarded as an example of a morphological 
novelty (Prum [1999]; Miller and Wagner [2003]). In what follows, I summarise 
an account of the evolution of feathers from (Prum [1999]). Prum takes a purely 
developmental approach to explain the evolution of feathers, explicitly ignoring 
their adaptive function. 
Feathers are a complex branched epidermal growth. The main branch is 
called the rachis, off this come the barbs, consisting of a barb ramus and 
variably shaped projections called barbules (see Figure 3A). Although feathers 
consist of a branched structure, they do not grow by branching at the tips in the 
way that trees do. Instead, they grow from the base, like hair or fingernails, and 
the different levels of branching are each constructed by various mechanisms 
located in the feather follicle. This follicle is a cylindrical invagination in the 
epidermis—like the indentation you would get on your skin if you held a small 
metal pipe on it for a while. Feather growth is fed from nutrients in the follicular

--- Page 66 ---

Brett Calcott 
Rachis 
Production 
Saaaeeaea = “, 
- OneN Barbules i 
ii. 
feo > 
Continuity 
Figure 3. Feather structure and evolution. (A) The hierarchical structure of a typical 
feather. The closeup shows the finer detail of the interlocking barbules that form 
the planar vane for a flight feather. (B) Proposed evolution of the feather. On the 
bottom are cross sections of the follicle collar during feather development. The 
changes here explain the resultant changes in feather form on the top row. Again, 
the two dimensions of the explanation are present. Redrawn from (Prum [1999]) 
and (Yu et al. [2002]). 
cavity (the base of the indentation), the form is constructed on the follicle collar 
(the outside of the tube projecting in the middle), and is displaced upward to 
produce a keratin matrix that constitutes the mature feather. The production 
of the complex branched structure is derived from the interaction of several 
processes of cellular differentiation occurring on the follicle collar. 
Prum proposes a series of stages to explain the evolution of feathers based 
on changes that occurred in the feather follicle.» Each of these changes in the 
feather follicle produces a new stage in feather morphology. So although the 
target of this explanation is the changing form of feathers, the explanation is 
given in terms of the follicle. 
A simple analogy can help to understand the relevance of these modifica- 
tions. We can think of the feather as being extruded from the follicle, rather 
like toothpaste coming out of a tube when you squeeze it. If someone asks 
how stripy toothpaste is produced (a phenomenon causing much puzzlement 
when I was a child), we tell them about the modifications to the nozzle of the 
toothpaste tube that lay down the stripes as the toothpaste is squeezed out. The 
modifications that Prum proposes can be likened (very roughly) to modifica- 
tions to the nozzle of the toothpaste tube. They are changes in the components 
of the mechanism that generate the form of the feather (see Figure 3B). 
} 
8 See (Prum [1999]) for details. Here, I have reproduced the stages that Prum labelled I through III, 
and assumed that IIIb comes before Illa. This is the same sequence suggested as more likely by 
Yu et al. ({2002)).

--- Page 67 ---

Lineage Explanations: How Biological Mechanisms Change 61 
The ancestral follicle consists of a simple cylindrical indentation. From 
this, the initial growth produces a hollow cylinder. (Imagine a toothpaste 
nozzle which has a round piece stuck in the middle). 
The epidermal collar then differentiates into longitudinal barb ridges. 
This produces a tuft of unbranched barbs (like stripy toothpaste, but 
only the stripes come out). 
Peripheral paired barbule plates then appeared on the sides of the barb 
ridges. These produced the first fine-level structure on the outgrowths, 
the rami and the barbules (the toothpaste analogy runs out of steam 
here). 
The point of barb growth becomes displaced as the barb is 
growing—producing helical growth around the follicle (following the 
arrows in the Figure 3B (iv)). New growth begins at the barb locus and 
eventually comes together to fuse and produce the rachis. This change 
created the large-scale branch structure, and enabled an indeterminate 
number of barbs to be produced for any outgrowth. 
Prum’s reconstruction seems plausible because, despite feather form changing 
radically, there are only small changes needed in the follicle to produce these 
changes. In addition, as with the eye, there are examples of these kinds of 
intermediate structures being produced in extant organisms. For example, the 
downy feathers found on birds resemble stage (iii). 
We might think that the inclusion of developmental information in the 
feather example means that it somehow ‘goes deeper’ than the adaptive account 
of the eye example. But although Prum’s account includes some developmental 
information, it still makes the same kind of bets that the eye account does. 
What produces change in feather morphology is small changes in the follicle 
that produce these feathers. But no account is given of how these small changes 
in the follicle map onto genotypic changes. So Prum is still making the same 
kind of bet about the genotype to phenotype mapping as Nilsson and Pelger. 
As with the eye example, Prum supports his assumptions by providing com- 
parative information. Each of the hypothesised feathers can be found in extant 
birds, as well as the developmental mechanisms responsible for the production 
of the variants in feather shape. 
4 The Two Dimensions of a Lineage Explanation 
Nilsson and Pelger’s explanation of eye evolution and Prum’s explanation of 
feather evolution are both examples of lineage explanations. In this section, I 
look more closely at the resemblance these examples have to the puzzles in the 
first section, and also the resemblance they have to each other. I'll do this by 
showing how, like the puzzles in Section 2, there are two dimensions present

--- Page 68 ---

62 Brett Calcott 
in these explanations, and the components making up the explanations play a 
role in both of these dimensions. 
4.1 The production dimension 
The production requirement in both of these examples is fulfilled by showing 
how some mechanism produces some phenomenon. The phenomenon pro- 
duced in each stage in Nilsson and Pelger’s series of eyes was a focussed image, 
with some degree of spatial resolution. The phenomenon produced bv each 
stage in Prum’s series of feathers was some variant of feather structure. Like 
the word puzzle, this output or phenomenon was produced by the components 
present at that particular stage: the combination and configuration of cell layers 
in the eye, or the construction and processes occurring in the feather follicle. 
Describing how each stage produces these phenomena results in a series of 
self-contained explanations. The last stage of the eye, for example, explains the 
spatial resolution of that particular type of eye by providing facts such as the 
ellipsoid shape of the layers of cells, the amount of light that this lets in and 
the optical density of the transparent cells. A similar independent explanation 
can be given for the other stages in the eye example. If we found some creature 
with an eye resembling, say, Figure 2 (ii), then the details that Nilsson and 
Pelger provide about that stage would explain how that particular creature had 
the visual acuity it did. Similarly, if we found a creature with feathers resembling 
those in Figure 3 (iii), the developmental explanation offered by Prum about 
that stage would explain why those feathers had the form that they did. So a 
lineage explanation contains a series of smaller, independent, explanations. 
These independent explanations at each stage are mechanistic explanations. 
There are a number of closely related modern accounts of mechanistic ex- 
planation (Glennan [1996]; Machamer et al. [2000]; Bechtel and Abrahamsen 
[2005]). The goal of these accounts is to capture the practices of many of the 
life sciences, where to give an explanation is just to give a description of a 
mechanism (Machamer et al. [2000]). Here is a definition of a mechanism from 
(Bechtel and Abrahamsen [2005]): 
A mechanism is a structure performing a function in virtue of its compo- 
nent parts, component operations, and their organization. The orchestrated 
functioning of the mechanism is responsible for one or more phenomena 
(Bechtel and Abrahamsen [2005}). 
The individual stages in both Nilsson and Pelger’s account of eye evolution 
and Prum’s account of feather evolution fit well with mechanistic explanation. 
In each stage of the model of eye evolution, the component parts (the different 
cell types) and their relations and operations (the effect their form has on light)

--- Page 69 ---

Lineage Explanations: How Biological Mechanisms Change 63 
explain how that eye can achieve the visual acuity it does. In each stage of 
the model of feather evolution, the component parts (of the follicle) and their 
relations and operations (differential cell death, helical growth) explain the form 
of the feather that is produced. It is not surprising that these intermediate stages 
fit well with mechanistic explanation. The goal of the mechanistic account of 
explanation was to deal with exactly the kinds of scientific domain present here: 
developmental biology and physiology. 
One feature of mechanistic explanation emphasised by some authors is their 
hierarchical nature. A component within one mechanism may itself be a mech- 
anism, and hence be the subject of another explanation. Given a particular 
mechanism, we can choose to decompose it in different ways by choosing 
different levels of description (Bechtel and Richardson [1993)). 
What guides the scientist in choosing the ‘right’ level of description to ex- 
plain the phenomena of interest? There appear to be four ways of choosing the 
‘right’ level that are described in the literature. First, the nested descriptions 
of mechanisms may simply ‘bottom out’ at some level relevant to the scientific 
domain of interest (Machamer et al. [2000]). Second, depending on the par- 
ticular mechanism being described, the ‘working entities’ playing a role in the 
mechanism may pick out different levels of descriptions. Thus, in the mecha- 
nism of meiosis, it is the chromosomes that are the working entities, whereas 
in other molecular processes (such as transcription) the working entities will 
be pieces of DNA that (partially) constitute the chromosome (Darden [2005]). 
Third, scientists may provide incomplete descriptions by abstracting away cer- 
tain details. The purpose of this may be to give a more compact description, 
or it may be that they wish a more general description that unifies a set of 
particular mechanisms (Machamer et al. [2000]). Finally, the process of de- 
composing a mechanism to provide an explanation is an ongoing one (Darden 
[2002]; Bechtel and Abrahamsen [2005]). So we might ‘black-box’ a particular 
component of a mechanism that we don’t yet fully understand, though we can 
describe enough of its apparent behaviour for it to play a role in some higher 
level description. 
Ascan be seen, the choice of a particular decomposition depends on different 
things. In the first case, we are guided by our broad domain of interest. in the 
second case, it is a narrower domain of interest, that of a particular mechanism, 
that guides us. In the third case, it is the role that we wish our explanation to 
play. In the last case, we are constrained by the current limits to our knowledge. 
This ability to decompose a mechanism in different ways plays an important 
part in how these mechanisms contribute to a lineage explanation. We'll see 
in the next section that the way we choose to decompose the mechanism in 
a lineage explanation is guided by additional concerns not mentioned above, 
concerns about relations of change between mechanisms.

--- Page 70 ---

Brett Calcott 
4.2 The continuity dimension 
A lineage explanation fills in plausible intervening stages between two pheno- 
typic endpoints that at first glance may look very different. We make a large 
change credible by positing a set of intermediate stages. But this is only part 
of the story. A mechanism is decomposed into a number of components, so we 
can explain this continuity in terms of the components, rather than at the focal 
level of the trait. Continuity between stages is achieved because the components 
used to describe a mechanism can be credibly redeployed or reorganised to con- 
struct the mechanism at a successive stage. So the vertical dimension—how we 
decompose the mechanisms into components—will be tied to the horizontal 
dimension—the ability of those components to provide a continuous story 
between each stage. 
For example, in the model of eye evolution, the same three layers of cells are 
present throughout the stages. Their shape, size and relation to one another are 
modified, but the credibility of continuity between the stages is justified because 
there is no radical change in parts, or their properties, during the transitions. 
The decomposition of the eye into these simple layers of cells not only explains 
the visual acuity of the eye, but it is also the right level at which we can 
explain the small differences that enabled increased acuity in the next stage. 
Similarly, the parts and organisation of the feather follicle—such as the barbules 
ridges and the helical growth—simultaneously explain the production of a 
particular feather form and provide a set of parts by which to account for the 
changes between different feather forms. 
Decomposition supplies an essential ingredient to explaining continuity be- 
tween stages, over and above adding more intermediate stages. A straightfor- 
ward gradualist proposal might be to explain change by simple operations on 
the entire focal trait, such as the Cartesian scaling and deformation famously 
portrayed in D’Arcy Thompson’s ‘On Growth and Form’ (Thompson [1961]). 
But by decomposing the trait into some mechanistic description, we can then 
explain change in terms of the components at this lower level. Now, deforma- 
tion of the individual components can be employed to explain change. More 
importantly, we can further explain change in terms of the addition, deletion, 
duplication, reuse and reorganization of components—something not possible 
if we remain at the singular focal level. 
For example, note that the transparent layer of cells in the eye example, orig- 
inally proposed to serve as a protective layer, gains the additional function of 
focussing light. So, although the function of the eye remains constant through- 
out all the stages, the components that make up the eye have changed their 
function. A modification to a single part of the eye plays a role in changing the 
function of the entire mechanism. Without decomposing the mechanism into 
parts it would not be possible to show this kind of change.

--- Page 71 ---

Lineage Explanations: How Biological Mechanisms Change 
4.3 The dual role of the parts 
Letters played a dual role in the puzzles in Section 2, participating in both 
dimensions of the puzzle. The letters chosen, and how they were organised 
produced the word at each stage, and small changes in the letters enabled a 
continuous change between stages. This dual role is similarly present in lineage 
explanations in biology. The components making up the mechanisms produce 
the relevant phenomena at each stage, and they serve as the grain of change for 
enabling continuity between stages. 
This dual role of the components in a mechanism distinguishes lineage ex- 
planations from other types of biological explanation. It also provides the basis 
for the recent focus on a number of concepts such as modularity and robustness 
in such fields as evo—devo. I'll show this by drawing some analogies between 
explanation in biology and planning in engineering. 
Consider some typical questions in proximate biology: how geckos stick to 
the ceiling (Autumn and Peattie [2002]), how ants find their way home (Wehner 
[2003]) or how fleas jump (Rothschild et al. [1975]). Providing an explanation 
for phenomena such as these goes hand-in-hand with knowing how we could, at 
least in principle, build something that does a similar job in a similar way. So if 
we really understand how a gecko sticks to the ceiling, we can build something 
that sticks to the ceiling /ike a gecko.’ Furthermore, the description of such 
mechanisms, often in the form of a diagram, provides (at least the outline) 
of a plan for how to build a machine that could reproduce these abilities. 
The suggestion (surely not original) is that there is some connection between 
explaining how a mechanism does X, and knowing how to build a mechanism 
that does X in a similar way. Perhaps this connection is not perfect, but it will 
suffice to make clear the importance of adding a continuity requirement to 
both explanations in biology and design in engineering. 
Note that the kinds of biological questions above are only about a production 
dimension: describing how a mechanism produces some output, or does some 
job. Suppose we introduce something like a continuity dimension to the picture. 
What would this mean for an engineering task? As it happens,-the addition of 
a continuity dimension is exactly what occurs in modern software engineering 
practice. The challenge is not simply to build some software that performs 
a particular task—the software must be built in such a way that it is easily 
modifiable. This is important as the functionality required from the software 
can change, sometimes radically, even whilst the software is still undergoing 
? In this case, this is exactly what has been done: gecko-like sticky material has been made based 
on the same principles as gecko feet.

--- Page 72 ---

66 Brett Calcott 
development.!° So the engineering task of software development often presents 
a dual challenge: the software is constructed to perform the current required 
function, but with an eye to isolating and localising functionality that can 
be independently modified, and perhaps reused. Many recent advances in the 
design of software have not been advances in algorithms, but in the tools and 
practices of software construction that enable easy modification and reuse of 
parts. 
What relevance does this have to lineage explanations? The dual challenge 
in software engineering mentioned above comes from the dual role that parts 
must play in the construction of software, much as the parts in a lineage 
explanation do. Producing a lineage explanation can be thought of as reverse- 
engineering the modifications that took place in a complex mechanism over 
time. It is not sufficient to show how some biological phenomenon works (say, 
for example, how geckos stick to the ceiling), but it must be shown how the 
mechanism they use is constructed of parts that stay functional as they change. 
Lineage explanations, by including a continuity dimension, introduce a new 
set of problems and principles, as they attempt to understand how something 
works and how it could have changed. 
The comparison I am making between certain engineering practices and the 
kind of reverse-engineering used to explain organismal change over time is not 
superficial. Similar concepts emerge in both disciplines when trying to discover 
and understand general principles relating to this dual constraint placed on the 
parts. Modularity, the focus of much recent discussion in evo—devo (Schlosser 
and Wagner [2004]; Callebaut and Rasskin-Gutman [2005]), is a key concept 
in software development practice. Kirschner and Gerhart’s list of ‘evolvability 
properties’ includes weak linkage, a concept that has many similarities with the 
idea of loose-coupling and indirection in software development (Kirschner and 
Gerhart [1998], [2005]). These concepts are connected to the dual role that parts 
must play in both disciplines: both producing some output and serving as the 
unit of change. Knowing what kind of structures enable working mechanisms 
to robustly and easily change over time is relevant to both reconstructing 
organismal change and designing systems subject to frequent modification. 
The analogy can be summarised in the following way. Good software de- 
sign requires that the software must not only perform a function, but also 
be easily modifiable. These dual constraints make a big difference to how the 
software will be constructed. Likewise in biology, understanding both how a 
mechanism can produce some phenomena, and how that same mechanism was 
modified over time, changes the kind of explanation given, for both issues must 
'0 Jacob famously likened the process of evolution to ‘tinkering’ rather than ‘engineering’ (Jacob 
[1977]). Jacob’s reasons for classifying something as tinkering rather than engineering suggest 
that a much of software engineering may rightly be called software tinkering.

--- Page 73 ---

Lineage Explanations: How Biological Mechanisms Change 67 
be simultaneously addressed in a lineage explanation. In each case, the addi- 
tional requirements, over and above explaining or designing how something 
works, correspond to adding what I have called a continuity dimension. This 
new dimension gives the components of the mechanisms an additional role as 
elements of change over time. 
Explaining what kinds of structural features enable change is a project of 
interest to both evo—devo and software development. Note that this is a different 
question from understanding what kinds of evolutionary processes could have 
evolved such structures. How modularity might enable future change, and how 
modularity evolved, are related but distinct issues. The analogy that I made 
between these two types of pursuits is only possible because both ignore the 
processes that cause change along the continuity dimension. It doesn’t matter 
why the software was required to be modified; what mattered was that it was 
modifiable. Likewise, in a lineage explanation, the fact that natural selection 
caused the change is not part of the explanation. What matters is that we 
describe the mechanism in a way that we can see how a small modification to 
some component could have, by whatever process, enabled such a change in 
the mechanism. I shall return to this issue in more detail in Section 7. 
5 Constraining the Explanations 
The production and continuity requirements constitute the core of a lineage 
explanation. Without these, there is no lineage explanation. There are, how- 
ever, a number of important ways that these explanations may be constrained 
by bringing additional information to bear on the explanation. Doing this is 
much like what I did to make the puzzle variants in Section 2. Introducing extra 
constraints on the puzzles made them harder, as there were fewer trajectories 
that met the imposed conditions. In the case of explanation, introducing con- 
straints likewise reduces the number of possible trajectories. The more we can 
constrain the possible trajectories, the more plausible our solution will look. 
Since the structure of a lineage explanation has two dimensions, additional 
constraints can be introduced in two ways: we can constrain what counts as a 
valid stage in our explanation, and we can constrain the way that we connect 
the stages. 
The requirement at each stage in a lineage explanation is to show how 
some particular mechanism performs some task. As I’ve said, each of these 
stages is itself an independent explanation about how some mechanism worked. 
Much of the time these reconstructions will be about mechanisms which no 
longer exist, so there will be familiar problems of inferring function from 
structure, or from similarity to features of extant organisms. For example, both 
lineage explanations I have given invoke analogues to extant organisms. These 
analogous organisms need not necessarily be part of the particular lineage

--- Page 74 ---

68 Brett Calcott 
that is being reconstructed. Invoking their existence can serve to increase the 
plausibility that a mechanism such as this is realistic and workable, without it 
necessarily being the case that the extant organisms’ particular mechanism is 
homologous with the one being explained. So we can constrain the plausibility 
of individual stages with respect to whether certain types of mechanisms are, 
in fact, biologically realisable. 
If we work along the other dimension of the explanation, then we can con- 
strain the relation between the different stages of the explanation. This can be 
done because we may have evidence for, or be able to assume, certain things 
about the order in which particular stages occurred. 
Both examples—the evolution of the eye and the evolution of the 
feather—imposed some sort of temporal ordering on the stages (in the same 
way that sort order was imposed on the word puzzle). It wasn’t simply that 
some stages were adjacent to one another; the changes were presumed to go 
in only one direction. This directionality was imposed in a very different way 
in each case. Consider the contrasting goals given by the authors of the two 
examples of lineage explanations: 
The first and most crucial task is to work out an evolutionary sequence 
which would be continuously driven by selection (Nilsson and Pelger 
[1994]). 
What is required is a theory of the origin of feathers that is based on avail- 
able evidence and that is independent of hypotheses about their presumed 
ancestral function (Prum [1999)). 
This difference in approach is partly explained by the subject matter. Note that 
the evolution of the eye (at least in this model) is simple in one important way. 
The adaptive function of the eye remains the same throughout its evolution- 
ary history. At every succeeding stage, the eye more accurately captures the 
incoming pattern of variation in light. 
But, unlike the eye, current adaptive function may differ from original adap- 
tive function. This is exactly the case for feathers. One function that feathers 
serve in birds today is flight. We can be fairly certain that the precursors of the 
feather could not have served this same function (Prum [1999]). Feathers are 
presumed to have performed various other adaptive roles, such as thermoreg- 
ulation, waterproofing or mate attraction. Adaptive accounts of the evolution 
of feathers assume that there has been a shift of function over time. So the kind 
of relationship we want between the stages in our lineage explanation is not 
necessarily one where the focal trait performs the same function. 
This multifunctional history of feathers, in contrast with the clear singular 
function of the eye, makes the task of lineage reconstruction from an adaptive 
stance much more difficult. It is not the case that we can clearly track and

--- Page 75 ---

Lineage Explanations: How Biological Mechanisms Change 69 
measure one function of feathers over time, as Nilsson and Pelger do with the 
eye. 
Prum’s insight was to see that adaptive concerns are not the only way to 
constrain the directionality of stages in a lineage explanation. He uses facts 
about the hierarchical nature of feather follicle development to constrain the 
order of the stages. Certain features must be in place before others are possible. 
For example, the filaments on the barbs are used to fuse to the rachis during 
helical growth. So it is presumed that production of this filamentous stage (iii) 
was prior to the introduction of helical growth in (iv). 
So although an optimising assumption—drawn from assumptions about 
the process of selection—may be used to order the stages, there are other 
general principles that can be used to order the stages in a lineage explanation. 
Both Simon’s argument for stability of hierarchical construction, and Wimsatt’s 
‘generative entrenchment’ provide principled methods of constraining the order 
of evolutionary change within mechanisms (Simon [1962]; Wimsatt [2001]). 
Although lineage explanations such as Nilsson and Pelger’s make some op- 
timality assumptions based on the way that natural selection works, it is im- 
portant to note that the explanations themselves do not include any explicit 
population-level processes. It is simply assumed that the performance of the 
task must get better with each successive mechanism, and the stages are ordered 
as such. Such a simple optimising assumption about the process of natural se- 
lection may not hold in many cases. For example, if there is some kind of 
frequency-dependent selection occurring, these assumptions would be suspect. 
Nor is it necessary that natural selection be responsible for the optimising 
process. Lineage explanations for change could well be sought for the kind 
of changes induced by artificial selection.'' As Prum’s explanation shows, an 
uphill adaptive drive is not the only way we apply ordering to the stages. In 
both cases, the ordering constraint on the stages is supplied by information 
about the kind of process that is responsible for the evolution of these changes. 
There may be other details which can support the claims about certain stages, 
or the relation between them, such as fossil or phylogenetic resources (Telford 
and Budd [2003]; Serb and Oakley [2005]). For example, Prum’s ordering of 
the stages in feather evolution derives primarily from assumptions about the 
hierarchical nature of development (as discussed above). But if good fossil 
evidence could be produced that showed a particular ordering of forms, this 
could be used to corroborate, falsify or constrain these assumptions. 
Is directionality a necessarily feature of these explanations? No. Sometimes 
a great deal of insight can be gained by simply knowing that there is some plau- 
sible path between certain kinds of mechanisms. Ijspeert et al. use a complex 
‘lA very simple example of this would be the reconstructions of lineages provided in Dawkins’ 
‘Biomorph’ evolution, which is explicitly an agent-driven optimising process (Dawkins [1986]).

--- Page 76 ---

70 Brett Calcott 
salamander-like robotic model to demonstrate the changes required in a neural 
circuit for it to switch from generating the limb movements for swimming to 
those for walking (Ijspeert et al. [2007]). They argue that this tells us something 
about the switch from aquatic to terrestrial locomotion. There is neither an 
adaptive nor a developmental constraint on their model. What is shown is that 
a complex model of the neural pattern generation (based on empirical work) 
can be modified to switch between these two modes of locomotion. Direction- 
ality of some kind can provide extra information to constrain the relationship 
between stages, but the central requirement is still one of continuity. 
6 Operational and Generative Lineages 
Thus far, I have claimed that the explanation for adaptive complexity (the eye), 
and the explanation for the origin of an evolutionary novelty (the feather) have 
a great deal in common. Both of these explanations possess a core structural 
similarity that identifies them as lineage explanations. But there are obviously 
some differences between the explanation for feather evolution and that for 
eye evolution. The previous section explained some of these differences by 
appealing to different ways that the trajectories could be constrained. In this 
section, I look at a deeper difference between the two. The relation between 
the focal phenotypic trait and the mechanism is very different depending on 
whether one is interested in adaptation or development. Understanding this 
difference sheds some light on why developmental details have a special role in 
the explication of evolutionary novelties. 
Consider the difference between these two questions: 
(1) How does that car work? 
(2) How was that car manufactured? 
Both questions could be answered by some sort of mechanistic explanation. An 
explanation of how the car works would include some description of the parts 
of the car, how they are organised, how they operate, and as a result how the 
car moves, steers and stops. An explanation of how the car was manufactured 
would also include a description ofthe parts, operations and their organisation. 
But this description of components would not include (for the most part) the 
components that one finds in a car. Rather, they would be the kinds of parts and 
processes that are found in a manufacturing plant. So, although both questions 
are about cars, and both explanations are about mechanisms, the role that the 
car plays in each explanation is different. In the first case, the components 
of the car are the components of the mechanism and the phenomena that 
these components produce is the behaviour of the car. In the second case, the 
components of the car—its parts, processes and organisation—are produced

--- Page 77 ---

Lineage Explanations: How Biological Mechanisms Change 71 
by the mechanism, and this mechanism is, for the most part, something quite 
different from the car. 
Note that, given either of these mechanisms, we can still make sense of a 
lineage explanation in which they might play a role. We can ask how changes 
in the car components affected the operation over time, and we can ask how 
changes in the components of the manufacturing plant affected the generation 
of cars over time. In both cases, we might say that such a coming up with 
such a series of changes describes ‘evolution’ of cars. But these would be quite 
different-looking explanations. 
The difference picked out by these two explanations is apparent when we 
contrast the eye and feather examples. If we focus on a single stage in the 
two examples of lineage explanations, it becomes apparent that the eye plays 
a different role in the lineage explanation than the feather does. The eye is the 
mechanism, and the phenomena that it produces are due to the operation of 
the eye mechanism. The feather, however, is what is generated by the feather 
follicle—the feather is what the follicle mechanism produces. The focal trait (the 
eye or the feather) plays a different role in each case. It is either the mechanism 
that is operating, or what is generated by the mechanism. 
This different role that the focal phenomenon plays is important when we 
examine how phenotypic novelties are explained. Explaining the origin of nov- 
elties poses a problem, because a requirement of these explanations is a degree 
of similarity between stages—what I’ve called the continuity requirement. The 
problem is addressed by switching the role that the focal trait plays in the lineage 
explanation. Rather than giving a continuous series of mechanisms that show 
how a trait’s operation changed over time, instead we give a continuous series 
of mechanisms showing changes in how that trait was generated over time. 
Why should a shift from the focal trait as something operating to something 
being generated help explain novelties? Recall the case of the car. A description 
of the mechanisms that manufacture a car will, for the most part, consist of very 
different components than those existing in the mechanism that explains the 
operation of the car. So, if our concern was that there was some discontinuity 
in the how the car operated, a lineage of manufacturing mechanisms is not 
constrained to telling this story in terms of parts that appear in the car. The 
upshot is that it is possible to satisfy a continuity constraint by the relations 
between the components of a generative decomposition, even when there is no 
continuity between the components in an operational decomposition. 
We can see how this works by comparing the two examples. In the first 
example, the mechanism in question is the operation of the eye. In order to 
provide continuity between the stages, Nilsson and Pelger decompose the eye 
into certain components. This decomposition results in components that are 
part of the structure and function of an eye: a lens, a retina, etc. This is very 
different from the second example. Here, the mechanism in question is the

--- Page 78 ---

12 Brett Calcott 
generation of the feather—something produced by the operation of the feather 
follicle. So our decomposition is into the components at the level of the feather 
follicle rather than the feather itself. We are still able to explain the changes 
in the feather because the feather follicle generates the feather. But note the 
difference: we can now meet the continuity requirement by describing changes 
in the follicle, rather than the feather itself. The generative decomposition into a 
feather follicle provides a level of description that has a continuity not available 
at the focal level of the feather. 
Shifting to a generative decomposition makes our focal trait the form or 
morphology of some part of the organism. We don’t try and describe conti- 
nuity at this level, for this is the phenomenon that the generative mechanism 
produces. Continuity is instead provided by showing changes in the compo- 
nents making up the mechanisms that produced the form. As I said above, the 
parts we use to understand the manufacturing of something may be entirely 
different from those that the manufacturing process produces. So any large, 
discontinuous changes in the phenomenon produced by the mechanism (nov- 
elties, for example) are not a concern. Developmental details are essential for 
these explanations of novelty, for it is only by using developmental information 
that we can give these generative decompositions. 
7 Explaining Change Without Populations 
In this section, I take a closer look at how lineage explanations relate to other 
kinds of explanations. I argue that they are genuine explanations of evolu- 
tionary change, but that they do not include populations. To see why this is 
so, it is important to distinguish two very different questions we might ask 
about evolutionary change. The first question concerns the processes driving 
change in populations over time. This will include such things as migration, 
mutation and natural selection. The second question is one about individuals 
rather than populations, and is the focus of this paper. This question ignores 
what processes drove the change and instead pays attention to how the compo- 
nents that produced some phenotypic phenomenon (morphology, behaviour, 
etc.) could be gradually modified to reflect some series of changes in that phe- 
nomenon (a fossil record, or perhaps some putative adaptive sequence). Both 
of these questions are about evolutionary change. But they are quite different, 
non-competing explanatory patterns. ~ 
Recognising the population as the unit of explanation was central to under- 
standing natural selection (Lewontin [1983]). Before Darwin, explanations for 
population change—such as that given by Lamarck—were simply a summation 
of changes within individuals. Although it is change at the population level that 
must be invoked when talking of natural selection, population-level change is 
not the only kind of change of interest in biology. The kind of explanations

--- Page 79 ---

Lineage Explanations: How Biological Mechanisms Change 73 
under focus in this paper are those that tell us the changes that could produce 
differences between individuals, not populations. Explaining what underwrites 
the differences between individuals is a common pursuit in biology. 
Consider how we might explain an illness such as cystic fibrosis. First we pick 
some symptomatic phenomenon that differs between individuals that do and 
don’t have cystic fibrosis. We might identify differences at many different levels 
of organisation; they may exist in the function of organs, cells or proteins. We 
then explain these differences by showing how a change in the operation of some 
underlying mechanism produced these differing phenomena. For example, we 
might explain the difference in cellular function between those with cystic 
fibrosis and those without by showing how a change in protein shape affects 
the transport of chloride across the cell membrane. Or we might explain the 
build-up of mucus in the lungs and subsequent bacterial infection by reference 
to this same lower level phenomenon we have just explained—a change in 
the ability to transport chloride. These explanations tell us the changes in a 
mechanism that produce the difference between healthy individuals and those 
with cystic fibrosis. 
This explanation invokes no population processes. It demonstrates how some 
small modifications to a mechanism operating in an individual can produce 
differences in some focal phenomenon. It is an individual-level explanation, 
for what is being described are mechanisms operating within individuals, how 
they function and how they can be changed. The processes of migration, drift 
and natural selection play no role here. We might want to give an explanation 
of why the recessive allele underlying cystic fibrosis continues to remain in a 
population, but this is to engage in a very different kind of explanation. 
This example demonstrates an important (and perhaps obvious) point. Ex- 
plaining how, in general, some phenomenon is produced by a biological mecha- 
nism provides us with capability to explain what underlies individual differences 
in that phenomenon. We began with an explanation of how some cellular mech- 
anism worked, and in virtue of some changes to the mechanism, we explained 
how a different sort of phenomenon (the cystic fibrosis) could be produced. 
This is unsurprising as understanding how something works is exactly the in- 
formation we need to know in order to understand how it can be manipulated 
or changed. 
This connection seems well founded, as it is widely accepted that underly- 
ing these mechanistic explanations is a difference-making, or manipulationist, 
It is important to note that the difference between population-level explanations and lineage 
explanations discussed here is not the same as the contrast drawn between explanations for 
evolutionary change by both Lewontin ([1983]) and Sober ([1984]). In both Lewontin and Sober’s 
case, the explanations for change are competing, as they have the same explananda. Lineage 
explanations, by contrast, seek to explain something different than population change. So they are 
not simply Lewontin’s ‘transformational’ explanations or Sober’s ‘developmental’ explanations. 
(Thanks to an anonymous reviewer for pointing this out.)

--- Page 80 ---

74 Brett Calcott 
account of explanation (Glennan [2002], [2005]; Woodward [2002]; Craver 
[2007]). On this account, to explain some target phenomenon is to show what it 
depends on and the structure of this dependence. This structure consists of a set 
of generalisations showing how the phenomenon changes under various inter- 
ventions or manipulations. This structure of dependence gives us the resources 
for answering what-if-things-had-been-different questions. So the parts, pro- 
cesses and organisation of the mechanism explain how it produces a particular 
phenomenon because changes to those same parts, processes and organisation 
would result in a change in the phenomenon produced. Equally, understanding 
how a particular phenomenon is produced by some mechanism provides us 
with a set of components whose modification can explain why different indi- 
viduals vary in the production of this phenomenon. This is just what we saw in 
the cystic fibrosis case: understanding the mechanism underlying a symptom 
of the disease allowed us to explain what underwrote the differences between 
individuals with the disease and those without it. 
Note what this observation entails for lineage explanations. The components 
of the mechanism play a dual role, because in lineage explanations the use of 
difference-making occurs in both dimensions. 
First take the production dimension. Any explanation of how some mech- 
anism works (of how it produces some phenomenon) is going to contain a 
description of certain parts and processes. The description is explanatory be- 
cause it shows how this phenomenon depends on the parts, processes and 
organisation of its constituent components. These dependencies are reflected 
by how changes in these various components produce differences. This is just 
standard mechanistic explanation, and this relates to the production dimension 
of a lineage explanation—it tells us how that particular mechanism works. 
But difference-making also plays a role in the continuity dimension of these 
explanations as well. For we explain how the mechanism could have changed 
over time by showing how small changes in the parts, processes and organisation 
occurred over time. Rather than identifying differences between individuals 
within a population (as was the case with the cystic fibrosis explanation), lineage 
explanations lay out of a series of such models that are related by descent, 
and rather than identifying the small changes in mechanisms responsible for 
differences between a healthy and a diseased individual, lineage explanations 
show how small changes between ancestral and derived mechanisms could have 
produced different behaviour, physiology and morphology. 
Although both dimensions engage in difference-making, there is an impor- 
tant difference between the two. As I have said, the production dimension 
is simply a mechanistic explanation. It shows how some putative mechanism 
could have functioned, or actually does function. The use of difference-making 
in the continuity dimension is different. For although the difference between 
stages is contrastive—showing how a change in some component produced a

--- Page 81 ---

Lineage Explanations: How Biological Mechanisms Change 75 
difference in phenomena—here we are asserting that the change was actually 
made, rather than it being some in-principle intervention. This change is what 
took the mechanism from one stage to the next. Furthermore, although we 
are assuming that a change like this took place, we explicitly do not say what 
caused these changes. Whatever processes actually induced the change do not 
play a direct part in the explanation. The same lineage explanation for some 
biological change could be given, whether the process driving that change was 
natural selection, artificial selection or genetic engineering.'* So although the 
production dimension is explicitly causal, the continuity dimension to a lineage 
explanation provides no causal connection between the processes that made the 
change between the mechanisms. 
It is important that a lineage explanation of evolutionary change is not 
in conflict with a population-level explanation that invokes natural selection. 
Both explain change over time, but they are asking very different questions 
(rather than answering the same question in different ways). One explains how 
populations change over time. The other explains how complex mechanisms 
can be modified gradually, yet continuously produces some specified outcome, 
be it behavioural, physiological or morphological. The lack of an explicit causal 
connection between adjacent stages along the continuity dimension is exactly 
the ‘causal gap’ that an explanation by natural selection can fill. 
The key assumption that connects these individual-based explanations with 
population processes is that the mechanistic descriptions in each stage are 
representative of some population or set of individuals, as they were in the cystic 
fibrosis example. A lineage explanation assumes that each such mechanistic 
description in the lineage was representative of the population, and enabled the 
next variation in the lineage to arise. In practice, it may be that the prototypes 
used to reconstruct a lineage are, in fact, particulars—such as fossils or a 
model organism. If we wish to generalise our results then, again, we have to 
assume that they are, or were, representative of some population. So a lineage 
explanation reconstructs a chain of individual (descriptions of) mechanisms, 
where these mechanisms are representative of those possessed by organisms 
assumed to have been fixed in the population. 
8 Conclusion 
Adaptationist: Individuals don’t evolve. Populations do. Species are the 
effects of the evolution of populations 
Structuralist: Individuals don’t evolve. Ontogenies do. Characters are the 
effects of the evolution of ontogenies (Amundson [2005]) 
'3 Lineage explanations could likewise be given for non-biological changes, such as the historical 
series of improvements to some engineered invention.

--- Page 82 ---

76 Brett Calcott 
Amundsen’s characterisation of the structuralist and adaptationist positions 
captures the differing explanatory interests one might have about change over 
evolutionary time. Lineage explanations capture this second explanatory in- 
terest. But I have argued that this second explanatory interest also comes in 
an adaptive flavour—lineage explanations where the mechanisms in question 
produce some adaptive behaviour, and the directionality of the stages are con- 
strained by optimising assumptions about natural selection. 
In either case, no population processes are present, but the explanations 
still answer important questions about evolutionary change. Showing how 
complex functioning mechanisms can gradually change really does present a 
puzzle, and is something genuinely in need of explanation. Knowing how a 
complex assemblage of parts does some particular task is one thing. Knowing 
how these parts and their organisation could be modified to improve or change 
the performance of that task is a different challenge. 
There is more than one kind of explanation for change over time, because 
there is more than one kind of question that we can ask. One is about the 
population-level processes that are responsible for that change. The other is 
about the properties of the individuals undergoing change: how can they both 
change gradually and continue to function? It is a mistake to think that all 
relevant questions about evolutionary change can be subsumed under a single 
explanatory pattern. 
Acknowledgements 
Many thanks to Kim Sterelny, Peter Godfrey-Smith, John Matthewson, Patrick 
Forber, Lindell Bromham and Rob Lanfear for advice, comments and sugges- 
tions. Two anonymous referees also helped make this a much better, and more 
comprehensible paper. 
Philosophy Program 
Research School of Social Sciences; 
Centre for Macroevolution and Macroecology 
School of Botany and Zoology 
Australian National University 
Canberra, ACT 0200 
Australia 
References 
Amundson, R. [2001]: ‘Adaptation and Development’, in S. H. Orzack and E. Sober 
(eds), Adaptationism and Optimality, Cambridge: Cambridge University Press. 
Amundson, R. [2005]: The Changing Role of the Embryo in Evolutionary Thought: Roots 
of Evo—Devo, New York, NY: Cambridge University Press.

--- Page 83 ---

Lineage Explanations: How Biological Mechanisms Change 77 
Autumn, K. and Peattie, A. M. [2002]: ‘Mechanisms of Adhesion in Geckos’, /ntegrative 
and Comparative Biology, 42, pp. 1081-90. 
Bechtel, W. and Abrahamsen, A. [2005]: ‘Explanation: A Mechanist Alternative’, Studies 
in History and Philosophy of Science Part C: Studies in History and Philosophy of 
Biological and Biomedical Sciences, 36, pp. 421-41. 
Bechtel, W. and Richardson, R. C. [1993]: Discovering Complexity: Decomposition and 
Localization as Strategies in Scientific Research, Princeton, NJ: Princeton University 
Press. 
Callebaut, W. and Rasskin-Gutman, D. (eds.). (2005). Modularity: Understanding the 
Development and Evolution of Natural Complex Systems, Cambridge, MA: MIT Press. 
Craver, C. F. [2007]: Explaining the Brain, Oxford: Clarendon Press. 
Darden, L. [2002]: ‘Strategies for Discovering Mechanisms: Schema Instantiation, 
Modular Subassembly, Forward/Backward Chaining’, Philosophy of Science, 69, 
pp. $354-S365. 
Darden, L. [2005]: ‘Relations among Fields: Mendelian, Cytological and Molecular 
Mechanisms’, Studies in History and Philosophy of Science Part C: Studies in Histor) 
and Philosophy of Biological and Biomedical Sciences, 36, pp. 349-71. 
Dawkins, R. [1986]: The Blind Watchmaker, Harlow: Longman Scientific & Technical. 
Glennan, S. S. [1996]: ‘Mechanisms and the Nature of Causation’, Erkenntnis, 44, pp. 49 
71 
Glennan, S. S. [2002]: ‘Rethinking Mechanistic Explanation’, Philosophy of Science, 69, 
pp. S342 
Glennan, S. S. [2005]: ‘Modeling Mechanisms’, Studies in History and Philosophy of Sci- 
ence Part C: Studies in History and Philosophy of Biological and Biomedical Sciences, 
36, pp. 443-64. 
Ijspeert, A. J., Crespi, A., Ryczko, D. and Cabelguen, J. M. [2007]: ‘From Swimming to 
Walking with a Salamander Robot Driven by a Spinal Cord Model’, Science, 315, 
pp. 1416-20. 
Jacob, F. [1977]: ‘Evolution and Tinkering’, Science, 196, pp. 1161-6 
Kirschner, M. and Gerhart, J. [1998]: ‘Evolvability’, Proceedings of the National Academ) 
of Sciences, 95, pp. 8420-7 
Kirschner, M. and Gerhart, J. [2005]: The Plausibility of Life: Resolving Darwin's 
Dilemma, New Haven, CT: Yale University Press. 
Lewontin, R. [1983]: ‘Darwin’s Revolution’, New York Review of Books, 30, pp. 21 
12, 
Machamer, P., Darden, L. and Craver, C. [2000]: ‘Thinking About Mechanisms’, Phi- 
losophy of Science, 67, pp. 1-25. 
Miiller, G. B. and Wagner, G. P. [2003]: ‘Innovation’, in B. K. Hall and W. M. Olson 
(eds), Keywords and Concepts in Evolutionary Developmental Biology, Cambridge, 
MA: Harvard University Press. 
Nilsson, D. E. and Pelger, S. [1994]: ‘A Pessimistic Estimate of the Time Required for 
an Eye to Evolve’, Proceedings of the Royal Society of London Series B-Biological 
Sciences, 256, pp. 53-8.

--- Page 84 ---

78 Brett Calcott 
Peel, A. and Akam, M. [2003]: ‘Evolution of Segmentation: Rolling Back the Clock’, 
Current Biology, 13, pp. R708-10. 
Prum, R. O. [1999]: ‘Development and Evolutionary Origin of Feathers’, Journal of 
Experimental Zoology, 285, pp. 291-306. 
Raff, R. A. [1996]: The Shape of Life: Genes, Development, and the Evolution of Animal 
Form, Chicago, IL: University of Chicago Press. 
Rothschild, M., Schlein, J., Parker, K., Neville, C. and Sternberg, S. [1975]: ‘Jumping 
Mechanism of Xenopsylla-Cheopis. 3. Execution of Jump and Activity’, Philosophical 
Transactions of the Royal Society of London Series B, 271, pp. 499-515. 
Schlosser, G. and Wagner, G. P. [2004]: Modularity in Development and Evolution, 
Chicago, IL: University of Chicago Press. 
Serb, J. M. and Oakley, T. H. [2005]: ‘Hierarchical Phylogenetics as a Quantitative Ana- 
lytical Framework for Evolutionary Developmental Biology’, Bioessays, 27, pp. 1158 
66. 
Simon, H. A. [1962]: ‘The Architecture of Complexity’, Proceedings of the American 
Philosophical Society, 106, pp. 467-82. 
Sober, E. [1984]: The Nature of Selection: Evolutionary Theory in Philosophical Focus, 
Cambridge, MA: Mit Press. 
Suzuki, Y. and Nijhout, H. F. [2006]: ‘Evolution of a Polyphenism by Genetic Accom- 
modation’, Science, 311, pp. 650-2. 
Tabery, J. G. [2004]: ‘“Synthesizing Activities and Interactions in the Concept of a Mech- 
anism’, Philosophy of Science, 71, pp. 1-15. 
Telford, M. J. and Budd, G. E. [2003]: ‘The Place of Phylogeny and Cladistics in Evo 
Devo Research’, International Journal of Developmental Biology, 47, pp. 479-90. 
Thompson, D. A. W. [1961]: On Growth and Form Abridged edition, edited by J. T. 
Bonner. Cambridge, UK: Cambridge University Press. 
Truman, J. W. and Riddiford, L. M. [1999]: ‘The Origins of Insect Metamorphosis’, 
Nature, 401, pp. 447-52. 
Wagner, G. P. [2000]: ‘What is the Promise of Developmental Evolution? Part I: Why is 
Developmental Biology Necessary to Explain Evolutionary Innovations?’, Journal of 
Experimental Zoology, 288, pp. 95-8. 
Wehner, R. [2003]: ‘Desert Ant Navigation: How Miniature Brains Solve Complex 
Tasks’, Journal of Comparative Physiology A, 189, pp. 579-88. 
Wimsatt, W. C. [2001]: ‘Generative Entrenchment and the Developmental Systems Ap- 
proach to Evolutionary Processes’, in S. Oyama, P. E. Griffiths and R. D. Gray (eds), 
Cycles of Contingency, Cambridge, MA: MIT Press. pp. 219-38. 
Winther, R. G. [2005]: ‘Evolutionary Developmental Biology Meets Levels of Se- 
lection: Modular Integration or Competition, or Both?’, in W. Callebaut and 
D. Rasskin-Gutman (eds), Modularity: Understanding the Development and Evolu- 
tion of Natural Complex Systems, Cambridge, MA: MIT Press. pp. 61-97. 
Woodward, J. [2002]: “What Is a Mechanism? A Counterfactual Account’, Philosophy of 
Science, 69, pp. S366—S377. 
Yu, M., Wu, P., Widelitz, R. B. and Chuong, C.-M. [2002]: ‘The Morphogenesis of 
Feathers’, Nature, 420, pp. 308-11.

--- Page 85 ---

Brit. J. Phil. Sci. 60 (2009), 79-100 
Focused Correlation and 
Confirmation 
Gregory Wheeler 
ABSTRACT 
This essay presents results about a deviation from independence measure called focused 
correlation. This measure explicates the formal relationship between probabilistic de- 
pendence of an evidence set and the incremental confirmation of a hypothesis, resolves a 
basic question underlying Peter Klein and Ted Warfield’s ‘truth-conduciveness’ problem 
for Bayesian coherentism, and provides a qualified rebuttal to Erik Olsson’s claim that 
there is no informative link between correlation and confirmation. The generality of the 
result is compared to recent programs in Bayesian epistemology that attempt to link cor- 
relation and confirmation by utilizing a conditional evidential independence condition. 
Several properties of focused correlation are also highlighted. 
Introduction 
Correlation Measures 
2.1. Standard covariance and correlation measures 
2.2 The Wayne—Shogenji measure 
2.3. Interpreting correlation measures 
2.4 Correlation and evidential independence 
3 Focused Correlation 
4 Conclusion 
Appendix 
1 Introduction 
Recent results in Bayesian epistemology have reinvigorated debate over the 
coherence theory of justification, particularly the version outlined by Laurence 
Bonjour ([1985]). One issue receiving attention is whether the idea behind 
coherentist justification is tenable, which is understood in this literature as 
whether an increase of coherence among a collection of beliefs is matched by 
an increase in the likelihood of those or related beliefs being true. Here critics 
pounce. 
The Author (2009). Published by Oxford University Press on behalf of British Society for the Philosophy of Science. All rights reserved 
doi:10.1093/bjps/axn048 For Permissions, please email: journals permissions@oxfordjournals org 
Advance Access published on January 16, 2009

--- Page 86 ---

80 Gregory Wheeler 
Peter Klein and Ted Warfield, for instance, have argued on measure-theoretic 
grounds that it is impossible for coherence to increase the likelihood of truth 
({1994], [1996]). They maintain that coherence is not ‘truth conducive’ in the 
way the coherence theory requires because there isn’t the right sort of ‘truth 
connection’ between coherent beliefs and true beliefs. Erik Olsson also doubts 
that there is an informative link between a measure of coherence and in- 
crease in the likelihood of truth (Olsson [2005], p. 135). The preconditions for 
Olsson’s result—constructed in terms of a ‘testimonial system’ in which witness 
reports are associated with a credibility measure and are independent of one 
another—are restrictive. But he thinks they are charitable to coherence theo- 
rists, arguing that ‘it is less plausible or even impossible that there could be an 
interesting measure of coherence that is truth conducive’ under circumstances 
in which these conditions are not met (Olsson [2005], p. 135). 
What Bayesian coherence theorists and critics alike are doing is devising 
various types of deviation from independence (dfi) measures for sets of binary 
variables, or proposing methods for inducing a partial ordering on sets of vari- 
ables, and then evaluating the prospects for such constructions to explain the 
relationship between coherence and incremental confirmation. The reduction of 
coherentist justification to dfi-measures is not direct, however, because coher- 
entist justification is a diachronic notion and all Bayesian coherence measures 
to date, along with the so-called impossibility results, concern a synchronic no- 
tion of coherence.' Although it is widely assumed in this literature that positive 
and negative results about synchronic coherence are relevant to a theory of 
diachronic coherence, the relationship between these two notions of coherence 
is still to be worked out. 
Nevertheless, the relationship between dfi-measures and confirmation is 
worth exploring quite apart from Bayesian coherentism and its critics. The 
relationship between dfi and confirmation is the basic issue behind Klein and 
Warfield’s counterexample, and also behind Olsson’s impossibility result. In- 
sofar as these two arguments suggest that there is no informative relationship 
between dfi-measures and confirmation, the results of this essay are a rebuttal. 
The paper presents a measure called focused correlation that resolves a gen- 
eral formal question behind truth-conduciveness objections. Focused correla- 
tion is the ratio of the degree of correlation among a pair of variables condi- 
tioned on a hypothesis to the degree of correlation of the evidence simpliciter. 
The measure may be used as an indicator function for when combining pieces 
of evidence for a hypothesis improves or degrades the incremental confirma- 
tion that the evidence provides separately for that hypothesis, and the mea- 
sure may be extended to an n-variable conditional dfi-measure without loss of 
' Bonjour repeatedly stresses that coherentist justification is a dynamic concept, not a static one. 
See pp. 144, 153, and 169 of (Bonjour [1985]), for instance.

--- Page 87 ---

Focused Correlation and Confirmation $1 
generality. Thus, this measure specifies the conditions under which combining 
probabilistically dependent evidence increases the incremental confirmation of 
a hypothesis. 
Focused correlation does not give a complete account of coherentist justifica- 
tion, however. In addition to worries about the relationship between synchronic 
coherence and diachronic coherence, there are features of correlation measures 
in general that raise concerns about their unqualified use in Bayesian episte- 
mology. Thus we begin the paper with a discussion of the relationship between 
correlation and coherence in Section 2 and highlight some limitations of the 
Wayne-Shogenji measure advocated by Tomoji Shogenji as a coherence mea- 
sure in (Shogenji [1999]). Then in Section 3 we present focused correlation, 
which is based upon a conditionalized form of the Wayne-Shogenji measure 
that was introduced by Wayne Myrvold ([{1996]). 
2 Correlation Measures 
Correlation is one type of relationship that can hold among a pair of variables, 
and the Wayne-Shogenji similarity measure is a species of correlation mea- 
sure. In this section, four points are discussed. First the relationship between 
the Wayne-Shogenji measure and Pearson’s product moment correlation co- 
efficient is addressed. Then the Wayne—Shogenji measure and a conditional 
form of the measure are extended to n-variable dfi-measures. Next, four exam- 
ples are introduced that demonstrate there is no direct dependence between 
the direction of the Wayne—Shogenji measure and the direction of incremental 
confirmation. Finally, a popular strategy for exploring indirect relationships 
between correlation and confirmation via conditional evidential independence 
assumptions is considered and criticized. These four points are addressed in 
the following four subsections. 
2.1 Standard covariance and correlation measures 
There are a variety of relationships that can hold among variables, but one of 
the most basic is covariance. The covariance of two random variables x and y is 
the average of x minus its mean, multiplied by the average of y minus its mean, 
1 n 
Cov(x, y) = - (4 — X)Qi — Y). 
P n a : : i=] 
In other words, Cov(x, y) is a measure for how variables vary together. 
We may also measure the strength of the /inear relationship between x and y 
as the covariance of x and y divided by the product of standard deviations of

--- Page 88 ---

82 Gregory Wheeler 
the two variables, 
Cov(x, y) 
Var(x)Var(y) 
r(x, y)= 
where r(x, y) is Pearson’s product moment correlation coefficient for x and y, 
which takes values between —1 and 1. 
Suppose now that x and y are binary variables, where x is | if A, 0 if 
=A, and y is 1| if B, 0 if -B. Suppose too that there is a joint probability 
distribution over these variables such that Pr( AN B), Pr(AN —B), Pr(—AN B), 
and Pr(— AN —B) are defined. If the probability model over the data for x and y 
satisfies both the mean assumption and the normal error assumption,’ and takes 
values in the open interval (0, 1), then we may define a correlation measure on 
events, Cor( A, B), in terms of the correlation coefficient r: 
Pr( AN B) — Pr( A) x Pr(B) 
ConA. 2) =f. = : 
J Pr( A) x Pr(— A) x Pr(B) x Pr(-B) 
This textbook result says the following about the probabilistic relationship 
between events over which Pr is defined: if and only if two events A and B are 
independent, i.e., Prt( AM B) = Pr( A) x Pr(B), then Cor( A, B) = 0; if and only 
if there is a positive correlation between A and B, i.e., Pr( A | B) > Pr( A), then 
Cor(A, B) > 0, where Cor( A, A) = 1 is stipulated to be the positive limit; if and 
only if there is a negative correlation between A and B, i.e., Pr( A | B) < Pr( A), 
then Cor(A, B) < 0, where Cor( A, —.A) = —1 is stipulated to be the negative 
limit. 
2.2 The Wayne—-Shogenji measure 
Rather than express degree of probabilistic independence among events A and 
B directly in terms of a linear correlation coefficient, Cor( A, B), we may instead 
express correlation as a weight function on probabilistic independence, 
Pr( A | B) = Pr(A) x S(A, B), (1) 
where A and Bare independent if and only if S.A, B) = 1, Aand Bare positively 
correlated if and only if S(A, B) > 1, and A and B are negatively correlated if 
and only if S(A, B) < 1. The measure S(A, B) is a nonlinear measure of the 
degree of probabilistic dependence between A and B. 
The mean assumption: the conditional mean of y given x is an unknown linear function of x; the 
error assumption: (i) errors are normally distributed with mean 0 and a known variance o7, and 
(ii) errors for each observation are independent.

--- Page 89 ---

Focused Correlation and Confirmation 
Table 1. Correlation measures 
(—) Correlation Independence (+) Correlation 
Pr( A | B) < Pr( A) Pr( A | B) = Pr( A) Pr( A | B) > Pr( A) 
Cor( A, B) < 0 Cor( A, B) = 0 Cor( A, B) > 0 
S(A, B) < 1 S( A, B) = 1 S(A, B) > 1 
Andrew Wayne ([1995]) proposed to interpret S as a ‘similarity measure’ and 
noted that (1) is equivalent to 
Pr(A|B) Pr(B| A) Pr( AN B) 
Pr(4) = Pr(B) ~—s~éPrr( A) x Pr B) SUA, B) = (2) 
This measure was also proposed by Tomoji Shogenji ({1999]) as a measure 
of coherence. Whereas Shogenji keeps the restriction on values of variables 
to the open unit interval, Wayne relaxes this restriction by stipulating that if 
either Pr( A) or Pr( B) is zero, then SCA, B) = 1. On either account the measure 
S( A, B) takes 0 as a negative limit but is without an upper limit: instead, the 
degree of positive similarity among variables is a function of the size and prior 
distribution of the data set. 
Both Wayne Myrvold ([{1996], p. 662) and Shogenji ({[1999]) extend (2) to 
bodies of evidence consisting of three or more variables, 
Pr( 4, N---M Ay) SUA, «..; A, ) 
~ Pr(A)) x «++ x Pr(A,)’ 
and Myrvold also defines a conditionalized form, 
Ay N---M A, | B) DUAL, «2 An |B) = a . (4) 
Pr( A; | B) x --- x Pr( A, | B) 
Despite the nonlinearity of S, the quantity measured in (2) is degree of cor- 
relation among pairs of random variables. Table | displays the relationships 
between S, Cor, and direction of correlation (+, —, or independent) for two 
variables. 
Strictly speaking, (3) and (4) are not correlation measures because correlation 
is a binary relation: it is well known that sets of three or more evidence variables 
may indicate a high degree of dependence whereas pairs of those variables may 
fail to be correlated. Yet (3) and (4) are dfi-measures. I will sometimes speak 
informally of correlation measures to include their three- or more evidence 
variable counterparts. 
2.3 Interpreting correlation measures 
In order to evaluate a correlation measure, it is necessary to know the signifi- 
cance of the correlation and the strength of the dependency between variables. 
Significance is a measure of reliability of the correlation of x and y. Tests for

--- Page 90 ---

84 Gregory Wheeler 
significance are typically based upon the assumption that error, i.e., the 
deviation of data from the ‘true’ linear function, is normally distributed. For 
Pearson’s coefficient r strength is the square of the correlation coefficient, which 
measures the proportion of common variation in x and y. Thus, correlation 
measures are sensitive to outlying data: since regression minimizes the sum of 
the squares of distances of data from a line plotted through that data, a single 
outlying data point can significantly alter the slope of the plotted regression 
line. Furthermore, variables may be nonlinearly correlated. Thus, applying the 
measure r may correctly indicate that there is no linear correlation, but may 
falsely indicate that there is only a weak degree of dependency among the vari- 
ables. It is therefore critical to examine the data underpinning a correlation 
measure to check against error, since bare correlation measures alone tell you 
little about the actual dependency between variables. 
The interpretations of the Wayne-Shogenji measure given by Bayesian epis- 
temologsts fail to address either the significance or the strength of correlated 
beliefs. Instead, they typically assume that a probability distribution is repre- 
sentative of the evidence, or representative of a rational agent’s assessment of 
likelihoods. But this assumption is suspect given the many qualifications that 
accompany the interpretation of correlation measures in statistics. Insofar as 
evidence is taken to be either representative of a state of affairs or an assess- 
ment of likelihood bound by objective criteria rather than simply a measure 
of an agent’s credence, critically assessing strength and significance is unavoid- 
able. Furthermore, once we take account of the objective basis for probability 
assessment, the interpretation of probability becomes a central concern to 
correlation-based theories in formal epistemology. 
For the sake of the argument here, assume there is a means to evaluate signif- 
icance and strength of correlated beliefs. Then an open problem for Bayesian 
accounts of confirmation, testimony, and coherence is to specify what impact, 
if any at all, combining coherent evidence has upon confirmation. This is the 
problem that Klein and Warfeld’s counterexample and Olsson’s results address, 
which has (misleadingly) been referred to as the ‘truth-connection’ problem. 
Concerning the measure S, intuitions are divided. Wayne proposed, but 
did not endorse, interpreting S to represent the diversity of evidence thesis 
(Howson and Urbach [1989], p. 114). This thesis holds that the /ess similar 
pieces of evidence for some hypothesis are to one another then the stronger the 
support this combined set of evidence would give to that hypothesis. On this 
view, S( A, B) = | represents that evidence A and B are maximally diverse, and 
maximally diverse evidence offers more support for a hypothesis than ‘narrow’ 
evidence, i.e., when A and B are either positively or negatively correlated. 
Shogenji proposed S as an account of epistemic coherence, which holds that 
the more similar evidence is to each other, the greater the support it offers 
to a hypothesis. On Shogenji’s interpretation S( A, B) = 1 represents neutral

--- Page 91 ---

Focused Correlation and Confirmation 85 
evidence, negatively correlated evidence represents incoherent evidence, and 
positively correlated evidence represents coherent evidence. 
As these clashing intuitions about S might suggest, correlation and confir- 
mation may vary independently. Contra Klein and Warfield and the diversity 
of evidence thesis, combining correlated confirming evidence may increase con- 
firmation of a conditioning hypothesis. Contra Shogenji, ‘incoherent’ evidence 
may increase confirmation of a conditioning hypothesis. Furthermore, both 
positively correlated evidence and negatively correlated evidence may each de- 
crease incremental confirmation of a conditioning hypothesis. Each possibility 
is demonstrated in four examples. 
But first, some terminology will be helpful for discussing these examples. 
Following L. Jonathan Cohen ([1977]), we say that evidence A and B converge 
upon hif Pr(h | AN B) > Pr(h | A). Furthermore, we say that the ratio of Pr(h | 
AN B) to Pr(h | A) is a measure of incremental convergence from A to { A, B} 
on A, and that this incremental convergence is positive if and only if Pr(h | 
AN B)/Pr(h | A) is greater than 1, negative iff less than 1, and neutral iff 1. 
Finally, billiard balls numbered | through 8 are classified as ‘solids’, 9 through 
15 as ‘stripes’, and the cue ball is neither numbered, striped, nor solid. See the 
Appendix for calculations. 
Example 1. An urn contains the balls numbered 1, 2, 3, 14, and 15. Consider 
the hypothesis 4; and two pieces of evidence, A; and A: 
h,: The drawn ball is the 2 ball. 
A: The drawn bail is solid. 
A>: The drawn ball is even. 
Note the following values: Pr(/,) = 0.2, Pr( A,) = 0.6, Pr( Az) = 0.4, Pr(A, | 
A\) = +, Pr(h; | 42) = 0.5, and S(A;, Ar) ¥ 0.833. So, the evidence set 
{ Aj, Ay} is negatively correlated. However, the incremental convergence from 
A, to { A), A} is positive, so it is not the case that the evidence set { A), A2} 
offers less confirmation than each piece of evidence alone. Note that 
Pr(h, | A;) L 
—_———. © 1.667 
Pr(h)) 
3 
Allexamples are constructed from the feature categories parity, color, and number. Some examples 
form mutually exclusive classes by treating values of each of these categories as features, e.g., 
the classes ‘odd’ and ‘even’. Although redefining values of a feature as distinct features is not a 
sound practice in general, this classification scheme does not affect the point under discussion 
but is adopted both to simplify the examples and to illustrate the power of focused correlation 
to correctly classify the relationship between correlation and confirmation even when there are 
logical relationships that hold among the categories.

--- Page 92 ---

Gregory Wheeler 
Pr(h, | A; A A>) oi 
Pr(h, | A)) oo 
Pr(hy | Ay ON Ap) 
Pr(h, | A>) 
5 
Hence, a negatively correlated evidence set { A,, A2} can increase the confirma- 
tion of A. Oo 
Example 2. An urn contains the billiard balls numbered 1, 2, 14, 15, and the 
cue ball. Consider the hypothesis 42 and two pieces of evidence, A; and Aq: 
hz: The drawn ball is striped. 
A3: The drawn ball is odd. 
Aq: The drawn ball is either an even solid or an odd striped. 
Note the following values: Pr(h2) = 0.4, Pr(.A3) = 0.4, Pr( Aq) = 0.4, Pr(hp | 
A3) = 0.5, Pr(h2 | A>) = 0.5, and S(A3, Ay) = 1.25. So, the evidence set 
{A3, Aq} is positively correlated. And on this example the incremental con- 
vergence from both A3 and A, to { A3, Aq} is positive; the evidence set { A3, Aq} 
confirms hz more than A; and Ag individually, i.e., 
Pr(h> | A3) ee Pr(h> | Aq) 
Pro) = —~™~*«~é(Ad) 
Pr(h> | AM Ag) Pr(h> | A3M Ag) 
2= 
Pr(h> | A3) Pr(h> | Ag) 
Hence, a positively correlated evidence set { A3, Ay} can increase the confirma- 
tion of hp. Oo 
Example 3. An urn contains the billiard balls numbered 1, 2, 14, 15, and the 
cue ball. Consider the hypothesis 43 and two pieces of evidence, As and Ag: 
h3: The drawn ball is solid. 
As: The drawn ball is odd. 
Ag: The drawn ball is even. 
Note the following values: Pr(h3) = 0.4, Pr( As) = 0.4, Pr( As) = 0.4, Pr(h3 | 
As) = 0.5, Pr(h3 | Ag) = 0.5, and S(.As, Ag) = 0. So, the evidence set { As, Ag} 
is negatively correlated. And on this example the incremental convergence from 
both As and Ag to { As, Ag} is negative; the evidence set { As, As} together does

--- Page 93 ---

Focused Correlation and Confirmation 
not confirm /3 more than As and A, individually, i.e., 
Pr(h3 | As) we Pr(h3 | Ag) 
———_—<—_—_——_ = l i= 
Pr(h3) oP r(h3) 
Pr(h3 | As N Ag) _0 Pr(h3 | As Ag) 
Pr(h3 | As) Pr(h3 | Ae) , 
Hence, a negatively correlated evidence set { As, Ag} can fail to increase the 
confirmation of /3. 
Example 4. An urn contains the billiard balls numbered 2, 4, 13, 15, and the 
cue ball. Consider the hypothesis 44 and two pieces of evidence, A7 and Ag: 
hs: The drawn ball is even. 
A;: The drawn ball is the 2 ball. 
Ag: The drawn ball is solid. 
Note the following values: Pr(/j4) = 0.4, Pr(A7) = 0.2, Pr( Ag) = 0.4, Pr(/g | 
Az) = 1, Pr(hg | Ag) = 1, and S(A47, Ag) = 2.5. So, the evidence set { A7, Ag} is 
positively correlated, therefore coherent. But on this example the incremental 
convergence from both A; and Ag to {A7, Ag} is neutral; the evidence set 
{A7, Ag} together does not confirm 44 more than A; and Ag individually, i.e., 
Pr(/g | Az) Pr(hg | Ag) einen. te DS ie 
Pr(/4) es Pr(/4) 
Pr(hg | A7M Ag) ' Pr(hg | A7M Ag) 
Prihg| 47)  ~—sO#Prr(ag| Ag) 
Hence, a positively correlated evidence set {A7, Ag} can fail to increase the 
confirmation of /4. 
Therefore, the interpretation of S( A, B) as either a measure of coherence of 
{ A, B}, or a measure of this set’s diversity is mistaken: S(.A, B) is a measure of 
correlation of A and B, and the confirmation boost of a hypothesis conditioned 
on the evidence set { A, B} is indeterminate given only the information that A 
and B are correlated. 
2.4 Correlation and evidential independence 
Since correlation and confirmation are not directly related to one another, 
it is natural to consider whether there are indirect relationships between the 
two. One way to do this is to add conditions to a correlation measure in 
order to find dependencies between (positively) correlated evidence and an 
increase in confirmation of an hypothesis. A popular condition among formal

--- Page 94 ---

88 Gregory Wheeler 
coherence theorists is a conditional independence assumption called evidential 
independence (Earman [2000]; Bovens and Hartmann [2003]; Olsson [2005]; 
Shogenji [2007]), which may be illustrated with two evidence variables, A and 
B, and a hypothesis variable, h. 
Evidential independence (EI). A is evidentially independent of B with respect to 
h if and only if Pr(A | BN h) = Pr(A| A), and Pr(A | BNA) = Pr(A| A). 
Proposition | collects some facts about EI. 
Proposition 1. [f/f Pr(A| BONh)=Pr(A|h) and Pr(A| BNh) = Pr(A| h), 
then 
(i) Pr(AN B|h) = Pr(A|h) x Pr(B | A), 
(ii) Pr(AN B|h) = Pr(A|h) x Pr(B| A), and 
Pr(A | A) Pr(B | h) Pr(h) 
Pr(A | A) Pr(B | A) Pr(h) + Pr(A | h) Pr(B | A) Pr(h) 
EI says that pieces of evidence about / do not influence one another’s proba- 
(iii) Pr(h | AN B)= 
bility except through their effect on h. Coherence theorists have appealed to EI 
in various guises to make plain Bonjour’s claims about cognitively spontaneous 
beliefs, which are noninferential beliefs that nevertheless bear some relationship 
to one another. On Bonjour’s account, if the contents of several independent, 
cognitively spontaneous beliefs ‘agree’ on a proposition p, then that collec- 
tion of beliefs generates evidence for p (Bonjour [1985], p. 148). The challenge 
for probabilistic theories of coherence then is to explain what ‘independence’ 
means in this context, and how a set of independent, cognitively spontaneous 
beliefs can positively influence the likelihood that those beliefs are true. 
Bovens and Hartmann ((2003]) address the challenge by adopting a witness 
model whereby a witness report is distinguished from the content of that report. 
On this account coherence occurs among the contents of reports, and EI is 
a condition governing sets of report variables about the conjunction of the 
contents of those reports. Olsson ({[2005]) also proposes a witness model that 
distinguishes between reports and the contents of reports. According to Olsson, 
a Bonjour belief system is a set of pairs, T = {( Rep(p), p)1 (Rep(p’), p’)n}, 
where a belief system T is coherent just in case the size n muttiset O consist- 
ing of the propositional contents of T is coherent.* Evidential independence 
on Olsson’s account is a condition governing witness reports rather than the 
propositional contents of those reports, since Olsson is interested in the case 
where n independent witnesses give identical reports. 
Shogenji ([2007]) presents a simplified witness model that removes the distinc- 
tion between evidence reports and the contents of those reports, and removes 
* Olsson states that O is a tuple, rather than a multiset, but order is not important for his proposal. 
Multiplicity is.

--- Page 95 ---

Focused Correlation and Confirmation 89 
the logical restriction that identifies the confirmable hypothesis with the con- 
junction of the evidence contents. EI on his account holds between the evidence 
contents A),..., An, given a logically unrelated hypothesis, /. 
Shogenji’s generalized ElI-witness model nevertheless preserves the basic dis- 
tinction between reports and the contents of those reports, which is the key 
idea underpinning the witness-report strategy. By incorporating evidential in- 
dependence directly into a model of evidence, Shogenji shows that 
... under the condition of evidential independence, the degree of coherence 
is simply a function of the individual strengths of the pieces of evidence. 
Thus, although there is a sense in which coherence is truth-conducive... 
the lateral relation, such as coherence, has no independent role to play in 
the confirmation of the hypothesis (Shogenji [2007], p. 371). 
So, given that the point behind Bonjour’s cognitively spontaneous beliefs was 
to generate coherentist justification without depending upon the individual 
strengths of each belief, Shogenji’s result appears to be bad news for the coher- 
ence theory of justification. 
Perhaps. But Bonjour’s outline of the coherence theory is a rough one, and 
nowhere does he explicitly endorse EI or the underlying constraints of EI- 
witness models. Furthermore, it is misleading to draw a general conclusion 
about the relationship between deviation from independence and confirmation 
on the basis of Shogenji’s results. Shogenji writes that under EI ‘the conditional 
probability of the hypothesis, Pr(h | A; N--- A,), is a strictly increasing func- 
tion of Pr(h | A;) for each i = 1 n’ (Shogenji [2007], p. 366, my notation), 
which we can see from Proposition |. But this is not true in general, since the 
second position of the conditional probability function Pr(h | A; N---M A,) is 
not a strictly monotone function on the size of n in Pr(h | A;), for | <i <n. 
The conditional probability function Pr(- | -) itself is monotone, just as all prob- 
ability functions are monotone: if AC B then Pr(A| -) < Pr(B| -); If A isa 
smaller set of possibilities than B, then the probability of A must be less than 
(or equal to) the probability of B. But Pr(- | A;) in general is not a strictly in- 
creasing function of the size of the set of possibilities { A;} for 1 < i <n, since if 
A B then Pr(- | A) may be greater than, less than, or equal to Pr(- | AN B). 
Conditioning on a smaller set of possibilities may either increase or decrease 
the probability of the conditioning event. 
It is the restriction to strictly increasing conditional measures that is driving 
Shogenji’s result, not evidential independence per se. So, we might think that a 
clearer picture of Shogenji’s observation would be given by replacing EI by a 
weaker monotonicity condition. 
Monotone evidence (ME). A and B are monotone evidence for / if and only 
if Pr(A| BNh) > Pr(A|h)and Pr(A| BOA) < Pr(A|h).

--- Page 96 ---

90 Gregory Wheeler 
Adopting ME rather than EI would seem to increase the scope of Shogenji’s 
result. 
Even so, Bonjour nowhere endorses anything like either ME or EI. Even if 
we assume that there is some static notion of coherence behind Bonjour’s claims 
about cognitively spontaneous beliefs, there are a variety of ways in which those 
beliefs may be understood to be independent of one another. One candidate 
is the error assumption mentioned in Section 2.1. Rather than start with a 
heavy-handed witness-report representation of cognitively spontaneous beliefs 
that gives you independence and dubious structural constraints on the class 
of models to evaluate, perhaps a better approach would be to investigate the 
relationship between probabilistic dependence and confirmation where much 
weaker constraints are maintained, like the observational independence as- 
sumption. Assuming observational independence leaves open how to describe 
the mechanism by which the independence of each observation is generated. 
One is then left free to consider whether there is an informative relationship 
between correlation and confirmation after all. 
3 Focused Correlation 
There is no direct relationship between correlation and confirmation but there 
is an indirect one: the degree of confirmation of by both A and B combined is 
determined by the product of the degree of confirmation of / by A, the degree 
of confirmation of h by B, and the ratio of the correlation of the evidence 
set {A, B} conditioned on / to the correlation of A and B. This point is also 
made by Myrvold ([1996], p. 663). The relationship between these three factors, 
expressed by 
Pr(h| ANB) Pr(h| A) Pr(h|B) SA, Bh) = x x 
Pr(h) Pr(h) Pr(h) S(A, B) 
is proved in the Appendix. 
Equation (5) suggests a new correlation measure, which is relativized to a 
particular hypothesis of interest, which we shall call focused correlation: 
S(A,Blh)  Pr(h| AN B) Pr(h) Pr(h) = x x - 
S( A, B) Pr(h) Pr(h | A)” Pr(h | B) For,(A, B) := 
The focused correlation of A and B relative to a hypothesis h, For;,(A, B), tells 
us what impact there is on the confirmation of A, if any at all, from combining 
A and B. 
When we restrict ourselves to cases in which each piece of evidence 
positively confirms /, i.e., when Pr(h | A) > Pr(A) and Pr(h | B) > Pr(h), then 
values of For,(A, B) greater than | tell us that the evidence set { A, B} offers

--- Page 97 ---

Focused Correlation and Confirmation 
Pr(h|-) > Pr(h) 
For, (A,B) > 1 
‘when For; 
Figure 1. Interpreting positive and negative values for For; on incrementally con- 
firming and disconfirming evidence. 
more confirmation to / than A or B alone. Likewise, when Pr(/ | A) < Pr(/) 
and Pr(h | B) < Pr(h), then values of For;(A, B) less than | tell us that the 
evidence set { A, B} offers even less confirmation to / than A and B alone. 
When Pr(/ | A) > Pr(h) and Pr(h | B) > Pr(h), then values for For;(A, B) 
less than | tell us that the evidence set { A, B} does not offer more confir- 
mation to / than A and B alone. Even so, the evidence set { A, B} may offer 
confirming evidence for h. The point is that combining A and B won’t gain 
an advantage. Similarly, when Pr(/ | A) < Pr(h) and Pr(A | B) < Pr(h), then 
values for For;,(A, B) greater than | tell us that the evidence set { A, B} does 
not offer less confirmation to A than A and B alone. The evidence set { A, B} 
may still offer disconfirming evidence for h, but the combination will not be 
a greater disadvantage so long as For;( A, B) # 0. What these cases illustrate 
is that combining confirming (disconfirming) evidence, whose focused correla- 
tion is negative (positive), will not improve (worsen) the confirmation of 4. The 
interpretation of these four cases is encapsulated in Figure 1. 
The lower limit of For;(A, B) is 0, which means that the evidence set 
{ A, B} offers no information about h. We stipulate that if For;( A, B) = 3, then 
For;(A, B) = 0. When For,(A, B) = 1, there is no difference in information 
about / between conditioning on the information set { 4, B} and conditioning 
on A and B separately. Notice that there are cases in which For,(A, B) = 2 but 
where there is a difference in information about / between conditioning on the 
evidence set versus conditioning on focal pieces of evidence. Example 3 is an 
illustration. 
When Pr(h | A) = Pr(h) and Pr(A | B) = Pr(h), then For;,(A, B) may be posi- 
tive, negative, or neutral. This is to say that when both A is independent of A, and 
B is independent of h, then combining A and B may increase the confirmation 
of h, decrease the confirmation of h, or have no impact on h.

--- Page 98 ---

92 Gregory Wheeler 
Finally, we may generalize (6) to a focused dfi-measure for finite evidence 
sets of 1 < n distinct variables: 
a A, | h) 
"ey singe 
Application of focused correlation to the four examples illustrates how this 
For;,( A, (7) 
measure may be interpreted as an indicator function to identify when combining 
confirming pieces of evidence offers more (or less) confirmation to a hypothesis. 
See the Appendix for details. 
Observation 1. Example | presents a case where the evidence set { A), A2} 
is negatively correlated, but the degree of confirmation of 4; on { A), A>} is 
greater than the degree of confirmation of /; on the event 4), and greater than 
the degree of confirmation on the event A2. Note, however, that A) provides 
more information about /; than A; does. A consequence of this asymmetry is 
that learning A> either before or after learning A; provides more information 
about /, than learning A, either before or after learning A. Nevertheless, the 
evidence set { A;, A>} offers positive confirmation for h;, more so than either 
A, or A2 alone. The For, (A), Az) is (approximately) 1.20. 
Observation 2. Example 2 presents a case where the evidence set { A3, Ag} is 
positively correlated, and the degree of confirmation of h2 on { A3, Ag} is greater 
than the degree of confirmation of h2 on each event, A; and Ag, individually. 
The For;,(A3, Ag) is 1.6. 
Observation 3. Example 3 presents a case where the evidence set { As, Ag} 
is negatively correlated, and the degree of confirmation of /3 is less than the 
degree of confirmation of 3 on each event, As aiid Ag, individually. No new 
information is learned from As after observing Ag, nor by Ag after observing 
As. In fact, no new information can be learned since As and Ag are mutually 
exclusive events. The For;,.(.As, Ag) is 0. 
Observation 4. Example 4 presents a case where the evidence set { A7, Ag} is 
positively correlated, but the degree of confirmation of hg is /ess than the degree 
of confirmation of 4 on each event, A7 and Ag, individually. The For;,(A7, As) 
is 0.4. 
Although not intended to be a full explication of coherentist justification, 
focused correlation nevertheless has four attractive features worth noting. 
First, focused correlation specifies a clear relationship between correlation 
and confirmation, one that does not involve strong conditional independence 
assumptions. Second, the measure does not place restrictions on the logical 
relationships between the evidence and the hypothesis. Third, focused correla- 
tion incorporates the basic structural features necessary to link confirmation

--- Page 99 ---

Focused Correlation and Confirmation 93 
to correlation/divergence from independence. Finally, although the measure 
is commutative, 1.e., For;,(.A, B) = For;(B, A), focused correlation nevertheless 
can reveal important asymmetries in the incremental confirmation of / on 
pieces of evidence. There is often a difference between learning A before learn- 
ing B and learning B before learning A, and focused correlation can be used to 
exploit such asymmetries in information content. Let’s now examine each of 
these points in some detail. 
Regarding the role of independence assumptions, note that none of the first 
four examples satisfies EI, yet the measure indicates when combining evidence 
increases the confirmation of the hypothesis. We of course may add constraints 
such as EI without affecting the behavior of focused correlation, since this is 
but a restricted case of Equation (5). The next example gives an illustration. 
Example 5. An urn contains the billiard balls numbered 1, 2, 14, 15, and the 
cue ball. Consider the hypothesis 4s and two pieces of evidence, Ag and Ajo: 
hs: The drawn ball is an odd stripe. 
Ay: The drawn ball is not solid. 
Ajo: The drawn ball is not even. 
Note the following values: Pr(hs) = 0.2, Pr( Ay) = Pr( Ajo) = 0.6, Pr(hs | Ag) = 
Pr(hs | Ajo) = a and S( Ao, Ajo) © 1.11. So, the evidence set { Ao, Ajo} 1s pos- 
itively correlated. Note also that the condition of evidential independence is 
satisfied: 
Pri Ag y Aio | hs) = Pr( Ag hs) x Pr( Ajo | hs) = l, 
Pr( Ay M Ajo | is) = Pr( Ao | hs) x Pr( Ajo | As) = 0.25 
However, the set { Ao, Ajo} neither confirms 4; more than Ag, nor more than 
Ajo, 1.€., 
Pr(hs | Ao) iM Pr(hs | Ajo) 
~ 1.665 = 
Pr(hs) Pr(hs) 
Pr(hs | Ag NM Ajo) a _ Pr(hs | Ag N # —_ ee ~ 
Pr(hs | Ajo) Pr(hs | Ajo) 
Hence, a positively correlated evidence set { A9, Ajo} offers less confirmation 
for hs than each piece of evidence alone. 
This result is captured by the measure For;,( A9, Ajo). Since S(A9M Ajo | 
hs) = 1, therefore 
ST Ay NM Ajo | hs) 
For;,.( Aj, Alo) = — =~ 0.901. i ( Ag 10 S( Ao A Ao)

--- Page 100 ---

94 Gregory Wheeler 
The second attractive feature of focused correlation is that it does not place 
restrictions on the logical relationships between the hypothesis and the evidence 
set {A, B}. On Olsson’s model and Bovens and Hartmann’s model, the event 
h is assumed to be equivalent to the joint event AM B. Like evidential inde- 
pendence, this assumption is a strong structural constraint that is motivated by 
representing cognitively spontaneous beliefs in terms of an El-witness model. 
Focused correlation does not place this constraint on the logical form of the 
hypothesis. 
The third benefit is that the structure of focused correlation builds in the 
fact that the relationship between correlation and confirmation is expressed 
by a specific relationship between a particular correlated evidence set and 
a particular hypothesis. It makes no more sense to talk about the generic 
impact that a correlated evidence set has on confirmation without specifying a 
hypothesis than it does to talk of between as a 2- rather than a 3-place relation. 
The structure of focused correlation makes explicit the basic parameters that 
are necessary to express the relationship between a correlated evidence set and 
the confirmation of a specific hypothesis of interest: values for conditional and 
unconditional forms of the measure S(-,-), a prior distribution for h, and the 
contribution of confirmational strength that the combination of evidence has 
over each piece of focal evidence. Note the special case when h is replaced by 
the evidence set { A, B}, 
Pr( A) x Pr( B) 
Pr(A | AN B) x Pr(B| AN B) x Pr(AN B)’ For 4ng(A, B) = (8) 
which can be generalized to evidence sets of size n: 
[]j=1 Pr( Ai) 
Ties Pr; | Ar ---Ay) x Pr( A Ay)” (9) 
For 4,.7..n4(Ai, «-+s 4n) = 
While the first two features of focused correlation highlight the generality 
of the measure, the third feature highlights an important restriction, i.e., in 
order to assess the impact of correlated evidence on the confirmation of a 
hypothesis, it is necessary to select a particular hypothesis. These constraints 
are captured by the structure of the measure, and Equations (8) and (9) simply 
apply the result to instances where one is interested in the evidence set itself. This 
last detail leads to our final point, which concerns the tracking of information 
gain as an agent learns new evidence. For;(A, B) is commutative because it 
represents a static evaluation of the factors contributing to confirmation that are 
embedded in the particular distribution underlying For;,(A, B). Nevertheless, 
we might be interested in different paths through the evidence that an inquiry 
may follow. And here the order in which evidence is learned may be very 
important.

--- Page 101 ---

Focused Correlation and Confirmation 95 
To illustrate consider again Example 1. This example highlights an asym- 
metry in the information that each focal piece of evidence reveals about /. In 
addition to knowing the overall impact that the evidence set { A;, A} has on 
h, we might also be interested in exploiting our knowledge that observing A> 
is more informative than observing A,. The advantage of this ability to rank 
information impact becomes more apparent for larger evidence sets, for in such 
cases we may in effect rank evidence by impact on a hypothesis, from greatest 
impact to least. Furthermore, we may exploit ordered evidence as a method 
for efficiently moving from a coarse to precise value for h. The measure For, 
alone does not yield this information, but calculating values necessary to apply 
focused correlation to a problem does yield this information. 
4 Conclusion 
The measure of focused correlation explicates the relationship between 
correlation (deviation from independence) and incremental confirmation. 
There are several insights this result offers, but the main point is that it re- 
solves a formal question underlying ‘truth-conduciveness’ arguments found 
within Bayesian coherentism. Insofar as the question raised by probabilistic 
theories of coherence is whether there is an informative relationship between 
deviation from independence measures and incremental confirmation, the 
answer is Yes. How independence of observations is construed is crucial for 
EI-witness model results, but not for the results obtained here. The key to 
the results here is to assume that observational independence holds without 
specifying a particular mechanism that generates those independencies. So, 
insofar as Bayesian coherence theorists are interested in the relationship 
between correlation and confirmation irrespective of particular mechanisms 
that guarantee that observations are independent, focused correlation provides 
an account. 
Focused correlation is a static dfi-measure, and there are several precautions 
and caveats that attend drawing inferences from correlation measures on sta- 
tistical data that are generally ignored by Bayesian epistemologists. Even so, 
there is an interesting relationship between correlation and confirmation that, 
with due care, may be exploited for informative inference. 
Appendix 
Proof (Equation (5)). Show: 
Pr(h | AN B) Pr(h | A) Pr(h|B) SA, Blh) — ee x : 
Pr(h) Pr(h) Pr(h) SCA, B)

--- Page 102 ---

96 Gregory Wheeler 
By the definition of S: 
Pr( AN Bih) 
SCA, B | h) Pr( Ajh) x Pri Bh) 
S(A, By) PHANB) Pr( A)x Pr( B) 
Pr(AN B|h) Pr(A) __—— Pr(B) 
~ “PAN B) ~ Pr(Alh) ~ Pr(BI A) 
Pr(h | AN B) Pr(h) Pr(h) x - - Xx 
Pr(h) Pr(h | A) Pr(h |B) 
Example 1: 
S(A), 4) = Pr(.A; M A>) 2 0.2 
Pr(A,)Pr(A2) = 0.24 
Pr( A, M Az | hy) x Pr(hy) 
Pr(A, 9 A> | hy) x Pr(hy) + Pr( A, A Ap | Ay) x Pr(hy) 
1x 0.2 
~ (1 x 0.2) +0 x 0.8) 
~ 0.833. 
Pr(h | A M A>) = 
Pr(hy | Ay MN A>) a 
Pr(h, | A\) i 
l 
1 3 
Pr(h; | Ay A Ap) - l 
Pr(h; | 42) ~—*02.5 
=— 49 
Pr(h; | A ‘ eS ion ake EET, Pry) 0.2 
Pr(h; | 42) 0.5 
Prih,;) _—sO.z. =. 
Example 2: 
; Pr(.A3M Ag) 0.2 Bs 
ih, A) = aya) O16 
Pr(A39 Ag | ho) x Pr(ho) 
Pr(.A39 Ag | Az) x Pr(hz) + Pr(.A3 9 Ag | hz) x Pr(h2) 
0.5 x 0.4 
= 05x04)+0x06)' 
Pr(h2 | A3M Ay) = 
Pr(h> | A3M Ag) a l . Pr(h> | A3M Ag) 
Pr(h2 | A3) ~ 0.5 wis Pr(h> | Ag)

--- Page 103 ---

Focused Correlation and Confirmation 
Pr(h> | A3) 0.5 
- = 1.25, : Pr(h>) | ~ 0.4 
Pr(h> | Ag) 
Prih>) 
Example 3: 
Pr(.As M Ae) Q 0 
Pr(As)Pr( 4s) 0.16 
Pr(h3 | As N Ag) Pr(h3 | As M Ag) 
ty Becton = - Ou ” Pr(h3 Ag) Pr(ih3| 4s) 0.5 | 
Pr(/3) 
Pr(h3 | Ae) 
Pr(h3) 7 
Example 4: 
Pr(A7M Ag) 0.2 S( 47, As) = — see tan 
, Pr( A7) Pr( Ag) 0.08 
= 2.5 
Pr( A719 Ag | Ag) x Pr(hg) 
Pr(hg | 47M Ag) = - enema anette aaa 1 Ag | ha) x Pr(h4) + Pr( 47 6 
0.5 x 0.4 ei ed wel (0.5 x 0.4) + (0 x 0.6) 
As ha) x Pr(h4) 
Pr(hg | A7M Ag) l 7 Pr(h4g | Az Ag) 
Prihg| 47) =1  ~=Prthg| As) 
Pr(h4 Az) 
Pr(h4) 
Pr(hg | Ag) = 
Pris)

--- Page 104 ---

98 Gregory Wheeler 
Example 5: 
Pr( Ay M Ajo) 0.6 x ; 
(4s, Ato) = Bay) x Pr Ai) ~ 0.6 x 0.6 
Pr( Ay A Ajo | hs) 1 S( Ay, Aio | hs) = = (49, Ato | fs) Pr( Ay | hs) x Pr(Aio |s) 1 
~ 1.111. 
Evidential independence: 
Pr( Ay M Ajo | As) = Pr( Ag | hs) x Pr Ajo | As) = 1 
Pr( Ag M Ajo | hs) = Pr(4 | hs) x Pr( Ajo | hs) = 
Shogenji’s: 
Pr(hs | Ag) — Pr(As))(Pr(hs | Ajo) — Pr(hs)) ik slat hen (Pr(hs ) r(As)(Pr(hs | Ajo) r(A: 
Pr(hs)(1 — Pr(hs)) 
1_9.2)(4 -0.2 = (3 ~ 0.2)(5 ~ 0.2) ~ 0.1111. 
(0.2)(0.8) 
From Example |, S(.4), 42) © 0.833, and Proof (Observation 1). 
Pr( Aj M Abd | hy) a l 
S(A}, 42 | hi) = me: - (Ai, A2 | 11) Pr( A; | A,) x Pr( Az | 1) Ixl 
SA, A> | F I MA, Athy) 1 4g, For), (A, A2) = - 7 
S( A), A>) 0.833 
From Example 2, S(.A3, Ay) = 1.25, and 
Pr(.A3 9 Ag | A) 0.5 S A 3 4 h> —- - = —_—- = = 
(4s, Ay | 2) = BA | hin) x Pr(Ag |g) 05X05 
Proof ( Observation 2). 
3 S(A3, Ag | A2) 
For), (A3, A4) — a er oe = 
From Example 3, S( As, 4s) = 0, and 
Pr( As Ag | h3) 0 S( As, Ag | 43) = = = 
(As, As | 3 Pr( As | 3) x Pr(Ag | h3) 0.5 x 0.5 
Proof ( Observation 3 ). 
S(As, Ag | 43) 0 ee 
Forn,( As, Ag) = aoa “_e 0, by definition. 
From Example 4, S(.A7, Ag) = 2.5, and 
Pri Ay M Ag | hy) 0.5 
S(A7, Ag | ha) = = = 
\Ar, Ae | M4) Pr(A7 | hg) x Pr(Ag| fag) 0.5 x 1 
Proof ( Observation 4).

--- Page 105 ---

Focused Correlation and Confirmation 
S(A7, Ag | ha) | 
For),,(A7, Ag) = a eee an soe oni Gl S(.A7, Ag) aa 
Show that 
= Pr(A|h) & Pr( AN Bl A) = Pr( Ath) x Pr( BA), 
= Pr(A|h) & Pr(AN Bh) = Pr( AJh) x Pr( BIA), and 
Proof (Proposition 1). 
(i) Pr(A| BNA) 
(ii) Pr(A| BNA) 
(iii) Pr(A| BN h) = Pr(A|h) & 
Pr( A|h) Pr( B| hh) Pr(h) Pr(h| An By = ———___PUAIA)Pr(BIAyPr(h) Pr( A| A) Pr( B| A) Pr(h) + Pr( A| A) Pr( BI A) Pr(h) 
Pr(AN BNh) Pr( ANA) (i) Pr(A| BN h) =Pr(A|h) > ———— = — — 
Pr( BO h) Pr(h) 
Pr(iAN BNh) Pr(ANh) Pr(Bnh) 
Pr(h) ~  Pr(h) Pr(h) 
<=> Pr(AN B| A) = Pr( Ah) x Pr(B| fh) 
(ii) Substitute A for h in (i) 
(iii) Iff (7) and (ii), then 
Pr( AN B| h)Pr(h) 
Pr( h | A M B) = EEE — a 
Pr(AN B|A)Pr(h) + Pr( AN B| A) Pr(h) 
Pr( A | h) Pr(B | A) Pr(h) 
~ Pr(A| A) Pr(B | h)Pr(h) + Pr(A | h) Pr(B | h) Pr(h) 
Proof (Equation (8) ). 
d S(A, B| AN B) 
RE eae Pr(AN B| AN B) Pr( A) Pr( B) 
~~ Pr(AN BB) Pr(A| ANB) -Pr(B| ANB) 
— Pr(A)x PB) si 
Pr( AN B) x Pr(A| AN B) x Pr(B| AN B) 
Proof (Equation (9) 
For { 1, ( A geeog 
Pr( Aj M.A 
[[i_) Pr( Aj | 419---9 A, 
| 
~ PY, Pr( 4 | An -- 
_ P(A. O-+-A An) 
[]/_, Pr( Ai)

--- Page 106 ---

Gregory Wheeler 
l [T;_ Pr( Ai) 
sa T]p_) Pr(Ai | 419---9 An) z Pr( A; N---M Ay) 
4 [Ti Pr( Ai) 
~ TTL, Pr(4i | Ay N+ Ay) x Pr( Ap +++ An) 
i=! 
Acknowledgements 
Thanks to Marco Castellani, Stephen Fogdall, Tomoji Shogenji, Phiniki 
Stouppa, Jon Williamson, and the audience at FEW 2008, Madison. Special 
thanks to Peter Bréssel for spotting an error in one of my proofs. The work 
was supported by the Leverhulme Trust; Portuguese Fundagao para Ciencia e 
a Tecnologia (SFRH/BPD-13699-2003). 
Artificial Intelligence Center-CENTRIA 
Department of Computer Science 
Universidade Nova de Lisboa 
2829-516 Caparica, Portugal 
grw@fct.unl. pt 
References 
Bonjour, L. [1985]: The Structure of Empirical Knowledge, Cambridge, MA: Harvard 
University Press. 
Bovens, L., and Hartmann, S. [2003]: Bayesian Epistemology, Oxford: Oxford University 
Press. 
Cohen, L. J. [1977]: The Probable and the Provable, Oxford: Clarendon Press. 
Earman, J. [2000]: Hume's Abject Failure, New York: Oxford University Press. 
Howson, C., and Urbach, P. [1989]: Scientific Reasoning: The Bayesian Approach, La 
Salle, IL: Open Court. 
Klein, P., and Warfield, T. [1994]: “What Price Coherence?’, Analysis, 54, pp. 129-32. 
Klein, P., and Warfield, T. [1996]: ‘No Help for the Coherentist’, Analysis, 56, pp. 118-21. 
Myrvold, W. [1996]: ‘Bayesianism and Diverse Evidence: A Reply to Andrew Wayne’, 
Philosophy of Science, 63, pp. 661-5. 
Olsson, E. [2005]: ‘Against Coherence: Truth, Probability and Justification’, Oxford: 
Oxford University Press. 
Shogenji, T. [1999]: ‘Is Coherence Truth Conducive?’, Analysis, 59, pp. 338-45. 
Shogenji, T. [2007]: “Why Does Coherence Appear Truth-Conducive?’, Synthese, 157, 
pp. 361-72. Manuscript for 2006 APA Eastern Division Meeting. 
Wayne, A. [1995]: ‘Bayesianism and Diverse Evidence’, Philosophy of Science, 62, 
pp. 111-21.

--- Page 107 ---

Brit. J. Phil. Sci. 60 (2009), 101-133 
When Empirical Success Implies 
Theoretical Reference: A 
Structural Correspondence 
Theorem 
Gerhard Schurz 
ABSTRACT 
Starting from a brief recapitulation of the contemporary debate on scientific realism, 
this paper argues for the following thesis: Assume a theory T has been empirically 
successful in a domain of application A, but was superseded later on by a superior 
theory T*, which was likewise successful in A but has an arbitrarily different theoretical 
superstructure. Then under natural conditions T contains certain theoretical expressions, 
which yielded T’s empirical success, such that these T-expressions correspond (in A) to 
certain theoretical expressions of T*, and given T* is true, they refer indirectly to the 
entities denoted by these expressions of T*. The thesis is first motivated by a study of the 
phlogiston-oxygen example. Then the thesis is proved in the form of a logical theorem, 
and illustrated by further examples. The final sections explain how the correspondence 
theorem justifies scientific realism and work out the advantages of the suggested account 
Introduction: Pessimistic Meta-induction vs. Structural Correspondence 
The Case of the Phlogiston Theory 
Steps Towards a Systematic Correspondence Theorem 
The Correspondence Theorem and Its Ontological Interpretation 
Further Historical Applications 
Discussion of the Correspondence Theorem: Objections and Replies 
Consequences for Scientific Realism and Comparison with Other Positions 
7.1 Comparison with constructive empiricism 
7.2 Major difference from standard scientific realism 
7.3. From minimal realism and correspondence to scientific realism 
7.4 Comparison with particular realistic positions 
The Author (2009). Published by Oxford University Press on behalf of British Society for the Philosophy of Science. All nghts reserved 
doi: 10.1093/bjps/axn049 For permissions, please email: journals. permissions@oxfordjournals.org 
Advance Access published on January 26, 2009

--- Page 108 ---

Gerhard Schurz 
1 Introduction: Pessimistic Meta-induction vs. Structural 
Correspondence 
Can we infer from the empirical success of scientific theories the approximate 
truth of their theoretical representations of reality? A celebrated argument that 
supports this inference is the no-miracles argument, or NMA for short (cf. 
Putnam [1975], p. 73). It says, roughly, that without the assumption of realism 
the empirical success of science would be a sheer miracle. More precisely, 
the best if not the only reasonable explanation of the continuous empirical 
success of scientific theories is the realist assumption that their theoretical 
‘superstructure’ is approximately true and, hence, their central theoretical terms 
refer to real though unobservable constituents of the world. 
However, it is doubtful whether the NMA in its unrestricted form is a re- 
liable argument. Theoretical reasons against the reliability of the NMA will 
be given in Sections 7.1—-2. The major empirical reason against the NMA is 
the pessimistic meta-induction argument (put forth by Laudan [1981]), which 
points to the fact that in the history of scientific theories one can recognize 
radical changes in the ontology, and hence in the theoretical superstructure of 
these theories, although there was continuous progress on the level of empirical 
success. On simple inductive grounds, it is unreasonable to expect that our con- 
temporary theories are the only ones in the history of science that could escape 
this fate. One rather should expect that the ontology and also the theoretical 
superstructure of our presently accepted theories will be radically overthrown 
in the future, and hence can in no way be expected to be approximately true. 
While the NMA is the point of support for a full-fledged scientific realism, 
the pessimistic meta-induction supports either an anti-realist attitude towards 
theories, or at least an instrumentalist position for which theories can only be 
regarded as more or less empirically adequate but not as true or false (van 
Fraassen [1980]). 
In the face of Laudan’s challenge, Boyd ({1984]) has pointed towards the 
existence of relations of correspondence between successive scientific theo- 
ries, which reflect that even on the theoretical level something is preserved 
through historical theory change and, thus, has a justified realist interpretation. 
Laudan replies that there is no evidence for systematic retention of theoreti- 
cal structures through successive theory-change with growing empirical suc- 
cess. Laudan ({1981], p. 121) gives a much debated list of examples of sci- 
entific theories that were strongly successful at their time but have assumed 
an ontology that is incompatible with or at least divergent from contempo- 
rary theories. Laudan concludes that scientists are well advised not to follow a 
retention strategy because this would impede scientific progress (ibid. p. 117, 
pp. 126f).

--- Page 109 ---

Structural Correspondence Theorem 103 
According to Worrall’s structural realism ([1989], p. 122), what are preserved 
in successive theories are certain structural relations between the terms of the 
theories, but not the content of theoretical terms. In the view of other au- 
thors, however, the distinction between ‘structure’ and ‘content’ is problem- 
atic, because all we know about the ‘content’ of a theoretical entity is specified 
by the structure of the theory.' I agree with Psillos ({1995], p. 44) that we 
should replace the structure—content thesis by the thesis that not all but cer- 
tain ‘parts’ of the theory’s content are preserved through theory-change—parts 
that are explicable by structural relations. Can this thesis be defended? Wor- 
rall has supported his preservation-of-structure thesis by one example from 
Laudan’s list, the correspondence relation between Fresnel’s and Maxwell’s 
equations describing the relative intensities of incident, reflected and refracted 
light beams. But as Worrall himself notices ({[1989], p. 120), this is an espe- 
cially pleasant case. Other examples such as the relation between the phlo- 
giston and oxygen theory of combustion, the caloric and the kinetic theory 
of heat, or between classical and quantum mechanics, are much more diffi- 
cult to handle. Even for simple cases as the Fresnel-Maxwell case, Laudan 
points out that one cannot say that the older theory is preserved in the later 
one, because the older theory assumes entities and mechanisms which accord- 
ing to the contemporary theory do not or even cannot possibly exist ({1981], 
pp. 127-31). 
In my opinion, what is of primary importance is to find good answers to the 
following three questions: 
Question 1; Do correspondence relations between seemingly disparate theo- 
ries occur in the history of science for systematic reasons, and not as mere 
exceptions? 
Question 2: Given that question | has a positive answer: are these systematic 
reasons objective ones? More precisely: are they the (necessary) result of the 
cumulatively increasing empirical success of the theories? Can we exclude that 
they are merely a consequence of the cognitive constitution of the human mind 
or brain, which prefers certain models over others? 
Question 3: Given that question 2 has a positive answer: does this yield an 
improved way of justifying scientific realism, which can resist the objections 
against the NMA in its unrestricted version? 
The debate in philosophy of science has produced sophisticated versions of 
realist positions, but it seems to me that a convincing answer to these questions 
' Cf. (Psillos [1995], pp. 31 fF; [1999], p. 155; Papineau [1996], p. 12; Votsis [2007])

--- Page 110 ---

104 Gerhard Schurz 
has so far not been achieved. In this paper, I will give a positive answer to these 
three questions. I will try to establish the following 
Thesis: Assume a theory T has been strongly (empirically) successful in a domain 
of application A, but was superseded later on by a superior theory T*, which was 
likewise successful in domain A but has an arbitrarily different theoretical su- 
perstructure (or ‘ontology’). Then (under natural conditions) T contains certain 
theoretical terms or expressions that correspond (in A) to certain theoretical 
expressions of T* and, given T* is true, they refer indirectly to the entities denoted 
by these expressions of T*. 
I will first support my thesis with the historical example that has been 
most resistant against the discovery of correspondence relations, namely the 
phlogiston—oxygen case (Section 2). Then I will show that my thesis is a logical 
consequence of the assumption that the following five rather natural require- 
ments are satisfied (Sections 3-4): 
(Requirement 0:) The vocabulary of the compared theories can be divided into 
a non-theoretical or empirical vocabulary, which is shared by the theories, and 
a theoretical vocabulary that is specific for each theory. The non-theoretical 
vocabulary contains either perceptual concepts or concepts that depend on 
unproblematic pre-theories (e.g., theories of measurement) which are shared 
by the compared theories. For a large part of the modern history of science 
this requirement seems to be met—even if one assumes that what counts as 
non-theoretical is relative to given background beliefs or measurement tech- 
nologies (cf. Laudan and Leplin [1991], p. 451), as long as these background 
parameters are shared by the compared theories. Logically speaking, the satis- 
faction of requirement (0) is a presupposition for the possibility of comparing 
the success of competing theories, and hence a presupposition of the entire 
problem. 
While requirement (0) is common in the debate, the next four requirements 
(1-4) arise from specific insights into the logical situation, which underlies my 
correspondence theorem. Here I give only a brief exposition of them; they are 
motivated and precisely defined in Sections 3-4: 
(Requirement 1: ) The past theory T must have been capable of producing novel 
predictions, which ai the time of the theory construction were neither known, 
nor expected by means of empirical induction alone. This requirement has also 
been defended by Worrall ({1989], p. 113), Psillos ({1999], p. 106) and Ladyman 
and Ross ((2007], Chapter 2.1.3). Specific to my account is my logical definition 
of strong (i.e. potentially novel) empirical—predictive success. The requirement 
of novel predictions alone is not sufficient to refute Laudan’s pessimistic meta- 
induction, because several of his historical examples meet this condition.

--- Page 111 ---

Structural Correspondence Theorem 105 
(Requirement 2:) The strong success of the predecessor theory T must be 
yielded by one (or several) theoretical terms or expressions of T, which can be 
empirically indicated or measured in certain circumstances by way of several 
bilateral reduction sentences. 
( Requirement 3: ) The entailment of T’s strong empirical success by the succes- 
sor theory T* must depend on the theoretical part of T*; and finally, 
( Requirement 4: ) the compared theories T and T* are ‘causally normal’. 
In Section 4, I will prove that if requirements 0-4 are satisfied, a cerrespon- 
dence relation between that expression gy of T which yielded T’s strong success, 
and a certain expression t* of T* follows from the union of T* with T,.<,, where 
Trestr 18 a T*-consistent content-part of T, which entails T’s strong success. The 
correspondence relation has the form of an equivalence relation restricted to 
the domain A of T’s strong success. This g—t* correspondence entails the possi- 
bility of a so-called g—t* reference shift, by which I mean the assignment of t*’s 
intended interpretation I(r*) to g such that this shifted interpretation makes 
true the content-part T,.,; of T, provided that T* is true. I express this situation 
by saying that Tyest; is indirectly true (i.e., true under the shifted interpretation), 
and that ¢ refers indirectly to I(t*) (i.e., a g—t*-correspondence holds, which 
entails the possibility of a g—t* reference shift). The content-part T,.s of T, 
which is preserved by the g—rt* correspondence is called g’s outer structure be- 
cause it covers y’s (causal) relations to the observable phenomena (and maybe 
to other T-expressions), while g’s so-called inner structure may be completely 
lost in g’s shifted interpretation within T*. 
In Section 5, I illustrate how my theorem applies to further historical cases, 
and in Section 6, I explain the philosophical substance of my theorem in the 
style of ‘objections and replies’. In the final section, Section 7, I point out 
the epistemological advantages of my account in regard to the prospects of 
justifying scientific realism. 
2 The Case of the Phlogiston Theory 
The phlogiston theory of combustion was developed by J. J. Becher and his stu- 
dent G. E. Stahl in the late seventeenth and early eighteenth century. Accord- 
ing to this theory, every material that is capable of being burned or calcinated 
contains phlogiston—a substance different from ordinary matter, which was 
thought to be the bearer of combustibility. When combustion or calcination 
takes place, the burned or calcinated substance delivers its phlogiston, usually 
in the form of a hot flame or an evaporating inflammable gas, and a dephlogis- 
ticated substance-specific residual remains. In the 1780s, Lavoisier introduced 
his alternative oxygen theory according to which combustion and calcination 
consists in the oxidation of the substance being burned or calcinated, that is, in

--- Page 112 ---

106 Gerhard Schurz 
the formation of a chemical bond of its molecules with oxygen. The assump- 
tion of the existence of a special bearer of combustibility became superfluous 
in Lavoisier’s theory. In modern chemistry, Lavoisier’s theory is accepted in a 
generalized form and nobody believes in the existence of phlogiston any more. 
Oxygen theory describes the calcination of a metal as a process of oxidation, . . . . , 
i.e., a process in which oxygen is added to the metal:- 
Metal + oxygen — calx (= metal—oxide). 
In terms of the phlogiston theory, this reaction is described as de-phlogistication, 
i.e., a process in which the metal delivers its phlogiston: 
Metal — calx + phlogiston ¢. 
Metals, coal and oils were assumed to be rich in phlogiston. The extraction of 
metals from calxes (mineral ores) through heating in charcoal was described 
by phlogiston theory as phlogistication, i.e., the inversion of the process of 
dephlogistication, in which the charcoal gives phlogiston to the calx to form a 
metal: 
Calx + coal > metal + ash (= dephlogisticated coal) 
+ byproduct: phlogisticated air ¢. 
Oxidation theory describes this process as reduction, i.e., the inversion of the 
process of oxidation, in which the calx delivers its oxygen to the coal: 
Metal—oxide + carbonium (coal) — metal + carbon dioxide 7 
+ byproduct: ash. 
‘Proper’ end products and ‘by-products’ exchanged their role. Phlogiston theory 
identified the evaporating fume (carbon dioxide) with phlogisticated air (not all 
of the coal’s phlogiston combines with the calx) and the ash as dephlogisticated 
coal; for oxygen theory, carbon dioxide is a proper end product, and the ash is 
a residual of incompletely oxidized coal. 
A further domain explained by the phlogiston theory was salt-formation 
through dissolution of metals in acids. In terms of modern chemistry, the 
qualitative reaction is the following (every acid has the chemical composition 
‘hydrogenium—X’, where the hydrogenium atoms give off their electrons to their 
negatively charged partner X): 
Metal + acid (= hydrogenium—X) — metal—X (= salt) + hydrogenium f. 
Since strong heating of the salt yielded the calx (oxide) of the metal, phlogiston 
theorists described this process as a dephlogistication of the metal, in which 
2 Chemical notation: the substances mentioned on the left of the arrow are input substances and 
those mentioned on the right of the arrow are output substances of the chemical reaction. ‘ft’ at 
a substance means that the substance is an evaporating gas.

--- Page 113 ---

Structural Correspondence Theorem 107 
calx and acid combine into the salt (McCann [1978], p. 32). Cavendish believed 
that the evaporating ‘inflammable air’ (hydrogenium) was pure phlogiston. So, 
phlogiston theory modelled salt-formation as follows: 
Metal + acid — calx—acid (= salt) + phlogiston (inflammable gas) f¢. 
Phlogiston theory entailed a variety of potentially novel predictions. For ex- 
ample, phlogiston theory predicted that the process of saltification should be 
possible for every kind of metal, provided that heat and acid are sufficiently 
strong—even for metals that have never been put into acid before, which were 
chemically so far unexplored, or for rare metals with atypical phenomenolog- 
ical properties. A particularly striking example of a novel prediction made by 
phlogiston theory has been described by Carrier ({2004]): after Cavendish had 
identified inflammable air with phlogiston, Priestley predicted in 1782 that it 
should be possible to invert the process of calcination by adding inflammable 
air to a metal calx. He heated several metal calxes in inflammable air and ob- 
served that the inflammable air was almost completely absorbed and that the 
calxes were slowly reconverted into the metals. Priestley had also recorded the 
emergence of water droplets in this reaction, but he assumed that the water was 
contained in the inflammable air from the beginning (Carrier [2004], p. 151). 
In modern terms, Priestley had performed the following reaction of reduction 
Metal—oxide + hydrogenium — metal + water 
while in terms of the phlogiston theory he had performed the following reac- 
tion: 
Calx + phlogiston (+ water) > metal (+ water). 
This was celebrated as a further success of the phlogiston theory. 
There were several empirical reasons why Lavoisier later concluded that his 
oxygen theory was true and that the postulate of phlogiston was superfluous. 
For example, in some cases the process of combustion or calcination resulted 
in an increase of weight of the dephlogisticated substance, which was explained 
by different ad hoc assumptions (one of them was the attribution of ‘negative 
weight’ to phlogiston). Moreover, phlogiston theorists were unable to isolate 
phlogiston in a coherent way (Cavendish’s identification of phlogiston with 
hydrogenium gas did not work in other domains). Nevertheless, if we restrict 
the phlogiston theory to a certain domain of application, such as the oxidation 
and saltification of metals and the retransformation of metal calxes into pure 
metals, then the phlogiston theory was strongly empirically successful with 
respect to these domains. Although Lavoisier’s oxygen theory surpassed the 
success of the phlogiston theory, it also faced severe difficulties: for example, 
Lavoisier assumed that the saltification of metals through acid is always the 
effect of the oxygen contained in the acid. But oxygen is not contained in all

--- Page 114 ---

108 Gerhard Schurz 
acids, e.g., it is not contained in hydrochloric acid. These difficulties had been 
overcome only by the generalized oxidation and reduction theory of modern 
chemistry. 
Let us now turn to our thesis of Section 1: if this thesis were true, then we 
could conclude from the strong empirical success of phlogiston theory that 
there is a certain correspondence between some of its central theoretical con- 
cepts and modern chemistry, so that from the modern viewpoint there is at least 
something that phlogiston theory has got right. For Carrier ({2004], pp. 154f), 
what phlogiston theory has got right is the classification of dephlogistication 
(oxidation) and phlogistication (reduction) as inverse chemical reactions. This 
is certainly true, but it seems to be too weak, because the inversions of cer- 
tain chemical processes had already been recognized before, and independentl) 
of the theoretical part of phlogiston theory. What we are after is something 
in the theoretical superstructure of the phlogiston theory that corresponds to 
something in modern chemistry. What could this be? 
There is nothing that directly corresponds to ‘phlogiston’ from the view- 
point of modern chemistry. But this is no wonder, because as we have already 
explained, phlogiston theory did not provide a general criterion of how phlo- 
giston can be empirically identified. So the theoretical term ‘phlogiston’ was 
empirically underdetermined in phlogiston theory. The theoretical expressions 
of phlogiston theory, which did all the empirically relevant work and were 
not empirically underdetermined, were the expressions of phlogistication = 
assimilation of phlogiston, and of dephlogistication = release of phlogiston. For 
these two expressions there indeed exists a correspondence with modern chem- 
istry, which goes much further than the identification of phlogistication with 
oxidation in Lavoisier’s sense. To explain this correspondence we need a bit 
more of modern chemistry.” 
Every substance consists of molecules, and molecules consist of atomic ele- 
ments bound together by chemical bonds. The e/ectropositivity of an element 
measures its tendency to contribute electrons to its neighbouring atoms in 
electrically polarized or ionic bonds. Conversely, the electronegativity measures 
the tendency of an element to attract electrons from the neighbouring atom 
in polarized or ionic bonds. Metals and hydrogenium are electropositive; car- 
bonium is in the middle of the spectrum, and nonmetals such as oxygen are 
electronegative, with the extremes being the halogens. Oxidation of an elemen- 
tary substance X (a metal, coal, etc.) in the generalized sense consists in the 
formation of a polarized or ionic bond of X with a electronegative substance Y, 
in which the atomic elements of X are electropositive and thus donate electrons 
to the electronegative neighbour Y in the bond. Every process of combustion, 
calcination or saltification consists of such an oxidation process. The inversion 
For the following cf. (Oxtoby et al. [1999], Chapters 3, 6.3).

--- Page 115 ---

Structural Correspondence Theorem 109 
of the process of oxidation is called the process of reduction: here the polar- 
ized or ionic bond between an electropositive X-ion and its electronegative 
neighbour is broken, X regains its missing electrons and reappears in its pure 
elementary form. Therefore we have the following correspondence relations 
between phlogiston theory and modern chemistry: 
Correspondence relations between phlogiston theory and modern chemistry: 
Dephlogistication of X corresponds to (and hence indirectly refers to) the 
donation of electrons of X-atoms to their bonding partner in the formation 
of a polarized or ionic chemical bond. 
Phlogistication of X corresponds to (and hence indirectly refers to) the 
acceptance of electrons by positively charged X-ions from their bonding 
partner in the treaking of a polarized or ionic chemical bond.4 
What was wrong in phlogiston theory was that phlogiston was thought of 
as a special substance that is emitted during a dephlogistication process. The 
electrons do not /eave the chemical substance but just move a little bit to the 
electronegative neighbours in the molecule. What really is emitted as the end 
product of an oxidation process (besides the oxidized material) depends on the 
oxidans, that is, the input substance, which causes the oxidation and which pro- 
vides the electronegative partner. If the oxidans is an acid, then what is emitted 
is hydrogenium, whence in these cases phlogiston could be identified with hy- 
drogenium. If the oxidans is pure oxygen and the oxidized material is coal, then 
carbon dioxide is emitted. In the combustion of phosphorus and sulphur noth- 
ing is emitted, and therefore the weight increases after dephlogistication—this 
was the most problematic case for the phlogiston theory. But apart from such 
cases, p‘ilogiston theory was strongly successful, and this strong empirical suc- 
cess is explained by the above correspondence relation. 
The proposed correspondence relation surely does not preserve all of the 
meaning of ‘phlogistication’. In particular, the assumption that phlogiston is 
a special substance contained in other substances and leaving them during the 
process of combustion is not preserved under this correspondence. Therefore, 
the correspondence relation cannot be regarded as an analytic truth, as a 
semantic translation. There is nothing in modern electron theory that would 
analytically entail a connection with phlogiston theory; nor the other way 
round. Rather, the above correspondence principle has to be regarded as a 
synthetic statement that is true only in the domain of application in which 
phlogiston theory was empirically successful. 
4 As was pointed out to me by James Ladyman, a similar correspondence holds between the 
‘phlogiston-richness’ and the ‘electropositivity’ (or ‘phlogiston-poorness’ and ‘electronegativity’) 
of an element

--- Page 116 ---

Gerhard Schurz 
3 Steps Towards a Systematic Correspondence Theorem 
Now I start my attempt to establish the existence of correspondence relations 
of the above sort on systematic grounds. Where should these correspondences 
come from? I will try to show that, under natural conditions, correspondence 
relations can be obtained as consequences of the union of (parts of) the two 
theories in their joined language. Kuhnian philosophers of science will probably 
object: how can it be possible to unify the conceptual frameworks of two 
theories whose theoretical superstructures are incommensurable? 
Assuming that theories are represented by sets of axioms and that require- 
ment (0) of Section | is satisfied, two cases of incommensurability are to be 
considered: on the one hand, the two theories may be theoretically incompara- 
ble in the sense of a nonoverlap of theoretical terms, and on the other hand, 
they may be (non-theoretically or theoretically) incompatible. The first case is 
logically harmless. As Hintikka ({1988], p. 27) has emphasized, a situation of 
theoretical (or even total) incomparability does not prevent us from joining 
together the conceptual frameworks of two theories as follows: If L; is the lan- 
guage of T;, and L» the language of the T>, where L; and Lz have a common 
logic and logical vocabulary, then the united language of both theories, Lj, is 
simply obtained as the set of all formulas expressible in the united vocabulary. 
In this united language we formulate the united theory T; U T> and see what 
follows from it. 
The second case is (logical) incompatibility between T; and T2. According 
to Carrier ({2001], pp. 67-9) and Hoyningen-Huene ({1993], Chapter 6.3(b)), 
this is a crucial aspect of incommensurability. We can distinguish two kinds 
of incompatibility. Non-theoretical (empirical) incompatibility arises when the 
two theories entail contradictory consequences in their joint non-theoretical 
vocabulary. More interesting are cases of theoretical incompatibility, because 
they are historically more stable. Theoretical incompatibility presupposes that 
the two theories have at least some theoretical concepts in common. Phlogiston 
and oxygen theories were theoretically incompatible, because they shared cer- 
tain theoretical concepts, such as ‘substance’, ‘mass’, etc. To repeat their basic 
theoretical incompatibility: for phlogiston theory, combustion is the effect of a 
special substance (phlogiston), but there is no such substance for oxygen theory. 
Also (non-theoretical or theoretical) incompatibility is not an obstacle for 
establishing correspondence relations, provided we restrict the outdated theory 
T. We cannot simply form a union of two incompatible theories T U T*. Doing 
this would trivialize our claim that this union entails correspondence principles: 
since this union is contradictory, it implies everything whatsoever. Therefore we 
take pains to consider only a certain part of the content of the outdated theory 
T, which yielded T’s strong empirical success but is nevertheless consistent with 
the contemporary theory T*.

--- Page 117 ---

Structural Correspondence Theorem 111 
Our next task is to formulate natural conditions under which a correspon- 
dence between theoretical expressions of the two theories T and T* can be 
established. I propose the following condition on the predecessor theory T, 
which comprises requirements | and 2 of Section 1: T must contain one (or 
several) theoretical expressions py, which yielded T’s strong empirical success by 
way of empirical indication or measurement laws connecting g with several phe- 
nomena in several circumstances. The general format of an empirical indication 
or measurement law for a theoretical term ¢g is a (Carnapian) bilateral reduction 
sentence of the following form: 
(BR;) (V) Aj — (g(x) Rj), where V>(x) C Ve(R;), Ve(x) C Ve(Aj). 
In words: Under empirical circumstances Aj, the presence of g is indicated or 
measured by an empirical phenomenon or process Rj. 
Explanation of the notation: g(x) is a T-theoretical term or expression in the 
free variable(s) x, which describes the state of the individual system x in the T- 
theoretical language (x may also be a vector of variables describing a composed 
system consisting of several parts). The BRs for g are indexed (BR;), because it 
is a crucial condition that g is characterized by many such BRs. The formulas A; 
and R; are possibly complex non-T-theoretical formulas, whose free variables 
obey the condition at the right side (where V;(a) = the set of variables occurring 
free in expression a). Hence, the formulas A; and R; contain all variables (x) 
that refer to the individual system under consideration, but may in addition 
contain further variables—especially the time variable t, or variables referring 
to individuals in the external circumstances.* The bilateral reduction sentence 
is understood as universally quantified, indicated by the ‘(V)’ in front of it. I will 
frequently omit the variables and mention them only when necessary. 
I understand bilateral reduction sentences, differently from Carnap, in a 
‘modernized’ and nonreductionist sense. They are not analytically true (as 
assumed by the early Carnap), because as we shall see soon, several BRjs 
joined together have synthetic consequences (which was recognized by Carnap 
[1936], p. 451). Rather, they are synthetic statements, which express conditions 
for the empirical indication or measurement of the theoretical entity g under 
specific conditions A;. They are usually not part of T’s axiomatization (so-called 
Certain subtleties are involved concerning the time variable t, which I mention only in the 
footnote. If g is an intrinsic property of x that does not depend on time, then the bilateral 
reduction sentence has the explicit form (i) ¥x,t: Ajxt — (g(x) — Rjxt). This sentence is empirically 
creative insofar it logically implies (ii) Vx: 3t(Ajxt A Ryxt) > Vt(Ajxt > Rjxt). This is okay: if a 
certain behaviour Rjxt under circumstances A;xt (such as a certain chemical reaction) empirically 
indicates an intrinsic time-independent property of x, then it must be expected that x will exhibit 
this behaviour every time when it is put into circumstances Aj. More generally, if ‘t’ stands for 
the vector of variables that are free in (Aj,Rj), but not in g(x), then BR; has the explicit form 
(i) and logically implies (ii). This subtlety causes further subtleties discussed in footnotes 6 and 
8. Note that our notation also admits g being a temporal property—in this case, x contains the 
variable t; or more generally, V»(Aj,Ri) = V¢(x), and the explained subtlety does not arise.

--- Page 118 ---

112 Gerhard Schurz 
‘rules of correspondence’, as assumed by the late Carnap), but are obtained as 
logical consequences of a suitably rich version of the theory. For example, if 
v(x) stands for ‘x delivers phlogiston’, and A; stands for ‘x is a metal that is put 
into hydrochloric acid (at a certain time t)’, then Rj stands for ‘x dissolves in the 
acid and inflammable air evaporates’. Bilateral reduction sentences subsume all 
important kinds of statements expressing empirical indication or measurement 
methods. In particular, this format subsumes quantitative measurement laws 
for theoretical terms, in which instead of the equivalence we have a numerical 
identity, because the two statements (1) and (2) are logically equivalent (where 
ris a variable ranging over real numbers, and fj is a function term): 
(Vr :) Aj(x, u) > (g(x) =r @ f(x, u) = r). (1) 
Aj(x, u) > (g(x) = fi(x, u)). (2) 
For example, if ‘g(x) = r’ stands for the expression ‘mass-of-x = r grams’ and 
A; for the circumstance ‘x is put on a balanced beam scales’, then Rj stands for 
the condition ‘the number of one gram units on the other side of the balanced 
beam is r’, formalized as ‘f;(x,u) = r’. The BR-sentence (1) is logically equivalent 
with (2), which is the standard format of a quantitative measurement law. 
To avoid misunderstandings, that a theoretical term g of T can be em- 
pirically indicated or measured does, of course, not mean that it becomes a 
non-theoretical term of T. The difference is, rather, that the measurement of y 
presupposes the theory T, in other words, T is needed to derive g’s measurement 
conditions, while T’s non-theoretical terms can be empirically measured inde- 
pendently of T, based on pure perception or non-theoretical knowledge (this 
insight goes back to Sneed [1971]). In the model-theoretic reading, a bilateral 
reduction statement for y expresses a class of measurement models for ¢ in the 
sense of Balzer et al. ({1987], p. 64). 
It is easy to see how the theoretical expression g of T yields strong (i.e., 
potentially novel) empirical success: because the class of bilateral reduction 
sentences, which characterize yg, BR(T) := {BR,(T), ..., BR,(T)}, entail such 
a potentially novel success as follows: 
A; — (g(x) @ R)) Potential novel predictions entailed by BR(T): 
A2 — (g(x) > R2) (i) (A; A Ry) > (A2 > Rg), 
etc. (ii) (Ay A >R,) > (A2 > -R2) etc. 
With ‘(A; A +R)) > (A2 > +R>)’ I mean a conditional of one of the two 
forms (i) or (ii) above. The class of conditionals {(Aj A +Rj) > (Aj > +R)): 
1 <i+j <n} is what I call the strong (potential) empirical success of T. These 
conditionals figure as potentially novel predictions because with their help one 
can infer something from what has happened in one domain of application 
about what will happen in another domain of application, without the other

--- Page 119 ---

Structural Correspondence Theorem 113 
domain of application having already been investigated.° For example, when ¢ 
represents a chemical model of oxidation, A; may describe the exposure of a 
metal to air and water, and R, the end products of the reaction of oxidation, 
A> the exposure of a metal to hydrochloric acid, and R> the end products of 
the reaction of salt-formation, etc. Even if the predictions asserted by scientific 
theories do not always have this explicit form, they can very often be recon- 
structed in this form.’ The causal interpretation of the role of gy as described by 
BR(T) is the following: g figures as a common cause of the observable behaviour 
Rj; in circumstances A;. The philosophical importance of this common cause 
condition is explained in Section 6. 
Now let us see how for the theoretical term g of T (satisfying requirements 
1 and 2) a correspondence can be established to a theoretical expression of 
another theory T*. This is possible whenever T* contains a (possibly complex) 
expression t* for which T* entails the same measurement conditions as T entails 
for gy. Then we have the following situation (‘IF’ for ‘logical consequence’): 
BR(T): A; — (g(x) = Rj) (where T | BR(T)) (3) 
BR(T*): A; — (t*(x) @ Rj) (where T* |+ BR(T*)). (4) 
The union of (3) and (4) entails that g and t* are equivalent in the circumstances 
of the measurement: 
TU T* lt BR(T) U BR(T") |t Aj — (g(x) & 1*(x)), (5) 
which is exactly a domain-restricted correspondence principle of the sort we 
are after. Since T and T* have radically different theoretical superstructures, the 
existence of such a T*-expression t* is neither obvious nor can it be expected 
to be directly evident from T*’s axioms. What our theorem will show is that the 
existence of such a T*-expression can indeed be derived from the assumption 
that our requirements (3) and (4) are satisfied. 
© If g(x) is a time-independent property, then these potentially novel predictions have the exact 
form: Vx: 3t(AjxtAR;xt) > Wt(Ajxt—> Rjxt), saying that whenever x has behaved at some time in 
subdomain A; in the way R;, then every time x is put into subdomain A, it will behave in the way 
Rj. 
For example, the novel prediction may have the form ‘an x of kind @ will exhibit reactions R 
under conditions A;’, where ‘a’ is a theoretical statement such as ‘being rich of phlogiston’ 
But the empirical indicators for ‘x is of kind @’ will (after some transformations) again have the 
form ‘under conditions Ajx, Rix has happened’. Even Cavendish’s novel prediction following 
from the inversion principle for chemical reactions may be reconstructed in this form. The inver- 
sion principle asserts that for each BR; describing a dephlogistication (oxidation) process, an 
inverted BR holds, abbreviated as InvBR;, which describes a phlogistication (reduction) proces: 
(vie{1 n}: BR; — InvBR;). Let In(x) and Out(y) denote that x is an input and y an out- 
put substance of a chemical reaction (where y = fx), and ‘g(x,y)’ stand for ‘phlogiston moves 
from x to y’. Then a BR for dephlogistication of x has the form In(x,t)AOut(fx,t)AA(x) > 
(o(x,fx)<>B(fx)), where A and B denote kinds of substances under certain conditions, and the 
inverted InvBR for phlogistication of x has the form In(fx,t)AOut(x,t)A B(fx) — (g(fx,x)<>A(x)) 
Taken together, BR and InvBR entail the novel prediction 3t(In(x,t)AOut(fx,t)AA(x)A B(fx)) > 
Vt(In(fx,t)AOut(x,t)A B(fx) — A(x)).

--- Page 120 ---

114 Gerhard Schurz 
Requirement (3) demands that the successor theory T* entails T’s strong 
empirical success in a way that depends on the theoretical part of T*. The de- 
pendence on T* is very natural, because from empirical descriptions of what 
goes on in a system x in domain Aj, nothing can be concluded by means of em- 
pirical induction alone about what goes on in system x in a qualitatively different 
domain Aj. For example, from observing reactions of metals in hydrochloric 
acid, nothing can inductively be concluded about the behaviour of metals in 
oxygen or water, and from observing projectiles on the earth, nothing can in- 
ductively be concluded about the planets moving around the sun. Connections 
of this sort can only be provided by a theory, and they are possible because in 
the theory one can infer from A;x A R,;x a certain theoretical description t(x) 
of the intrinsic properties of x, which in turn entails in the theory that (Ajx > 
R,x) will hold. Therefore I assume that for every conditional of the form (Aj/A 
+Rj) — (A; — +Rj), which the new theory T* entails, there exists a medi- 
ating theoretical description or theoretical mediator t*(x) such that T* entails 
(AjxA+Rjx > 1*(x)) as well as (t*(x) > (Ajx > +Rjx)).° Ifa theory T has this 
property w.r.t. a partition {Aj:1 <i <n} of a domain A into subdomains, then 
I say that the strong (potential) empirical success entailed by T is 7-dependent. 
The theoretical mediator t*(x) may depend both on i and j, 
(A; A Rj) > T;; (x), tT; — (Aj > Rj) for all j 41, j € {1 
in which case the theory T* utilizes for the prediction of the system’s behaviour 
in each new domain of application Aj (j ¥ i) a different theoretical description. 
However, we can join these descriptions into a big conjunction 
T(x) A{t5(X) : Ly Sn. st 
which is T*’s maximal theoretical description, which can be inferred from 
(A; A Rj) and contains all theoretical details necessary to infer what goes on 
in all other domains. Thus we can assume that the theoretical mediator 1;* 
depends only on i. In Section 4, we shall see that we can even get rid of this 
dependence on index i. 
With our requirement (3), we are a/most where we want to be. As the proof of 
our theorem will show, the satisfaction of this requirement implies that T* en- 
tails unilateral reduction sentences for the mediating T*-theoretical expression 
t* in terms of the circumstances Aj and reactions Rj, which imply implications 
but not equivalences between g of T and t* of T*. To obtain equivalences and 
hence the desired correspondence principles, we will need one further natural 
assumption (requirement 4 of Section 1), namely that the considered theories 
8 If r* is time-independent (recall footnote 6), the T*-entailed mediator statements have the form 
Wx: (St(AixtARjxt) > t*(x)) and Wx: (t*(x) > Vt(Ajxt— Rjxt)). These two statements are logically 
equivalent with the universally quantified statements Wx,t: (AjxtARjxt > t*(x)) and Yx,t: (t*(x) > 
Vt(Ajxt— Rjxt)), respectively. So it is possible to represent these statements without quantifiers.

--- Page 121 ---

Structural Correspondence Theorem 115 
are causally normal in the following sense. The non-theoretical predicates or 
parameters of the theory divide into a set of causally independent parameters, 
which describe the circumstances A; of the special subdomains, and a set of 
causally dependent parameters. The behaviour of the system x w.r.t. its de- 
pendent parameters under given values of the independent parameters can be 
deduced from the theory. But it is impossible to derive from a purely theoret- 
ical description t(x) of a system x any empirical assertion about the status of 
the independent parameters of x. This is again a very natural condition. For 
example, nothing can be concluded from the theoretical nature of a certain 
substance about what humans do with it, about whether they expose it to hy- 
drochloric acid or to heat or whatever. Or, nothing can be concluded from a 
purely mechanical description of a physical body about its initial conditions, 
about whether the body was thrown into the air, or split into pieces, etc. 
4 The Correspondence Theorem and Its Ontological Interpretation 
The following definition summarizes the central notions that have been ex- 
plained in the previous section (note: by definition, T entails a set of sentences 
> iff T entails all sentences in £). 
Definition: 
(1) A is a partitioned domain iff A = A; U... U Ay, n > 2, where the 
A; are mutually exclusive and qualitatively different subdomains described by 
non-theoretical means. 
(2.1) A strong potential empirical success of a theory T w.r.t. partitioned 
domain A = A; U... UA, isa set of conditionals of the form (Aj A +Rj) > 
(A; — +R;) fori#j € {1,...,n}, which are entailed by T, where the Rj describe 
the empirical (non-theoretical) behaviour of the system x under consideration 
in circumstances Aj. 
(2.2) Such a strong potential empirical success is yielded by a theoretical 
expression yg of T iff T entails the bilateral reduction statements BR(T) = 
{A; > (g(x) @ Rj): 1 <i<n}. 
(2.3) Such a strong potential empirical success of T is 7-dependent iff for 
every conditional of the form (Aj A +R;) — (A; — +R;) following from T, 
there exists a theoretical description 1;(x) of the underlying system x such that 
(A; A +R; — 1;(x)) and t;(x) — (A; — +R)) follow from T (cf. footnote 8). 
(3) A theory T is causally normal w.r.t. a partitioned domain A = A; U...U 
A, iff (i) the non-theoretical vocabulary of T divides into a set of independent 
and a set of dependent parameters (predicates or function terms), (ii) the 
descriptions ‘A;’ of the subdomains A; are formulated solely by means of the 
independent parameters (plus logico-mathematical symbols) and (iii) no non- 
trivial claim about the state of the independent parameters of a system x can 
be derived in T from a purely T-theoretical and T-consistent description of x.

--- Page 122 ---

116 Gerhard Schurz 
Correspondence theorem: Let T be a consistent theory that is causally normal 
w.r.t. a partitioned domain A = A; U... U A, and contains a T-theoretical 
expression g(x), which yields a strong potential empirical success of T w.r.t. 
partitioned domain A. 
Let T* be a consistent successor theory of T (with an arbitrarily different 
theoretical superstructure), which is likewise causally normal w.r.t. partitioned 
domain A and which entails T’s strong potential empirical success w.r.t. A ina 
T*-dependent way. 
Then T* contains a theoretical expression t*(x) such that T and T* together 
imply a correspondence relation of the form 
(cy: A —> (g(x) @ 1*(x)) 
(in words: whenever a system x is exposed to the circumstances in one of the 
subdomains of A, then x satisfies the T-theoretical description ¢ iff x satisfies 
the T*-theoretical description t*), 
which implies that g(x) indirectly refers to the theoretical state of affairs de- 
scribed by t*(x), provided T* is true. 
Corollary 1. BR(T) U T* is consistent, and (C) follows already from 
BR(T) U T* (even from BR(T) U BR(T*); see proof step (14)). 
Corollary 2. t* is unique in domain A modulo T*-equivalence. 
Remark. The theorem applies to all theoretical expressions g of T which yield 
strong potential empirical success by way of bilateral reduction statements. We 
speak of potential success because the logical part of the theorem is independent 
of the factual success of the considered theories. If the potential success of T 
is preserved by T* in a T*-dependent way, and both theories are causally 
normal, the correspondence relation (C) follows. Corollary | tells us that this 
correspondence principle follows in a non-trivial way, from a certain part of T 
that is consistent with T*. Corollary 2 informs us that if T* contains several 
different theoretical descriptions t;* which satisfy the correspondence theorem, 
then T* entails that they are equivalent in domain A. 
Proof of the correspondence theorem: 
We assume the following bilateral reduction sentences follow from T: 
T + BR(T), where BR(T) := {Aj > (g(x) @ Rj): 1 <i <n}, (6) 
and because T is causally normal w.r.t. A; U...U Aj, the Ajs are expressed in 
terms of independent parameters and the Rjs in terms of dependent parameters 
of T. 
We assume that the strong potential empirical success of T, which follows 
from the bilateral reduction sentences in (6), is also entailed by T*, and so the 
following must hold (recall footnote 6): 
n} : T* It (Aj A Rj) > (Aj > Rj), and 
T* It (A; A —R;) > (Aj — —R)).

--- Page 123 ---

Structural Correspondence Theorem 117 
Because this strong potential empirical success entailed by T* is T*-dependent, 
(7) implies the following: 
For every i there must exist T*—theoretical descriptions 1;*(x) and ju7(x) 
such that (8) 
T* Ik (Aj A Ri) > 17(x), and 
one 
It is sufficient for our purpose to choose one fixed i, say i = k. We abbreviate 
T,* = t* and py* = p*. Recall footnote 8, which implies that the following 
proof remains essentially propositional. 
One can see why the condition of 7*-dependent strong potential success is 
crucial. Without this condition it would be impossible to say how the empirical 
consequences in (7) are obtained from T*. With this condition we gain two T*- 
theoretical descriptions of x: one mediates the empirical consequences involving 
positive Rjs, and the other one mediates the empirical consequences involving 
negative Rijs. 
From (8) it follows by propositional logic that: 
T* It (Ay > (Rx > t*(x)), ViFk, je fl ; + A; > (t*(x)— Rj) 
(9) 
T* Ik (Ay > (AR, > *(x)), ViFk, je{l : T* lk Aj > (*(x) > 7R)). 
So far we have two unilateral reduction sentences for two different T*-theoretical 
descriptions, t* and *, in the measurement conditions of g. We want to derive 
a bilateral reduction sentence for a T*-theoretical description that has the same 
form as the bilateral reduction sentence for g. Now the condition of causal 
normality comes into play. 
It follows from (9) by propositional logic that 
T* Ik (>t*(x) A my*(x)) > Ag, 
Vj #k, je {l > Tt lk (r*(x) A *(x)) > Aj. (10) 
But since T* is causally normal w.r.t. the given partition of A, A; is an as- 
sertion described in terms of independent parameters, and so it follows that 
t*(x)Ap*(x) and —1*(x)A—y*(x) must be T*-inconsistent, for otherwise T* 
could not be causally normal. Therefore we have 
T* lk (t*(x) & -y*(x)). (11)

--- Page 124 ---

118 Gerhard Schurz 
Together with (11), (9) gives us the following: 
T* It (Ay > (Rx <= T*(x)), as well as 
n} : T* lt Aj > (t*(x) @ R)). 
By summarizing the two statements in (12), we get 
T* I+ BR(T*), where BR(T*) := {Aj — (t*(x) @ Ri): 1<i<n)}. (13) 
In other words, T* entails for t* the same bilateral reduction sentences as T 
entails for ¢. 
From BR(T) in (6) and BR(T*) in (13), we can derive by propositional logic 
the intended correspondence relation: 
n} : BR(T)U BR(T") It Aj > (g(x) @ t*(x)), (14) 
and since A := A; V ... V Ag, (14) gives us 
(C): TUT* It BR(T) U BR(T*) It A > (g(x) & 1*(x)). 
Concerning Corollary 1: (C) entails the second conjunct of this corollary. 
For its first conjunct, we must show that BR(T) is consistent with T*. Proof by 
reductio ad absurdum. If T* U BR(T) were inconsistent, then T* |k ~ABR(T) 
and hence (i) T* Ik v{-(A; > (g(x) = Rj)): 1 < i < n} would hold.’ Since 
—(A — B) It A, we would obtain from (i) and propositional logic (ii): T* IF Ay v 
... VApg. (If the anonymous time variable t is present, as described in footnote 
6, we obtain, strictly speaking, T | St(Aj(x,t)v ... VAn(x,t)), which says that 
it follows from T that the system under consideration has been at least once in 
one of the circumstances A;.) But since T is causally normal, this is impossible. 
Concerning Corollary 2: Recall that we have defined t* = 1° for some fixed 
k. Since we can carry out the proof for every such k, we can obtain in step (13): 
Vk € {1 n} : T* lt Aj > (1(x) @ R)). (15) 
Step (15) implies by propositional logic: 
Vie {l n}: T* lt A; > (7;"(x) o T; (x)). (16) 
For example, A; — (tj < R,) and A; —> (ty <— R;) imply A; > (tf @ 75), 
etc. Since A := A; V ... V An, (16) gives us by propositional logic 
Vj¢zke {l n} : T* lt A (7(x) @ x(x). (17) 
° For Sa finite set of statements, AS denotes the conjunction and VS the disjunction of the elements 
of S.

--- Page 125 ---

Structural Correspondence Theorem 119 
So T* entails that all the theoretical descriptions t;*(x) (1 <i <n) are equivalent 
in the domain A. a 
Against our proof of Corollary 1, the consistency of BR(T) with T*, one 
may object that it relies on the logical properties of material implication, i.e., 
-=Vx(A — B) implies 3x(A A —-B). For a formulation with the help of nomologi- 
cal or counterfactual implications, this is not valid. But there are also other ways 
to establish the consistency of BR(T) with T*. For example, the consistency of 
BR(T) U T* can be proved from the following condition:'° 
Vx(y(x) <— t*(x)) is T"—consistent. (18) 
In almost all situations I can think of, (18) will be satisfied, because in the 
context of this condition the expression ¢g is deprived of all content which is 
not already contained in BR(T). In particular, (18) must hold whenever the 
following two stronger conditions are satisfied: (18a) g’s nonlogical terms are 
not contained in T*, and (18b) g’s possible extensions are logically unrestricted 
(as defined in footnote 11).!! 
The correspondence theorem presupposes that T’s theoretical description ¢ 
can be empirically characterized by means of bilateral reduction sentences. If 
y were only characterized by unilateral reduction sentences of the two forms 
Ai > (g(x) > Rj), and 
Ax > (Rx > ¢(x)), 
then the proof of this theorem would not work. It is an open question whether 
under this condition a weaker version of the correspondence theorem can 
be established. However, I think that theories that do not provide bilateral but 
merely unilateral empirical characterizations of their theoretical concepts occur 
only in early stages of science. In particular, as soon as reduction sentences are 
formulated in terms of quantitative concepts, the difference between unilateral 
and bilateral reduction sentences vanishes, because functions are right-unique. 
More precisely, the following two unilateral reduction sentences 
A; — Vr(g(x) = r > f(x) =r), and Aj > Vr(f(x) = r > g(x) =r) 
are mutually logically equivalent, and are both equivalent with 
Aj > (g(x) = f(x)), 
10 Proof: By (18) there exists a model M for T* U ¥x(y(x)<+r*(x)). T* entails BR(T*), whence T* 
Vx(y(x)<>r*(x)) entails BR(T), since BR(T) = BR(T*)[g/rt*]. Thus M verifies T* U BR(T), and 
so T* U BR(T) is consistent. 
That g’s possible extensions are logically unrestricted means that for every domain D and X C 
D* (where ¢ has k free variables) there exists an interpretation I such that I(g) = X. Given a 
model M = (D,I) of T*, then by condition (18a + b) we can expand M to a model M’ for g, 
which assigns to g the same extension as to t*. This entails condition (18).

--- Page 126 ---

120 Gerhard Schurz 
which is in turn logically equivalent with the bilateral reduction sentence 
Aj + Vr(g(x) =ro f(x) =f). 
At this point let us reflect on the ontological interpretation of the correspondence 
relation (C). Of course, (C) is not meant to say that whenever T’s intended model 
is realized (phlogiston leaves the substance), also T*’s intended model is realized 
(electrons move to the bonding partner). This would be a strange scenario of 
‘causal overdetermination’: two distinct causal scenarios were simultaneously 
realized, each sufficient to explain the observable behaviour. What (C) expresses 
is the possibility of a g—t*-reference shift: instead of the reference assigned to 
in T’s intended model (phlogiston leaves the substance), we can assign to g the 
reference of t* in T*’s intended model (electrons move to the bonding partner), 
under preservation of the strong empirical success of T. 
The claim of the correspondence theorem concerning the indirect reference 
of g means (by our definition in Section 1) that the g—r*-reference shift makes 
BR(T) indirectly true, i.e., true under the shifted interpretation (we identify 
T’s restriction Tyestr of Section 1 with BR(T)). The model-theoretic proof of g’s 
indirect reference and BR(T)’s indirect truth is straightforward: assume T is 
true in the intended (but ‘non-real’) model (D,I), T* is true in the intended (and 
‘real’) model (D*,I*) and I(2;) = I*(zj) for all (shared) non-theoretical terms 
x; of T and T*. This implies that D M D* ¥ @, and 77's extensions are taken 
from DN D*. The g-t*-shifted interpretation I’ of BR(T) differs from I in that 
I'(g) = I*(t*). By the correspondence theorem, T* | BR(T*), and so (D*,I*) 
verifies BR(T*), where BR(T*) results from BR(T) by replacing g by t* (recall 
step (13) of the proof). Hence (D*,I’) verifies BR(T). 
It is important that the expression g, which yielded T’s strong success, need 
not be a primitive term but may be composite, which leaves room for either 
an ontological underdetermination or the nonreference of T’s primitive terms. 
In the case of phlogiston, the complex terms ‘dephlogistication’ and ‘phlogis- 
tication’ can be related to corresponding electron donations or acceptances, 
but not so for the primitive term ‘phlogiston’. More generally, whenever T’s 
expression gy corresponds to t* of T*, but the ontology of the old theory T con- 
cerning the entities involved in g is incompatible with the contemporary theory 
T*, then it will be the case that ¢ is not a primitive but a complex expression 
of T, and T will contain certain theeretical assumptions about g’s inner struc- 
ture or composition, which, from the viewpoint of T*, are false—for example, 
that ‘dephlogistication’ is a process in which a special substance different from 
ordinary matter, called ‘phlogiston’, leaves the combusted substance. While T 
has a right model about g’s outer structure, i.e., the causal relations between the 
complex entity g and the empirical phenomena, it has a wrong model about 
g’s inner structure. This situation is typical even for most advanced contempo- 
rary theories. For example, we are confident that protons exist because they

--- Page 127 ---

Structural Correspondence Theorem 121 
are measurable common causes of a huge variety of BRs. But concerning the 
hypothesis about the inner composition of protons consisting of three quarks, 
things are different: physicists cannot measure quarks in isolation and, hence, 
are much more uncertain about their reality. 
My notions of the outer and inner structure of a complex expression or 
entity g reflect Worrall’s distinction between ‘structure’ and ‘content’ in an 
ontologically unproblematic way, which I think can be defended against the 
objections of Psillos and Papineau (recall footnote 1): the ‘structure’ that is 
preserved is y’s outer structure, while the ‘content’ that is not preserved is 
y’s inner structure. Often, the preserved outer structure of a T-expression 9 
does not only contain ¢’s relations to observable phenomena, but covers also 
y's relation to other T-theoretical terms g2 for which a T*-correspondence 
can also be established. In this sense, the relation between dephlogistication 
and phlogistication as inverse chemical reactions, as described in footnote 7, is 
preserved in modern chemistry. To elaborate the notions of ‘inner’ and ‘outer 
structure’ in a more general way would be an important task for future work. 
5 Further Historical Applications 
We now illustrate the correspondence theorem with some more historical ex- 
amples. First, let us see how phlogiston theory fits into the logical corset of 
the correspondence theorem. The phlogiston theory implies bilateral reduction 
sentences of the following form, for 1 < i< n,n > 2: 
If an input substance x of kind Xj; (e.g., a metal) is exposed to the influence 
of an input substance y of type Yj (e.g. hydrochloric acid), then x gets 
dephlogisticated (or phlogisticated, resp.) iff the chemical reaction produces 
output substances z = f(x,y) of type Z; = f(X;, Yj) (e.g., the metal dissolves 
and inflammable air evaporates). 
Given that modern chemical oxidation and reduction theory entails the 
empirical consequences of these bilateral reduction sentences that make up the 
strong success of phlogiston theory, the correspondence theorem entails that 
the two theories together must imply a correspondence relation, saying: 
If x is an input substance of one of the kinds X; and is exposed to the 
influence of chemical input substances y of type Yj, then x gets dephlogis- 
ticated (or phlogisticated, resp.) during the reaction iff this reaction satisfies 
a certain theoretical description in terms of modern chemistry 
and we have found such a description, namely the following: 
... iff during this reaction the atoms/molecules of x donate electrons to 
(or accepts electrons from, resp.) the atoms/molecules of y during the 
formation (or breaking, resp.) of a chemical bond.

--- Page 128 ---

122 Gerhard Schurz 
Another example from Laudan’s list is the caloric theory of heat (cf. Carrier 
[2004], pp. 151ff). According to this theory (which was, for example, believed 
by Lavoisier), every material substance contains some amount of caloric, and 
this amount is responsible for the heat or temperature of the substance—the 
more caloric it contains, the hotter it will be. Thereby caloric was assumed to 
be a substance consisting of weightless particles. While the particles of material 
substances attract each other in a substance-specific way, which is demon- 
strated by the forces of cohesion, the caloric particles repel each other, which is 
confirmed by thermal expansion, i.e., by the fact that (almost) all substances ex- 
pand in their volume when their temperature and hence their amount of caloric 
increases. In the solid state of substances, the attractive forces among the ma- 
terial particles dominate the repulsive forces among the caloric particles, and 
this holds the solid substance together. In the fluid state, the repulsive forces be- 
tween the caloric particles become stronger but not yet dominant. Finally, in the 
gaseous state these repulsive forces become completely dominant, so that the 
attractive forces between the material particles are negligible. These principles 
of the theory of caloric imply that the thermal expansion of gases, that is, the 
dependence of their volume on their temperature, should be entirely caused by 
the increase of caloric and, hence, should be the same in all gases, independent 
of their material nature. This was a prediction of a novel phenomenon, which 
was confirmed independently by John Dalton and Joseph-Louis Gay-Lussac 
in 1802 in their phenomenological gas law, which asserts that under constant 
pressure, the volume of any (sufficiently ideal) gas is proportional to its absolute 
temperature. 
The modern theory of thermal expansion is based on the kinetic theory of 
gases. According to this theory, a special immaterial substance such as caloric 
does not exist; temperature is nothing but the mean kinetic energy of the or 
S 
molecules. In the gaseous state, the distance between the molecules of the gas 
is so large that the volumes of its molecules and the attractive forces between 
its molecules are negligible. Therefore gases with equal temperature under the 
same pressure will have the same volume. 
In the case of caloric theory we enter the same situation as in phlogiston 
theory, namely that an empirical identification of caloric particles was impossi- 
ble. The crucial theoretical expression of caloric theory, which was empirically 
measurable and did the relevant work, was the amount of caloric particles con- 
tained in a substance that repe/ each other. What corresponds to this theoretical 
expression in modern theory is the mean kinetic energy of the molecules. So we 
have the following correspondence relation:

--- Page 129 ---

Structural Correspondence Theorem 123 
Correspondence relations between the caloric theory and modern physical 
chemistry: 
The amount of caloric particles in a substance X = the mean kinetic energy 
of X’s molecules. 
The repulsion force between the caloric particles in X = the expansion 
forces of X’s molecules, which in the gaseous state correspond to the pressure 
of X. 
These correspondence relations explain the strong success of the caloric 
theory in the described domain of application (though the caloric theory was 
less successful in other domains of application; cf. Psillos [1999], 115ff.). 
The quantitative version of this correspondence follows from the famous 
Maxwell—Boltzmann correspondence between the (absolute) temperature T of a 
gas and the mean kinetic energy of its molecules: 
2-Na m-v- 
id “olen aaa (19) 
Here A describes the empirical conditions for approximately ideal gases 
(high temperatures), mo is the mean kinetic energy of one gas molecule; R 
(Rydberg’s constant) expresses the uniform rate of thermal expansion, and Na 
is Avogadro’s number. The standard derivation of (19) is remarkably similar 
to the proof of our general logical theorem: the laws for the average pressure 
of the gas molecules (which are derivable from the theory of mechanics) entail 
2-Na . m-v i — has the same measurement that the complex mechanical expression 
conditions as the temperature T in the phenomenological gas law.'? From (19) 
together with the quantitative law of caloric theory (20), which relates the 
amount of caloric x with absolute temperature 
A — (x =k-T) (20) 
(where k is a gas-independent proportionality constant), we obtain the follow- 
ing quantitative correspondence between caloric and mean kinetic energy: 
2-k-Na m- v2 
A- aad x al > |: (21) 
I merely indicate how my account applies to the Fresnel-Maxwell case. Here, 
the domain of application A are incident, reflected and refracted light beams 
'2 Written as a measurement law for temperature, the phenomenological gas law has the form 
(i) A> (T= pe ) (V volume, p pressure, n mole number). The mechanical law for the pressure 
of the gas molecules can be rewritten into the isomorphic form (ii) A > oS = be ). 
(i) + (ii) entail the Maxwell—Boltzmann correspondence (19). (Cf. Barrow [1966], Chapter 2.2). 
The phenomenological gas law (i) contains the strong success shared by caloric theory and kinetic 
gas theory. Logically speaking, it unifies BRjs of the form Aj — (T = re ) for different kinds of 
gases i by the postulate of a uniform thermal expansion rate: Vi,j: Rj = Rj, which was crucial for 
the entailment of novel predictions 
m-V

--- Page 130 ---

124 Gerhard Schurz 
at the interface of two media, their angles and their intensities. Both theories 
entail the same non-theoretical equations relating these expressions. Worrall 
({1989]) and Psillos ({1995]) have worked out (though they disagree on other 
points) that the non-theoretical notion of the intensity of light has a direct 
theoretical correlate in both theories: in Fresnel’s mechanical wave theory it 
is directly proportional to the square of the oscillation velocity of the ether 
molecules (Psillos [1995], p. 36), and in Maxwell’s electromagnetic theory of 
light it is directly proportional to the square of the oscillating strength of the 
electromagnetic field (cf. Young and Freedman [1996], p. 1036f). So we have 
a correspondence relation between the oscillation velocity of ether molecules 
in Fresnel’s theory and the oscillation strength of the electromagnetic field in 
Maxwell’s account. The application of my account to further examples (e.g., 
Kepler vs. Newton, classical vs. quantum mechanics) remains to be carried out 
in the future. 
6 Discussion of the Correspondence Theorem: Objections and 
Replies 
Objection 1: The non-realist (e.g., van Fraassen [2006], p. 298, second section) 
will also grant that the old theory T and the contemporary theory T* are 
logically related to each other insofar as a certain class of correct empirical 
predictions, denoted by S (for ‘success’), is logically entailed both by T and by 
T*. Does your correspondence theorem say more than this? 
Reply 1: Yes, in many ways. That the two theories T and T* entail the same (true) 
empirical predictions S implies nothing about a logical relationship between the 
theoretical parts (axioms) of T and T*, let alone a logical relation between some 
T-theoretical and T*-theoretical expressions. In contrast, the correspondence 
theorem establishes an equivalence between the T-theoretical expression g and 
some T*-theoretical expression t* in the partitioned domain A = A; U...UAy, 
of T’s success. This surprising result depends crucially on the satisfaction of the 
two explained requirements (1) and (2) concerning the predecessor theory T: 
(1) the class S is a case of strong (potential) success, which by definition means 
that S is a set of conditionals of the form (Aj A +Rj) > (Aj > +R)j) fori ¥j 
€ {1, ...,n}; and (2) this strong (potential) success S is yielded by a (possibly 
compound) theoretical expression g of T by means of the bilateral reduction 
sentences BR(T): (Aj — (g(x) — Rj)) (1 < i < n), which follow from T. From 
the satisfaction of requirements (1) and (2), together with the satisfaction of 
the rather self-evident requirements (0) (T and T* share their non-theoretical 
terms), (3) (T* entails S in a T*-dependent way) and (4) (T and T* are causally 
normal), the correspondence theorem follows. 
Objection 2: Requirements (1) and (2) express intricate formal conditions, but 
what is their philosophical substance?

--- Page 131 ---

Structural Correspondence Theorem 125 
Reply 2: Requirement (1) of strong (potential) success expresses the capability 
of T to produce novel predictions: from what was observed in one domain of 
application (Aj A Rj), one may infer what will happen in another and potentially 
novel domain of application (A; — R;). Requirement (2) expresses in its causal 
interpretation that the entity (or property) posited by g figures as a common 
cause of the mutually correlated regularities or dispositional properties ‘if Aj, 
then R;’ (i.e., g(x) — (Aj > Rj)), ina way that allows g’s empirical measurement 
(i.e., Aj A Rj > g(x)). In (Schurz [2008], Section 7.1) it is argued at length that 
it is exactly this common cause property that distinguishes scientific abductions 
to theoretical entities from speculative abductions: while typical speculations 
postulate for each new phenomenon a new kind of theoretical cause, science 
introduces new theoretical entities only if they figure as common causes of 
several intercorrelated phenomena. If g were characterized by only one BR- 
sentence A; — (g(x) <— R)), the derivation of the correspondence principle 
would be impossible. What the proof of the correspondence theorem amounts to 
from the causal viewpoint is the following: if g of T yields T’s strong (potential) 
success by playing the role of a measurable common cause of the correlated 
dispositional properties ‘if A;, then Rj’, and T* entails T’s success in a T*- 
dependent way (where both T and T* are causally normal), then also T* must 
contain some expression t* whose designatum figures as a measurable common 
cause of the correlated dispositional properties. This is the reason why in the 
domain A, g’s reference can be understood as that entity/ property which in T* 
is denoted by t*. 
Objection 3: What is the exact part of the old theory T which can be said to 
have reference within the ontology of T*, or which is preserved in T*? And 
whatever this ‘part’ may be, isn’t its determination only possible ex post, i.e., 
when the new theory T* is already known, by way of ad hoc manoeuvres? (this 
is van Fraassen’s challenge in [2006], p. 290). 
Reply 3: No. According to the correspondence theorem, the respective ‘part’ 
of T can be determined in advance, without knowing the new theory T*. Every 
theoretical expression g of a (causally normal) theory T which satisfies the re- 
quirements (1) and (2), together with the bilateral reduction sentences BR(g,T) 
for g, is such a ‘part’ of T. It corresponds to some T*-expression t*, for any 
(causally normal) theory T* which entails T’s strong (potential) empirical suc- 
cess in a T*-dependent way. Hence if the theory T* is (approximately) true, then 
the ‘part’ of T expressed by the bilateral reduction statements BR(g,T) and ev- 
erything that follows from it is indirectly true in the sense explained in Section 
1, i.e., true under the g—t* reference shift, which assigns t*’s interpretation to 
gy. I regard this notion of ‘indirect truth’ as a scientifically important kind of 
‘partial’ truth.

--- Page 132 ---

126 Gerhard Schurz 
Let me compare the ‘reference shift’ of my correspondence theorem with 
Psillos’ description of a reference shift. According to Psillos ({1999], p. 296), 
y and t* denote the same entity iff (a) their putative reference plays the same 
causal role in a network of phenomena, and (b) the core causal description of t* 
takes up the kind-constitutive properties of the core causal description of g. In 
my account, g and t* can be understood to denote the same entity iff both g and 
t* play the role of a measurable common cause of a set of empirical regularities 
as described by the same (or: isomorphic) bilateral reduction sentences BR(T) 
and BR(T*). So, instead of Psillos’ ‘same causal role’ condition, I have the 
more precise ‘isomorphic common cause’ condition. However, I do not assume 
that the ‘kind-constitutive properties’ of the core causal description of g in T 
are preserved in T*, since many of these kind-constitutive properties of g will 
belong to g’s inner structure which—as I have argued—is typically not preserved 
in T*. In conclusion, Psillos’ preservation-requirement for kind-constitutive 
properties seems to be rather problematic: contra Psillos ({1999], p. 158) I think 
that even the luminiferous ether had one kind-constitutive property which was 
not taken over by Maxwell’s electromagnetic theory, namely its composition of 
ether molecules, whose displacement velocity is proportional to the amplitude 
of light. 
The content-part of T which is preserved in T* is BR(T) and what follows 
from it in domain A. This is logically underpinned by the observation that 
the translation of g into t* preserves the following kinds of consequences, 
where S(¢) is a sentence about gy, and S(t*) is the analogous sentence about t* 
(obtained by substituting r* for g)!?: 
(Content—preservation) If A, BR(T) !+ S(g), then A, BR(T*) It S(z*). 
However, the content part of T which is preserved in T* is not a simple con- 
junctive part of T’s axioms. It is a ‘structural’ content-part of T. This contrasts 
with Psillos’ ‘theoretical constituents’, which seem to be such conjunctive parts 
of the given theory (cf. [1999], p. 80f). Lyons ([2006]) has shown that, contra 
Psillos, many conjunctive parts of Kepler’s theory T, which were used in the 
derivation of Kepler’s area law, can in no way be said to be preserved in the 
ontology of modern physics. The preserved content-part of T in my account is 
not beset by Lyons’ objections. The shifted interpretation of BR(T) within T*’s 
ontology presupposes forgetting T’s hypotheses about ¢’s inner structure and 
considering only g’s relations to the observable phenomena. Properly speak- 
ing, the preserved content-part BR(T) has to be understood as a Ramsey-type 
'3 The proof assumes that ¢’s possible extensions are logically unrestricted in the sense explained 
in footnote 11. Proof by contraposition: Assume A, BR(T*) |F S(r*) does not hold. Then there 
is a model M = (D,J) satisfying A and BR(T*) but falsifying S(r*). By our assumption, we can 
expand M to M’ for ¢ by assigning the extension I(t*) to g. The model M’ verifies BR(T) and 
falsifies S(y). So, A, BR(T) |F S(@) would not hold, too.

--- Page 133 ---

Structural Correspondence Theorem 127 
existential quantification in which one quantifies over g as a whole: if BR(T) 
has the form S(g), where g has the form f(z) (e.g., release-of-phlogiston), then 
what is preserved within T*’s ontology is not 3XS(f(X)) but the logically weaker 
4IXS(X). It is an open question that deserves future investigation whether this 
preservation of structural content-parts can be adequately captured by one of 
the existing accounts of truthlikeness. 
Objection 4: Is the correspondence theorem analytic or synthetic? And if it is 
analytic, how can that be? For example, a ‘witchcraft theory’, which has got 
some empirical predictions right, cannot correspond to any entity posited by 
modern science—so it seems that the correspondence theorem can’t hold for 
analytic reasons alone. 
Reply 4: The correspondence theorem has the form ‘if assumptions, then corre- 
spondence’. It holds on analytic reasons, because it is a /ogical theorem. But the 
truth of the ‘assumptions’—which consist in the satisfaction of requirements 0 
to 4— is, of course, a synthetic question. And my claim that these requirements 
are often (and typically) satisfied for scientific theories is an empirical-historical 
discovery. Therefore, my conclusion that usually (and typically) certain theo- 
retical ‘parts’ of past theories are preserved in later theories depends on analytic 
argument as well as on empirical-historical discovery. 
The worry about correspondence for ‘witchcraft theories’ is related to the 
following challenge of Laudan ([1984], p. 160), Psillos ({1999], p. 292) and 
van Fraassen ({2006], p. 301): a notion of correspondence would certainly be 
trivial if it allowed all sorts of outdated theories, even medieval astronomy or 
Aristotelian physics, to correspond ‘in some way’ to contemporary science. This 
complaint is correct, but it does not apply to my account, which is based on 
the condition that the outdated theory T satisfies the requirements (1) and (2). 
Aristotelian physics is a case in point. Van Fraassen remarks that ‘Aristotelian 
science felt free to multiply theoretical terms for so-called occult properties’ 
({2006], p. 281). More specifically, Aristotelian physics did not explain different 
kinds of motion in terms of common causes, but posited a distinct cause for 
each kind of motion: ‘earthly’ sublunary bodies (when free of ‘violent’ forces) 
tend to fall down, because their ‘natural place’ is the centre of the earth; light 
sublunar entities (like flames) move upwards because their natural place is the 
heaven; etc. Logically speaking, the theoretical concept g;(x) := ‘attraction of 
x towards the natural place g;’ is not characterized by a multitude of bilateral 
reduction sentences; rather each special theoretical term ¢j; is characterized by 
just one bilateral reduction sentence of the form: Aj — (g)(x) <> Rj), where A; 
describes the type of moved entity, yg; the natural place of x, and R; the direction 
of the movement. Hence conditions (1) and (2) do not apply to Aristotelian 
physics, and therefore correspondence to, e.g., the gravitation force of modern 
physics cannot be established. The same is true for most speculative theories of 
the medieval era.

--- Page 134 ---

128 Gerhard Schurz 
On the other hand, whenever the outdated theory T satisfies requirements (1) 
and (2), correspondence to a contemporary theory T* (satisfying the other re- 
quirements) exists, even if the outdated theory uses terms with ‘witchcraft-like’ 
meaning-components. Here is an example (cf. Jammer [1957], Chapter 5): in his 
early astronomical writings Kepler stipulated a force, which holds the planets 
on their orbits around the sun, but he conceived this force as a kind of soul 
(‘anima’). Later Kepler argued that this force is mathematically describable. 
Newton, who found out the correct mathematical description of the gravita- 
tional force, was aware that gravitation, being an action-at-a-distance, resists a 
mechanical explanation. Newton continued to be agnostic about the ‘nature’ 
of gravitation and emphasized that his theory is compatible with a material as 
well as a spiritual (non-material) conception of it (cf. Jammer [1957], p. 137). 
The message is this: even if gravitational force is interpreted as a spiritual (i.e., 
‘witchcraft-like’) entity, this does not prevent its correspondence to the modern 
classical concept of force, as long as it is mathematically described in a way 
that yields correct predictions of movements of bodies in gravitational fields. 
7 Consequences for Scientific Realism and Comparison with Other 
Positions 
Does my correspondence theorem support scientific realism? If yes, which kind 
of scientific realism? Does it improve the justifications of scientific realism which 
have been proposed in philosophy of science? In this section, I will answer these 
questions. By way of comparing my account to other accounts, I will try to 
show that the answer to the first and the last question is positive. 
7.1 Comparison with constructive empiricism 
In van Fraassen’s account, what is preserved in theory-succession is merely the 
entailment of strong empirical success. In contrast, my correspondence theorem 
entails that also on the theoretical level, a structural part of T’s content (as 
specified above) is preserved in T*. Van Fraassen ({2006], p. 298) thinks that 
the mere preservation of empirical predictions from T to T* is sufficient to 
explain why the old theory T was successful. In my view, this is not enough 
for an explanation of T’s success. My account explains why the theory T had 
strong empirical success as follows: because that expression of T which yielded 
T’s success corresponds to a theoretical expression of T* that yields the same 
success within T*. 
On the other hand, what my account has in common with van Fraassen’s 
account is a sceptical attitude concerning the unrestricted Putnam—Boyd ver- 
sion of the no miracles argument, NMA for short (cf. van Fraassen [2006], p. 
296). There are both strong theoretical and strong empirical arguments that

--- Page 135 ---

Structural Correspondence Theorem 129 
undermine the NMA. The major empirical argument against the NMA is Lau- 
dan’s pessimistic meta-induction from which this paper started in Section |. The 
major theoretical argument against the NMA in my view is the fact explained 
in Section 6 (replies 1 and 2): if the theory T does not satisfy requirements 
(1) and (2), but speculates for each kind of phenomenon a special cause, then 
there is simply no way to infer from its empirical success something about the 
truth-status of its theoretical part. Therefore I think the unrestricted version of 
the NMA is simply fa/se. Only a restricted version of it is tenable. My account 
attempts to show why and under which restriction the NMA is defensible. This 
brings me to the next subsection. 
7.2 Major difference from standard scientific realism 
Most scientific realists base their arguments on some version of the NMA. In 
contrast, my account in no place presupposes the NMA or some other form of 
inference to the best explanation (IBE for short). My account is based on an 
analytic theorem. Together with empirical-historical evidence concerning the 
truth of the theorem’s assumptions, my account establishes independently of 
the NMA or some other form of IBE as to why a specifically restricted version 
of the NMA can be defended. Herein I see the major difference and advantage 
of my account to scientific realism. 
In Section 7.1, I have sketched two strong arguments against the reliability 
of the NMA in its unrestricted form. There are also other reasons why a justifi- 
cation of scientific realism that does not presuppose the NMA is desirable. For 
example, many authors have pointed out that the standard justification of the 
NMA by an IBE is circular, because the reliability of IBE is in turn justified by 
the NMA (cf. the discussion in Psillos [1999], pp. 81ff). Some NMA-adherents 
have replied that the ‘rule-circularity’ involved in the justification of NMA is 
a nonvicious kind of circularity. I strongly doubt this point: Salmon ({1957], 
p. 46) and Achinstein ({1974], p. 137) have convincingly demonstrated that ab- 
surd rules such as the rule of anti-induction, or the obviously invalid rule “No 
F is G, Some Gs are Hs; therefore: All Fs are Hs’ can be justified in a ‘rule- 
circular’ manner. Hence, ‘rule-circularity’ is as vicious as ‘premise-circularity’. 
In conclusion, there are many independent reasons why a justification of scien- 
tific realism that does not presuppose the reliability of NMA is desirable—and 
my account attempts to offer such a justification. 
7.3 From minimal realism and correspondence to scientific realism 
How does my correspondence theorem justify scientific realism without pre- 
supposing the NMA? First of all, the correspondence theorem alone justifies 
only a conditional realism: if one assumes the (approximate) realistic truth of

--- Page 136 ---

130 Gerhard Schurz 
the presently accepted theory T*, then also outdated theories T satisfying the 
requirements (1) and (2) contain a (theoretico-structural) content-part which 
is indirectly true and hence partially true (since ‘indirect truth’ is an important 
case of ‘partial truth’). This conditional realism weakens Laudan’s pessimistic 
meta-induction. But conditional realism alone is not sufficient to justify scien- 
tific realism. For someone who, on independent epistemological grounds, does 
not believe that contemporary or future scientific theories are approximately 
true, this conditional realism cannot tell anything about the partial truth of 
earlier theories. 
But the situation changes if one makes in addition the following assumption 
of minimal realism (MR): 
(MR): The observed phenomena are caused by an external reality whose 
structure can possibly be represented in an approximate way by an ideal the- 
ory T*, which is causally normal, entails the observed phenomena in a T*- 
dependent way, and whose language is in reach of humans’ logico-mathematical 
resources. 
(MR) is a minimal realist assumption because it merely says that an approxi- 
mately true theory describing the reality that causes the observed phenomena is 
possible—independent of whether humans will ever be able to find this theory. 
Together with (MR), the correspondence theorem entails that the abductive in- 
ference from the strong empirical success of theories to their partial realist truth 
is justified. For if (MR) is true, then there exists an approximately true ideal 
theory T*, which need not be known to us and preserves all of the strong em- 
pirical success that our accepted theories have. So the correspondence theorem 
implies that every (theoretico-structural) content-part of our contemporary 
theories satisfying the requirements (1) and (2) corresponds to a content part 
of the ideal theory T*, and hence is indirectly true. By this line of argumenta- 
tion, the abductive inference from strong success to partial (approximate) truth 
is replaced by the analytic inference from (MR) plus strong success to partial 
(indirect) truth. In this way my account provides an independent justification 
of NMA and IBE. 
7.4 Comparison with particular realistic positions 
What kind of scientific realism is supported by (MR) plus the correspondence 
theorem, in comparison with other positions? This kind of scientific realism is 
weaker than the scientific realism defended by Putnam ([1975]), Boyd ({1984]) 
and Psillos ({1999]) in three respects. First, it does not assert partial realistic 
truth for all kinds of predictively successful theories, but only for theories 
satisfying requirements (1) and (2). Second, the partial truth is not asserted 
for some conjunctive part of the (axioms of the) theory, but for a part that

--- Page 137 ---

Structural Correspondence Theorem 131 
is ‘structural’ in nature (cf. reply 3 of Section 6). Third, the partial truth has 
been called indirect truth because it is obtained from a g—t* reference shift, 
which treats the theoretical expression ¢ of T as primitive, abstracting from the 
theory’s hypotheses about g’s inner structure. For this reason, the scientific 
realism supported by my account is compatible with a certain amount of 
empirical underdetermination, even in our most advanced theories (quantum 
mechanics is a case in point; cf. Ladyman [1998], pp. 418f). 
On the other hand, the kind of scientific realism supported by my account 
is stronger than that of Worrall ({1989]). It shares with Worrall the ‘structural’ 
nature of that part of the theory that is asserted to be (indirectly) true, but does 
not assume that this structural part refers merely to a mathematical structure; it 
corresponds rather to a certain real structure among real entities or properties. 
My kind of scientific realism is also stronger than that of Carrier ({2004], 
pp. 154f), who merely assumes that strongly successful theories have got the 
classification of the empirical phenomena right, while I argue that also a certain 
content-part of their theoretical superstructure is (indirectly) true. 
The fact that the kind of scientific realism supported by my account is weaker 
than that of Putnam, Boyd or Psillos may be seen as a disadvantage, but this 
disadvantage is the price of the two major advantages of my account. First, my 
justification of scientific realism does not presuppose the reliability of NMA or 
IBE. Second, in my account the ‘structural’ parts of a strongly successful theory 
can be specified in advance (independent of the successor theory) and with 
logical precision. These are my reasons for hoping that my account of scientific 
realism is better supported and more credible than those strong accounts of 
scientific realism that are based on dubious versions of the NMA. 
Acknowledgements 
For valuable comments I am indebted to Timothy Lyons (in particular for 
Section 6), Ioannis Votsis, Martin Carrier, Paul Hoyningen-Huene, James 
Ladyman, Theo Kuipers, Hannes Leitgeb, Johan van Benthem, Ulises 
Moulines and an unknown referee. 
Philosophy Department 
University of Duesseldorf 
Universitaetsstrasse 1, Geb. 23.21 
D-40225 Duesseldorf, Germany 
schurz@phil-fak.uni-duesseldorf.de 
References 
Achinstein, P. [1974]: ‘Self-Supporting Inductive Arguments’, in R. Swinburne (ed.), The 
Justification of Induction, Oxford: Oxford University Press, pp. 134-8.

--- Page 138 ---

132 Gerhard Schurz 
Balzer, W., Moulines, C. U. and Sneed, J. D. [1987]: An Architectonic for Science, Dor- 
drecht: Reidel. 
Barrow, G. M. [1966]: Physical Chemistry, New York: McGraw-Hill. 
Boyd, R. [1984]: ‘The Current Status of Scientific Realism’, in J. Leplin (ed.), Scientific 
Realism, Berkeley: University of California Press, pp. 41-82. 
Carnap, R. [1936]: ‘Testability and Meaning (Part I)’, Philosophy of Science, 3, 
pp. 419-71. 
Carrier, M. [2001]: ‘Changing Laws and Shifting Concepts’, in P. Hoyningen-Huene and 
H. Sankey (eds), Incommensurability and Related Matters, Dordrecht: Kluwer, pp. 
65-90. 
Carrier, M. [2004]: ‘Experimental Success and the Revelation of Reality: The Miracle 
Argument for Scientific Realism’, in M. Carrier, G. Kiippers, J. Roggenhofer and 
P. Blanchard (eds), Knowledge and the World: Challenges Beyond the Science Wars, 
Heidelberg: Springer, pp. 137-61. 
Hintikka, J. [1988]: ‘On the Incommensurability of Theories’, Philosophy of Science, 55, 
pp. 25-38. 
Hoyningen-Huene, P. [1993]: Reconstructing Scientific Revolutions: Thomas Kuhn's Phi- 
losophy of Science, Cambridge: Cambridge University Press. 
Jammer, M. [1957]: Concepts of Force, Cambridge, MA: Harvard University Press. 
Ladyman, J. [1998]: “What Is Structural Realism?’, Studies in the History and Philosophy 
of Science, 29, pp. 409-24. 
Ladyman, J., and Ross, D. [2007]: Every Thing Must Go. Metaphysics Naturalized, (with 
D. Spurrett and J. Collier), Oxford: Oxford University Press. 
Laudan, L. [1981]: ‘A Confutation of Convergent Realism’, reprinted in D. Papineau 
(ed.), The Philosophy of Science, Oxford: Oxford University Press, pp. 107-38. 
Laudan, L. [1984]: ‘Discussion: Realism Without the Real’, Philosophy of Science, 51, 
pp. 156-62. 
Laudan, L. and Leplin, J. [1991]: ‘Empirical Equivalence and Underdetermination’, 
Journal of Philosophy, 88, pp. 449-72. 
Lyons, T. D. [2006]: ‘Scientific Realism and the Stratagemata de Divide et Impera’, 
British Journal for the Phiiosophy of Science, 57, pp. 537-60. 
Oxtoby, D., Gillis, H. and Nachtrieb, N. [1999]: Principles of Modern Chemistry, Orlando, 
FL: Saunders College Publishing. 
McCann, H. G. [1978]: Chemistry Transformed: The Paradigm Shift from Phlogiston to 
Oxygen, Norwood, NJ: Ablex Publishing Corporation. 
Papineau, D. [1996]: ‘Introduction’, in D. Papineau (ed.), The Philosophy of Science, 
Oxford: Oxford University Press, pp. 1—20. 
Psillos, S. [1995]: ‘Is Structural Realism the Best of Both Worlds?’, Dialectica, 49, 
pp. 15-46. 
Psillos, S. [1999]: Scientific Realism. How Science Tracks Truth, London and New York: 
Routledge. 
Putnam, H. [1975]: ‘What is Mathematical Truth?’, in H. Putnam (ed.), Mathematics, 
Matter and Method, Cambridge: Cambridge University Press, pp. 60-78.

--- Page 139 ---

Structural Correspondence Theorem 133 
Salmon, W. C. [1957]: “Should We Attempt to Justify Induction?’, Philosophical Studies, 
8, pp. 45-7. 
Schurz, G. [2008]: ‘Patterns of Abduction’, Synthese, 164, pp. 201-34. 
Sneed, J. D. [1971]: The Logical Structure of Mathematical Physics, Dordrecht: Reidel 
Van Fraassen, B. [1980]: The Scientific Image, Oxford: Clarendon Press. 
Van Fraassen, B. [2006]: ‘Structure: Its Shadow and Substance’, British Journal for the 
~ 
Philosophy of Science, 57, pp. 275-307. 
Votsis, I. [2007]: ‘Uninterpreted Equations and the Structure-Nature-Distinction’, Philo- 
sophical Inquiry, 29, pp. 57-71 
Worrall, J. [1989]: ‘Structural Realism: The Best of Both Worlds?’, Dialectica, 43, 
pp. 99-124. 
Young, H. D., and Freedman, R. A. [1996]: University Physics, 9th edition, Reading, 
MA: Addison-Wesley.

--- Page 141 ---

Brit. J. Phil. Sci. 60 (2009), 1 
Newman’s Objection 
Peter M. Ainsworth 
ABSTRACT 
This paper is a review of work on Newman’s objection to epistemic structural realism 
(ESR). In Section 2, a brief statement of ESR is provided. In Section 3, Newman’s 
objection and its recent variants are outlined. In Section 4, two responses that argue 
that the objection can be evaded by abandoning the Ramsey-sentence approach to ESR 
are considered. In Section 5, three responses that have been put forward specifically to 
rescue the Ramsey-sentence approach to ESR from the modern versions of the objection 
are discussed. Finally, in Section 6, three responses are considered that are neutral with 
respect to one’s approach to ESR and all argue (in different ways) that the objection 
can be evaded by introducing the notion that some relations/structures are privileged 
over others. It is concluded that none of these suggestions is an adequate response to 
Newman’s objection, which therefore remains a serious problem for ESRists. 
Introduction 
Epistemic Structural Realism 
2.1 Ramsey-sentences and ESR 
2.2 WESR and SESR 
The Objection 
3.1 Newman's version 
3.2 Demopoulos and Friedman’s and Ketland’s versions 
Replies that Abandon the Ramsey-Sentence Approach to ESR 
4.1 Redhead’s reply 
4.2 French and Ladyman’s reply 
Replies Designed to Rescue the Ramsey-Sentence Approach 
5.1. Zahar’s reply 
5.2 Cruse’s reply 
5.3 Melia and Saatsi’s reply 
Replies that Argue that Some Structures/ Relations are Privileged 
6.1. A Carnapian reply 
6.2 Votsis’ reply 
6.3 The Merrill/ Lewis/ Psillos reply 
Summary 
The Author (2009). Published by Oxford University Press on behalf of British Society fer the Philosophy of Science. All rights reserved 
doi:10.1093/bjps/axn051 For permissions, please email: journals. permissions@oxfordjournals.org 
Advance Access published on January 19, 2009

--- Page 142 ---

Peter M. Ainsworth 
1 Introduction 
The first fully elaborated statement of epistemic structural realism (ESR) can 
be found in the work of Russell ({1912], [1927]) (although Worrall [1989], [1994] 
argues that the doctrine can also be found in the writings of Duhem [1906] 
and Poincaré [1903]); Russell himself abandoned the position in the face of 
Newman’s objection (Newman [1928]). The doctrine was revived four decades 
later by Maxwell ({1968], [1970a], [1970b]), who both coined the term ‘structural 
realism’ and introduced the Ramsey-sentence approach to the doctrine, which 
is adopted by most contemporary ESRists. 
Demopoulos and Friedman ([1985]) and Ketland ((2004]) have put forward 
variants of Newman’s objection aimed at the Ramsey-sentence approach to 
ESR and numerous authors (Cruse [2005]; French and Ladyman [2003]; Melia 
and Saatsi [2006]; Psillos [1999]; Redhead [2001]; Votsis [2003], [2004]; Zahar 
[2001], [2004]) have suggested a variety of ways in which one might attempt to 
evade the objection. 
This paper consists of five main sections. In Section 2, a brief statement 
of ESR is provided. In Section 3, Newman’s objection and its recent variants 
are outlined. In Section 4, two responses that argue that the objection can 
be evaded by abandoning the Ramsey-sentence approach to ESR are con- 
sidered. In Section 5, three responses that have been put forward specifically 
to rescue the Ramsey-sentence approach to ESR from the modern versions 
of the objection are discussed. Finally, in Section 6, three responses are con- 
sidered that are neutral with respect to one’s approach to ESR and all ar- 
gue (in different ways) that the objection can be evaded by introducing the 
notion that some relations/structures are privileged over others. It is con- 
cluded that none of these suggestions is an adequate response to Newman’s 
objection. 
2 Epistemic Structural Realism 
The ESRist upholds two main theses. On the one hand there is the ‘realist’ 
thesis: 
Mature scientific theories provide us with a substantial amount of knowl- 
edge about both the observable and the unobservable world.! 
' The qualification ‘mature’ is important: the ESRist, like the conventional realist, does not commit 
himself to realism with respect to all scientific theories. Moreover, the ‘knowledge’ that science is 
supposed to provide is taken to include claims that are not strictly true, but only approximately 
true. Spelling out what maturity and approximate truth are may not be easy (although Worrall 
[1989] has argued that it is straightforward to explicate maturity: he suggests that a theory 
is mature if and only if it correctly predicts an empirically confirmed result that it was not 
engineered to yield) but these issues will be held in abeyance throughout this paper.

--- Page 143 ---

Newman's Objection 137 
On the other hand there is the ‘structuralist’ thesis, which, as a very rough 
first approximation, we may state as follows: 
All we know of the unobservable world is its structure. 
2.1 Ramsey-sentences and ESR 
From a formal point of view, languages are built out of two types of term:* 
logical terms and non-logical terms. In a language of second-order logic these 
two groups consist of the following: 
[1] Logical terms: 
(i) logical connectives (*—’, *&’, etc.) 
(ii) quantifiers (“V’, “3”) 
(iii) individual and predicate variables (x;, x2,... and X;, X2,...) 
and possibly: 
(iv) the identity predicate (“= °) 
[2] Non-logical terms: 
(i) a number of names (a), a2, .. .) denoting objects 
(ii) a number of predicates (P;, P2, . . .) denoting properties and relations. 
The claim that ‘all we know of the external world is its structure’ might 
suggest that the ESRist thinks that our knowledge of the external world is 
purely structural (that it consists of only logical terms). Despite the way they 
sometimes talk, this is not a view held by any serious contemporary ESRist, 
but it is a view sometimes imputed to ESRists by their critics. 
The Ramsey-sentence approach to ESR was first proposed by Maxwell 
({1968]) and is adopted by most (but not all) modern ESRists. Let an ob- 
servational term be a non-logical term that refers to an observable object, 
property, or relation and a theoretical term be a non-logical term that refers to 
an unobservable object, property, or relation. The Ramsey-sentence of a theory 
is obtained from a sentence expressing the theory by first replacing the theo- 
retical terms (names and predicates) in the sentence with new variables (using 
the same variable for each occurrence of the same term, and different variables 
- Sometimes logicians use the word ‘term’ as a synonym for ‘name’. Throughout this paper it is 
used in a broader sense, as explicated here.

--- Page 144 ---

138 Peter M. Ainsworth 
for different terms). The resulting formula is then turned into a sentence (the 
theory’s Ramsey-sentence) by binding the variables with the appropriate exis- 
tential quantifiers (placed at the start of the formula, so that every occurrence 
of the same new variable is in the scope of the same quantifier). Note that, in 
general (as long as the original sentence contains at least one theoretical pred- 
icate) constructing the Ramsey-sentence of a theory will require a language of 
second-order logic.* 
Maxwell provides an example (Maxwell [1970a], p. 186). Consider the ‘the- 
ory’ expressed by the sentence: 
Y¥x([Ax&Dx] — JyCy), 
where ‘A’ and ‘D’ are ‘theoretical’ predicates such that ‘Ax’ means ‘x is a radium 
atom’ and ‘Dx’ means ‘x radioactively decays’, and ‘C’ is an ‘observational’ 
predicate such that ‘Cx’ means ‘x is a click in a suitably located Geiger counter’.4 
Its Ramsey-sentence is: 
IXSYVx([Xx&Yx] —> SyCy). 
The ESRist (if he takes the Ramsey-sentence approach to ESR), elaborates 
his structuralist thesis as follows: 
A theory’s Ramsey-sentence is as much as a theory reliably tells us about 
the world. 
Notice that, when it is elaborated this way, the ESRist’s structuralist thesis 
has the following corollary: 
The knowledge provided by our mature scientific theories consists (in its 
ultimate form) of statements constructed using only logical and observa- 
tional terms. 
This corollary is a claim the instrumentalist would also endorse. ESR re- 
mains distinct from instrumentalism because the instrumentalist claims that 
our theories provide knowledge only about the observable world, whereas the 
ESRist maintains that the Ramsey-sentences of our theories provide knowledge 
about both the observable world and about the structure of the unobservable 
world. 
> Zahar ((2001], p. 236) points out that we can construct an equivalent sentence using a first-order 
language that contains the predicates ‘being a set’ and ‘e’. However, we would of course have to 
leave these unRamseyfied, and it is hard to see what justification there could be for that, unless 
one was prepared to take them as logical predicates, like * = °. 
Of course this doesn’t express any real ‘theory’ and is, moreover, false: if a radium atom decays 
on Mars, there will not be a click in a suitably located Geiger counter, because there will not be 
a suitably located Geiger counter, but it will suffice to give the general idea. 
4

--- Page 145 ---

Newman's Objection 139 
At first sight, it may seem that as Ramsey-sentences are constructions formed 
using only logical and observational terms, they could not tell us anything about 
the unobservable world. This is certainly not true. As van Fraassen ([{1980], 
p. 54) notes, the claim that ‘there are entities that are not O; and not O; 
and ...’, where each QO; is an observational predicate and the sentence ascribes 
to the entities it refers to the negation of every O; in the language, successfully 
makes the claim that there are unobservables (which is a claim about the un- 
observable world) using only logical and observational vocabulary. (However, 
the essential claim of Newman’s objection is that Ramsey-sentences can’t tell 
us anything substantial about the unobservable world.) 
Given that ESR has been explicated in terms of Ramsey-sentences, one may 
wonder if ESR is any more ‘structural’ than conventional scientific realism: 
after all, the Ramseyfied version of a sentence is no more closely linked to 
the structures that satisfy it than is the original sentence. However, although 
there isn’t any particularly intimate relation between structural realism and 
structures (in the set-theoretic sense) the term ‘structural’ is not totally inap- 
propriate: the ESRist maintains that theoretical predicates (and the sets that 
provide the extensional interpretation of these predicates) should not be given 
an intensional interpretation, but should be treated purely extensionally, i.e. 
treated in a purely mathematical or ‘structural’ way. 
2.2 WESR and SESR 
ESR can be subdivided into two doctrines: weak ESR (WESR) and strong ESR 
(SESR) (as they will be called in this paper). SESR presupposes a particular 
metaphysical and epistemological doctrine called ‘indirect realism’. This is the 
view that although the external world exists, we do not have direct access to 
it. It implies that there is a distinction between the ‘internal world’ of our 
own consciousnesses, to which we have direct access, and the external world, 
to which we do not (this internal/external distinction is sometimes called the 
phenomenal/noumenal distinction or the mental/physical distinction). 
It is important to separate the internal/external distinction from the ob- 
servable/unobservable distinction. Within the indirect realist framework the 
observable/unobservable distinction may be roughly characterized as a dis- 
tinction between those external objects, properties, and relations (e.g. tables, 
redness”) that have a direct counterpart in internal experience and those ex- 
ternal objects, properties and, relations (e.g. quarks, strangeness) that do not. 
Because the phrase ‘direct counterpart’ is so vague this characterization does 
In fact, the indirect realist might say that we need to distinguish two ‘redness’ predicates: one 
that refers to the redness of internal objects (sense-data) and another that refers to the redness 
of external (physical) objects.

--- Page 146 ---

140 Peter M. Ainsworth 
not succeed in unambiguously drawing the intended distinction, but hopefully 
the general idea is clear enough. 
The essential difference between WESR and SESR is that while the WESRist 
thinks that theoretical terms (i.e. terms referring to unobservable objects, 
properties, and relations) need to be Ramseyfied (leaving Ramsey-sentences 
containing only logical and observational terms), the SESRist thinks that ex- 
ternal terms (i.e. terms referring to external objects, properties, and relations) 
need to be Ramseyfied (leaving Ramsey-sentences containing only logical and 
internal terms). The version of ESR outlined in the previous section is thus 
WESR. The SESRist’s structuralist thesis can still be stated as: 
A theory’s Ramsey-sentence is as much as a theory reliably tells us about 
the world. 
But because of his different approach to Ramseyfication, the SESRist’s struc- 
turalist thesis has a somewhat different corollary, viz: 
The knowledge provided to us by our mature scientific theories consists (in 
its ultimate form) of statements constructed using only logical and internal 
terms. 
In fact, although the example of Ramseyfication that was given in the previ- 
ous section is Maxwell’s own and it suggests WESR, he himself was a SESRist. 
He makes this quite clear when he states that ‘My own view [. . .] is that all items 
should be considered theoretical [meaning that terms referring to them should 
be Ramseyfied] unless they occur in direct experience; since I reject any form of 
direct realism, this means that the observable [meaning the things referred to 
by terms that do not need to be Ramseyfied] is instantiated only in inner events 
of observers’ (Maxwell [1970a], p. 181). 
On the face of it, however, Ramseyfying all terms except those that refer 
to items of internal experience will result in the Ramsey-sentences of most 
(if not all) theories being purely formal, and thus entirely devoid of empiri- 
cal content, because, on the face of it, most (if not all) theories do not deal 
with items of internal experience at all. Take for example the toy theory given 
above. Clicks in suitably placed Geiger counters are not items of internal ex- 
perience, so (taking the SESRist line) the predicates referring to them should 
also be Ramseyfied away, leading to the following Ramsey-sentence for the 
theory: 
AXAYIAZVx([Xx&Yx] — dyZy), 
which is purely formal, and therefore has no empirical content. 
The proponent of SESR can avoid this unwelcome conclusion by arguing 
that although most theories, in themselves, are not directly about internal

--- Page 147 ---

Newman's Objection 141 
experience, they are nonetheless connected with internal experience by auxiliary 
theories, which are always implicitly held. For example, we implicitly hold that 
we will have an experience of a sense-data Geiger counter click only if, (i) there 
is a (real) Geiger counter click, (ii) we are hallucinating Geiger counter clicks, 
(iii) we are dreaming about Geiger counter clicks, (iv) somebody is playing a 
practical joke, ... alongside the theory: 
Vx([Ax&Dx] — dyCy); 
we would thus also hold: 
4x(C*x & Eax) — (Ay[Cy] v Ha...), 
where a is a constant referring to oneself, “C*x’ means ‘x is a sense-data Geiger 
counter click’, “Exy’ means ‘x experiences y’, ‘Cx’ means the same as before and 
‘Hx’ means ‘x is hallucinating Geiger counter clicks’. The combined theory we 
hold is thus expressed by the sentence: 
Vx([Ax & Dx] > dyCy) & (Ax[C*x & Eax] — [By(Cy) v Ha...]). 
Taking the SESRist approach to Ramseyfication, one obtains something like 
the following Ramsey-sentence of the combined theory: 
IW3AXIYIAZ...Wx((Wx & Xx] > SyYy) & (Ax[C*x & Eax]— [ByYy v Za...]), 
which does contain some non-logical terms (*C*’, ‘E’ and ‘a’) and thus does 
not fail to make an empirical claim for lack of them. 
3 The Objection 
3.1 Newman’s version 
Newman ({1928]) takes Russell’s ESR to imply that the most that we can 
know about the external world is its structure. He ascribes this view to Russell 
on the basis of passages like the following (which Newman quotes in [1928], 
p. 144): ‘Thus it would seem that wherever we infer from perceptions it is only 
structure that we can validly infer; and structure is what can be expressed by 
mathematical logic’ (Russell [1927], p. 254), ‘The only legitimate attitude about 
the physical world seems to be one of complete agnosticism as regards all but 
its mathematical properties’ (Russell [1927], p. 270). Newman then launches 
the following objection to this view: 
Any collection of things can be organised so as to have the structure W 
[where W is an arbitrary structure], provided there are the right number of 
them. Hence the doctrine that on/y structure is known involves the doctrine 
that nothing can be known that is not logically deducible from the mere 
fact of existence, except (‘theoretically’) the number of constituting objects. 
(Newman [1928], p. 144, original emphasis)

--- Page 148 ---

142 Peter M. Ainsworth 
For example, being told that a system has domain D = {a, b, c} (where 
a, b, and c are arbitrary names for three distinct but unspecified objects) and 
instantiates a relation R = {(a, b), (a,c), (b, c)} tells us no more than that the 
system consists of three objects, because some elementary set-theory reveals 
that any three objects instantiate seven non-empty one-place relations, 511 
non-empty two-place relations (of which R is one) and 134,217,727 non-empty 
three-place relations.° Being told that they instantiate R is both trivial (insofar 
as it follows from some elementary set-theory) and perversely specific (insofar 
as R is just one of the 134,218,245 non-empty relations they instantiate). Thus 
being told that the system has structure (D, R) is being told no more than 
that it contains three objects, because any system containing three objects can 
be taken to have this structure, along with a vast number of other structures 
(any tuple whose first member is D and whose other members are amongst the 
134,218,245 relations instantiated by the members of D is a structure that can 
be taken to be possessed by any system containing three objects). 
The objection arises because our purely structural knowledge gives us only 
extensional information about the structure of the system: if we had an inten- 
sional interpretation of R, we would not have this problem. For instance, if 
we knew that R was the ‘heavier than’ relation (restricted to the system) then 
we would have some more useful information: we would know that the three 
objects in the system were of unequal weights. However, the claim that we 
have any such intensional information about the external world is exactly what 
Newman thinks that Russell denies. 
Newman considers two possible responses to this objection. The first is to 
distinguish ‘real’ relations from ‘fictional’ relations and assume that when the 
Russellian tells us what relations hold in a system he is talking only about 
real relations. A fictional relation is defined as ‘one whose only property is 
that it holds between the objects that it does hold between’ (Newman [1927], 
p. 145). By this Newman does not mean to call real only those relations that 
have interesting formal properties (such as reflexivity, transitivity etc.) because 
the Russellian would presumably not wish to ignore all relations that lack in- 
teresting formal properties (for example, all one-place relations lack interesting 
formal properties, but the Russellian would presumably not wish to ignore all 
one-place relations). Rather he means that real relations are those that are 
the extensions of intensionally interpreted predicates. At first it seems that the 
problem here is that all relations are fictional for Russell, because, according to 
Newman, he denies that our knowledge claims about the external world involve 
any intensionally interpreted predicates (except perhaps equality). However, 
® Aset of n objects has 2" — 1 non-empty subsets. n objects can be arranged into a set of n.n distinct 
pairs, which has 2"" — 1 non-empty subsets. n objects can be arranged into a set of n.n.n triples, 
which has 2""" — | non-empty subsets.

--- Page 149 ---

Newman’s Objection 143 
Newman claims that the problem is just the opposite, i.e. that all relations are 
real, because, having named the objects in the domain, each relation will be 
the extension of some intensionally interpreted predicate. In the above case, for 
example, R is the extension of the relation that holds between x and y just in 
case (x = a and y = b) or (x = aand y = c) or (x = band y=c). 
This leads Newman to consider a second possible response, which is to 
distinguish between ‘important’ and ‘trivial’ relations and assume that when 
the Russellian tells us what relations hold in a system he is talking only about 
important relations. Newman dismisses this response as follows: 
we should have to compare the importance of relations of which nothing is 
known save their incidence (the same for all of them) in a certain aggregate. 
For this comparison there is no possible criterion, so that ‘importance’ 
would have to be reckoned among the prime unanalysable qualities of 
the constituents of the world, which is, I think, absurd. (Newman [1928], 
p. 147) 
Although Newman thought that this response was absurd, a number of 
philosophers have put forward variants of it. These will be discussed in 
Section 6. 
However, isn’t there a much more obvious response to Newman: hasn’t he 
misunderstood the position he attacks? Newman imputes to Russell something 
like the following claim: 
Our knowledge of the world is purely structural (i.e. it consists of claims 
constructed using only logical terms). 
However, it may be that Russell actually held only the following (weaker) 
view: 
Our knowledge of the world consists of claims constructed using only 
logical and internal terms. 
It is true that the passages of Russell that Newman quotes seem to suggest he 
would go along with the former (stronger) of these claims, but elsewhere Russell 
is quite explicitly committed to the view that we know how external objects 
are connected with internal experience, which he allows us to legitimately de- 
scribe with non-logical terms. For example, Russell says that ‘My knowledge of 
the table is of the kind which we shall call “knowledge by description”. The 
table is “the physical object which causes such-and-such sense-data”’. (Russell 
[1912], p. 26). This seems to imply that he held only the latter (weaker) claim. If 
we understand Russell this way then it seems that Newman has misunderstood 
his position.

--- Page 150 ---

144 Peter M. Ainsworth 
However, Russell did not respond to Newman this way (Russell’s response 
is discussed in passing in Section 5.2, as it relates to Cruse’s [2005] response). 
It is difficult to believe that Russell just missed such an obvious rejoinder. 
Perhaps the reason he did not offer it is that he could see that although 
Newman’s objection, as Newman states it, is not strictly speaking to the point, 
nonetheless Newman’s line of thinking does lead one to the conclusion that 
ESR is not significantly distinct from standard antirealism. This point has 
been made by Demopoulos and Friedman ([{1985]) and Ketland ((2004]) with 
respect to the Ramsey-sentence approach to ESR and is discussed in the next 
subsection. 
3.2 Demopoulos and Friedman’s and Ketland’s versions 
Demopoulos and Friedman (anachronistically) impute to Russell a form of 
the Ramsey-sentence approach to ESR (Demopoulos and Friedman [1985], 
p. 622) whereby the knowledge a scientific theory provides is expressed by the 
Ramsey-sentence of that theory, which will contain non-logical terms that refer 
to either internal objects, properties, and relations (for the SESRist) or observ- 
able objects, properties, and relations (for the WESRist) but will in either case 
not be purely structural. They claim (although without substantial argument) 
that if ‘our theory is consistent, and if all its purely observational consequences 
are true, then the truth of the Ramsey-sentence fo//ows as a theorem of set the- 
ory or second-order logic, provided our initial domain has the right cardinal- 
ity’ (Demopoulos and Friedman [1985], p. 635, original emphasis). Although 
Demopoulos and Friedman do not really back up this claim, Ketland ((2004]) 
does provide a strong argument for (a slight variant of) the claim. 
To understand Ketland’s argument we must distinguish between intended 
and arbitrary interpretations of a language. An intended interpretation is a 
function from the non-logical terms of the language to the objects, properties, 
and relations in a structure that respects the intended meanings of the non- 
logical terms. For example, under its intended interpretation, the name ‘Julius 
Caesar’ is assigned the person Julius Caesar. Similarly, under its intended 
interpretation, the predicate ‘larger than’ is assigned the set of pairs (x, y) such 
that x is larger than y. An arbitrary interpretation does not respect intended 
meanings in this way. In an arbitrary interpretation, the name ‘Julius Caesar’ 
might be assigned any arbitrary object and the predicate ‘larger than’ might be 
assigned any arbitrary set of pairs. 
Ketland assumes that we have a two-sorted second-order language. A two- 
sorted language (see, for example, Enderton [2001], pp. 295-6) has two types 
of individual variables that range over two different domains. Many-sorted 
languages are harmless in the sense that they can be reduced to standard one- 
sorted languages without loss (except of convenience) (see Enderton [2001],

--- Page 151 ---

Newman's Objection 145 
pp. 296-9). In this case the two domains (in the intended interpretation of the 
language) are observable objects and unobservable objects. 
The language is also assumed to have three types of predicates: observational 
predicates, which (in the intended interpretation of the language) refer to ob- 
servable properties and relations (which Ketland takes to be sets [of tuples] 
of observable objects), theoretical predicates, which (in the intended interpre- 
tation of the language) refer to unobservable properties and relations (which 
Ketland takes to be sets [of tuples] of unobservable objects) and mixed pred- 
icates, which (in the intended interpretation of the language) refer to mixed 
relations (which Ketland defines as sets of tuples such that each tuple contains 
at least one observable object and at least one unobservable object).’ 
Let (Do, O;, O2,...) be the structure associated with the intended inter- 
pretation of the observational part of the language, i.e. let Do be the set of 
observable objects in the world and let O;, O2 etc. be (the sets corresponding 
to) the observable properties and relations referred to by the observational 
predicates of the language. 
We can now define what it means for an arbitrary structure for the language, 
((D;, D2), Ria, Riz R31, R3.2,...) to be empirically correct 
((D;, D2) is an arbitrary two-sorted domain, each Rj; is an arbitrary interpre- 
tation of an observational predicate over D, (i.e. a subset of D; or a subset of 
D; x D, etc.), each R3j; is an arbitrary interpretation a theoretical predicate 
over D> (i.e. a subset of D> or a subset of D> x D> etc.) and each R3>j; is an 
arbitrary interpretation of a mixed predicate over D; U D> (i.e. a subset of D, 
x D> ora subset of D> x D, etc.)). We do this as follows: 
Definition 1: D,, Do), Riz, Ri2 >1, Roo, ‘np RRy Bea < 1S 
empirically correct if and only if its reduct (D;, Rj.;, Ri.2, . . .) is isomorphic 
to (Do, O}, O2,...). (cf. Ketland’s ‘Definition E’ [2004], p. 296) 
In other words, a structure is empirically correct if and only if the appropriate 
reduct of the structure is isomorphic to the structure of the observable world 
(relative to some choice of predicates). This definition of empirical correctness is 
in line with van Fraassen’s ([1980]) notion of empirical adequacy: van Fraassen 
says that ‘a theory is empirically adequate exactly if [...] [it] has at least one 
model that all the phenomena fit inside’ (van Fraassen [1980], p. 12). We will 
Ketland himself notes that the characterization of observable (etc.) properties and relations as 
sets (of tuples) of observable (etc.) objects has some counter-intuitive consequences. In particular 
he notes that ‘many scientifically significant relations and quantities (e.g. various space-time 
relations and quantities, various scientific quantities such as mass, length, duration, location, 
etc.) will “decompose” into three strangely distinct relations, depending on the observational 
status of their relata’ (Ketland [2004], p. 289, footnote 5). Cruse ({2005]) argues that the ESRist 
(or at least the WESRist) can respond to Ketland’s version of Newman’s objection by denying 
that this is an accurate characterization of the observable/unobservable distinction that he wishes 
to draw. This response is discussed in Section 5.2

--- Page 152 ---

146 Peter M. Ainsworth 
return to the issue of how this definition of empirical correctness compares with 
other notions of empirical correctness (or adequacy) later. Let’s assume that 
the Ramsey-sentence of a theory in this language is obtained by Ramseyfying 
away the mixed and theoretical predicates that appear in the theory (cf. Ketland 
[2004], p. 292). It follows that: 
Theorem 1: The Ramsey-sentence of a theory A is true if and only if 
there is some sequence of relations R2, R22 R31, R32,... such that 
((Do, Dr), O1, O2 21, R22 , R32,...) F A (cf. Ketland’s 
‘Theorem 4 [2004], p. 293) 
where Dy is the set of unobservable objects in the world. (Ketland gives a 
proof of this result; this has been omitted, as the result itself seems intuitively 
obvious.) We need one more definition: 
Definition 2: (D;, D2), Ry, Ri2 21, K22 ee a= 
cardinality correct if and only if |D2| = |Dr|. (cf. Ketland’s ‘Definition G’ 
[2004], p. 298) 
and we can then prove: 
Theorem 2: The Ramsey-sentence of a theory A is true if and only if A has 
a model that is empirically correct and T-cardinality correct. (cf. Ketland’s 
‘Theorem 6’ [2004], p. 298) 
The proof is in two steps: 
[1] Left-to-right: Suppose the Ramsey-sentence of A is true. Then, by The- 
orem 1, there is some sequence of relations R2;, R22 
such that (Do, Dr), O01, Or 2 R31, R32,...) F A, ice. 
(Do, Dr), 01, O2,..-, Roi, R22 R3;, R32,...) is a model of A. 
Clearly, (Do, O;, O2,...) is isomorphic to (Do, O;, O2,...), so (Do, Dr), 
QO), O> R31, R32,...) is empirically correct (by 
Definition 1). Equally clearly, |Dr| = |Dr|, so (Do, Dr), O1, O2 
R31, R32,...) is T-cardinality correct (by Definition 2). 
[2] Right-to-left: Suppose A has a model, ((D;, D2), Ri, Ri2 
22 R31, R32,...), that is empirically correct and T-cardinality cor- 
rect. As ((D;, D2), Ria, Ri2 > R31, R3.2,...) is empir- 
ically correct, (D;, Rj.;, Ri2,...) is isomorphic to (Do, O;, O2,...), ie. 
there is a bijection, f: D; ~ Do, such that, for every R; ; and every n-tuple, 
Xn), Of elements of D;: 
(x), X2,---,Xn) € Ry, if and only if (f(x;), f(x2) f(xn)) € Oj. 
As ((D;, D2), Rui, Ri2 21, R22 R31, R32,...) is T-cardinality 
correct, |D>| = |Dr|, i.e. there is a bijection, g: D2 — Dy. We can use fand g 
to define a new function, fg: (D,;, D2) —~ (Do, Dr), such that f* g(x) = f(x)

--- Page 153 ---

Newman’s Objection 
if x € D,; and f*g(x) = g(x) if x € D2 and we can use f*g to define new 
relations such that, for every R2; and every R3;: 
(i) Roi = ae {(f2(x1), fg(x2), 
(ii) = R3i = ar {(f 2x1), f g(x2) 
By the construction of the R> js and R3js, fg is an isomorphism between 
21, R22 3.1, R32,-.- and ((Do, Dy 
R31, R32,...). We know that ((D;, D2), Ry), 
3 F A,so ((Do, Dr), O;, O2,... , Ro, 
A. So by theorem 1, the Ramsey-sentence of A is 
How does Ketland’s result compare to Demopoulos and Friedman’s claim 
that if ‘our theory is consistent, and if all its purely observational consequences 
are true, then the truth of the Ramsey-sentence follows as a theorem of set the- 
ory or second-order logic, provided our initial domain has the right cardinality’ 
(Demopoulos and Friedman [1985], p. 635, original emphasis)? Ketland notes 
that his result is, strictly speaking, weaker, because it is in principle possible 
that all a theory’s purely observational consequences could be true whilst it 
might not have an empirically correct model but not vice versa. (An obser- 
vational consequence here is assumed to be any statement formed using only 
observational predicates and logical terms, excluding predicate variables. Ob- 
servational consequences in this sense are thus assumed to include empirical 
generalizations. Zahar’s reply to the modern version of the objection is based 
on the claim that this is inappropriate; this is discussed in Section 5.1). 
It is not easy to give an example that demonstrates how all a theory’s purely 
observational consequences could be true whilst it might not have an empiri- 
cally correct model, but Ketland gives an example that demonstrates how some- 
thing analogous can occur in number theory. Say that a model is ‘arithmetically 
correct’ if and only if it has a reduct isomorphic to the standard natural number 
structure. Say that a theory has true ‘arithmetical consequences’ if and only if 
all the consequences of the theory that are stated in the language of arithmetic 
are satisfied in the standard natural number structure. Let L be the language 
of arithmetic, and let Ly be the language of arithmetic extended by a monadic 
predicate symbol, T (which is intended to behave like a truth predicate). Now 
consider the theory FS*: this contains the axioms of Peano arithmetic and 
certain axioms concerning the predicate T (these are axioms that, intuitively 
As a referee pointed out, this proof assumes that we are dealing with a full (standard) second- 
order model (i.e. a model in which the second-order quantifiers range over all the relations 
on the domain{s]). If we were dealing with a non-full (Henkin) model then there would be no 
guarantee that the relations that are defined in the proof would be in the scope of the second-order 
quantifiers, so the proof would not go through.

--- Page 154 ---

148 Peter M. Ainsworth 
speaking, a truth predicate should satisfy) (see Halbach [1999], pp. 368-9). 
It can be shown that, (i) if M is a model of FS* then M does not have a reduct 
isomorphic to the standard natural number structure, so FS* does not have a 
model that is arithmetically correct, but nonetheless, (11) every consequence of 
FS* that does not involve T is true in the standard natural number structure, 
so FS* has only true arithmetical consequences. (For more on this see Ketland 
[2004], pp. 295-8). 
However, despite the fact that Ketland’s result is strictly speaking weaker 
than the one Demopoulos and Friedman claim, the difference appears to be 
immaterial: Ketland’s result still suggests that ESR is not significantly distinct 
from antirealism. As noted, Ketland’s notion of empirical correctness formal- 
izes van Fraassen’s notion of empirical adequacy. Since van Fraassen argues 
that it is rational to believe that our mature theories are empirically adequate 
(but not necessarily true) Ketland’s result shows that what the knowledge that 
a theory’s Ramsey-sentence is (approximately) true would amount to, beyond 
what van Fraassen’s antirealism allows, would be (at most) only knowledge of 
the cardinality of the unobservable world. Thus ESR (in its Ramsey-sentence 
form) is just van Fraassen’s antirealism, augmented by the peculiar claim that 
we can (perhaps) know the cardinality of the unobservable world.’ 
4 Replies that Abandon the Ramsey-Sentence Approach to ESR 
In this section, two arguments that claim that Newman’s objection can be 
evaded if one abandons the Ramsey-sentence approach to ESR are discussed. 
On the face of it, this appears to be a strange line to take, because although 
Demopoulos and Friedman’s and Ketland’s versions of the objection are di- 
rected at the Ramsey-sentence approach to ESR, Newman’s original version of 
the objection is not. This first impression remains on closer inspection of these 
arguments. 
4.1 Redhead’s reply 
Redhead ((2001]) argues that the Ramsey-sentence approach to ESR is indeed 
undermined by Newman’s objection, commenting that ‘the Ramsey sentence 
AIR(S[R]), asserting the existence of a relation R which has structure S, is in 
fact a logical truth, modulo the specification of the cardinality of the domain 
over which the relation is defined’ (Redhead [2001], pp. 345-6). This is false: 
the Ramsey-sentence of a theory is not satisfied by every model of the right 
The qualifications ‘at most’ and ‘perhaps’ appear here because knowing that a theory’s Ramsey- 
sentence is true doesn’t guarantee that we know the cardinality of the unobservable world, it 
only guarantees that we know that the theory has some model with the right cardinality (and the 
theory might have models of different cardinalities).

--- Page 155 ---

Newman’s Objection 149 
cardinality, so the Ramsey-sentence is not, ‘a logical truth, modulo the speci- 
fication of the cardinality of the domain’ (on the most obvious reading of this 
expression). For example, consider the ‘theory’ expressed by the sentence Jw3x 
([w 4 x] & Vy [(y = w) Vv (y = x)]) & Vz(Pz) (which says that there are two things, 
and that everything is P). Its Ramsey-sentence is 3X[Sw3x ([w # x] & Vy [(y = 
w) V (y = x)]) & Vz(Xz)] (assuming P needs to be Ramseyfied). This is satisfied 
by the structure ({1, 2}, {1, 2}) but not by every structure with two elements in 
its domain. For example, it is not satisfied by the structure ({1, 2}, {1}). Hence 
it is not ‘a logical truth, modulo the specification of the cardinality of the do- 
main’. What (Ketland’s version of) Newman’s objection actually shows is that 
if the theory has an empirically correct model then the theory’s Ramsey-sentence 
is bound to be satisfied by some structure (instantiated by the world) as long as 
the world contains an appropriate number of (unobservable) things. 
Redhead puts forward an alternative approach to ESR, which, he claims, 
avoids Newman’s objection. He describes this alternative approach as 
follows: 
We need not deny that there are real physical relations posited by physical 
theories [...] Thus S(R), where R refers to a specific relation having the 
structure S, is of course logically stronger than the Ramsey sentence, and is 
by no means a logical truth. But this means [. . .] that the reference of R must 
be picked out in non-structural terms. But this is not denied in the above 
account. Our claim is merely that R is hypothesised in some explanatory 
theoretical context so it exists as an ontological posit, but all that we have 
epistemic warrant for is the second-order structure S. (Redhead [2001], 
p. 346). 
Redhead appears to suggest that the Ramsey-sentence approach to ESR 
denies that there are real physical relations posited by physical theories. This is 
also false. The Ramsey-sentence approach does claim (roughly speaking) that 
all we know about (some of) these relations is structural, and this is (roughly 
speaking) why Newman’s objection operates against it. But this is a claim that 
Redhead apparently endorses. The point of Newman’s objection is (roughly 
speaking) that if all we know is that there is some (real) relation R (of which we 
have only structural knowledge) then we know nothing more than a cardinality 
constraint on the domain over which the relation is defined. 
On the other hand, in parts of the above quote Redhead seems to suggest 
that we can specify the relation R intensionally. If he does think this then he 
appears to have abandoned ESR, and it is hard to see how he can maintain the 
claim that we have only structural knowledge of R. 
Perhaps the most charitable reading of Redhead’s position would be to 
substitute the word ‘important’ or ‘natural’ for the words ‘real’ and ‘specific’ in 
the above quote. This would lead us back to Newman’s own ‘absurd’ response 
to his objection (variants of which are discussed in Section 6). However, it

--- Page 156 ---

150 Peter M. Ainsworth 
is doubtful that this is really what Redhead intended. Neither ‘natural’ nor 
‘important’ means the same as ‘real’, much less is either synonymous with 
‘specific’. Moreover, there seems to be no reason why taking this line forces one 
to abandon the Ramsey-sentence approach (as will be seen when this approach 
is discussed in Section 6). 
4.2 French and Ladyman’s reply 
French and Ladyman ([2003]) appear to suggest that Newman’s objection does 
not arise if one adopts the semantic view of theories (whereby a theory is taken 
to be a collection of structures) as opposed to the syntactic view (whereby a 
theory is taken to be a collection of sentences): 
Worrall’s approach is thoroughly embedded in the so-called syntactic view 
of theories that adopts first-order quantificational logic as the appropriate 
form for the representation of physical theories. [Footnote omitted] We 
will not rehearse our reasons here, but we consider this approach to be 
deeply flawed, not only because of its inadequacy in reflecting scientific 
practice, but also because of the pseudo-problems that arise once one has 
adopted it. So for example, the Newman problem is obviated if one does not 
think of structures and relations in first-order extensional terms. One of us 
(Ladyman [1998]) has suggested an alternative descriptive framework for 
SR [structural realism], namely the ‘semantic’ or model theoretic approach 
to theories. (French and Ladyman [2003], p. 33) 
On the face of it, it seems highly unlikely that moving to the semantic view 
would really allow the ESRist to evade Newman’s objection. In fact, Newman’s 
original version of the objection is posed against the view that scientific theories 
directly specify a structure that represents the world. It is true that Demopoulos 
and Friedman ([{1985]) and Ketland ((2004]) aim their objections at the Ramsey- 
sentence approach, which does assume that science presents us with a linguistic 
representation of the world. However, it is easy to show that an analogue of 
their objections applies to a version of ESR framed using the semantic view. 
Framed in terms of the semantic view, ESR would imply something like the 
following limitation on our knowledge: 
The most that we can know about the world is that some structure (pro- 
vided by our scientific theories) is empirically correct and isomorphic to a 
structure instantiated by the world. 
Let (Do, O;, O2,...) be the structure instantiated by the observable world 
(relative to some chosen observational predicates): i.e. let Do be the set of 
observable objects in the world and let O;, O> etc. be the intended extensions 
of observational predicates. Let a theory present us with a structure, ((D,, D>), 
Ry, Riz,.--, Roa, Ro2,..., R31, R32,..-), where D; is a domain that is to

--- Page 157 ---

Newman's Objection 151 
represent the set of observable objects, D> is a domain that is to represent the set 
of unobservable objects, each Rj ; is to represent an observable relation (where 
an observable relation is taken to be a set of [tuples of] observable objects), 
each R>j; is to represent a mixed relation (where a mixed relation is taken to 
be a set of tuples that each contain at least one observable object and at least 
one unobservable object) and each R3, is to represent an unobservable relation 
(where an unobservable relation is taken to be a set of (tuples of) unobservable 
objects). Define such a structure to be empirically correct as before: 
Definition 1: ((D;, D2), Ry1, Ri2,..., Ro, Ro2,..., Rai, Ra2,..-) is 
empirically correct if and only if its reduct (D;, Ry.;, Ry. is isomorphic 
to (Do, 01, O2,...). (cf. Ketland’s ‘Definition E’ [2004], p. 296) 
Let Dy be the set of unobservable objects in the world. Define ((D;, D>), 
Ri 1, 2 21, R22 3.1, R3.2,...) to be T-cardinality correct as 
oefore: 
Definition 2: D;, Dz), Ry, Ri2,.--, R21, R22 : » R31, R32,- -) 1S 
T-cardinality correct if and only if |D2| = |Dy|. (cf. Ketland’s ‘Definition 
G’ [2004], p. 298) 
We can now prove that: 
Theorem 3: ((D;, Dz), Ry, Ry2,..., Roi. Ro2,.... R31, R32, is em- 
pirically correct and isomorphic to a structure instantiated by the world if 
and only if it is empirically correct and T-cardinality correct 
The proof comes in two stages: 
[1] Left to right: Suppose ((D;, D2), Ry), Ri2,..., Roi, Ro2,.-., Rs 
R3.2,...) is empirically correct and isomorphic to a structure instantiated 
by the world. That is, ((D;, D2), Ry1,Ry2,...,R21,R22,..-,R31, R32, 
is isomorphic to a structure of the form, ((Do, Dr), O;, O2,..., Ro, 
R>> R31, R32,...), where O;, O> etc. are the intended extensions 
of observational predicates. By stipulation it is empirically correct. And 
clearly, |D2| = |Dr|, ie. (D;, Dz), Ry, Ri2 21, R22,.-.., R3 
R32,...) is T-cardinality correct. 
[2] Right to left: Suppose ((D;, Dz), Ry1, Ryi2..-. 21, R22, , R; 
R32,...) is empirically correct and T-cardinality correct. By stipulation 
it is empirically correct. As ((D;, D2), Ri). 2? 
R32,...) 1s empirically correct there is some isomorphism between (D), 
R) 1, Ryi2,...) and (Do, O;, Oo, . . .). That is, there is a bijection f: D;} + Do, 
such that, for every R;; and every n-tuple, (x; x2,..., Xn), of elements 
of D;: 
te 3.15 
(X1, X2,.--,Xn) € Ry, if and only if (f(x;), f(x2) f(x,)) € Oj.

--- Page 158 ---

Peter M. Ainsworth 
As ((D;, D2), Ria, Riz 21, R22 31, R32,...) is T-cardinality 
correct there is some bijection, g: D2 + Dry. We can use these functions to 
define a new function, fg: (D;, D2) —~ (Do, Dr) such that f* g(x) = f(x) if 
x € D, and f* g(x) = g(x) if x € D2 and we can use f*g to define new relations 
such that, for every Rp; and every R3j: 
(i) Roi =ar {(f (x1), fg(x2) Pete) ofp iKss os «5 Xn) € Roi}. 
(ii) Ri = ar {(f (x1), e(x2) 
By the construction of the R;s and R33 ;s, fg is an isomorphism between 
(D,, D>), Ruy, 2 R21, 72 R31, R32,...) and (Do, Dr), 
R31, R32,...). (Do, Dr), O1, Or 
R>> R3,,, R3.2,...) is a structure instantiated by the world: we know 
that O,, O> etc. are the intended extensions of observational predicates 
and each R; is an arbitrary mixed relation that is obviously instantiated 
by the world (all its tuples are built from objects in the set Do U Dr, 
i.e. the set of objects in the world) and similarly, each R33; is an arbitrary 
unobservable relation that is obviously instantiated by the world (all its 
tuples are built from objects in the set Dy, i.e. the set of unobservable 
objects in the world).'° @ 
Theorem 3 states that the semantic view formulation of ESR (i.e. the 
claim that the most that we can know about the world is that some 
structure—provided by our scientific theories—is empirically adequate and 
isomorphic to a structure instantiated by the world) is equivalent to the claim 
that the most that we can know about the world is that some structure (pro- 
vided by our scientific theories), ((D;, D2), Ri, Riz 3. 
R3.2,...), is empirically correct and T-cardinality correct, so Theorem 3 pro- 
vides a Newman-esque reductio of the semantic view formulation of ESR. It 
is true that there is a slight difference between the ESRist who works with the 
semantic view and the ESRist who works with the syntactic view/Ramsey- 
sentence approach (because the latter’s claim is equivalent to the even less 
optimistic view that the most we can know about the world is that one of 
the—possibly many—structures that satisfy a given theory is empirically cor- 
rect and T-cardinality correct) but this difference does not appear to be very sig- 
nificant. (The difference arises because the proponent of the semantic view—at 
least as he has been characterized here—thinks that the theory specifies a unique 
structure, whereas the proponent of the syntactic view thinks that a theory is 
a sentence that picks out only a family of structures, all of which satisfy the 
sentence). 
'0 Tt may be objected that the R2;s and the R'3,;s, presumably unlike the Os, may not correspond 
to any natural relations; this again leads to essentially Newman’s own ‘absurd’ response to his 
objection (variants of which are discussed in Section 6).

--- Page 159 ---

Newman’s Objection 
5 Replies Designed to Rescue the Ramsey-Sentence Approach 
In this section, three replies that are designed to save the Ramsey-sentence 
approach to ESR from the modern versions of Newman’s objection are con- 
sidered. 
5.1 Zahar’s reply 
Zahar ({2001], Appendix 4, co-written with John Worrall; [2004]) takes issue 
with Demopoulos and Friedman’s version of Newman’s objection, which he 
characterizes as the claim that it is ‘only what the Ramsey-sentence asserts 
over and above its observational content [that] is reducible to [. . .] a cardinality 
constraint’ (Zahar [2004], p. 10, original emphasis). Zahar goes on to say 
that: 
This ‘over and above’ however proves to be essentially indefinable; for 
on the one hand, the Ramsey-sentence does not normally follow from 
its empirical basis, i.e. from the set of true and empirically decid- 
able, hence singular sentences. If, on the other hand, all the—generally 
undecidable—empirical generalisations’ were included in the observa- 
tional content of a theory, then the Ramsey-sentence might well turn out 
to be one of them; in which case Demopoulos’s and Friedman’s thesis col- 
lapses into the trivial claim that the Ramsey-sentence follows from itself 
(Zahar [2004], p. 10, original emphasis). 
Zahar’s first ‘hand’ holds the proposition that only singular sentences form 
the observational content of a theory. He then demonstrates that Ramsey- 
sentences can go beyond such observational content. He asks us (Zahar [2004], 
p. 11) to consider a theory expressed by the sentence 
A: Wx(Fx — Tx) & Vy(Ty — Ky), 
where F and K are observational predicates and T is a theoretical predicate. 
The Ramsey-sentence of this theory is 
A* : 3X(Wx[Fx — Xx] & Vy[Xy — Ky]), 
which is equivalent to 
Vx(Fx — Kx) 
and this last sentence (and hence the equivalent A*) goes beyond any number 
of singular statements of the form 
Fa; > Ka; 
in the sense that no matter how many statements of this form we have, there 
is always a model in which they are all true but in which the generalization

--- Page 160 ---

154 Peter M. Ainsworth 
Vx(Fx — Kx) (and hence the equivalent A*) is false. So there is a clear sense in 
which the Ramsey-sentence of the theory goes beyond (Zahar’s understanding 
of) the observational content of the theory. 
However, given Zahar’s understanding of the observational content of a 
theory as consisting of the singular sentences (containing only observational 
terms) entailed by the theory, then not only do Ramsey-sentences typically go 
beyond observational content but, as Zahar’s example clearly illustrates, uni- 
versal generalizations that involve only observational predicates (i.e. “empirical 
generalisations’) also go beyond observational content. Even the antirealist 
would typically agree that we can know such generalizations to be true, so 
even the antirealist would agree that we can know more than the observational 
content of a theory, in Zahar’s sense of observational content.'' Unless the 
ESRist can demonstrate that Ramsey-sentences also go beyond empirical gen- 
eralizations, he has failed to distinguish his position from antirealism. In fact, 
Zahar clearly states (in the above quote) that the Ramsey-sentence of a theory 
might often be (equivalent to) an empirical generalization. At this point he 
seems to concede to Demopoulos and Friedman even more than they ask for: 
they claim that ESR is antirealism plus a cardinality constraint, while Zahar 
seems to concede that ESR is (often) plain antirealism (because he claims that 
Ramsey-sentences are [often] equivalent to empirical generalizations, so the 
ESRists’ claim that we can have knowledge of at most Ramsey-sentences is 
[often] equivalent to the antirealists’ claim that we can have knowledge of at 
most empirical generalizations). 
Nonetheless, Zahar denies that ESR is just antirealism. This denial seems 
to rest on an equivocation over the meaning of ‘observational content’. Zahar 
claims that the difference between realists and antirealists is that the latter deny 
that we can have knowledge that goes beyond the observational content of a 
theory, which is true, but only if the observational content of a theory is taken 
to include empirical generalizations. He then demonstrates that the Ramsey- 
sentence of a theory goes beyond the observational content of a theory, where 
this is now taken to exclude empirical generalizations. Thus his conclusion that 
to know the Ramsey-sentence of a theory is to know more than the antirealist 
would allow does not follow. 
'! Typically, the antirealist would say that we can have knowledge of singular empirical state- 
ments and empirical generalizations (i.e. generalizations that do not involve theoretical terms or 
second-order variables) but would deny that we generally have knowledge of a theory’s Ramsey- 
sentence (since, notwithstanding Zahar’s example, a theory’s Ramsey-sentence is not typically 
equivalent to an empirical generalization). (Although what Newman’s objection purports to show 
is—roughly speaking—that knowing a theory’s Ramsey-sentence to be true is knowing very little 
beyond knowing that the singular empirical statements and empirical generalizations that fol- 
low from the theory are true, so ESR collapses into a position not significantly distinct from 
antirealism.)

--- Page 161 ---

Newman's Objection 
5.2 Cruse’s reply 
Cruse’s ((2005]) reply is directed at Ketland’s version of the objection. In 
particular, Cruse objects that not all ESRists need draw the observational 
term/theoretical term distinction in the way that Ketland suggests (and given 
a suitably different construal of the distinction, Ketland’s proof of Theorem 2 
would not go through). Recall that Ketland takes observational predicates 
to refer exclusively to sets of (tuples of) observable objects, theoretical predi- 
cates to refer exclusively to sets of (tuples of) unobservables objects and mixed 
predicates to refer to sets of tuples that each contain at least one observable 
and one unobservable object. As noted, Ketland acknowledges that this has 
some rather counter-intuitive consequences. Cruse emphasizes this point: 
consider the relation denoted by the predicate ‘larger than’. On Ketland’s 
taxonomy, there is no such single relation; there are three. First, there is the 
relation we might call observably larger than, which ranges entirely over ob- 
servable objects. Second[,] there is the relation we might call unobservabl 
larger than, which ranges entirely over unobservable objects. Third, there is 
the relation we might call miscellaneously larger than, which applies to all 
and only pairs of objects such that the first is observable, the second unob- 
servable, and the first larger than the second.'? On Ketland’s terminology, 
only the first class of relations—those which range entirely over observable 
objects—count as observable [. . .] I will call this the strong version of the 
observational-theoretical (O/T) distinction. (Cruse [2005], p. 561, original 
emphasis, footnote added) 
Cruse’s reply is based on rejecting the strong observational term/theoretical 
term distinction. 
Cruse notes that some ESRists do appear to be committed to this form of the 
observational term/theoretical term distinction or rather (as he puts it) ‘some- 
thing isomorphic to it’ (Cruse [2005], p. 563). Translated into the terminology 
of this paper, he suggests that the internal term/external term distinction em- 
ployed by SESRists (such as Russell) must take this form because ‘the mental 
and physical domains are entirely disjoint, so no (non-mathematical) property 
which applies to a mental event applies to a physical event or vice versa. Sim- 
ilarly, no (non-mathematical) predicate which applies to a mental event will 
apply to a physical event’ (Cruse [2005], p. 563). 
In fact, in the face of Newman’s objection, Russell decided that he was not 
committed to this form of the distinction. In a letter to Newman (reprinted in 
his autobiography [1968] and by Demopoulos and Friedman [1985]) he wrote: 
\2 This is splitting hairs, but it seems that we could also have an unobservable object that was larger 
than an observable object if, for example, ‘the universe’ or ‘the nearest black hole’ qualifies as 
unobservable object

--- Page 162 ---

Peter M. Ainsworth 
It was quite clear to me, as I read your article, that I had not really intended 
to say what in fact I did say, that nothing is known about the physical world 
except its structure. I had always assumed spacio-temporal continuity with 
the world of percepts, that is to say, I had assumed that there might be co- 
punctuality between percepts and non-percepts [...] And co-punctuality 
I regarded as a relation which might exist among percepts and is itself 
perceptible. (Russell [1968], p. 176) 
Russell is here putting forward the view that there is at least one non-logical 
predicate (‘co-punctuality’) that refers to a relation that can hold between 
(i) pairs of external objects, (ii) pairs consisting of one external object and one 
internal object (in either order), and (iii) pairs of internal objects. Russell’s own 
reply to Newman is actually essentially the same as Cruse’s reply to Ketland, 
which is perhaps surprising, as Cruse cites Russell as the sort of ESRist for 
whom this reply is not available. 
There are good reasons to think that this sort of reply isn’t available to 
Russell. As Demopoulos and Friedman ({1985]) point out, Russell’s move is 
completely ad hoc: 
in the earlier theory [i.e. Russell’s [1927] theory] we could not assume ac- 
quaintance with a cross category notion such as spacio-temporal contiguity 
or causality; but in the light of the difficulties of that theory we now find 
that we can assume this! [Footnote omitted] We are not saying that one 
cannot resolve the issue in this way. But it seems quite clear that without 
a considerable advance in the theoretical articulation of this rather elusive 
Russellian concept [i.e. acquaintance], no such resolution of the difficulty 
can be very compelling. (Demopoulos and Friedman [1985], p. 632, original 
emphasis) 
I would go further: it seems that, given the supposedly radical difference 
between external and internal objects, it is very unlikely that the issue could be 
satisfactorily resolved this way. Moreover, if Russell makes this concession then 
he seems to be left at the top of a slippery slope: if we can assume that external 
objects can be ‘co-punctual’ with one another in the same way that internal 
objects sometimes are, why can’t we assume that they can be ‘bigger than’ one 
another in the same way? It thus does seem (as Cruse suggests) that SESRists 
(such as Russell) are committed to an internal term/external term distinction 
of the form Ketland suggests (so they cannot evade the Newman/Ketland 
objection this way). 
However, Cruse’s main point (translated into the current idiom) is that the 
WESRist is not committed to anything like Ketland’s form of the observa- 
tional term/theoretical term distinction. He proposes an alternative form of 
the distinction according to which:

--- Page 163 ---

Newman’s Objection 
observational predicates refer to, broadly speaking, perceptible, or observ- 
able properties such as redness or squareness. A natural understanding 
of this would be that these observable properties are unproblematic not 
because they are a/ways observable, but simply because we can in at least 
some cases observe them (Cruse [2005], p. 565, original emphasis) 
This is supposed to capture the intuition that ‘we can meaningfully (and for 
a realist, truly) assert the existence of red blood cells, or microscopic square 
grids, for example’ (Cruse [2005], p. 564); i.e. that observational predicates can 
be applied to unobservable objects. A natural interpretation of this suggestion 
(natural in the light of the foregoing discussion, at any rate) is that rather than 
taking an observational predicate to be one whose intended extension is a set 
consisting only of (tuples of) observable objects (as Ketland suggests), we are 
to take an observational predicate to be one whose intended extension is a set 
consisting of at least some (tuples of) observable objects. Theoretical predicates 
would then be those whose intended extension is a set consisting entirely of (tu- 
ples of) unobservable objects. However, interpreted this way, Cruse’s suggestion 
is also deeply counter-intuitive, because it classes as observational a number 
of predicates that are, intuitively speaking, theoretical. It is true that ‘being a 
superstring’ is on this account a theoretical predicate, because no superstring is 
observable, but ‘being a collection of superstrings’ is observational, because it 
applies to some (in fact, if the theory is correct, all) observable objects. And it is 
(to say the least) counter-intuitive to classify “being a collection of superstrings’ 
as observational. 
However, this is not the only possible interpretation of Cruse’s suggestion. 
Like Ketland’s original proposal, this interpretation of Cruse’s proposal makes 
the assumption that either a predicate is observational if and only if its extension 
contains only (tuples of) observable objects or that a predicate is theoretical 
if and only if its extension contains only (tuples of) unobservable objects. But 
why make this assumption? 
The assumption seems to rest on the idea that we can determine whether an 
object is observable or unobservable, but that we cannot (directly) determine 
whether a property or relation is observable or unobservable, so whether or 
not a property or relation is unobservable must be defined in terms of whether 
or not the objects to which it applies are. But this idea is surely wrong. Surely 
we know that red is an observable property as surely and directly as we know 
that Jeff Ketland is an observable object. And surely we know that being a 
collection of superstrings is an unobservable property as surely and directly as 
we know that the nearest black hole is an unobservable object. It is true that 
there are some properties and relations that we might hesitate to class either 
way. But it is equally true that there are some objects that we might hesitate to 
class either way (small particles of dust, for example). So the idea that we must

--- Page 164 ---

158 Peter M. Ainsworth 
define the observability or unobservability of properties and relations in terms 
of the observability or unobservability of the objects to which they apply is at 
least questionable. 
This suggests that the extensions of observational predicates can contain 
both observable and unobservable objects, and that the extensions of theoretical 
predicates can contain both observable and unobservable objects, depending on 
what the criteria of observability are. If the WESRist adopts an observational 
term/theoretical term distinction along these lines then it is true that Ketland’s 
proof of Theorem 2 does not go through (Ketland’s proof crucially assumes 
that observational predicates apply only to sets of [tuples of] observables, which 
is not the case with this characterization of the observational term/theoretical 
term distinction). The WESRist can thereby evade the conclusion that: 
Theorem 2: The Ramsey-sentence of a theory A is true if and only if A has a 
model which is empirically correct and T-cardinality correct. (cf. Ketland’s 
‘Theorem 6’ [2004], p. 298) 
However, the WESRist would be well advised to leave the champagne on 
ice, if not in the cellar. Even using this more liberal characterization of the 
observational term/theoretical term distinction, we can still prove a theorem 
that casts doubt on the view that knowledge of a theory’s Ramsey-sentence is 
the sort of knowledge that the WESRist wants to claim that we have. 
We assume that we have a language containing a number of observational 
predicates (construed as above, so that they may apply to observable and unob- 
servable objects) and a number of theoretical predicates (construed as above, so 
that they too may apply to observable and unobservable objects). The structure 
associated with the intended interpretation of the observational predicates is: 
(Da, O;, O>, cay 
where Da is the domain of (observable and unobservable) objects in the world 
that instantiate some observable property or relation that is referred to by one 
of the observational predicates of the language and each Oj is the intended 
extension of an observational predicate of the language. Now, given a theory, 
A, the Ramsey-sentence of A is obtained by Ramseyfying away the theoretical 
predicates. As before, it is obvious that: 
Theorem 4: The Ramsey-sentence of a theory A is true if and only if there is 
some sequence of relations, R2), R22,... such that ((Da, Dg), O;, O2 
R21, R22,...)F A 
where Dg is the domain of objects in the world that do not instantiate any 
observable property or relation that is referred to by one of the observational 
predicates of the language (depending on the choice of these predicates, Dg

--- Page 165 ---

Newman's Objection 159 
may well be the empty set, e.g. if one of the observational predicates is ‘larger 
than’, Dg will presumably be empty as every object is surely on at least one side 
of this relation to at least one other object). We will say of an arbitrary structure 
for the language, ((D;, D2), Ri, Ri2,..-, Ro, R22....), (where (D;, D>) is 
an arbitrary two-sorted domain, each R,; is an arbitrary interpretation of an 
observational predicate over D;, and each R2; is an arbitrary interpretation of 
a theoretical predicate over D; U D>) that: 
Definition 3: ((D;, D2), Ry... Ri2,....R21, Ro2,...) gets the extensions of 
the observational predicates right if and only if its reduct (D,, Ry, R 
is isomorphic to (Da, O;, Oo, 
We will also say that: 
Definition 4: (( D), D>), R, ls Ry Do 0+ «BRD Fy 2822,---7 18 B-cardinality correct 
if and only if |D>| = |Dg}. 
It is easy to prove: 
Theorem 5: The Ramsey-sentence of a theory A is true if and only if A 
has a model which gets the extensions of the observational predicates right 
and which is B-cardinality correct. 
The proof is in two steps: 
[1] Left-to-right: Suppose the Ramsey-sentence of A is true. Then, by 
Theorem 3, there is some sequence of relations R2;, R22,... such that 
((Da, Dg), O1, Oo, ..., R21, R22,...) F A, ie. (Da, Dg), O;, Or, ..., Roi, 
R>>,...) isa model of A. Clearly, (Da, O;, Or, ...) is isomorphic to (Da, 
O;, O2,...), so (Da, Dg), O1, O2,..., Rai, Ro2,...) gets the extensions 
of the observational predicates right (by Definition 3). Equally clearly, 
|Dg| = |Dgl, 1-e. (Da, Dg), O1, O2 21, R22,...) is B-cardinality 
correct (by Definition 4). 
[2] Right-to-left: Suppose A has a model, ((D;, Dz), Rit, Ri2,..-,R2i, 
22 ), which gets the extensions of the observational predicates right 
and which is B-cardinality correct. As ((D,;, Dz), Ri, Riz R>| 
R22,...) gets the extensions of the observational predicates right, (D;, 
R,1, Ry.2,...) is isomorphic to (Da, O;, Oo,...), i.e. there is a bijection 
f: D; — Da, such that, for every R;; and every n-tuple, (x), X2,...Xn), 
of elements of D;: 
(X1,X2,.-.,Xn) € Rj, jif and only if (f(x;), f(x2) f(x,)) € Oj. 
As ((D;, D2), Rut, Riz 21, R22,...) is B-cardinality correct, there 
is a bijection g: D. — Dg. We can use f and g to define a new function, 
f*g: (D;, D2) — (Da, Dg) such that f* g(x) = f(x) ifx € D; and f* g(x) = g(x)

--- Page 166 ---

Peter M. Ainsworth 
if x € D2 and we can use f*g to define new relations such that, for every 
R3;: 
R3; =ar {(f* 2(x1), f g(x) X,)) 2(X1, Xo 
By the construction of the R2;’s, fg is an isomorphism between ((D;, D2), 
Ri, Rj2 R>1, R22,...) and (Da, Dg), O;, O2 R31, Ro5,...). 
We know that ((D;, D2), Ri, Ri2 ...) F A, so (Da, Dg), 
R>;, R22,...) F A. So by Theorem 3, the Ramsey-sentence of 
A is true. £ 
Theorem 5 implies that the Ramsey-sentence of a theory can tell us something 
substantial about the world, beyond what the antirealist typically allows: it tells 
us about the ‘observable’ properties and relations of the unobservable world. So 
ESR, construed on these lines, does represent a halfway house between realism 
and antirealism. However, it doesn’t look like the house the ESRist claims to 
inhabit: the position implies that we have no non-trivial knowledge of unob- 
servable properties and relations (and, in particular, no interesting ‘structural’ 
knowledge of such properties and relations). This implies that predicates such 
as ‘strangeness’ or ‘being a collection of superstrings’ serve only an instrumen- 
talist function in our theories, which seems to be at odds with the traditional 
ESRist’s claims. !* 
5.3 Melia and Saatsi’s reply 
Melia and Saatsi’s ({2006]) response to Newman’s objection is based on the 
observation that: 
The properties postulated in scientific theories are typically taken to stand 
in certain intensional relations to various other properties. Some properties 
counterfactually depend on others, some are correlated in a law-like manner 
with others, some are independent of others, and some are explanatory of 
others. (Melia and Saatsi [2006], pp. 579-80, original emphasis) 
Melia and Saatsi point out that such relations between properties (i.e. second- 
order relations) cannot be expressed in standard second-order logic.'* More- 
over, they claim that if we formulate scientific theories and their Ramsey- 
sentences in a language that is capable of expressing such relations then 
Newman’s objection will be blocked. 
'3 Discussing whether or not this position was actually plausible would be tangential to the aims of 
this paper. It would also be a highly scholastic exercise, because it is not a position that anyone 
holds. 
The terminology could become confusing here. I call properties and relations of properties and 
relations second-order properties and relations. It is important to note that second-order logic is 
so called because it allows for quantification over sets of (tuples of) objects as well as objects and 
not because it accommodates second-order properties and relations in this sense. 
14

--- Page 167 ---

Newman's Objection 161 
As Melia and Saatsi note ({2006], p. 580) the obvious way to formulate such 
relations would be to introduce higher-order predicates into one’s language. 
This is not the approach they ultimately favour, but let’s consider this approach 
first. Consider a ‘theory’ that states that there is a click on a Geiger counter if 
and only if there is an atom in the vicinity of the Geiger counter that radioac- 
tively decays. We might attempt to formalize this theory as follows: 
IxCx — dy(Ay & Dy), 
where ‘A’ and ‘D’ are ‘theoretical’ predicates such that ‘Ax’ means ‘x is an atom 
in the vicinity of a Geiger counter’ and ‘Dx’ means ‘x radioactively decays’ and 
‘C’ is an ‘observational’ predicate such that ‘Cx’ means ‘x is a click in a Geiger 
counter’. Melia and Saatsi would say that implicit in the theory is the claim 
that the correlation between Geiger counter clicks and radioactive decay is a 
lawful (as opposed to accidental) correlation. Consequently, they would argue 
that a more faithful formalization of the theory would be as follows: 
(AxCx — dy[Ay & Dy]) & LDC, 
where L is a second-order predicate such that LX Y means ‘X is lawfully corre- 
lated with Y’. Consequently the Ramsey-sentence of the theory is not 
AXSY (SxCx — dy[Xy & Yy)), 
s 
which would be contentless!> but is rather: 
AX3SY[(AxCx <— Sy[Xy & Yy]) & LXC], 
which is not so trivial: it states that some property has a lawful correlation with 
clicks on Geiger counters. 
However, this assumes that the second-order predicate L does not need to be 
‘Ramseyfied’. If it does then we would obtain the following ‘Ramsey-sentence’: 
IXSX3Y[(4xCx <— Jy[Xy & Yy]) & XXC], 
which is, again, effectively contentless: it states that there is a click on a Geiger 
counter if and only if there is something that has two (not necessarily distinct) 
properties and that there is some (second-order) relation between one of these 
properties and clicks on Geiger counters. This is contentless because there is 
always some second-order relation between any two properties: given any two 
properties P and Q we can construct the second-order relation {(P, Q)} between 
them. 
So this response is only viable if it is reasonable to suppose that second-order 
relations between properties like ‘is lawfully correlated with’ do not themselves 
'5 The Ramsey-sentence of this ‘theory’ (formalized this way) is completely contentless: it states that 
there is a click on a Geiger counter if and only if there is something that has two (not necessarily 
distinct) properties. This is utterly trivial since a click on a Geiger counter is something that has 
at least one and hence two (not necessarily distinct) properties.

--- Page 168 ---

162 Peter M. Ainsworth 
need to be Ramseyfied. But this is surely not something the ESRist could 
consistently accept: ‘lawful correlations’ and their ilk are surely not observable, 
much less internal, relations. 
As noted, Melia and Saatsi do not advocate the use of higher-order predi- 
cates to formalize the relations between properties. They suggest that we should 
instead augment the language with a number of modal operators that express 
the pertinent relations: ‘So, for instance, let Lp express “it is physically necessary 
that. ..”. Then JXLpVx(Xx <Gx) says that there is a property that is /awfully 
coextensive with G.’ (Melia and Saatsi [2006], p. 581). The claim is that scien- 
tific theories, and their Ramsey-sentences, would typically (perhaps invariably) 
include such modal operators. This sidesteps the problem that undermines the 
previous approach, because there can be no question of ‘Ramseyfying’ modal 
operators. This approach is, however, open to the objection that it requires us 
to take these modal operators as logical primitives and we surely cannot accept 
that modal operators expressing things like ‘it is physically necessary that...’ 
can be taken as /ogical primitives, since whether or not something happens as 
a matter of physical necessity is an issue that must be decided empirically, not 
as a matter of logic. 
6 Replies that Argue that Some Structures/ Relations are Privileged 
In this section, three variants of Newman’s own ‘absurd’ response to his objec- 
tion are considered. This response is founded on the claim that some relations 
are more important than others. Newman took this response to be absurd be- 
cause, as the ESRist believes, we have only structural knowledge of the relations 
in question, we lack a criterion to distinguish the important relations from the 
unimportant relations, so ‘importance’ must be left as an unanalyzed primitive, 
a mysterious quality that attaches to some relations but not to others. How- 
ever, the idea of a primitive important/unimportant distinction (or something 
similar) has not appeared absurd to everyone. The proposal most similar to 
Newman’s own was put forward by Carnap ([1967]) to resolve an analogue of 
Newman’s problem that faces the theory he puts forward in the Aufbau. The 
adaptation of this proposal to resolve Newman’s objection to ESR is discussed 
in Section 6.1. The other two variants of this approach discussed here both 
in some sense deny that ‘importance’ needs to be taken as primitive. Votsis’ 
({2003}; [2004], Chapter 4) proposal (discussed in Section 6.2) grounds the im- 
portance of a relation on the means by which it is discovered. Psillos’ ({1999], 
Chapter 3) proposal (discussed in Section 6.3) takes the importance ofa relation 
to be a contingent physical property of the relation. Psillos himself argues that 
taking the approach ultimately amounts to abandoning ESR. (Both Merrill 
[1980] and Lewis [1983] make suggestions similar to Psillos’ in response to

--- Page 169 ---

Newman's Objection 163 
Putnam’s [1977] ‘model-theoretic’ argument against realism: an argument that 
is very closely related to Newman’s objection to ESR). 
6.1 A Carnapian reply 
This reply has some similarities to Melia and Saatsi’s ({2006]) reply (discussed 
in Section 5.3). The essence of the proposal is the suggestion that we should 
take importance (or as Carnap [1967] calls it, ‘foundedness’) as a primitive 
(second-order) logical property that attaches to some relations (in the way that 
identity is sometimes taken as a primitive logical relation that holds between 
some pairs). With a little adaptation of the Ramsey-sentence approach, this 
enables the proponent of ESR to evade Ketland’s variant of the Newman 
objection. Let the symbol for the foundedness property be ‘Found’. Instead 
of advocating belief in a theory’s Ramsey-sentence, the ESRist who takes this 
approach should advocate belief in the theory’s Ramsey-sentence*, where the 
latter is just like a normal Ramsey sentence, except that, for each predicate 
variable, X, we add (in the scope of the quantifier 3X) the phrase ‘&Found(X)’. 
For example, consider the toy theory: 
¥x(O,x — Tx) & Vy(T2y > Ory), 
where O; and O> are observational predicates and T,; and T> are theoretical 
predicates. This theory yields the Ramsey-sentence 
AXISY(Vx[O,x > Xx] & Vy[Yy > O>y]) 
and the Ramsey-sentence* 
3X3Y(Vx[O,x > Xx] & Vy[Yy > Ory] & Found[X] & Found[Y]). 
As ‘Found’ is taken as a logical primitive the Ramsey-sentence* contains only 
logical and observational terms. It is thus hygienic, by the WESRist’s standards 
(in that the WESRist’s claim does not imply that it is impossible to know the 
Ramsey-sentence*). However, if Ramsey-sentences are swapped for Ramsey- 
sentence*s then Ketland’s argument no longer goes through. In particular, the 
relevant analogue of: 
Theorem 1: The Ramsey-sentence of a theory A is true if and only if there 
>,... such that ((Do, 
Theorem |’: The Ramsey-sentence* of a theory A is true if and only if 
there is some sequence of relations, R2;,R 3.1, R3.2,... such that 
((Do, Dr), O1, Or 21, R22 R31, R32,...) = A and such that 
each member of the sequence is a founded relation.

--- Page 170 ---

164 Peter M. Ainsworth 
This blocks Ketland’s proof of Theorem 2 at the last step: although, given 
that a theory has some model ((D;, Dz), Ri, 
R3.2,...) that is empirically correct and T-cardinality correct, we can construct 
a model of the theory of the form ((Do, Dr), O1, O2 R51, Ro2 
R32,...), this does not guarantee that the theory’s Ramsey-sentence* is true, 
as there is no guarantee that the relations R;, R22,... we have constructed 
will be founded. 
However, even Carnap does not have licence to invent logical predicates at 
whim; if we are to accept ‘Found’ as a new logical term we surely must be given 
reasons to do so. Here is what Carnap says: 
[Found] does not belong to any definite extralogical domain, as all other 
nonlogical objects do. Our considerations concerning the characterisation 
of the basic relations of a constructional system as founded relation exten- 
sions of a certain kind hold for every constructional system of any domain 
whatever. It is perhaps permissible, because of this generality, to envisage 
the concept of foundedness as a concept of logic and to introduce it, since 
it is undefinable, as a basic concept of logic. (Carnap [1967], p. 237, original 
emphasis) 
We might, just possibly, think that the following was plausible: 
If a property or relation is instantiated in every possible system then it is a 
logical property or relation. 
However, if I understand him correctly, Carnap seems to assume something 
like the following: 
If a property or relation is instantiated in every possible constructional 
system then it is a logical property or relation. 
The notion of a constructional system is central to Carnap’s theory in the Auf- 
bau, but it is not the case that every possible system is a possible constructional 
system, so it is hard to see why we should accept this assumption. Compare 
Carnap’s assumption with the following assumption: 
If a property or relation is instantiated in every possible system whose 
domain contains human beings then it is a logical property or relation. 
No one would accept this: it would lead to the conclusion that properties like 
‘being a human being’ and ‘being a mammal’ are logical properties. So it seems 
we at least need an argument that shows that constructional systems are more 
important (founded, as it were) than systems whose domain contains human 
beings.

--- Page 171 ---

Newman's Objection 165 
It is instructive to contrast foundedness with identity, a relation we might 
sensibly take to be logical. In the first place we can see that the identity relation 
will be instantiated by some pairs of objects from any possible non-empty 
domain; by contrast the foundedness property need not be instantiated by any 
relations from a given set of relations. In the second place, given a domain, we 
can determine the extension of the identity relation over that domain a priori; 
by contrast we cannot determine the extension of the foundedness relation over 
a set of (extensionally specified) relations a priori. 
It seems that it is fair to say that taking the notion of the importance of a 
relation as a primitive logical notion is absurd and cannot form the basis of a 
reasonable response to Newman’s objection. 
6.2 Votsis’ reply 
Votsis claims that Newman’s objection purports to show that ‘the knowledge 
claims of SR [structural realism] [are] of littlke worth or importance’ (Votsis 
[2003], p. 886) by showing that ‘the information they offer can also be de- 
rived a priori from set theory modulo a cardinality constraint, hence the only 
important information contained in the structural realist claims concerns the 
cardinality of the domain’ (Votsis [2003], p. 886).'° He also claims that the 
inference from the latter to the former rests on the assumption that ‘any infor- 
mation contained in a statement that is also derivable a priori lacks importance’ 
(Votsis [2003], p. 886). Votsis takes issue with this assumption. There is a sense 
in which everyone will surely agree that this assumption is false. The statement 
that there is no largest prime is surely of some importance, at least in some 
contexts, but the claim is derivable a priori. 
However, Votsis does not dispute the assumption that ‘any information con- 
tained in a statement that is also derivable a priori lacks importance’ by pointing 
out that there is a sense in which some results that are obtained a priori are 
important. Rather, he seems to make the extraordinary suggestion that how a 
claim is arrived at affects its importance. In particular, he seems to suggest that 
a claim is more important if it is arrived at empirically than if it is arrived at 
a priori. He claims that ‘The method of arriving at the abstract structures is at 
least partly empirical [. . .] The fact that set theory also allows me to derive the 
same structure a priori does not mean that the information I have reached is 
'6 This isn’t strictly true, at least if we take the modern version of Newman’s objection that is 
directed at the Ramsey-sentence approach to ESR. In this form, what Newman’s objection shows 
is that knowing that a theory’s Ramsey-sentence is true is only knowing that the theory has a 
model that is T-cardinality correct and empirically correct. Moreover, it seems that the essence of 
Newman’s objection is not so much that ESR implies that scientific theories are of little worth 
but rather that it implies that all they tell us about the unobservable world is its cardinality, so 
ESR is not significantly distinct from antirealism. So it seems that Votsis fails to address the right 
issue in his reply to Newman’s objection. However, this is not too important: it will be argued 
that he does not satisfactorily address the wrong issue.

--- Page 172 ---

166 Peter M. Ainsworth 
devoid of importance’ (Votsis [2003], p. 887, original emphasis). But if some 
fact is of no importance when it is acquired by set-theoretical reasoning, that 
surely does mean that it is of no importance when it is acquired empirically. A 
fact that is unimportant, insofar as it can be easily discovered, does not become 
more important because it can also be discovered by an unnecessarily difficult 
route. For example, suppose someone claims to have made the important dis- 
covery that ‘eggs is eggs’. We might well reply that their discovery is not in fact 
important, insofar as it is easy to show (a priori) that ‘eggs is eggs’: the claim 
follows directly from the a priori principle that everything is self-identical. It 
would be ridiculous if they were to reply that the discovery was important 
because they arrived at it by an empirical study of eggs. Indeed, it seems that 
Votsis’ contention comes down to the claim that ‘two’ results can be the same 
but of different importance, a claim that violates the law of the indiscernibility 
of identicals. 
Votsis attempts to support his contention with a thought experiment: 
Take the numbers 133 and 123. I can, restricting myself solely to arithmetic, 
perform various operations on these numbers. One such operation is ad- 
dition. Similarly, if I had two collections of 133 and 123 physical objects 
respectively, I could count them one by one, and would reach the same 
result. Despite the similarities, there is an important difference between 
the two cases. The latter case is one in which the result is a property that 
is then ascribed to the physical world, in particular to the physical objects 
under consideration, and not merely an exercise of arithmetic. This claim is 
warranted by the employment of an empirical method to arrive at the given 
number. The main point is quite simple: The fact that arithmetic allows 
me to do this a priori does not mean that the information I have reached 
counting objects is of little or no importance. One need only consider the 
consequences if I had made an error in counting. (Votsis [2003], p. 886, 
original emphasis) 
This case does not at all support Votsis’ contention. In this case the two 
procedures, the a priori arithmetical procedure of adding 133 and 123 and 
the empirical procedure of counting the objects in two collections, do not 
achieve the same results. The former enables us to determine that 133 + 123 = 
256, whereas, as Votsis notes, the latter enables us to determine that there are 
256 objects in a particular collection. These results may well be of different 
importance but we cannot infer from this, as Votsis does, that two procedures 
yielding the same result can yield results of different importance, as the results 
in this case are not the same: the former is a theorem of arithmetic, the latter is 
a contingent fact about the world. 
Votsis goes on to note that ‘Using the [. . .] a priori method, set theory allows 
us to set up any structure we like [...] No structure is privileged in this sense. 
The structural realist’s a posteriori method guarantees that some structures are 
privileged over others.’ (Votsis [2003], p. 887, original emphasis). There is a

--- Page 173 ---

Newman’s Objection 167 
trivial sense in which the structures that have been arrived at a posteriori are 
privileged compared with those that have not been arrived at a posteriori, but 
it is not clear that there is an important sense: it is not obvious why the fact 
that some structures have been arrived at a posteriori guarantees that these 
structures are more important than those structures that have been arrived at 
‘merely’ a priori. Simply being arrived at via an a posteriori method does not 
seem to be sufficient to make a result important, especially if that result could 
have been arrived at a priori. After all, if the claim ‘eggs is eggs’ had been 
discovered to be true a posteriori, it would not thereby be more important than 
those identity claims that had been arrived at ‘merely’ a priori. 
Newman argued (in effect) that the ESRist (unlike the conventional realist) 
does not have the resources to distinguish the important structures instantiated 
by a system from the unimportant structures, or even to say in what sense one 
structure could be more important than another. Votsis seems to be suggesting 
that the ESRist can make the distinction, because a structure is made important 
simply in virtue of the fact that it has been arrived at via an a posteriori method. 
This is surely untenable. 
6.3 The Merrill/ Lewis/ Psillos reply 
This proposal was first suggested as a possible response to Putnam’s model 
theoretic argument against realism by Merrill ({1980]). It was adopted by Lewis 
([1983]) and has been discussed in connection with Newman’s objection by 
Psillos ({1999]). The key to the proposal is the suggestion that it is a contingent 
fact that some relations instantiated by the world are more important than 
others. The importance, or as proponents of this approach usually put it, 
‘naturalness’, ofa relation is not a logical property of the relation, nor a property 
the relation somehow acquires via the method by which it is discovered, but a 
physical property. Proponents of this view would not, presumably, deny that 
there is a perfectly good sense in which objects in the domain of the world, Dw, 
instantiate every relation compatible with the cardinality of Dw. However, they 
would add that only some of these relations are natural relations. The idea is 
that the world isn’t just a collection of objects that can be grouped howsoever 
we please; rather, it is a collection of objects that also have preferred natural 
groupings. The world itself determines that some relations are more important 
than others and in this way comes pre-structured. 
This is just the ‘natural kinds’ doctrine (or something very similar) and so 
this response is only open to those ESRists who are prepared to buy into this 
doctrine (or something very similar).'’ However, if one does accept it then 
'7 Worrall (personal communication) says that he takes natural kinds to be, by definition, the 
properties and relations that we refer to with the predicates of our best theories. This response 
to Newman’s objection is also not open to the ESRist who accepts only this form of the natural 
kinds doctrine: saying that the predicates of our best theories refer to natural kinds does not limit

--- Page 174 ---

168 Peter M. Ainsworth 
Newman’s objection misses the point. Let’s call a structure a structure of the 
world if its domain is Dw (the set of objects in the world). Let’s call a structure 
a natural structure of the world if in addition the relations in it are (the sets 
corresponding to) natural relations. It is true that any structure whose domain 
has the same cardinality as Dw is isomorphic to some structure of the world, 
but it is certainly not true that any such structure is isomorphic to some natural 
structure of the world (and presumably science aims to discover not just any 
old structures of the world but only the natural structures). 
In terms of the Ramsey-sentence approach to ESR, the claim would be 
that the relations over which the quantifiers in the Ramsey-sentence range are 
restricted to the natural relations. It follows that Theorem | 
Theorem |: The Ramsey-sentence of a theory A is true if and only if there 
is some sequence of relations, R2;, R22 R31, R3.2,... such that ((Do, 
Dr), O;, O2 2.15 K2 31, R32,...) FA 
must be replaced by 
Theorem 1’: The Ramsey-sentence of a theory A is true if and only if there 
is some sequence of natural relations, R2,, 
that ((Do, Dr), 01, O2 
because the relations R2;, R2> R31, R32,... are now only in the scope 
of the existential quantifiers if they are natural. This blocks Ketland’s proof 
of Theorem 2 at the last step: although given a theory has some model ((D,, 
D2), Ria, Riz 21, R22 31, R32,..-) that is empirically correct 
and T-cardinality correct we can construct a model of the theory of the form 
((Do, Dr), O1, O2 51, R’ R31, R3.2,...), it does not guarantee that 
the theory’s Ramsey-sentence is true, as there is no guarantee that the relations 
R31, R22,... we have constructed are natural. As Merrill puts it: 
So long as we ignore any intrinsic structuring of the world, there is nothing 
to forbid us imposing a structure along any lines we chose. But if, as the 
realist surely must hold, the real world is a structured domain, then we are 
not free to ignore its intrinsic structuring in playing our model-theoretic 
tricks. (Merrill [1980], p. 74, original emphasis) 
It might be objected that the ESRist could have no warrant for the claim 
that the second-order variables in the Ramsey-sentence range over only natural 
properties and relations, because we can have no idea what the natural kinds 
are independently of our theories. However, the ESRist needn’t claim that we 
do know what natural kinds are independently of our theories. He will claim 
that whatever reason we have to believe that our theories are true is also a 
the properties and relations to which these predicates can refer if any property or relation that is 
referred to by these predicates is, by definition, a natural kind.

--- Page 175 ---

Newman's Objection 169 
good reason to think that the terms in our theories refer to natural kinds and 
that the second-order variables in the Ramsey-sentence range over only natural 
properties and relations. '* 
As noted, Psillos ([1999]) considers this response to Newman’s objection but 
argues that it is not available to the ESRist: 
in order for them [i.e. ESRists] to distinguish between natural and non- 
natural classes they have to admit that some non-structural knowledge is 
possible, viz. that some classes are natural, while others are not. (Psillos 
[1999], p. 66) 
As long as we do not go down Carnap’s route and take naturalness to be a 
logical property of properties and relations then it is true that the knowledge 
that some property or relation is natural is not purely structural. However, 
neither the WESRist nor the SESRist claims that we have only purely struc- 
tural knowledge (WESRists claim that our knowledge is restricted to claims 
constructed using logical and observational terms, SESRists claim that our 
knowledge is restricted to claims constructed using logical and internal terms). 
However, Psillos’ point is essentially unaffected by this consideration, because 
the ‘naturalness’ of a property or relation is surely not an observable (or in- 
ternal) property, so neither the WESRist nor the SESRist can consistently 
treat ‘naturalness’ as a primitive second-order non-logical predicate, and if the 
predicate ‘naturalness’ must itself be ‘Ramseyfied’, this response will not work; 
cf. Section 5.3. 
7 Summary 
It has been argued that none of the attempts that have been made to evade 
Newman’s objection is successful. Consequently, Newman’s objection remains 
a very serious problem for the ESRist. Of course, one cannot rule out the 
possibility that ESRist may in the future come up with a satisfactory reply, but 
in the absence of such a reply it seems that the sensible attitude towards his 
position is one of considerable scepticism. 
'8 He might also suggest, as Lewis does, that, ‘It takes two to make a reference, and we will not 
find the constraint [on what properties and relations we refer to] if we look for it always on the 
wrong side of the relationship. Reference consists in part of what we do in language or thought 
when we refer, but in part it consists in eligibility of the referent.’ (Lewis [1983], p. 371). Lewis’ 
suggestion is that there is some feature of the world that restricts the range of our variables for 
us. So it doesn’t matter whether we know that our terms refer to natural kinds or not. It is just 
a fact that only certain properties and relations are eligible referents of terms, so, like it or not, 
our (second-order) variables can only range over certain properties and relations. However, this 
response is incoherent. It amounts to the claim that ‘there are some properties and relations 
(the unnatural ones) that lie outside the scope of our quantifiers’ and this claim is obviously 
self-defeating. Suppose (for reductio) that it is true. Then there is something in the scope of the 
‘some properties and relations’ that lies outside the scope of the ‘some properties and relations’. 
So the claim is false. Or, to put it in another way, we clearly can refer to unnatural relations, even 
if we don’t typically do so.

--- Page 176 ---

Peter M. Ainsworth 
Acknowledgements 
I’m grateful to Roman Frigg and John Worrall for their comments on this 
paper. 
Department of Philosophy, Logic and Scientific Method 
London School of Economics and Political Science 
Houghton Street, London, WC2A 2AE, UK 
P.M. Ainsworth@lse.ac.uk 
References 
Carnap, R. [1967]: The Logical Structure of the World: Pseudoproblems in Philosophy, 
translated by R. A. George, London: Routledge & Kegan Paul. 
Cruse, P. [2005]: ‘Ramsey Sentences, Structural Realism and Trivial Realization’, Studies 
in the History and Philosophy of Science, 36, pp. 557-76. 
Demopoulos, W. and Friedman, M. [1985]: ‘Bertrand Russell’s The Analysis of Mat- 
ter: Its Historical Context and Contemporary Interest’, Philosophy of Science, 52, 
pp. 621-39. 
Duhem, P. [1906]: The Aim and Structure of Physical Theory, translated by P. P. Wiener, 
reprinted 1991, Princeton, NJ: Princeton University Press. 
Enderton, H. B. [2001]: A Mathematical Introduction to Logic, 2nd edition, San Diego, 
CA: Harcourt. 
French, S. and Ladyman, J. [2003]: ‘Remodelling Structural Realism: Quantum Physics 
and the Metaphysics of Structure’, Synthese, 136, pp. 31—S6. 
Halbach, V. [1999]: ‘Conservative Theories of Classical Truth’, Studia Logica, 62, 
pp. 353-70. 
Ketland, J. [2004]: ‘Empirical Adequacy and Ramsification’, British Journal for the 
Philosophy of Science, 55, pp. 287-300. 
Ladyman, J. [1998]: ‘What Is Structural Realism?’, Studies in the History and Philosophy 
of Science, 29, pp. 409-24. 
Lewis, D. [1983]: “New Work for a Theory of Universals’, Australasian Journal of 
Philosophy, 61, pp. 343-77. 
Maxwell, G. [1968]: ‘Scientific Methodology and the Causal Theory of Perception’, in 
I. Lakatos and A. Musgrave (eds), Problems in the Philosophy of Science: Proceedings 
of the International Colloquium in the Philosophy of Science, London, 1965, Vol. 3, 
Amsterdam: North-Holland Publishing, pp. 148-60. 
Maxwell, G. [1970a]: ‘Structural Realism and the Meaning of Theoretical Terms’, in 
S. Winokur and M. Radner (eds), Analyses of Theories and Methods of Physics and 
Psychology, Minneapolis, MN: University of Minnesota Press, pp. 181-92. 
Maxwell, G. [1970b]: ‘Theories, Perception, and Structural Realism’, in R. Colodny (ed.), 
Nature and Function of Scientific Theories, Pittsburgh, PA: University of Pittsburgh 
Press, pp. 3-34. 
Melia, J. and Saatsi, J. [2006]: ‘Ramseyfication and Theoretical Content’, British Journal 
for the Philosophy of Science, 57, pp. 561-85.

--- Page 177 ---

Newman’s Objection 171 
Merrill, G. H. [1980]: ‘The Model-Theoretic Argument Against Realism’, Philosophy of 
Science, 47, pp. 69-81. 
Newman, M. H. A. [1928]: “Mr. Russell’s “Causal Theory of Perception” Mind, 37, 
pp. 137-48. 
Poincaré, H. [1903]: Science and Hypothesis, reprinted in S. J. Gould (ed.), The Value of 
Science: Essential Writings of Henri Poincaré, New York: Random House. 
Psillos, S. [1999]: Scientific Realism. How Science Tracks Truth, London: Routledge. 
Putnam, H. [1977]: ‘Realism and Reason’, Proceedings and Addresses of the American 
Philosophical Association, 50, pp. 483-98. 
Redhead, M. [2001]: ‘Quests of a Realist’, Metascience, 10, pp. 341-7 
Russell, B. [1912]: The Problems of Philosophy, Oxford: Oxford University Press. 
Russell, B. [1927]: The Analysis of Matter, London: Kegan Paul, Trench, Trubner. 
Russell, B. [1968]: The Autobiography of Bertrand Russell, IT, London: Allen & Unwin. 
van Fraassen, B. C. [1980]: The Scientific Image, Oxford: Clarendon Press. 
otsis, I. [2003]: ‘Is Structure Not Enough?’, Philosophy of Science, 70, pp. 879-90. 
Votsis, I. [2004]: The Epistemological Statue of Scientific Theories: An Investigation of the 
Structural Realist Account, PhD thesis, London School of Economics and Political 
Science. 
Worrall, J. [1989]: ‘Structural Realism: The Best of Both Worlds?’, Dialectica, 43, 
pp. 99-124. 
Worrall, J. [1994]: ‘How to Remain (Reasonably) Optimistic: Scientific Realism and the 
“Luminiferous Ether”, PSA 1994, 1, pp. 334-42. 
Zahar, E. G. [2001]: Poincaré’s Philosophy: From Conventionalism to Phenomenology, 
Chicago: Open Court. 
Zahar, E. G. [2004]: ‘Ramseyfication and Structural Realism’, Theoria: Revista de Teoria, 
Historia y Fundamentos de la Ciencia, 19, pp. 5—30.

--- Page 179 ---

Brit. J. Phil. Sci. 60 (2009), 173-193 
Determinism and the Mystery of 
the Missing Physics 
Mark Wilson 
ABSTRACT 
This article surveys the difficulties in establishing determinism for classical physics within 
the context of several distinct foundational approaches to the discipline. It explains that 
such problems commonly emerge due to a deeper problem of ‘missing physics’. 
The Problems of Formalism 
Norton’s Example 
Three Species of Classical Mechanics 
3.1 Mass point physics 
3.2 The physics of perfect constraints 
3.3 Continuum mechanics 
Conclusion 
The physicist, as his theories develop, often finds himself forced by the 
results of his experiments to make new hypotheses, while he depends, with 
respect to the compatibility of the new hypotheses with the old axioms, 
solely upon these experiments or upon a certain physical intuition, a prac- 
tice which in the rigorously logical building up of theory is not admissible. 
David Hilbert ({1976]) 
1 The Problems of Formalism 
For the reasons Hilbert indicates, ‘classical mechanics’ resembles a stool con- 
structed of six or seven legs of unequal lengths: if we unwisely place too much 
weight in the wrong place while it perches primarily upon legs 1, 3, and 5, it’s 
liable to rock over to legs 2, 3, and 6 in response, perhaps depositing us upon 
the floor in the process (Figure 1). Throughout our educations, we have been 
encouraged to speak of ‘classical mechanics’ as if it represented a unitary and 
well-understood doctrine. “Tain’t so, but, for many purposes, this little fiction 
creates no difficulties simply because the speaker may have intended to draw 
a mathematical contrast whose content is completely clear from the context, 
© The Author (2009). Published by Oxford University Press on behalf of British Society for the Philosophy of Science. All rights reserved 
doi: 10.1093/bjps/axn052 For permissions, please email: journals. permissions@oxfordjournals.org 
Advance Access published on January 7, 2009

--- Page 180 ---

Mark Wilson 
Figure 1. Stool. 
as usually occurs when, e.g., one compares ‘quantum and classical statistical 
mechanics’. But, for many of the questions that philosophers pose, such easy 
disambiguation is not available and such inquiries can become seriously con- 
fused unless the wobbly architecture hidden behind the implacable facade of 
‘classical mechanics’ is adequately recognized. For example, much contempo- 
rary commentary on philosophical theories of matter in the eighteenth and 
nineteenth centuries strikes me as greatly compromised by its inclination to 
assume that phrases such as ‘classical mechanics’ or ‘the Newtonian picture’ 
capture surgically precise meanings, when, in fact, such terminology can be 
readily applied to deeply incompatible doctrines. 
Philosophers of science often select some specific mathematical formalism 
(sometimes only implicitly) to represent ‘classical physics’ in their argumenta- 
tion despite the fact that the formalism selected is often incapable of bearing the 
doctrinal weight expected of it. As a common case in point, many writers begin 
their investigations by blithely stating ‘I here adopt Lagrangian formalism as a 
suitable embodiment of Newtonian theory’ without apparently recognizing the 
serious descriptive holes to which that specific formalism is heir. Indeed, I freely 
confess to have fallen into this error in my callow years (I began to study ‘clas- 
sical mechanics’ more critically because I dimly recognized that there had to be 
something fishy in what I had claimed). In point of fact, classical Lagrangian 
mechanics, under its normal interpretation, tolerates rather pronounced lapses 
in its scope of coverage and, in applied engineering practice, one is commonly 
forced to say, ‘Oh, those tools simply won’t work in this case; you need to utilize 
formalism X instead’ (I’ll supply some examples later). A failure to recognize

--- Page 181 ---

Determinism and the Mystery of the Missing Physics 175 
these descriptive gaps often vitiates the plausibility of whatever lesson it was 
that our unwitting author had hoped to extract. 
Unfortunately—and this is the chief moral of the essay—we’re unlikely to 
find any wholly stable X upon which the phrase ‘classical mechanics’ can 
permanently and happily rest. The reason for this is simple: classical mechanics 
represents a set of doctrines admirably suited to macroscopic experience and, 
in consequence of this large-scale mission, must inevitably compress swatches 
of very complicated physical behavior into simplified rules of thumb. In this 
essay, we shall be especially concerned with some of the popular recipes that 
classical modelers utilize to conveniently patch over the complicated manners 
in which extended solids contact one another. Much of the multi-leggedness 
we witness in the classical stool traces to the fact that such ‘rules of thumb’ 
for contact action exist in a variety of flavors that are not compatible with one 
another from a foundational point of view, yet possess equal bragging rights 
to qualify as ‘the classical approach’ to the contact between solids. 
2 Norton’s Example 
To demonstrate how dramatically such seemingly technical issues affect mat- 
ters of philosophical concern, let us examine how the question ‘Is Newtonian 
mechanics deterministic?’ turns upon these issues. Recently John Norton! has 
described a situation involving a ball sliding down a peculiarly shaped dome, 
which looks, at first appearance, as if it must prove indeterministic from a clas- 
sical point of view. However, the conceptual situation is not as straightforward 
as first appears, for foundational multi-leggedness complicates the discussion 
in hidden yet quite significant ways. An examination of Norton’s example can 
thus serve as an excellent illustration of the unstable gappiness that represents 
the natural price classical mechanics must pay to achieve the extraordinary 
successes it achieves on the macroscopic level (this is a chief theme in Wilson 
[2006]; in this essay, I’ll be able to identify some of the gaps, but not adequately 
defend their virtues). 
Norton’s example represents an improved recasting of a circumstance that 
has been long familiar in the physical literature. He allows a ball of unit mass 
to slide frictionlessly under terrestrial gravitation down a concave hillside track 
with the resultant equation of motion d*r/dr? = r'/?, where r is arc length 
along the track. The lack of a Lipschitz bound upon the active force at the 
apex creates a situation where this equation tolerates many possible solutions 
for a ball situated at the crest with no initial velocity (it can stay put or slowly 
start to roll down the track ad libitum with r(t) = (1/144)7*). The curvature 
' Norton ({2006]) calls his supporting structure a ‘dome’, but some unstated constraint allows 
us to ignore the ball’s potential angular displacements, producing the net effect of a track-like 
constraint. Hence I shall describe the situation as a ‘hillside track’ in this paper.

--- Page 182 ---

Mark Wilson 
point mass 
Figure 2. Types of foundational object. 
of the track ‘turns off the active part of the gravitational force acting on the 
ball as the origin is approached, yet ‘turns it back on’ very rapidly as the bail 
becomes displaced from the apex, allowing our moving-ball solutions to de- 
celerate smoothly enough to display a limiting zero velocity as the origin is 
approached. Given such a counterexample (many others can be easily con- 
structed), why has Newtonian mechanics heretofore so often been described as 
‘deterministic’? Norton presumes that he has shown that this familiar charac- 
terization is simply wrong, but, in fact, various ‘six-legged stool’ considerations 
concerning the phrase ‘Newtonian mechanics’ affect this example in substan- 
tive ways, as we shall establish in the course of this review. For such reasons, 
Norton’s case nicely illustrates the care we must observe as we ponder the 
‘content’ of ‘Newtonian mechanics’. 
3 Three Species of Classical Mechanics 
To this end, we should distinguish three basic families of foundational approach 
to ‘classical mechanics’ (they split into further subdivisions as further ques- 
tions are pressed): (MP) mass point particle mechanics (the ‘classical physics’ 
usually taught to freshman physics majors), (PC) the physics of rigid bodies 
and perfect constraints (commonly introduced as ‘analytical mechanics’ in that 
same freshman course in a somewhat surreptitious manner) and (CM) coniin- 
uum mechanics (taught systematically only to theoretical engineers and applied 
mathematicians nowadays) (Figure 2). The ‘objects’ basic to these formulations 
are, respectively, (MP) unextended point-masses, (PC) extended yet perfectly 
rigid geometrical shapes such as balls and curved tracks, and (CM) shapes

--- Page 183 ---

Determinism and the Mystery of the Missing Physics 
Figure 3. Ball on track. 
that are thoroughly flexible at every size scale, such as distorting balls, pliable 
tracks, wooden beams, and fluids. Probably class (CM) represents the most nat- 
ural embodiment of the doctrines we traditionally expect to find in ‘classical 
mechanics’ but articulating its operative mathematics precisely is rather tricky 
and often reserved for specialists. For this reason, point-mass-like formulations 
appear to be ‘foundational’ within historical textbooks such as Thompson and 
Tait ({1867]) simply because a proper framework for articulating CM principles 
directly lay beyond their technical reach. But their intended ‘world view’ was 
one of continua, not point masses. In any case, our three basic approaches are 
not, from a foundational point of view, strictly compatible with one another: 
the modes of description favored as basic in one approach can only be treated as 
convenient approximations within the others. Into which of these foundational 
categories does Norton’s case fall? 
His ball (really, a point mass) and track comprise objects of a fixed geometry 
and hence represent a standard arrangement within PC. This approach typi- 
cally makes two characteristic assumptions about cases like Norton’s (Figure 3). 
(a) A particle bound to a constraining surface but free of all opposing forces 
will follow a geodesic of the surface with a constant speed relative to the surface 
(this venerable doctrine is commonly called ‘generalized inertia’).” (b) Any total 
force exerted against the system can be perfectly decomposed into two disjoint 
components: the part that attempts to drive the particle unsuccessfully into the 
surface and the piece able to affect its generalized inertial motion (Figure 4). 
Forces in the first group are commonly dubbed ‘constraint forces’, ‘forces of 
reaction’ or ‘forces that do no work’ and the latter, ‘active forces’ or ‘forces 
that perform work’. In Norton’s example, the total applied force appears as 
the constant downward directed gravitational force, which is then apportioned 
into my two sub-categories in different amounts at different positions along 
his dome. At the very top the gravitational force proves completely ineffectual 
2 As such, generalized inertia captures a chief manner in which orthodox analytical mechanics 
diverges from point mass mechanics. Norton’s particle is not ‘free’ in this sense, because some of 
the applied gravitational force ‘performs work’ upon the particle, as well as supplying the ‘forces 
of constraint’ that bind it to its track (if only the latter are active, the particle qualifies as ‘free’) 
It might be noted that the resulting motion will represent neither a true sliding or rolling, either 
of which can engender surprisingly divergent behaviors upon a two-dimensional hillside.

--- Page 184 ---

Mark Wilson 
No work 
performed 
Constraint 
force 
Active part 
‘\ of force 
Figure 4. Ball on hillside. 
in its capacity ‘to perform work’. It operates entirely as a ‘constraint force’ 
there. However, as the particle moves away from the summit, some portion of 
the impressed gravitational force begins to ‘turn on’ as an active force. Appor- 
tioning ‘forces’ into these bifurcated categories is a very ancient practice—it 
is implicit in Greek thinking about statics—and an average textbook evokes 
the distinction without a murmur of apology (this ‘sneakiness’ then allows 
analytical mechanics to creep on stage). Nonetheless, from the points of view 
of our alternative foundational starting points, this kind of ‘active/constraint’ 
decomposition may prove strictly unwarranted and can only be justified as a 
form of convenient approximation. Any friend of determinism should be cau- 
tious about allowing forces to be glibly divided into ‘reactive’ and ‘constraint’ 
categories, for that’s how Norton’s loss of determinacy secretly enters the scene. 
In orthodox mass point mechanics (MP), by contrast, its unextended par- 
ticles obey strict First Law inertia: unaffected particles travel in straight lines 
without change of velocity. Moreover, Newton’s Third Law, as it is commonly 
understood in an MP context, rules out the possibility of true ‘constraint forces’ 
entirely: it can’t tolerate forces that strictly bind a particle to a fixed geomet- 
rical constraint. At best, the particle will stay pretty near a geodesic on the 
constraining surface while simultaneously displaying a lot of rapid wiggling. It 
can then prove convenient to factor away the wiggling as a ‘fast motion’ that is 
superimposed upon a ‘slow motion’ in the manner described in every textbook 
on approximation theory, but this decomposition is to be tolerated only as an 
approximation. In other words, orthodox MP mechanics rejects the assumption 
that forces can be strictly apportioned into ‘active’ and ‘constraint’ classes: all 
forces are wholly ‘active’. 
To see this, observe that a standard textbook treatment of Newton’s Third 
Law replaces its vague original ‘action = reaction’ formulation by three tacit 
assumptions: that fundamental forces always arise in inter-particle pairs that

--- Page 185 ---

Determinism and the Mystery of the Missing Physics 
unconstrained 
destination 
e 7 ~ 
force of 
reaction 
Figure 5. Force of reaction. 
are (1) central, (2) balanced, and (3) dependent only upon relative positions, not 
velocities. These assumptions then allow the textbook to associate any particle 
system with a potential energy function and to prove the conservation of energy 
on that basis. Within rival foundational stories such as CM, the conservation 
of energy is often adopted as an independent postulate, but the Third Law 
derivation just sketched represents the usual way its status is approached within 
freshman textbooks and in the more rigorous developments based squarely 
upon the MP picture. 
3.1 Mass point physics 
Accordingly, let us now scrutinize the ‘constraint force’ that binds our particle 
a to Norton’s track from the MP point of view. According to First Law inertia, 
a should normally wish to whoosh ahead along a tangent running off the 
surface but we have also assumed that the track’s matter supplies just enough 
‘constraint force’ f, to pull a back to the surface with exactly the right velocity 
to satisfy the expectations of generalized inertia (Figure 5). Okay, but let us 
now run a particle b along the track that is exactly like a except that it scoots 
a little faster along the path. Once again, the track’s ‘constraint force’ must 
exert a binding force f;, of the right magnitude to pull 5 down to its appointed, 
generalized inertia rendezvous with the hillside.* f, clearly needs to be different 
from f,. But how can the constraining surface prove smart enough to exert 
the exact degree of force required? Answer: the track must be able to ‘see’ 
the velocity difference between a and b and adjust its strength accordingly. 
But permitting such a sensitivity to velocity is contrary to the Third Law and 
the canonical ‘conservation of energy’ story sketched above. In fact, the MP 
foundational framework can legitimately tolerate strong binding forces that can 
only approximately drag a and b along similar ‘slow motion’ paths, with residual 
differences showing up in the different ‘fast motion’ ways in which a and b wiggle 
3 ae il : To articulate the situation more accurately, the gravitational force can convey the particle down- 
ward, but the repulsive forces arising from the dome’s surface must be smart enough to halt its 
descent in the right places with the correct tangential velocities.

--- Page 186 ---

180 Mark Wilson 
upon the hillside. This is the fundamental reason why the MP particle picture 
can’t, strictly speaking, tolerate true ‘forces of constraint’. Indeed, with respect 
to their fundamental qualities, ‘active forces’ and ‘constraint forces’ should be 
regarded as quite different sorts of critter and the conceptual tensions they 
engender stand at the root of many of the great historical disputes about the 
nature of ‘force’.4 
It is rather surprising that these elementary issues are rarely discussed clearly. 
A notable exception can be found in Gallavotti ({1983]), which seems flabber- 
gasted by the manner in which most textbooks sneak analytical mechanics on 
stage through nothing more than inadequate ‘forces that do or don’t do work’ 
hand-waving: 
However, the principle of the conservation of the difficulties makes it clear 
that there must be some serious obstacle to the actual applications of such a 
shining but simplistic vision. The true constraints are, in fact, generated by 
forces that .. . generally are neither simple nor conservative . .. but depend 
upon the velocities of the points as well as their positions. (Gallavotti 
[1983], p. 155)° 
He credits V. I. Arnold with first recognizing that appeals to constraints 
should be properly approached within the framework of MP as a kind of 
approximation theorem involving rapidly jittery motions superimposed upon 
a slower trend. 
The only significant comment I would add to Gallavotti’s discussion is that 
we should remain aware of the fact that PC can also be approached profitably 
from the continuum physics side of things (CM), where the alternative tale of 
approximation we will then weave will look significantly different from the MP 
story just sketched. In terms of predictive realism, the CM version of the story 
is preferable because it provides better warnings of the many circumstances 
where ‘analytical mechanics’ ( = PC) winds up treating real-life tracks, domes, 
and balls quite poorly.® We shall come back to the CM side of the ledger later. 
What, then, should we say about the ‘determinism’ of MP particle physics? 
Here we confront the basic phenomena of ‘missing physics’ that represents 
my central theme in this essay. One can’t immediately produce a definitive 
answer to the ‘determinism’ question simply because standard presentations 
* Heinrich Hertz ([1956]) struggled to resolve our dilemma by nominating the constraint forces as 
primary while relegating the ‘active forces’ to the humbler status of ‘artifacts of an approximation 
policy’. 
Gallavotti ({1983]) also elects to tolerate velocity-dependent forces in his initial postulates, spe- 
cializing to forces derived from a potential only later (p. 169). 
In real life, ragged surfaces lubricated by an intervening fluid are required before any simulacrum 
of frictionless sliding can be achieved (otherwise the ball binds tightly to its track). The ‘con- 
straints’ favored in analytic mechanics model the contact between solids in a quite unrealistic 
manner whereas continuum mechanics contains better resources for this purpose.

--- Page 187 ---

Determinism and the Mystery of the Missing Physics 181 
invariably weasel quite a bit with respect to foundational assumptions that must 
be settled before a feature such as determinism can be coherently adjudicated. In 
particular, we must know more than we are usually told about the exact kinds of 
inter-particle forces that our MP physics will tolerate. To be sure, our textbook 
will tell us about universal gravitation and Coulomb’s law, but what about the 
strong local forces responsible for molecular binding and repulsion? In real-life 
practice, molecular modelers evoke various inverse sixth and twelfth power 
laws (such as the well-known Lennard-Jones potential) for these purposes, but 
they are usually rather evasive about the classical validity of these ‘rules of 
thumb’. Instead, they wax weasely: ‘I don’t really know any classical contender 
for a short range force law that should be accepted as a canonical part of MP 
physics, but my simple power law rules work pretty well as a stopgap.’’ Here 
we witness a typical ‘missing physics’ gap within point mass mechanics where 
practitioners commonly evoke fairly coarse rules of thumb to fill foundational 
gaps that they will be happy to abandon if things don’t work out right. 
Yet as long as such gaps remain unfilled, it becomes misleading to clas- 
sify point particle physics as ‘deterministic’ or not—the question simply can’t 
be adjudicated on a rational basis with such incomplete information. In the 
breech, we will do better as philosophers, I think, to concentrate directly upon 
the descriptive holes presently tolerated within assembled MP doctrine, rather 
than focusing upon indeterminism per se. In 1962 Montague ({1974]) carefully 
formalized the treatment of ‘point particle physics’ that had been provided 
by Patrick Suppes and his group (McKinsey et al. [1953]) in hopes of rigor- 
ously establishing traits like determinism and discovered, to his dismay, that 
his collected materials were far too feeble to address this question in a remotely 
interesting fashion. Indeed, McKinsey et al. ({1953]) had been so timid in their 
formulations that they had omitted the Third Law altogether!* In such a con- 
text, asking ‘Is classical physics deterministic?’ resembles ‘Have you stopped 
beating your wife?’: The question of why so many vital background presump- 
tions are missing needs to addressed first. 
However, if we are willing to follow current modeling practice and strengthen 
our Third Law principles sufficiently to further demand that all inter-particle 
interactions must obey some kind of power law principle, then we can guarantee 
that only analytic functions (away from the collision singularities) will appear 
in our governing ordinary differential equations. If so, then Cauchy’s original 
local existence proof based upon power series expansions kicks in where the 
uniqueness of trajectories comes along for free (we only need worry about 
Lipschitz conditions and all that when some lapse of smoothness intrudes 
In practice, such modelers often evoke square well potentials as an expedient, but this coarse rule 
of thumb isn’t compatible with the smoothness presumed in Newton’s Second Law. 
8 Cf. the critical remarks in (Truesdell [1984]).

--- Page 188 ---

182 Mark Wilson 
upon our starting set of equations). Under this supplementary assumption, 
MP physics becomes provably deterministic. 
Which is not to say that point mass physics is therefore left in descriptively 
great shape, because it is notorious that its local trajectories are not always 
globally extendible: its particles can either bump into one another or allow 
some required quantity (such as position) to blow up into an intolerable infinity 
within finite time (Saari [2005]). Such unpleasant failures of ‘global existence’ 
have prompted classical physicists to invent all sorts of excuse for papering over 
these descriptive holes within their doctrines, a point to which I'll soon return. 
But it is as descriptive holes such issues should be addressed, not as failures of 
determinism: perfectly harmless-looking point particle setups often engender 
descriptive impossibilities after a time when left to their own devices. 
I stress the advisability of looking at matters in this manner because John 
Earman ([{1986]) in his well-known book (and John Norton follows him in this) 
artificially converts a classic ‘blow up’ phenomenon into an alleged ‘failure 
of determinism’ through recasting the proper meaning of ‘initial condition’ 
in a manner that I regard as terminologically unfortunate and contrary to 
well-established mathematical practice.? The only motive I can discern for this 
reclassification is that it allows one to inform other philosophers that ‘I have 
discovered that classical particle mechanics isn’t really deterministic’ rather 
than the more apt ‘I have discovered that classical particle mechanics sometimes 
tolerates strange descriptive gaps.’ The former asseveration may sound more 
thrilling, but I think the latter statement better captures the circumstances to 
which we should pay closer attention as philosophers of science. 
When asked about the collision and ‘blow up’ problems characteristic of 
point particle mechanics, physicists commonly reply in one of two ways: (i) 
they claim that some unspecified ‘missing physics’ will kick in to prevent the 
blowup from occurring or (ii) they’ll suggest some relatively crude rule of 
thumb to ‘continue’ trajectories past their apparent breakdown calamities. As 
an example of the first reply, it is often suggested (see below for a citation) that 
the celebrated Xia ([1992]) blow up in point particle gravitation will be ‘cured’ 
in real life by the repulsive mechanisms that supply planets with their real-life 
‘size’, for such barriers should prevent the system from milking the infinite 
potential wells tolerated within the unadulterated Xia setup. But, as we noted 
° Specifically, Earman and Norton argue that we should examine MP mechanics in a manner 
that tolerates hypothetical ‘space invaders’ allowed to enter a regular MP scenario from spatial 
infinity in finite time. If so, this tolerance should be described in terms of a novel ‘side condition at 
infinity’ added to conventional MP rather than an ‘initial condition’ per se (following Hadamard, 
mathematicians have performed a very valuable service in classifying the sundry kinds of natural 
‘side condition’ pertinent to differential equations precisely and I believe we should not trample 
upon their good terminological offices unduly). In view of its rather extensive foundational holes, 
I am unpersuaded that such novel ‘side conditions’ represent the best way to highlight what is 
troublesome in orthodox MP physics.

--- Page 189 ---

Determinism and the Mystery of the Missing Physics 183 
with the molecular modelers, the exact nature of these ‘repulsive mechanisms’ 
is left unspecified within canonical MP physics. A good example of the second 
response is the rule that colliding particles should rebound elastically. Here we 
simply plow past some of the problem’s singularities with a ‘fill-in rule’ of a 
non-differential equation type. 
Oftentimes, as John Norton implicitly observes, popular recipes for patch- 
ing over the descriptive holes in the vein of (ii) carry the risk of introducing a 
measure of certifiable indeterminacy in their wake. The classic illustration of 
this circumstance (in a CM context) can be found in the Riemann—Hugoniot 
recipe for repairing the shock wave blowups that otherwise commonly arise 
with respect to the natural equations for a confined gas (Smoller [1983]). This 
repair recommends that what are now called ‘weak solutions’ should be tol- 
erated for these equations, but, without some further constraints, this new 
allowance tolerates far too many new solutions of this generalized type, with 
an attendant loss of solution uniqueness. To be sure, by evoking the celebrated 
Lax conditions as additional rules of thumb, we can restore unique weak so- 
lution developments within many one-dimensional applications, although it is 
dubious that this repair will prove entirely adequate in three dimensions as well 
(resolving these matters has proved enormously difficult; cf. Yudovich [2006]). 
Generically, there are good reasons to doubt whether blow-up problems of 
this ilk can be completely repaired by such adjoined ‘rules of thumb’, simply 
because such principles rarely track the stage-by-stage evolution of the system 
in the tight manner provided by a set of differential equations. Once again, the 
central phenomenon to which we should pay the greatest attention is the ap- 
pearance of the ‘missing physics’ descriptive holes engendered by the blowups: 
the apparent failures of determinism enter largely as an artifact of the fact that 
the ‘rules of thumb’ commonly cited in repair are often unable to plug the 
descriptive holes thoroughly. 
In any case, from a strict MP point of view that demands power law forces, 
Norton’s hillside display of ‘indeterminism’ shouldn’t seem troubling at all, for 
advocates of these foundations shouldn’t accept Norton’s proffered equation 
of motion as an acceptable ‘slow variable’ decomposition for the situation at 
hand. Indeed, the complications that Malament ({2006]) has discussed indicate 
that, in any proper point particle modeling of the conditions near the top 
of the track, the real motions are likely to prove so elaborate that no ‘fast 
variable’/‘slow variable’ decomposition will prove admissible in that region at 
all. Accordingly, a stout MP advocate can properly retort, “Norton appeals 
to a perfectly rigid track in setting up his problem, but no such constraint 
can be properly justified as an approximation within this setting, for the great 
complexities of how the point mass comprising the “ball” will interact with 
the point masses comprising the “track” have been improperly idealized away 
through an unwarranted appeal to a “rigid track”’. Indeed, I augur that this

--- Page 190 ---

184 Mark Wilson 
MP response accurately captures the background thinking that explains why 
few working physicists are likely to be swayed in their conviction that ‘classical 
mechanics is deterministic’ by Norton’s example.!° 
Observe that these complications in our discussion all stem from the under- 
lying factor that puzzled Richard Montague: ‘Why have practicing classical 
physicists been so reluctant to complete the MP picture by embracing power 
law supplements (or some other expedient) in a manner that would make “de- 
terminism” a provable or refutable mathematical feature of the formalism?’ 
The proper rationale traces simply to the brute fact that, when we begin in- 
specting smallish things at close range, Mother Nature stops supplying any 
firm indication of any appropriate MP rules at all, but steers us in the direction 
of quantum physics instead. As all molecular modelers know, their sixth and 
twelfth power rules possess a first-order experimental accuracy at best and, 
insofar as one can successfully model close interactions in a ‘classical’ vein at 
all, one must seat the sources of intermolecular attraction and repulsion upon 
some kind of extended blobs (some of the relevant experimental considera- 
tions arguing against point mass foundations had become well known by the 
mid-nineteenth century; cf. Maxwell [1952]). Because Mother Nature refuses 
to supply suitable guidance with respect to possible short-range laws for point 
particles, practitioners often dismiss Montague’s request to ‘fill in the missing 
physics’ in MP by observing, ‘Oh, extended blobs actually become important at 
that scale size, not true point particles.’ In much this same ‘mass points are not 
our real concern’ spirit, Gallavotti ({1983], p. 30) appeals to the extended size 
of planets when he dismisses the ‘physical relevance’ of the Xia-like collision 
singularities arising within MP physics. When physicists offhandedly offer such 
excuses, we tacitly witness the unstable stool of ‘classical mechanics’ slowly set- 
tling upon other foundational legs: if ‘extended blobs’ become tolerated as our 
‘fundamental classical entities’, we must shift the parameters of our discussion 
of determinism into the realms of either PC or true CM. As we do this, we find 
that distinct sets of considerations need to be canvassed in these arenas than 
prove pertinent when we operate in strict MP mode. 
And there is a basic methodological tension that complicates these issues 
in real-life practice. Although from a modeling point of view we are inclined 
to object to the appearance of singularities where some density or velocity 
blows up to infinity, from a mathematical point of view we often greatly value 
these same breakdowns; for, as Riemann and Cauchy demonstrated long ago, 
the singularities of a problem commonly represent the precise features of the 
mathematical landscape we should seek in our efforts to understand how the 
qualitative mathematics of a set of equations unfolds. Insofar as the project of 
achieving mathematical understanding goes, singularities frequently prove our 
10 Arnold ({1997]) simply assumes determinism as an axiom!

--- Page 191 ---

Determinism and the Mystery of the Missing Physics 185 
best friends, not our enemies. Accordingly, if we have already decided that 
our MP formulations overlook the missing physics pertinent to extended bod- 
ies anyway, why should we gussy up the mathematical formalism of MP with 
artificial assumptions concerning repulsion at close quarters? Such unwanted 
supplements may only camouflage the very singularities that we need to un- 
cover in our attempts to understand how our system behaves when no danger 
of close contact looms. Once we recognize that MP can’t happily serve as a 
foundational basis for everything that we would normally consider to be a 
‘classical process’, we might as well allow its parochial singularities to emerge 
in as nakedly transparent a manner as possible, for, from a mathematical point 
of view, taming the singularities only impedes understanding, without provid- 
ing any compensating gain in overall modeling accuracy. In short, once we 
have decided that the point masses of MP cannot adequately serve as ‘founda- 
tional entities’ for classical physics considered as a whole, we simultaneously 
lose any motive for regarding MP’s sundry breakdowns as deficiencies. Plainly, 
such methodological considerations will shape textbook presentations of MP 
doctrine along considerably different axes than our naive ‘How does classical 
mechanics describe the world?’ expectations anticipate. And this background 
explains why the strange ‘gaps’ that Richard Montague noted within conven- 
tional formalizations of MP physics aren’t really so surprising after all. 
3.2 The physics of perfect constraints 
Once we foundationally substitute extended objects for point masses, we have 
shifted to a different framework (PC or CM) where the question of determinism 
requires a considerably different discussion. In fact, Norton’s example nicely 
indicates how this can happen, for its particulars suit the expectations native to 
PC, where extended objects!! are allowed to frame perfectly rigid surfaces upon 
which a suitable finite set!” of generalized coordinates can be installed and with 
!! A related subtlety merits a passing remark, even if it does not touch upon determinism per se. 
Strictly speaking, Norton’s example involves a dimensional mismatch between an extended and 
an unextended object: a zero-dimensional point situated upon the two-dimensional surface of a 
hillside. It is common practice to study examples of this sort, but should we really wish to accept 
such mismatches as foundationally basic? A concrete experience with continua suggests otherwise. 
The history of the subject is full of treatments where a three-dimensional system such as a plank 
has been mathematically treated as a two-dimensional or one-dimensional array, often by appeal 
to some apparent symmetry in its configuration. Nonetheless, it is now recognized that these 
reduced treatments are hard to justify rigorously and that it is a mistake to apply fundamental 
mechanical axioms to such lowered-dimensional systems directly. With respect to the dome case, 
three-dimensional balls often act quite unexpectedly on frictionless planes and one should be 
careful about presuming that Norton’s predicted motion will emerge in any reasonable limit 
as a three-dimensional ball is reduced in size toward a point. However, this quibble, although 
important for foundational work, does not affect Norton’s example in any material way, for we 
can reproduce analogs of his circumstances in higher dimensions. 
> Historically, the great interest in Lagrangian mechanics and other forms of variational principle 
lay in the hope that detailed hypotheses about local contact action could be evaded by such

--- Page 192 ---

Mark Wilson 
Figure 6. Ball on single track. 
respect to which the principle of generalized inertia proves completely valid. 
However, a number of fresh subtleties immediately intervene, the first of which 
is that ‘analytical mechanics’ is rarely presented in a manner where its intended 
scope is clearly specified. The most common frameworks for implementing this 
flavor of physics utilize either a Lagrangian or a Hamiltonian operator upon 
generalized coordinates in the context of holonomic constraints (although, as 
we shall soon observe, this last requirement is strangely limiting). The basic 
trouble is that most presentations are vague as to what should be tolerated 
under the heading of a ‘generalized coordinate’ and this ambiguity makes it 
hard to evaluate the precise pertinence of Norton’s example to PC in turn. 
For example, consider a ball freely sliding along a tube with a sharp bend and 
let the quality S(t) mark its increasing arc length displacements along its path 
(Figure 6). Does this ball-and-tube system fall within the proper ambit of PC? 
The answer depends upon whether S(t) qualifies as an acceptable ‘generalized 
coordinate’ or not. But this issue looks as if it can be reasonably resolved 
in three ways. Answer 1: No, because S(t) doesn’t possess completely smooth 
derivatives with respect to regular Cartesian coordinates. Answer 2: Yes, because 
the ball’s generalized inertial motion proves perfectly smooth relative to S(t) 
itself (I'll supply answer 3 shortly). However, if we accept this second answer, 
shouldn’t we also allow a tube-and-ball system that splits both to the left and 
means; cf. (Darrigol [2005], Chapter 1). The intended arena is continuum mechanics, with finite- 
dimensional modelings emerging only as first-order approximations through ‘lumping’. The 
notion that finite-dimensional ‘analytical mechanics’ might prove foundationally central emerged 
only later, with no clear parent insofar as I am aware. But Norton’s example is troubling only if 
we adopt such a point of view.

--- Page 193 ---

Determinism and the Mystery of the Missing Physics 
Figure 7. Ball on split track. 
right as an acceptable PC system as well, for S(t) still appears as if it can serve 
as a suitable generalized coordinate for a bail rolling along a bifurcated tube 
(Figure 7)? Granting S(t) this status, our split tube setup looks to be prima 
facie indeterministic under the assumption that generalized inertia must carry 
our ball along one tube or other past the forking. In fact, I have seen precisely 
this case, with these same implicit assumptions, cited to prove that ‘classical 
mechanics is indeterministic’ (Truesdell [1966]). However, we can reasonably 
reject this possibility if we either retreat to answer | or (this is the delayed ‘answer 
3’) we simply declare S(t) unacceptable as a ‘generalized coordinate’ in this case 
simply because no complete set of admissible coordinates'? manages to fix the 
ball’s state uniquely! In other words, answer 3 renders PC deterministic by fiat: 
any appeal to ‘generalized coordinates’ for a system already presupposes that 
the system enjoys a set of coordinates that certify its motions as deterministic. 
In truth, split tube arrangements are usually not tolerated as ‘part of analytical 
mechanics’ simply because no one really wants to bother with such outré 
states of affair when systems are studied from this point of view. To those who 
fancy that PC aspires to embrace ‘all of classical mechanics’, this exclusion 
should seem arbitrary, but, in fact, ‘analytical mechanics’ has no business 
pretending that it can successfully accommodate every intuitively expected 
‘classical situation’ anyway (its credentials for this foundational office are much 
worse, I think, than even those for MP). But once its inherent descriptive gaps 
are cheerfully acknowledged, applied mathematicians can reasonably demand 
that mathematical understanding should trump complete physical modeling when 
they carve out a proper arena for ‘analytical mechanics’, for reasons similar to 
'S To have a complete set of coordinates for the split tube case, we also need a variable to mark 
lateral position within the tubes and this variable won’t normally prove smooth (although we can 
improve matters by tricks like Norton’s).

--- Page 194 ---

Mark Wilson 
Figure 8. Locomotive wheel. 
those that we canvassed in point mass circumstances. From this point of view, 
answer 3 isn’t unreasonable at all. 
Incidentally, if we adopt answer |’s approach to ‘generalized coordinates’ and 
'4 we might save determinism in this case add a few supplementary assumptions, 
by claiming that the unique ‘correct resolution’ to our divided tube problem 
is one where the ball rebounds up the tube in a reverse direction (indeed, I 
have often heard this response offered as a ‘solution’ to the indeterminism 
problem). However, Norton’s example nicely demonstrates that this kind of 
answer is not sufficiently general—the singularity at the tube’s bifurcation point 
can be sufficiently smoothed to make the non-unique continuations appear 
‘more normal’. To see how this can happen, let us consider another example, 
which is often cited to demonstrate ‘classical non-determinism’ in the older 
literature (Figure 8). Suppose we have a locomotive wheel and rod in the 
configuration sketched, where some large motive thrust F is applied along 
the piston rod (engineers call such configurations the ‘dead points’ of the 
mechanism; their presence creates great headaches in real-life design work). In 
which direction will the wheel turn under F’s influence? As matters presently 
stand, F is perfectly matched by the ‘forces of constraint’ that arise within 
the wheel, so the summed applied force ‘performs no work’ on the wheel. 
However—and here is where the underlying parallels with Norton’s hillside 
case become palpable—as soon as the piston becomes slightly inclined from 
the horizontal, the altered geometry will allow some of the thrust F to ‘do work’ 
on the wheel and accelerate its turning motion. Sometimes it is objected that 
‘Really the wheel can never move from its “dead spot” configuration because its 
acceleration must display an unacceptable jump to do so,’ in the same vein as we 
‘solved’ the tube indeterminacy. However, if our wheel happens to be shaped 
'4 In accepting the rebounding state of affairs as a ‘solution’, we tacitly tolerate a trajectory that falls 
outside the usual requirements of analytic mechanics. An adequate investigation of ‘determinism’ 
relevant to such circumstances will require a more precise delineation of the range of ‘weak 
solutions’ tolerated.

--- Page 195 ---

Determinism and the Mystery of the Missing Physics 189 
like the top of Norton’s dome, then such ‘constrained force converting to 
active force’ scenarios can be rendered smooth enough to bypass this standard 
objection. In any case, it’s hard to remain consistently prissy about smooth 
movements within the realm of mechanism, which frequently display impulsive 
reversals in all sorts of natural contexts. 
Such concerns represent but the tiny tip of a much larger iceberg of woes 
that lurk within the PC universe. As soon as we admit rigid objects of a finite 
geometry into ‘classical physics’, it comes hard to accommodate the full range 
of expected setup variations within the framework of any fixed formalism. We 
have just witnessed a basic prototype: if God can build a tube with a sharp bend 
to the left, why can’t He also build one that splits in both directions? ‘Because 
it doesn’t suit my formalism’ seems like a shabby answer. Nor are we likely to 
look favorably upon physicists who claim that balls can’t roll and skaters can’t 
glide down hillsides because such motions aren’t tolerated within their favored 
form of PC. ‘Surely it’s much easier to build a ball or a skate than anything 
that will “purely slide” in your fashion,’ we complain. ‘Surely you’ve omit- 
ted much of the basic physics that governs classical contact interaction from 
your formalism.’ Although careless readers often overlook the fact, the famil- 
iar forms of ‘analytical mechanics’ found in most textbooks tacitly demand 
‘holonomic constraints’, which can accommodate neither rolling nor skating. 
And even after these significant lapses in coverage are corrected by adopting 
a ‘virtual work’ framework for PC better suited to rolling and sliding, the re- 
vised formalism will still prove unable to handle situations of over-constraint as 
exemplified within a modification of Aristotle’s celebrated double-axled wheel, 
where the two hubs are mounted on rails in rack-and-pinion fashion. PC lacks 
any method for resolving how such a device will resolve the incompatible con- 
straints restricting its free motion (Figure 9).'> Our immediate intuitive reaction 
to this over-constraint is to note that the interior of the wheel must somehow 
flex in response to these applied stresses: “Your analytical mechanics has left 
out the physics of all that,’ we complain. Quite right, but to tolerate flexure is 
ipso facto to depart the happy land of PC and instead take up foundational 
residence within the realm of pure continua (CM) where perfectly rigid bodies 
are normally rejected as impossible, strictly speaking. Once again, the specter 
of ‘missing physics’ has caused the stool of ‘classical mechanics’ to rock over 
to new foundational legs. 
Before we briefly inspect the CM situation, let me comment upon a pecu- 
liar feature of classical analytical mechanics’ present centrality within physics. 
Although, rightly understood, it is hard to regard the formalism of analytical 
mechanics as capturing ‘the full world of classical mechanics’ in any acceptable 
'S Hertz’ system of rigid-body-based mechanics ({1956]) was often criticized for neglecting over- 
constraint of this sort.

--- Page 196 ---

Mark Wilson 
Figure 9. Aristotle’s wheel. 
tensions 
Figure 10. Notched rod. 
way, it nonetheless serves as an important guide, through standard quantiza- 
tion procedures, as to how the laws of the guantum world behave. For whatever 
reason, the symplectic structures etc. natural only to gliding but not rolling 
balls seem just the ticket to success with respect to quantum foundations. But 
we should not let this strange ‘success in guiding quantization’ trick us into 
overlooking the descriptive gaps that analytic mechanics tolerates within its 
originally intended classical home. 
3.3 Continuum mechanics 
Turning briefly to CM, we find that we can scarcely evade the problems caused 
by finite geometries, although they now arise in more subtle ways (Figure 10). 
‘If God can cut a rounded notch into a steel rod, why can’t He cut an en- 
tirely sharp notch as well?’ we wonder. Yet, orthodox models for a sharply 
notched rod demand a blowup singularity in the stress at the notch. Realisti- 
cally, we know that steel will flow plastically or even fragment long before any

--- Page 197 ---

Determinism and the Mystery of the Missing Physics 191 
extremely high stress is achieved, but our formalism has not demanded that the 
physics needed to activate these processes be installed within our steel. Should 
we require it to do this? Well, materials scientists have developed some fairly 
good stories for plastic flow along this line, but they don’t regard any of them 
as better than first-order approximations. So we again face an uncomfortable 
dilemma much like our reluctance to embrace wholeheartedly the power law 
repulsion rules utilized by the molecular modelers; we can tame our notch 
anomaly through a ‘missing physics’ rule of thumb that we don’t particularly 
trust. And even if we do this, certain natural geometries are apt to introduce 
singularities within these revised models as well. In the meantime, the math- 
ematicians continue to advise us, “Wait a minute! We /ike those singularities. 
Don’t smooth them over with some crude rule of thumb that you don’t really 
trust anyway.’ Accordingly, workers in CM have instead attempted to discover 
pleasing but rather ad hoc compromises between restrictions upon the range 
of setup geometries they tolerate and answers that accept certain singularities 
within some specified flavor of generalized ‘solution’. Efforts along these lines 
have forced the applied mathematician to consider very delicate flavors of func- 
tion spaces, often adapted to different sorts of problem in different ways. So, 
what do ‘classical mechanics’ foundational objects’ look like within CM in light 
of these methodological subtleties? It becomes very hard to say. We might look 
to Mother Nature for foundational guidance, but, when pressed about classical 
physics, she only smiles disagreeably and points to quantum chemistry as the 
correct story of what occurs at such scale lengths. 
4 Conclusion 
Accordingly, despite our philosophical wishes otherwise, it seems unlikely that 
we'll ever manage to get our classical stool to sit firmly at rest on solid legs, 
simply because some flavor of ‘missing physics’ consideration seems always 
ready to rock us off our present perch. We simply don’t know how to fill out 
‘the world of classical physics’ in any consistent manner that doesn’t tolerate 
strange gaps where some otherwise expected ‘classical situation’ becomes dis- 
allowed on seemingly arbitrary grounds (indeed, often the circumstances that 
stymie classical foundational story F can be nicely modeled within the frame- 
work of some alternative classical story G and vice versa—a foundational 
instability that I have elsewhere (Wilson [2006]) called ‘the lousy encyclopedia 
phenomenon’). As long as such gaps persist, Norton-like indeterminacies may 
sometimes creep in, largely as a consequence of having adopted some fill-in 
‘rule of thumb’ (e.g., the constraint provided by Norton’s perfectly rigid track), 
which we don’t believe truly ‘gets all of the classical physics of the real-life situ- 
ation right.’ Accusations of ‘indeterminism’ rarely seem definitive in such cases, 
simply because we’ve never really trusted the ‘rules of thumb’ upon which they

--- Page 198 ---

192 Mark Wilson 
trade in the first case. This is why ‘missing physics’ gaps of the sorts we have 
surveyed represent a more central feature of classical mechanics’ peculiar cir- 
cumstances than any of its potential indeterminacies. These basic foundational 
considerations, it seems to me, supply the true explanation of why long ago 
Richard Montague wasn’t able to locate enough ingredients to settle rigorously 
whether ‘classical mechanics’ is deterministic or not. 
Acknowledgements 
This essay was written for a 2006 UCLA symposium on determinism. I would 
like to thank the other participants (especially David Malament, John Norton, 
and Sheldon Smith) for helpful comments. 
Department of Philosophy, University of Pittsburgh 
1001 Cathedral of Learning, Pittsburgh 
PA 15260, USA 
mawilson@pitt.edu 
References 
Arnold, V. I. [1977]: Mathematical Methods of Classical Mechanics, Berlin: Springer. 
Earman, J. [1986]: Determinism: A Primer, Dordrecht: Kluwer. 
Gallavotti, G. [1983]: The Elements of Mechanics, New York: Springer. 
Darrigol, O. [2005]: Worlds of Flow, Oxford: Oxford University Press. 
Hertz, H. [1956]: The Principles of Mechanics, New York: Dover. 
Hilbert, D. [1976]: ‘Mathematical Problems’, in F. Browder (ed.), Mathematical De- 
velopments Arising from Hilbert Problems, Providence, RI: American Mathematical 
Society, pp. 1-34. 
McKinsey, J. C. C., Sugar, A. C. and Suppes, P. [1953]: ‘Axiomatic Foundations 
of Classical Particle Mechanics’, Journal of Rational Mechanics and Analysis 2, 
pp. 253-72. 
Maxwell, J. C. [1952]: ‘Atoms’, in W. D. Niven, (ed.), Scientific Papers of James Clerk 
Maxwell, Volume 2, New York: Dover, pp. 445-84. 
Malament, D. [2007]: ‘Norton’s Slippery Slope’, 20th Biennial Meeting of the 
Philosophy of Science Association, (Vancouver, BC), 2006. Available online at 
<philsci-archive.pitt.edu/archive/00003195/>. 
Montague, R. [1974]: Formal Philosophy, New Haven: Yale University Press. 
Norton, J. [2006]: ‘The Dome: An Unexpectedly Simple Failure of Determinism’, 
20th Biennial Meeting of the Philosophy of Science Association, (Vancouver, BC), 
2006. Available online at <philsci-archive.pitt.edu/archive/00002943/>. 
Saari, D. [2005]: Collisions, Rings and Other Newtonian N-Body Problems, Providence, 
RI: American Mathematical Society. 
Smoller, J. [1983]: Shock Waves and Reaction—Diffusion Equations, New York: Springer. 
Thompson, W. (Lord Kelvin) and Tait, P. [1867]: Treatise on Natural Philosophy, Oxford: 
Oxford University Press.

--- Page 199 ---

Determinism and the Mystery of the Missing Physics 193 
Truesdell, C. [1966]: Six Lectures on Modern Natural Philosophy, Berlin: Springer. 
Truesdell, C. [1984]: ‘Suppesian Stews’, in his An Idiot's Fugitive Essays on Science, 
New York: Springer, pp. 503-79. 
Xia, Z. [1992]: ‘The Existence of Noncollision Singularities in Newtonian Systems’, 
Annals of Mathematics, 135, pp. 411-68. 
Wilson, M. [2006]: Wandering Significance, Oxford: Oxford University Press. 
Yudovich, V. I. [2006]: “Global Solvability versus Collapse in the Dynamics of an Incom- 
pressible Fluid’, in A. A. Bolibruch, Y. S. Osipov and Y. G. Sinai (eds), Mathematical 
Events of the Twentieth Century, Berlin: Springer, pp. 501-28.

--- Page 201 ---

Brit. J. Phil. Sci. 60 (2009), 195-220 
What Are the New Implications of 
Chaos for Unpredictability? 
Charlotte Werndl 
ABSTRACT 
From the beginning of chaos research until today, the unpredictability of chaos has 
been a central theme. It is widely believed and claimed by philosophers, mathematicians 
and physicists alike that chaos has a new implication for unpredictability, meaning that 
chaotic systems are unpredictable in a way that other deterministic systems are not. 
Hence, one might expect that the question ‘What are the new implications of chaos for 
unpredictability?’ has already been answered in a satisfactory way. However, this is not 
the case. I will critically evaluate the existing answers and argue that they do not fit the 
bill. Then I will approach this question by showing that chaos can be defined via mixing, 
which has never before been explicitly argued for. Based on this insight, I will propose 
that the sought-after new implication of chaos for unpredictability is the following: 
for predicting any event, all sufficiently past events are approximately probabilistically 
irrelevant. 
Introduction 
Dynamical Systems and Unpredictability 
2.1 Dynamical systems 
2.2 Natural invariant measures 
2.3. Unpredictability 
Chaos 
3.1 Defining chaos 
3.2. Defining chaos via mixing 
Criticism of Answers in the Literature 
4.1. Asymptotic unpredictability? 
4.2. Unpredictability due to rapid or exponential divergence? 
4.3. Macro-predictability and Micro-unpredictability? 
A General New Implication of Chaos for Unpredictability 
5.1. Approximate probabilistic irrelevance 
5.2 Sufficiently past events are approximately probabilistically 
irrelevant for predictions 
Conclusion 
for the Ph The Author (2009). Published by Oxford University Press on behalf of British Societ 
doi:10.1093/bjps/axn053 For permissions, please email: j< ls. permissions@oxfo 
ess published on January 19, 2

--- Page 202 ---

Charlotte Werndl 
1 Introduction 
In the past decades, much ado has been made about chaos research, which has 
been hailed as having led to revolutionary scientific insights. Since the begin- 
nings of the systematic investigation of chaos until today, the unpredictability 
of chaotic systems has been at the centre of interest. 
There is widespread belief in the philosophy, mathematics and physics com- 
munities (and it has been claimed in various articles and books) that there is a 
new implication of chaos for unpredictability, meaning that chaotic systems are 
unpredictable in a way other deterministic systems are not. More specifically, 
what is usually believed is that there is at least one new implication of chaos for 
unpredictability that holds true in a// chaotic systems. 
The physicist James Lighthill, commenting on the impact of chaos on un- 
predictability, expresses this point as follows: 
We are all deeply conscious today that the enthusiasm of our forebears for 
the marvellous achievements of Newtonian mechanics led them to make 
generalizations in this area of predictability which, indeed, we may have 
generally tended to believe before 1960, but which we now recognize were 
false. (Lighthill [1986], p. 38) 
These features connected with predictability that I shall describe from 
now on, then, are characteristic of absolutely all chaotic systems. (Lighthill 
[1986], p. 42) 
Similarly, Weingartner ([1996], p. 50) says that ‘the new discovery now was that 
[...] a dynamical system obeying Newton’s laws [. . .] can become chaotic in its 
behaviour and practically unpredictable’. 
Thus, the question ‘What are the new implications of chaos for unpredictabil- 
ity?’ appears natural, and one might well suppose that it has already been 
satisfactorily answered. However, this is not the case. On the contrary, there is 
a lot of confusion about what exactly the new implications of chaos for unpre- 
dictability are. Several answers have been proposed, but, as we will see, none of 
them fit the bill. 
Fundamental questions about the limits of predictability have always been 
of concern to philosophy. So, the widespread belief and the various flawed 
accounts about the new implications of chaos for unpredictability demand 
clarification. The aim of this paper is to critically discuss existing accounts and 
to propose a novel and more satisfactory answer. 
My answer will be based on two insights. First, I,will show that chaos can 
be defined in terms of mixing. Although mixing is occasionally mentioned 
in connection with chaos, to the best of my knowledge, so far no one has 
explicitly argued that chaos can be thus defined. Second, I will argue that mixing 
has a natural interpretation as a particular form of approximate probabilistic

--- Page 203 ---

What Are the New Implications of Chaos for Unpredictability? 197 
irrelevance which is a form of unpredictability. On this basis, I will propose a 
general novel answer: a new implication of chaos for unpredictability is that 
for predicting any event at any level of precision, all sufficiently past events are 
approximately probabilistically irrelevant. 
The structure of the paper is as follows. Section 2 will provide the background 
of our discussion. I will introduce dynamical systems and discuss the concepts 
of unpredictability relevant for this paper. Section 3 will be about chaos. Here, I 
will show that chaos can be defined in terms of mixing. After that, in Section 4, 
I will examine the existing answers to the question of the new implications of 
chaos for unpredictability, which I dismiss as mistaken. In Section 5, I propose 
a general answer that does not suffer from the shortcomings of the other 
accounts. 
2 Dynamical Systems and Unpredictability 
2.1 Dynamical systems 
Chaos is discussed in dynamical systems theory. A dynamical system is a math- 
ematical model consisting of a phase space X, the set of all possible states of 
the system, and evolution equations that describe how solutions evolve in phase 
space. Dynamical systems often model natural systems. 
There are discrete dynamical systems and continuous dynamical systems. Dis- 
crete dynamical systems are systems in which the time increases in discrete 
steps. Formally, they consist of a set X as phase space anda map 7: X¥ > X 
as evolution equation; the dynamics of the system are given by x,4; = T(x), 
x € X,n € No. The solution through x is the sequence (7T”(x)),>0, which is 
also referred to as the iterates of x. If T is invertible (non-invertible), I speak of 
an invertible (non-invertible) discrete dynamical system, respectively. Contin- 
uous dynamical systems involve a continuous time parameter. They typically 
arise from differential equations. By definition, all dynamical systems and thus 
chaotic systems are deterministic. ! 
For simplicity, I will often confine my attention to discrete dynamical sys- 
tems. I can do this without loss of generality because all definitions of chaos 
I will be using can be directly carried over to continuous dynamical systems. 
Alternatively, a continuous dynamical system can be regarded as chaotic if and 
only if there is a suitable Poincaré section such that the discrete dynamical sys- 
tem defined by the Poincaré map is chaotic (e.g. Smith [1998], pp. 92-3). Hence, 
everything I will say about the new implications of chaos for unpredictability 
equally applies to continuous dynamical systems. 
' According to the conventional definition of Montague ([1962]) and Earman ({1971]), a dynamical 
system is deterministic if and only if any two solutions that agree at one time agree at all future 
times.

--- Page 204 ---

198 Charlotte Werndl 
Dynamical systems divide into two groups: volume-preserving systems, 
among them Hamiltonian systems, and dissipative systems. A volume- 
preserving system is defined as a system in which the phase-space volume 
is preserved under time evolution, i.e. the volume (formally the Lebesgue mea- 
sure) of any region of phase space remains the same as this region is evolved 
according to the evolution equations (Smith [1998], p. 16). Dissipative systems 
are systems which are not volume preserving. 
There are two types of dynamical systems relevant for our discussion. First, if 
for a discrete system there is a metric d, where d measures the distance between 
points in phase space, (X, d, T) is called a ‘topological dynamical system’. It is 
generally assumed in the literature (e.g. Devaney [1986], p. 51) that topologi- 
cal systems provide a possible framework for characterising chaos. This makes 
intuitive sense because it is often imagined that in case of chaotic behaviour 
there is some way of measuring the distance between points in the phase space 
X, and thus that there is a metric defined on XY. Moreover, to the best of my 
knowledge, there is always a natural metric for paradigmatic chaotic systems. 
Often the phase space is simply a subset of R”, n > 1, and the metric is the 
standard Euclidean metric. 
The second type of dynamical system is a measure-theoretic dynamical sys- 
tem. It isa quadruple (XY, ©, uw, 7) consisting of a phase space X, a o-algebra D 
on X,a measure yu with w( X) = | anda surjective measurable map T: X > X. 
If a property holds for all points in a subset X of ¥ for which w(X) = 1, it is 
said that it holds for almost all points. 
Important for us is what is called a ‘measure-preserving dynamical system’. 
It is a measure-theoretic system where for all A € X 
u(T~'(A)) = uA), (1) 
where T~'(.A) = {x € X: T(x) € A} (cf. Cornfeld et al. [1982], pp. 3-5). Con- 
dition (1) says that the measure yp is invariant under the dynamics of the system. 
Although there exist evolution equations that do not have invariant measures, 
for very wide classes of systems invariant measures can be proven to exist. For 
instance, if T is a continuous map on a compact phase space endowed with a 
metric, there exists at least one invariant measure (Mafié [1983], p. 52). 
As is sometimes claimed (e.g. Eckmann and Ruelle [1985]), measure- 
preserving systems provide a possible framework for characterising chaos. For 
volume-preserving systems, the natural invariant measure is typically the 
Lebesgue measure or a normalized Lebesgue measure, e.g. the microcanonical 
2 Descriptions of a dynamical system via metric spaces and measures are usually related in the 
following way: the o-algebra © of a measure-theoretic system is, or at least includes, the Borel 
o-algebra of the metric space (X, d) of the topological system. The Borel o-algebra of (X, d) 
is the o-algebra generated by all open sets of X (cf. Mané [1983], pp. 2-3). Intuitively, it is the 
o-algebra which arises from the metric space (X, d).

--- Page 205 ---

What Are the New Implications of Chaos for Unpredictability? 199 
measure of classical statistical mechanics. For dissipative systems, to the best of 
my knowledge, all systems that have ever been identified as chaotic have or are 
supposed to have a natural invariant measure if one considers the following. 
Many chaotic systems have attractors. For a topological system (Y, d, T) 
the set A C Y is an attractor if and only if (i) T(A)= A; (ii) there is a 
neighbourhood U 3D A such that all solutions are attracted by A, i.e. for all 
yin U limy_..0 inf{d(T"(y), x) | x € A} = 0; and (iii) no proper subset of A sat- 
isfies (i) and (ii). Liouville’s theorem implies that only dissipative systems can 
have attractors (Schuster and Just [2005], p. 162).° As we will see in the next 
section, for chaotic systems the evolution of any bundle of initial conditions 
eventually enters every region in phase space. This is impossible for the motion 
approaching an attractor since the attracted solutions never return to where 
they originated. Hence, chaotic behaviour can occur only on A. The chaotic 
motion is described by a system with phase space A, and the invariant measure 
is only defined on A. Generally, an attractor on which the motion is chaotic is 
called a ‘strange attractor’. 
Of course, in practice one is often concerned with solutions approaching 
a strange attractor. Yet after a sufficiently long duration, either the solutions 
enter the attractor or come arbitrarily near to the attractor. In the latter case, 
since the dynamics is typically continuous, when the solutions are sufficiently 
near to the attractor they essentially behave like the solutions on the attractor. 
And in applications such solutions which are sufficiently near to a strange 
attractor are considered to be chaotic for practical purposes. In particular, in 
the latter case the unpredictability of solutions very near to the attractor is 
practically indistinguishable from the one on the attractor. Consequently, for 
characterising the unpredictability of motion dominated by strange attractors, 
it is widely acknowledged that it suffices to consider the dynamics on attractors, 
where natural invariant measures can be defined. 
2.2 Natural invariant measures 
What are natural invariant measures, in particular for dissipative systems? From 
an observational viewpoint, it is natural to demand that the long-run time- 
averages of almost all solutions approximate the measure. Such measures are 
called ‘physical measures’. Let us look at them in more detail (cf. Eckmann and 
Ruelle [1985], pp. 626, 639-40). 
For measure-preserving systems (X, ©, u, T) with A(X) > 0, where A is the 
Lebesgue measure, the following method identifies physical measures. (M1) 
(i) Take any A C YX. (ii) Take an initial condition x € X. (iii) Consider L4(x), 
Some other definitions of ‘attractor’ allow that volume-preserving systems can have attractors; 
yet these definitions are not standard in our context.

--- Page 206 ---

200 Charlotte Werndl 
the long-run average of the fraction of iterates of x which are in A. (iv) Consider 
G4= {xe X| L4(x) = p(A)}. Then yp is a physical measure if and only if for 
any A € & Lebesgue-almost-all initial conditions approximate the measure of 
A, i.e. A(G 4) = A(X). If such a measure exists, it is unique. 
What are physical measures for strange attractors? I will be concerned with 
two kinds of strange attractors: first, the case where all solutions eventually 
enter an attractor A with A(A) > 0. Clearly, here method (M1) can be applied 
for X = A. Second, it can be that the solutions approach but never enter an 
attractor A with A(A) =0 but A(U) > 0, where U is the neighbourhood of 
A. Here the method has to be slightly modified. (M2) (i) Take any region 
AC A. (ii) Take an initial condition x € U. (iii) Consider L 4(x), the long-run 
average of the fraction of iterates of x which are close to A. (iv) Consider 
G4= {x € U| L4(x) = p(A)}. Then p is a physical measure if and only if for 
all A € © it holds that A(G 4) = A(U). If such a measure exists, it is unique. 
As we will see in the next section, chaotic systems are ergodic. A measure- 
preserving system (X, £, , 7) is ergodic if and only if for all A€ © with 
p( A) > 0 
H(Un>0 T~"(A)) —— (2) 
Now for ergodic volume-preserving systems the Lebesgue measure is the phys- 
ical measure. As we will see in the next section, typically for systems proven 
to be chaotic physical measures can be proven to exist (Lyubich [2002]; Young 
[2002]). For systems only conjectured to be chaotic numerical evidence gener- 
ally favours the existence of physical measures (Young [1997]). 
For an example, consider the logistic map 7(x) : [0,1] > [0,1], T(x) = 
ax(1 — x) with aw © 3.6785. Here the solutions enter an attractor of positive 
Lebesgue measure. Now we choose an initial condition on the attractor and 
draw a histogram of the fraction of iterates of x (up to an iterate T"(x),n > 1) 
which are in a particular part on the attractor. Then, for Lebesgue-almost-all 
initial conditions we chose on the attractor, we obtain what is illustrated in 
Figure 1: as n goes to infinity and the histogram becomes finer, the histograms 
approximate a particular measure on the attractor. Hence, this measure is phys- 
ical according to method (M1) (cf. Jakobson [1981]). 
For another example consider the Lorenz equations 
dx(t) 
dt 
dy(t) — =rx(t)- y(t) — x(t)z(t), 
dt 
dz om) = x(t) y(t) — bz(t), dt 
= a(y(t) — x(0)),

--- Page 207 ---

What Are the New Implications of Chaos for Unpredictability? 201 
(a) (b) 
Figure 1. (a) Histogram and (b) natural measure of the logistic map for a + 3.6785. 
Figure 2. Numerical solution of the Lorenz equations foro = 10,r = 28, b = 8/3. 
for the parameter values o = 10, r = 28 and b = 8/3, which Lorenz ([1963]) 
considered. Here it is proven that there is a strange attractor of Lebesgue 
measure zero such that all solutions originating in the neighbourhood of the 
attractor, which is of positive Lebesgue measure, approach but never enter 
the attractor. Figure 2 shows a numerical solution of these equations; one 
can vaguely discern the shape of the attractor, known as the Lorenz attractor, 
because the solution spirals towards it. According to the method (M2), the 
physical measure is the one for which the following holds for Lebesgue-almost- 
all initial conditions in the neighbourhood of the attractor: the long-run average 
of the fraction of time that the solution spends c/ose to a set A on the attractor 
approximates the measure of A (cf. Luzzatto et al. [2005]).4 
4 There are also other natural measures. For instance, v is absolutely continuous with respect to 
4, where v and yw are measures on a measurable space (X, 2), if and only if for all Ae = 
with (A) = 0 also v( A) = 0. Absolute continuity with respect to the Lebesgue measure can be 
justified (Malament and Zabell [1980]; van Lith [2001], p. 590). Hence, if there is a unique ergodic 
invariant measure absolutely continuous with respect to the Lebesgue measure, it is a natural 
one. For ergodic volume-preserving systems, the Lebesgue measure is such a unique measure. For 
many systems, e.g. wide classes of one-dimensional maps and, as we will see in the next section,

--- Page 208 ---

202 Charlotte Werndl 
Invariant measures are commonly interpreted as probability densities. This 
deep and controversial issue has, of course, been discussed in statistical me- 
chanics but is not the focus of this paper. I mention only two interpretations 
that naturally suggest interpreting measures as probability and relate to our 
discussion. According to the time-average interpretation, the measure of a set 
Ais the long-run average of the fraction of time that a solution spends in A. Ac- 
cording to the ensemble interpretation, the measure of a set A at ¢ corresponds 
to the fraction of solutions starting from some set of initial conditions that are 
in A at time ¢ (Berkovitz et al. [2006], p. 675). 
2.3 Unpredictability 
There are different kinds of unpredictability in dynamical systems. I will only 
introduce two concepts needed for the discussion of our main question. 
According to the first concept of unpredictability, a system is unpredictable 
when any bundle of initial conditions spreads out more than a specific diameter 
representing the prediction accuracy of interest (usually of larger diameter than 
the one of the bundle of initial conditions). When this happens, the system is 
unpredictable in the sense that the prediction based on any bundle of initial 
conditions is so imprecise that it is impossible to determine the outcome of 
the system with the desired prediction accuracy.* A well-known example is a 
system in which, due to exponential divergence of solutions, any bundle of 
initial conditions of at least a specific diameter spreads out over short time 
periods more than a diameter of interest. 
The second concept of unpredictability is probabilistic. It says that for prac- 
tical purposes any bundle of initial conditions is irrelevant, i.e. makes it neither 
more nor less likely that the state is in a region of phase space of interest. Accord- 
ing to this concept, it is not only impossible to predict with certainty in which 
region the system will be, but in addition, for practical purposes, knowledge of 
the possible initial conditions neither heightens nor lowers the probability that 
the state is in a given region of phase space. An example is that knowledge of 
any bundle of sufficiently past initial conditions is practically irrelevant for pre- 
dicting that the state of the system is in a region of phase space. Eagle ((2005], 
p. 775) defines randomness as a strong form of unpredictability: an event is ran- 
dom if and only if the probability of the event, conditional on evidence, equals 
the prior probability of the event. This idea relativised to practical purposes is 
at the heart of our second concept. Consequently, this second concept can also 
be regarded as a form of randomness. 
many paradigmatic dissipative chaotic systems including strange attractors, there is a unique 
ergodic measure absolutely continuous with respect to the Lebesgue measure (Lyubich [2002)). 
For instance, for the logistic map with x ~ 3.6785, the measure of Figure 1(b) is such a unique 
measure (Jakobson [1981}). 
Schurz ({1996], pp. 133-9) discusses several variants of this form of unpredictability.

--- Page 209 ---

What Are the New Implications of Chaos for Unpredictability? 203 
Clearly, the first and second concepts of unpredictability are different and 
cannot be expressed in terms of each other since the notions of ‘diameter’ and 
‘probability’ are not expressible in terms of each other. 
3 Chaos 
3.1 Defining chaos 
I base the discussion of defining chaos on the following assumption, which is 
widely accepted in the literature (e.g. Brin and Stuck [2002], p. 23; Devaney 
[1986], p. 51). A formal definition of chaos is adequate if and only if 
(i) it captures the main pretheoretic intuitions about chaos, and 
(ii) it is extensionally correct (i.e. correctly classifies essentially all systems 
which, according to the pretheoretic understanding, are uncontroversially 
chaotic or non-chaotic). 
Let us first direct our attention to (i). Roughly, chaotic systems are de- 
terministic systems showing irregular, or even random, behaviour and sensitive 
dependence to initial conditions (SDIC). SDIC means that small errors in initial 
conditions lead to totally different solutions. 
The logistic map T : [0, 1] —~ [0, 1], T(x) = ax(1 — x) fora = 4isa paradig- 
matic chaotic system. Figure 3 shows the first six iterates of a small bundle of 
initial conditions 7, and suggests that any bundle blows up substantially. Thus, 
the system appears to exhibit SDIC. This figure also suggests that any bundle 
blows up until it covers the whole phase space. Thus, the motion appears to 
exhibit not only SDIC but also irregular behaviour in the following sense: any 
bundle of initial conditions eventually intersects with any other region in phase 
space, a property called denseness. It is widely agreed that SD/C and denseness 
are necessary conditions for chaos (Niellsen [1999], pp. 14-5; Peitgen et al. 
[1992], pp. 509-21; Smith [1998], pp. 167-9). This motivates the following 
7) 
T2) 
Ta) 
0 
0 I TH MH TM1 O 
. Behaviour of the logistic map for a = 4.

--- Page 210 ---

204 Charlotte Werndl 
criterion: a definition applying to dynamical systems captures the main pretheo- 
retic intuitions about chaos if and only if it implies SDIC and denseness. 
Let us now discuss (ii), the requirement of extensional correctness. Imagine 
we are concerned with a pretheoretic property P. Further, assume that we are 
faced with a class of objects some of which uncontroversially have property P, 
others uncontroversially fail to have property P, and yet others are borderline 
cases or controversial in some sense. The task is to find an unambiguous 
definition of P. Then it is natural to say that an unambiguous definition of the 
property P is extensionally correct if and only if it classifies all objects correctly 
which uncontroversially have or do not have property P. For the borderline 
objects, it is unimportant how they are classified, and we defer to the definition. 
Being chaotic is such a property because the pretheoretic idea of chaos is 
somewhat vague. Among the dynamical systems whose behaviour is mathe- 
matically well understood, there is a broad class of uncontroversially chaotic 
systems and a broad class of uncontroversially non-chaotic systems. Moreover, 
there are a few borderline cases, for example the system discussed by Martinelli 
et al. ([1998], p. 199), where it is not clear whether they are chaotic (Brin and 
Stuck [2002], p. 23; Robinson [1995], pp. 81-5; Zaslavsky [2005], pp. 53-4). 
Consequently, I say that a formal definition of chaos is extensionally correct if 
and only if it correctly classifies essentially all mathematically well understood, 
uncontroversially chaotic and non-chaotic behaviour. 
Several definitions of chaos have been proposed (Lichtenberg and Lieberman 
[1992], pp. 302-9; Robinson [1995], pp. 81-6). While these definitions are very 
similar, they are all inequivalent. For want of space, I cannot discuss all these 
definitions here and instead focus on a definition of chaos in terms of mixing, 
which will be crucial later on. 
3.2 Defining chaos via mixing 
Intuitively speaking, the fact that a system is mixing means that any bundle of 
solutions spreads out in phase space like a drop of ink in a glass of water. 
A measure-preserving dynamical system (X, ©, uw, 7) is mixing if and only if 
for all A, Be & 
lim u(T~"(B)N A) = u(B)u( A). (4) n-> Oo 
Mixing is occasionally mentioned in connection with chaos, usually only in 
the context of volume-preserving systems (e.g. Lichtenberg and Liebermann 
[1992], pp. 302-3; Schuster and Just [2005], p. 177). Yet, to the best of my 
knowledge, so far no one has explicitly argued that chaos can thus be defined. 
I will argue for this and propose that mixing is chaos: a system is chaotic if and 
only if it is mixing on the relevant subset of X. More needs to be said about 
what qualifies as the relevant subset later on.

--- Page 211 ---

What Are the New Implications of Chaos for Unpredictability? 205 
Since mixing was introduced before the 1960s, the beginning of the systematic 
investigation of chaos, it might seem puzzling that chaos can be adequately 
defined via mixing. However, many formal definitions and measures of chaos 
were invented before the 1960s (Dahan-Dalmedico [2004], p. 70), but rather 
few systems were known to which these notions apply. Novel from the 1960s 
onwards was that many different highly interesting systems, surprisingly also very 
simple systems, were found to which these concepts apply. 
Let us first discuss whether mixing captures the pretheoretic intuitions. 
Mixing implies denseness: mixing systems are ergodic (Cornfeld et al. [1982], 
p. 25). By looking at Equation (2), we see that from this follows that any region, 
naturally interpreted as a set of positive measure, eventually visits every region 
in phase space. 
Mixing also implies SDIC. This can be seen as follows. Mixing implies 
that any bundle of initial conditions spreads out uniformly over the phase 
space. Therefore, any bundle eventually spreads out considerably, thus exhibit- 
ing SDIC. Formally, assume a mixing measure-preserving system (X, L, 1, T) 
is given, where a metric d is defined on X, and = contains every open set of 
X. Furthermore, assume that every open set has positive measure.° Consider 
two open sets O; and Q) with 0 < ¢ := infxeo, yeo,{d(x, y)}. Mixing implies 
that for any open set O, there is an n > 0 such that 7"(O)M O, #9 and 
T"(O)M O, # WB. But this means that ¢ < sup, ,-7.o){d(x, y)}. Hence, the fol- 
lowing condition holds, which in definitions like Devaney chaos is taken to be 
the SDIC implied by chaotic behaviour (Devaney [1986], p. 51). 
There is ane > 0 such that for all x € Xand for alld > 0 (5) 
there isa y € X and ann € No withd(x, y) < dandd(T"(x), T"(y)) = «. 
As SDIC is often linked to positive Lyapunov exponents, let us now turn to a 
discussion of this issue. For a continuously differentiable Ton an open X¥ C R 
the Lyapunov exponent of x € X is 
noon 
n—| 
A(x) := lim — }*log(|7(T'(x))), (6) 
i=0 
where 7” is the derivative of T (for a general definition, see Mane [1983], 
p. 263). For ergodic systems, the Lyapunov exponent exists and is equal for all 
points except for a set of measure zero (Robinson [1995], p. 86). Hence, one 
can speak of the Lyapunov exponent of a system. Accordingly, one definition 
of chaos that has been suggested is that the system is ergodic and has a positive 
Lyapunov exponent. 
® This is standardly assumed and, to the best of my knowledge, applies to all paradigmatic chaotic 
systems.

--- Page 212 ---

206 Charlotte Werndl 
From a positive Lyapunov exponent, it is commonly concluded that the 
SDIC shown by chaos consists of the exponential spreading of inaccuracies 
over finite time periods (e.g. Lighthill [1986]. p. 46; Ott [2002], p. 140; Smith 
[1998], p. 15).’? However, this is mistaken. Positive Lyapunov exponents im- 
ply that for almost all points x in phase space, the average over all i > 0 of 
log(|7’(7"(x))|)—the exponential growth rate of an inaccuracy at the point 
T'(x)—1s positive. Here the average is taken for the solution starting from x 
over an infinite time period. But, positive on average exponential growth rates 
over an infinite time period do not imply that nearby solutions diverge exponen- 
tially or rapidly over finite time periods. The growth rate over finite time periods 
can be anything; inaccuracies can even shrink (Smith et al. [1999], pp. 2861-—2).® 
Furthermore, it is not true that inaccuracies of chaotic systems spread exponen- 
tially or rapidly over finite time periods: for paradigmatic chaotic systems like 
the Lorenz attractor, there are regions where inaccuracies even shrink over finite 
time periods, and numerical evidence suggests such regions for many chaotic 
systems (Smith et al. [1999], p. 2881; Zaslavsky [2005], p. 315; Ziehmann et al. 
[2000], pp. 10-1). 
Mixing systems need not have positive Lyapunov exponents, and thus in- 
accuracies need not grow exponentially on average as time goes to infinity. Is 
this a problem for mixing as a definition of chaos? No. First, there is no agree- 
ment in the literature whether chaos should show this on average exponential 
growth. Some definitions do indeed demand it, others like Devaney chaos do 
not. Second, the arguments for requiring positive Lyapunov exponents are not 
convincing. The standard rationale is that the SDIC shown by chaotic system 
has to be exponential divergence of nearby solutions over finite time periods. 
But, as shown above, this is not implied by a positive Lyapunov exponent and 
also does not generally hold for chaotic systems. Another possible argument is 
that for chaotic behaviour, inaccuracies should spread out rapidly. Yet the rate 
of divergence of mixing systems not having positive Lyapunov exponents can 
be much faster for arbitrary long time periods than for systems with positive 
Lyapunov exponents; thus, it is not clear why positive Lyapunov exponents 
should be required (Berkovitz et al. [2006], p. 689; Wiggins [1990], p. 615). To 
conclude, mixing captures the pretheoretic intuitions about chaos. It remains 
to show that mixing is extensionally correct. 
To do this, I have to consider the main classes of uncontroversially chaotic 
and non-chaotic behaviour.’ I start with uncontroversially chaotic behaviour, 
With the qualification that the time periods have to be small enough such that the inaccuracy 
does not eventually saturate at the diameter of the system. 
Moreover, Lyapunov exponents only measure the average growth rate of an infinitesimal inaccu- 
racy around x, which is defined as the growth rate of a small ball of radius ¢ > 0 with centre x as 
€ — 0; yet in practice the uncertainty is finite and may not behave like the infinitesimal one (cf. 
Bishop [unpublished], p. 8). 
Obviously, I cannot discuss every single system regarded as clearly chaotic or non-chaotic. Yet 
the following discussion covers all main examples.

--- Page 213 ---

What Are the New Implications of Chaos for Unpredictability? 207 
and first discuss volume-preserving systems. There are (1) Hamiltonian systems 
which are chaotic on the whole hypersurface of constant energy. Three types 
of systems are mainly discussed here: first, chaotic billiards, which are mix- 
ing (Chernov and Markarian [2006]; Ott [2002], p. 296); second, hard sphere 
systems, which are either proven or conjectured to be mixing (Berkovitz et 
al. [2006], pp. 679-80); third, geodesic flows of space with negative Gaussian 
curvature, which are mixing (Schuster and Just [2005], p. 181). 
Another class are (11) Hamiltonian systems to which the KAM theorem ap- 
plies, e.g. the Henon—Heiles system or the standard map. This class also includes 
simplified versions of Poincare maps of systems to which the KAM theorem 
applies. The KAM theorem describes what happens when integrable systems 
are perturbed by a non-integrable perturbation. It says that tori with suffi- 
ciently irrational winding number survive the perturbation. Between the stable 
motion on surviving tori, there appear to be regions of random motion. As the 
perturbation increases, these regions become larger and often eventually cover 
nearly the entire hypersurface of constant energy. 
For these systems, the phase space is separated into regions, each of which 
has its own dynamics: in some of them the motion appears random, and in 
others it is stable. Because of this separation into regions, random behaviour 
can only be found in a region. Consequently, as is widely acknowledged, proper 
chaotic motion can only occur on a region (Ott [2002], pp. 267-95; Schuster 
and Just [2005], pp. 165-74). Thus, I have to show that the mathematically 
well-understood random motion in a region is mixing. Yet the conjectured 
chaotic motion of KAM-type systems is understood only poorly (Zaslavsky 
[2005], p. 139). It has only been proven that there is chaotic behaviour near 
hyperbolic fixed points, where the motion is indeed mixing (Moser [1973], 
Chapter 3). Apart from this, some numerical evidence suggests that the motion 
conjectured to be chaotic is mixing (e.g. Chirikov [1979]). Thus, Lichtenberg and 
Liebermann ([1992], p. 303) comment that we ‘expect that the stochastic orbits 
that we have encountered in previous sections are mixing over the bounded 
portion of phase space for which they exist’. 
I should mention that numerical experiments suggest that for a few KAM- 
type maps, there are sets on which the motion seems somewhat random, but 
these sets consist of m > 2 component areas, each of which is mapped succes- 
sively on to another, returning to itself after n iterations. There is no agreement 
whether such motion, which cannot be mixing, should be called ‘chaotic’ (e.g. 
Belot and Earman [1997], p. 154; vs. Ott [2002], p. 300). If it is, chaos can still 
be defined via mixing: one can say that a system is chaotic if and only if it 
is ergodic and its phase space is decomposable into n > | sets with disjoint 
interior such that the nth iterate is mixing on each of these sets. I call this the 
‘broad definition of chaos via mixing’. Numerical experiments suggest that the 
behaviour mentioned above may be chaotic according to this definition (Ott 
[2002], p. 303).

--- Page 214 ---

208 Charlotte Werndl 
Next in line are (iii) chaotic volume-preserving non-Hamiltonian systems. Here, 
the main examples discussed are discrete. First, the baker’s map and volume- 
preserving Anosov diffeomorphisms like the cat map are mixing (Arnold 
and Avez [1968], p. 75; Lichtenberg and Liebermann [1992], p. 303). Second, 
paradigmatic chaotic systems are expanding piecewise maps like the tent map 
or the sawtooth map, which are mixing too (Bowen [1977]). 
I now turn to dissipative systems, and first discuss strange attractors. One 
class are (iv) strange attractors where the attracted solutions never enter the 
attractor. Three main groups are treated here: first, for Smale’s Solenoid and 
generalised Solenoid systems there is a measure on which the motion is mixing 
(Mayer and Roepstorff [1983]). Second, for the system investigated by Lorenz 
({1963]) and the Lorenz model, and generalised versions thereof, there is a 
physical measure on which the motion is mixing (Luzzatto et al. [2005]). Third, 
for generalised Hénon systems like the Hénon map, there exists a physical 
measure such that the motion on the attractor is mixing (Benedicks and Young 
[1993}). 
Also important is the (v) visible chaotic behaviour of generalised logistic 
systems like the logistic map. For these discrete systems, for most parameter 
values, the solutions enter an attractor with a physical measure on which the 
motion is either mixing or chaotic according to the broad definition via mixing. 
But, for a few parameter values there is chaotic behaviour on the entire interval, 
e.g. for the logistic map with parameter 4; in these cases, there is also a physical 
measure on which the motion is mixing (Jakobson [1981]; Lyubich [2002]).'° 
Finally, another class is (vi) repelling chaotic behaviour on Cantor sets. Two 
main kinds of discrete systems are discussed here: first, geometric horseshoe- 
systems like Smale’s horseshoe, which are mixing (Robinson [1995], pp. 249-74). 
The second example is chaotic motion on Cantor sets for the logistic map with 
parameter greater than 4, which is also mixing (Robinson [1995], p. 33).!! 
Let us now turn to uncontroversially non-chaotic motion. I again start with 
volume-preserving systems. A paradigmatic class are (i) integrable Hamiltonian 
systems, where there is periodic or quasi-periodic motion on tori, which is not 
mixing (Arnold and Avez [1968], pp. 210-4). 
Another class is the (ii) motion on clearly non-chaotic regions of KAM-type 
systems. Again, this class also includes simplified versions of Poincaré maps 
of KAM-type systems. As already discussed, for KAM-type systems the phase 
space is separated into regions, and on some regions the motion is stable. Thus, I 
have to show that the stable motion is not mixing. And indeed, the behaviour in 
these regions, e.g. the motion on surviving tori or near specific elliptic periodic 
10 In all these cases, the invariant measure is also the unique ergodic measure absolutely continuous 
with respect to the Lebesgue measure (Jakobson [1981]; Lyubich [2002]). 
'l This follows because these systems are isomorphic to a Bernoulli shift.

--- Page 215 ---

What Are the New Implications of Chaos for Unpredictability? 209 
points, is not mixing (Arnold and Avez [1968], pp. 86-90; Lichtenberg and 
Liebermann [1992], Chapters 3-5). 
I now turn to dissipative systems. Important here are (iil) non-chaotic at- 
tractors. These are attracting periodic cycles and fixed points and also quasi- 
periodic attractors as discussed by Ott ((2002], Chapter 7), which obviously 
cannot be mixing. Moreover, the motion approaching such attractors, e.g. the 
behaviour around stable nodes or stable foci, clearly cannot be mixing (cf. 
Robinson [1995], p. 105).!? 
Finally, let us mention two further very broad classes of clearly non-chaotic 
behaviour. Since mixing captures SDIC, (iv) systems not exhibiting any kind of 
SDIC, e.g. the identity function, cannot be mixing. 
Moreover, since mixing captures denseness, (v) motions showing SDIC but 
where, in any sense, typical solutions do not come arbitrarily near to any region 
in phase space cannot be mixing. Examples are the system %,4; = cx, force > 1 
on (0, oo) or the motion around unstable nodes or unstable foci (cf. Robinson 
[1995], p. 105). 
In sum, I have first demonstrated that mixing captures the pretheoretic 
intuitions about chaos. After that, I have briefly shown that a definition of 
chaos in terms of mixing is extensionally correct in the sense explained above. 
Consequently, chaos can be adequately defined in terms of mixing. 
With this knowledge about chaos, we are ready to critically discuss the 
answers suggested in the literature to our main question. 
4 Criticism of Answers in the Literature 
4.1 Asymptotic unpredictability? 
Let us first discuss an answer based on the concept of asymptotic unpredictabil- 
ity. Roughly, systems whose asymptotic behaviour cannot be predicted with 
arbitrary accuracy for all times, even if the bundle of initial conditions is made 
arbitrarily small, are said to be asymptotically unpredictable. Let (X, d, T) bea 
topological dynamical system, ¢ be the desired prediction accuracy and 4 be the 
diameter of the bundle of initial conditions. For x € X, the solution (7"(x)),>0 
is asymptotically predictable if and only if 
Ve > 035 > OVy € X¥Vn > 0 (d(x, y) < 5 > d(T"(x), T"(y)) < €). (7) 
A dynamical system is asymptotically unpredictable if and only if for all x € X 
(T"(x))n>0 is not asymptotically predictable.'? In terms of the distinction 
'2 Here, there often exists no invariant measure of interest. 
'3, Bishop ({2003], pp. 174-7) also aims to formalise asymptotic unpredictability. However, he does 
not list the most obvious notion presented here.

--- Page 216 ---

210 Charlotte Werndl 
introduced in Section 2.3, this is clearly a version of the first concept of unpre- 
dictability. 
Miller ([1996], pp. 106—7) and Stone ({1989], p. 127) argue that the new im- 
plication of chaos for unpredictability is that chaotic systems are asymptotically 
unpredictable. Indeed, all chaotic systems discussed in the literature are asymp- 
totically unpredictable, and standard definitions of chaos imply asymptotic 
unpredictability. For instance, (5), a condition of Devaney chaos and, under 
plausible assumptions, a consequence of mixing clearly implies asymptotic 
unpredictability. 
However, as Smith ([1998], p. 58) has pointed out, many non-chaotic systems, 
e.g. one only showing SDIC as it happens in the system x4) = cX%, ¢ > 1 
(class (v) of clearly non-chaotic behaviour), are asymptotically unpredictable. 
Hence, this account is wrong. But maybe the account can be strengthened in 
the following way: the new implication is that chaotic systems are asymptotically 
unpredictable and bounded. 1 maintain that this is not correct either: there are 
unbounded chaotic systems (Smith [1998], pp. 168-9), a point which is reflected 
in usual definitions of chaos, which do not require boundedness. Furthermore, 
for many bounded integrable systems (part of class (i) of the clearly non- 
chaotic behaviour), the solutions loop around tori in such a way that they 
are asymptotically unpredictable (Arnold and Avez [1968], pp. 210-4). Hence, 
there are examples of non-chaotic, bounded and asymptotically unpredictable 
systems. 
I conclude that the sole connection between asymptotic unpredictability 
and chaos is this: while only some non-chaotic systems are asymptotically 
unpredictable, every chaotic system is asymptotically unpredictable. 
4.2 Unpredictability due to rapid or exponential divergence? 
It is widely believed and often claimed that the new implication of chaos for 
unpredictability is the following: due to rapid or exponential divergence of nearby 
solutions, bundles of initial conditions spread out a distance more than a diameter 
of interest over short time periods (e.g. Ruelle [1997], pp. 27-8); often it is added 
that this is so, and the systems are bounded (e.g. Lighthill [1986], p. 46). In terms 
of the distinction introduced in Section 2.3, this is a form of the first concept 
of unpredictability. 
As many unbounded non-chaotic systems like the system x4; = CX, with 
c > | show (part of class (v) of clearly non-chaotic behaviour) rapid or expo- 
nentially divergence everywhere is ‘nothing new’ (Smith [1998], p. 15). Thus, 
the version not requiring boundedness cannot be true. But also the version 
requiring boundedness is wrong: as mentioned above, there are unbounded 
chaotic systems. Furthermore, as argued in Section 3.2, it is often nor true 
that nearby solutions of chaotic systems diverge rapidly or exponentially over

--- Page 217 ---

What Are the New Implications of Chaos for Unpredictability? 211 
finite time periods as is so widely believed in the philosophy, physics and math- 
ematics communities (e.g. Eagle [2005], p. 767; Schurz [1996], p. 140; Smith 
[1998], p. 15). Hence, this is not the sought-after new implication of chaos for 
unpredictability. 
Why is it so widely believed that inaccuracies in chaotic systems spread 
rapidly or exponentially over finite time periods? One plausible reason is that 
because very simple systems like the cat map show this property, this claim 
is wrongly generalized to all chaotic systems. Also, the wrong belief stems 
at least in part from misinterpreting Lyapunov exponents. As pointed out in 
Section 3.2, positive on average exponential growth rates over an infinite time 
period are wrongly taken to imply that inaccuracies spread exponentially over 
finite time periods. 
The only connection between the unpredictability of chaos and the rapid or 
exponential increase of inaccuracies over finite time periods seems to be this: it 
is more often the case for chaotic than for non-chaotic systems that bundles of 
initial conditions spread out more than a diameter of interest over short time 
periods. 
4.3 Macro-predictability and Micro-unpredictability? 
Macro-predictable yet micro-unpredictable behaviour is a broad and interesting 
topic in physics. For instance, in statistical mechanics, systems are often macro- 
predictable but micro-unpredictable. Here, we concentrate only on whether 
there is any combination of macro-predictability and micro-unpredictability in 
chaotic systems that other deterministic systems do not have. 
To gain an understanding of this third proposed answer, recall the Lorenz 
Equations (3) and Figure 2. These equations exhibit macro-predictability: the 
solutions are attracted by an attractor, a small region of phase space. There is 
also micro-unpredictability since the motion on the attractor exhibits SDIC. 
Peter Smith argues that this combination of macro-predictability and micro- 
unpredictability is a new implication of chaos for unpredictability: 
This type of combination of large-scale order with small scale disorder, of 
macro-predictability with the micro-unpredictability due to sensitive depen- 
dence, is one paradigm of what has come to be called chaos. {...| So error 
inflation by itself is entirely old-hat. The novelty in the new-fangled chaotic 
cases that will concern us is, to repeat, the combination of exponential error 
inflation with the tight confinement of trajectories by an attractor (Smith 
[1998], pp. 13—5, original emphasis). 
Here, macro-predictability means that the system eventually shows the be- 
haviour corresponding to the motion on the attractor, a proper subset of phase 
space. Micro-unpredictability is understood as the unpredictability implied by

--- Page 218 ---

212 Charlotte Werndl 
exponential error inflation. Yet, as shown in Section 3, solutions of chaotic 
systems need not diverge exponentially or rapidly over finite time periods. 
Therefore, micro-unpredictability has to be interpreted as a weaker notion, e.g. 
asymptotic unpredictability (cf. Section 4.1). 
As becomes clear from the Lorenz system, strange attractors imply this 
combination of macro-predictability and micro-unpredictability. However, this 
combination is no new implication of chaos for unpredictability since there are 
many chaotic systems without attractors. As already pointed out, all chaotic 
volume-preserving dynamical systems like chaotic Hamiltonian systems or the 
baker’s map (classes (i), (ii) and (iii) of uncontroversially chaotic behaviour) 
cannot have attractors. And some chaotic dissipative systems, e.g. repelling 
chaotic motion on Cantor sets or the logistic map on [0, 1] (class (vi) and a part 
of class (v) of uncontroversially chaotic behaviour), have no attractors. Hence, 
these systems are not macro-predictable in the above sense, viz. that appeals to 
attractors. 
It could be that Smith ({1998]) only meant to say that this combination 
of macro-predictability and micro-unpredictability found in strange attractors 
is a novelty for systems with attractors. But this would not help. Clearly, this 
claim would be no satisfying answer to our main question because it does not 
apply to essentially all chaotic systems. Furthermore, also non-chaotic systems 
can be macro-predictable and micro-unpredictable as discussed here. For in- 
stance, in the plane let R be the region enclosed by a circle of radius r around 
the origin (boundary included). Imagine that all solutions in R go in circles 
around the origin and that all solutions outside Rare attracted by the periodic 
motion in Rsuch that all solutions are continuous. Such non-chaotic attractors 
(part of class (iii) of clearly non-chaotic behaviour) obviously imply macro- 
predictability and micro-unpredictability. Thus this combination of macro- 
predictability and micro-unpredictability is not even a novelty for systems with 
attractors. 
Of course, there are also other concepts of macro-predictability and micro- 
unpredictability (e.g. Smith [1998], pp. 60-1). However, to the best of my knowl- 
edge, none of them provides a combination of macro-predictability and micro- 
unpredictability that is characteristic of chaotic behaviour. 
To conclude, strange attractors are macro-predictable and micro- 
unpredictable in the above specified sense. However, it is not the case that 
a combination of macro-predictability and micro-unpredictability constitutes 
a new implication of chaos for unpredictability. 
None of the answers examined so far have proven to be correct. There is 
one more answer suggested in the literature: some physicists, e.g. Ford ({1989]), 
have defined chaos by the condition that almost all solutions have positive algo- 
rithmic complexity. In other words, they have argued that the unpredictability 
implied by positive algorithmic complexity is a new implication of chaos for

--- Page 219 ---

What Are the New Implications of Chaos for Unpredictability? 213 
unpredictability. However, Batterman and White ([{1996]) and Smith ({1998], 
p. 160) have made it clear that chaos cannot be defined via algorithmic com- 
plexity since many systems without SDIC (part of class (iv) of clearly non- 
chaotic behaviour) have positive algorithmic complexity too. Consequently, 
this is no new implication of chaos for unpredictability, and this is all we need 
to know. 
In sum, the answers in the literature do not fit the bill. 
5 A General New Implication of Chaos for Unpredictability 
5.1 Approximate probabilistic irrelevance 
The answer I propose starts from the well-known idea that mixing goes along 
with loss of information as recently discussed by Berkovitz et al. ({2006]). First 
of all, let us introduce approximate probabilistic irrelevance, the notion of 
unpredictability which will be crucial for our claim. 
Given a measure-preserving system (X, D, , 7), it is common to associate 
with a set A€ © a property P4, where Py, holds if and only if the system’s 
state is in A (Berkovitz et al. [2006], p. 671). For instance, for the logistic map 
with a = 4 interpreted as a model of population dynamics (May [1976]), the 
set A = [0, 1/2) corresponds to the property that the population is less than 
half of the maximum of the possible population. 
Because time is discrete, I can denote time points by ¢,, n € Z, such that n 
increases by one if the model is iterated once; for instance, if ts corresponds to 
the iteration stage T, fs corresponds to T° etc. Given this, I define the event A’ 
as the occurrence of the property P, at time 4,. To come back to our example, 
A is the event that the population is less than half of the maximum possible 
population at time 4, (Berkovitz et al. [2006], p. 671). 
Since the exact state of the system may not be known, I introduce p( A”), 
the probability of the event A”. I also introduce conditional probabilities: 
p(B | A), for arbitrary A, B € © with (A) > 0, is the probability that Ps, 
obtains at time 4,, given that P, obtains at 4, (Berkovitz et al. [2006], p. 671). 
By the usual definition, p(B” | A”) = p(B’ & A”)/ p( A”). 
Now recall the second conception of unpredictability of Section 2.3. For this 
conception, we have to say what it means that knowledge that the system is 
in a region A at #, is practically irrelevant for predicting that it will be in B at 
tm. We say that this is so if the probability of the event B™ given knowledge 
of the event A” approximately equals the unconditionalised probability of the 
event B™. Let ¢ > 0 be the level at which probabilities differing by less than « 
are considered as practically equivalent. Furthermore, assume that p( A”) > 0; 
I will later explain why I am justified to do so. Then formally this is captured

--- Page 220 ---

214 Charlotte Werndl 
by the following definition:'4 
A" is approximately probabilistically irrelevant for predicting B™ (8) 
(tm > t,) at levele > Oif and only if | p(B” | A”) — p(B™) |< «. 
How can we determine the values of the probabilities occurring in (8)? Be- 
cause the probabilities should reflect objective dynamical properties of sys- 
tems, I say that the probability of an event A” corresponds to the measure of 
A (Berkovitz et al. [2006], p. 673). As mentioned in Section 2.1, this is quite 
natural under certain interpretations. 
For all ¢, and for all A € £: p( A”) = (A). (9) 
This idea can be generalised to joint simultaneous events as follows: 
For all 4, and for all A, B € &: p( A"& B”) = u(AN B). (10) 
This implies: 
For all tf, t, and all A, Be &: p( B”& A") = w(T"-"(B)N A) (11) 
since 7”~( B) is the evolution of the set B backward in time from 1,, to t,.'° 
In the next section, we will see how approximate probabilistic irrelevance 
relates to chaos, and will finally propose an answer to our question. 
5.2 Sufficiently past events are approximately probabilistically 
irrelevant for predictions 
The argument I put forward to answer the main question of the paper is as 
follows. (P1) Chaos can be defined in terms of mixing. (P2) Mixing systems 
exhibit a particular pattern of approximate probabilistic irrelevance, which con- 
stitutes a form of unpredictability. Therefore, (C) a new implication of chaos for 
unpredictability is the particular pattern of approximate probabilistic irrelevance 
arising from mixing. 
In Section 3.2, we have seen that premise (P1) is true. Let me now argue for 
premise (P2). Recall the definition of mixing, Equation (4). I assume without 
loss of generality that the event we want to predict occurs at f. Then, assuming 
'4 T use what is basically the difference measure in confirmation theory to define the approximate 
probabilistic irrelevance. I should point out that our claims are independent of the measure 
involved, i.e. they would remain the same if I used any other measure with the indisputable 
property that it is continuous when the unpredictability is highest, i.e. when p( B™ | A”) = p(B’). 
Berkovitz et al. ({2006], p. 672) interpret the difference measure of events as a general measure 
of unpredictability. However, they do not justify this choice or address whether their results are 
independent of the measure. 
> I can infer (11) from (10) as follows: 7"~""(B) contains exactly those points that are in B at 
time f,,. Consequently, T”"~’"(B)/M A consists of exactly those points which pass A at time 4, 
and go through B at time 4,, > f,, i.e. for which B” & A’ is true. Thus, from (10) it follows that 
p( BY & A") = w(T"-"(B) 1 A).

--- Page 221 ---

What Are the New Implications of Chaos for Unpredictability? 215 
Equations (9) and (11), it follows that a system (XY, £, , T) is mixing if and 
only if 
lim p( B® | A") — p(B) = 0, (12) n—> Oo 
for all A, B € © with p( A) > 0. This equation holds for ail, i.e. invertible and 
non-invertible, measure-preserving systems. Berkovitz et al. ({2006], p. 676) 
show (12) only for invertible systems. Moreover, they interpret their results as 
applying only to Hamiltonian systems. Many chaotic systems, e.g. all strange 
attractors (classes (iv) and (v) of uncontroversially chaotic behaviour), are 
not Hamiltonian. Furthermore, many paradigmatic systems like generalised 
logistic systems or the tent map (class (v) and part of classes (iii) and (vi) of 
uncontroversially chaotic behaviour) are not invertible. Since I am interested 
in the unpredictability implied by chaos, I need (12) for all systems, and this 
general claim follows from (9) and (11). 
From the definition of the limit, I obtain that (12) can be expressed as 
For any event B”, any precisione > 0 and any A with z( A) > 0 (13) 
there existsmp € Nsuch that forall n > no:|p( B® | A") — p(B®)| < e. 
Hence, mixing means that for predicting an arbitrary event at an arbitrary level 
of precision ¢ >0, any sufficiently past event is approximately probabilistically 
irrelevant. Notice that due to the impossibility of determining initial conditions 
precisely, scientists always consider regions of phase space corresponding to 
possible initial conditions. Since these regions are not of measure zero, | am 
justified in assuming that (A) > 0. In terms of the distinction introduced in 
Section 2.3, this pattern of probabilistic irrelevance is a version of the sec- 
ond concept of unpredictability. Hence, mixing systems exhibit a particular 
pattern of approximate probabilistic irrelevance, which constitutes a form of 
unpredictability: i.e. premise (P2) is true.'® 
Now that I have argued for the premises (P1) and (P2) of the above argument, 
I conclude: (C) a general new implication of chaos for unpredictability is that for 
predicting any event at any level of precision ¢ > 0, all sufficiently past events are 
approximately probabilistically irrelevant. 
To fully understand this conclusion, consider the following: for strange at- 
tractors this claim applies in a strict sense to events only on the attractor. 
Yet for practical matters, there is chaotic behaviour when solutions are very 
'© This claim can be generalised. (X, £, u, T) is mixing iff for any p absolutely continuous with 
respect to 4 and any square-integrable function / lim, x ff x)dpn, = [f(x)du, where p, is the 
n-steps evolved measure. Interpret 4 as probability and p as measuring our knowledge of the 
initial condition. Then, assuming absolute continuity of p, mixing means that for arbitrary 
knowledge of the initial condition after a sufficiently long time, the prediction obtained by 
evolving the measure is practically no better than if we had no knowledge whatsoever of the 
initial conditions (cf. Berger [2001], pp. 126—32).

--- Page 222 ---

216 Charlotte Werndl 
near to the strange attractor (cf. Section 2.1); then my claim means that for 
predicting any event on or very near the attractor A at any level of precision 
€ > 0, all sufficiently past events in the neighbourhood U D A are approximately 
probabilistically irrelevant. For KAM-type systems my claim applies, as one 
would like it, to each chaotic region. Moreover, as explained in Section 3.2 in 
discussing the uncontroversially chaotic behaviour, some may want to adopt 
the broad definition of chaos via mixing, i.e. that the system is ergodic and 
its phase space is decomposable into n > | regions with disjoint interior such 
that the nth iterate is mixing on each set. When n > 1, my claim (C) has to be 
adapted in the following way: the unpredictability of mixing applies to the mth 
iterate on the region of interest. This means that for predicting any event in the 
region of interest at any level of precision ¢ > 0, all sufficiently past events that 
could have evolved to the region of interest are approximately probabilistically 
irrelevant. 
On the one hand, the unpredictability involved in my answer is strong: suffi- 
ciently distant events are practically as independent as coin tosses. On the other 
hand, it is weak since only sufficiently past measurements are approximately 
probabilistically irrelevant. Restricting my claim to sufficiently past events is 
essential: first, many chaotic systems are continuous, and continuity makes it 
impossible that for all past times, all events are approximately probabilistically 
irrelevant for predictions. Second, we have seen that to require rapid divergence 
of nearby solutions for chaotic behaviour is untenable. 
What is novel about my claim? Granted, in a few publications on chaos the 
notion of ‘irrelevance’ is discussed. In fact, there are two main foci; but none 
gives my claim. First, there is Berkovitz et al.’s ([2006]) explication of the ergodic 
hierarchy. Yet recall our main argument (cf. the beginning of this subsection). 
As pointed out, Berkovitz et al. only show premise (P2) for invertible systems, 
and they interpret their results as only applying to Hamiltonian systems. Hence, 
they do not argue for the general premise (P2), and, most importantly, they 
do not argue for the crucial premise (P1). Therefore, they could not arrive at 
the conclusion (C). Second, sometimes it is asserted that for chaos the input is 
irrelevant in the sense that prediction is exponentially expensive in the initial 
data, meaning that for an input string of length n all information is lost after 
n steps, at which point we are totally unsure what happens next (Leiber [1998], 
p. 361; Smith [1998], p. 53). However, as argued in Section 4.2, predictions 
for chaotic systems need not be exponentially expensive in the initial data; the 
irrelevance shown by chaos is more subtle. 
6 Conclusion 
The unpredictability of chaotic systems is one of the issues that has attracted 
most interest in chaos research. Nonetheless, nearly half a century after the

--- Page 223 ---

What Are the New Implications of Chaos for Unpredictability? 217 
start of the systematic investigation of chaos, there has been much confusion 
about, and no correct answer to, the question ‘What are the new implications of 
chaos for unpredictability?’, in the sense that chaotic systems are unpredictable 
in a way that other deterministic systems are not. 
I have criticised the answers in the literature to the above question. First, 
I rejected the answer that chaotic systems are asymptotically unpredictable 
on the grounds that also many non-chaotic systems are asymptotically unpre- 
dictable. Second, I rejected the answer that chaotic systems are unpredictable in 
the sense of exponential or rapid divergence of nearby solutions (often claimed 
with the added condition of boundedness). For, when not requiring bound- 
edness, many non-chaotic systems are also unpredictable in this sense. Fur- 
thermore, in the case of requiring boundedness, there are unbounded chaotic 
systems and, though unacknowledged in the philosophy literature, chaotic sys- 
tems need not be unpredictable in the sense of having exponential or rapid 
divergence of solutions. Third, I dismissed the answer that chaos shows a 
specific combination of macro-predictability and micro-unpredictability: there 
are chaotic systems which are not macro-predictable, and non-chaotic sys- 
tems which also show this combination of macro-predictability and micro- 
unpredictability. 
This prompted the search for an alternative answer. I approached this prob- 
lem by showing that chaos can be defined in terms of mixing, 1.e. that mixing 
captures the main pretheoretic intuitions about chaos and correctly classifies 
the various classes of uncontroversially chaotic and non-chaotic behaviour. 
This has never been explicitly argued for in the literature. Based on this insight, 
I proposed a novel general answer: a new implication of chaos for unpredictabil- 
ity is that for predicting any event at any level of precision ¢ > 0, all sufficiently 
past events are approximately probabilistically irrelevant. Chaotic behaviour is 
multi-faceted and takes various forms. Yet if the aim is to identify a general 
new implication of chaos for unpredictability, I think this is the best we can get. 
Acknowledgements 
I am indebted to Jeremy Butterfield, Roman Frigg, Peter Smith and two anony- 
mous referees for valuable comments on earlier versions of this paper. Many 
thanks also to Robert Bishop, Adam Caulton, Franz Huber, Paul Weingartner 
and the audiences at the Philosophy Workshop at the University of Cambridge 
and the 15th UK and European Meeting on the Foundations of Physics for 
discussions that led to improvements in this paper. I am grateful to St John’s 
College, Cambridge, for financial support. 
Faculty of Philosophy, University of Cambridge 
Sidgwick Avenue, Cambridge, CB3 9DA, UK 
csw39@cam.ac.uk

--- Page 224 ---

Charlotte Werndl 
References 
Arnold, V. I. and Avez, A. [1968]: Ergodic Problems of Classical Mechanics, New York: 
W. A. Benjamin. 
Batterman, R. W. and White, H. [1996]: ‘Chaos and Algorithmic Complexity’, Founda- 
tions of Physics, 26, pp. 307-37. 
Belot, G. and Earman, J. [1997]: ‘Chaos Out of Order: Quantum Mechanics, the Corre- 
spondence Principle and Chaos’, Studies in History and Philosophy of Modern Physics, 
28, pp. 147-82. 
Benedicks, M. and Young, L.-S. [1993]: ‘Sinai-Ruelle-Bowen Measures for Certain 
Heéenon Maps’, Jnventiones Mathematicae, 112, pp. 541-76. 
Berger, A. [2001]: Chaos and Chance: An Introduction to Stochastic Aspects of Dynamics, 
New York: de Gruyter. 
Berkovitz, J., Frigg, R. and Kronz, F. [2006]: ‘The Ergodic Hierarchy, Randomness 
and Hamiltonian Chaos’, Studies in History and Philosophy of Modern Physics, 37, 
pp. 661-91. 
Bishop, R. C. [2003]: ‘On Separating Predictability and Determinism’, Erkenntnis, 58, 
pp. 169-88. 
Bishop, R. C. [unpublished]: “What Could Be Worse than the Butterfly Effect?’. 
Bowen, R. [1977]: ‘Bernoulli Maps of the Interval’, Jsrael Journal of Mathematics, 28, 
pp. 161-8. 
Brin, M. and Stuck, G. [2002]: Introduction to Dynamical Systems, Cambridge: Cam- 
bridge University Press. 
Chernov, N. and Markarian, R. [2006]: Chaotic Billiards, Providence, RI: American 
Mathematical Society. 
Chirikov, B. V. [1979]: ‘A Universal Instability of Many-Dimensional Oscillator Systems’, 
Physics Reports, 52, pp. 264-379. 
Cornfeld, I. P., Fomin, S. V. and Sinai, Ya. G. [1982]: Ergodic Theory, Berlin: 
Springer. 
Dahan-Dalmedico, A. [2004]: ‘Chaos, Disorder, and Mixing: a New Fin-de-siécle 
Image of Science?’, in M. N. Wise (ed.), Growing Explanations: Historical Per- 
spective on the Sciences of Complexity, Durham: Duke University Press, pp. 67 
94. 
Devaney, R. [1986]: An Introduction to Chaotic Dynamical Systems, Menlo Park: 
Benjamin. 
Eagle, A. [2005]: ‘Randomness is Unpredictability’, British Journal for the Philosophy of 
Science, 56, pp. 749-90. 
Earman, J. [1971]: ‘Laplacian Determinism, or Is This Any Way to Run a Universe?’, 
Journal of Philosophy, 68, pp. 729-44. 
Eckmann, J.-P. and Ruelle, D. [1985]: ‘Ergodic Theory of Chaos and Strange Attractors’, 
Reviews of Modern Physics, 57, pp. 617-54. 
Ford, J. [1989]: ‘What is Chaos That We Should Be Mindful of It?’, in P. Davies (ed.), 
The New Physics, Cambridge: Cambridge University Press, pp. 348-71.

--- Page 225 ---

What Are the New Implications of Chaos for Unpredictability? 219 
Jakobson, M. V. [1981]: ‘Absolutely Continuous Invariant Measures for One-Parameter 
Families of One-Dimensional Maps’, Communications in Mathematical Physics, 81, 
pp. 39-88. 
Leiber, T. [1998]: ‘On the Actual Impact of Deterministic Chaos’, Synthese, 113, 
pp. 357-79. 
Lichtenberg, A. J. and Lieberman, M. A. [1992]: Regular and Chaotic Dynamics, Berlin: 
Springer. 
Lighthill, J. [1986]: ‘The Recently Recognized Failure of Predictability in Newtonian 
Dynamics’, Proceedings of the Royal Society of London, Series A, 407, pp. 35-50. 
Lorenz, E. N. [1963]: “Deterministic Nonperiodic Flow’, Journal of the Atmospheric 
Sciences, 20, pp. 130-41. 
Luzzatto, S., Melbourne, I. and Paccaut, F. [2005]: ‘The Lorenz Attractor is Mixing’, 
Communications in Mathematical Physics, 260, pp. 393-401. 
Lyubich, M. [2002]: ‘Almost Every Quadratic Map is Either Regular or Stochastic’, 
Annals of Mathematics, 156, pp. 1-78. 
Malament, D. B. and Zabell, S. L. [1980]. ‘Why Gibbs Phase Averages Work: The Role 
of Ergodic Theory’, Philosophy of Science, 47, pp. 339-49. 
Mane, R. [1983]: Ergodic Theory and Differentiable Dynamics, Berlin: Springer. 
Martinelli, M., Dang, M. and Seph, T. [1998]: ‘Defining Chaos’, Mathematics Magazine, 
71, pp. 112-22. 
May, R. M. [1976]: ‘Simple Mathematical Models with Very Complicated Dynamics’, 
Nature, 261, pp. 459-67. 
Mayer, D. and Roepstorff, G. [1983]: ‘Strange Attractors and Asymptotic Measures of 
Discrete-Time Dissipative Systems’, Journal of Statistical Physics, 31, pp. 309-26. 
Miller, D. [1996]: ‘The Status of Determinism in an Uncontrollable World’, in P. Wein- 
garnter and G. Schurz (eds), Law and Prediction in the Light of Chaos Research, Berlin: 
Springer, pp. 103-14. 
Montague, R. [1962]: ‘Deterministic Theories’, in D. Wilner (ed.), Decisions, Values and 
Groups, New York: Pergamon Press, pp. 325-70. 
Moser, J. [1973]: Stable and Random Motions in Dynamical Systems, Princeton, NJ: 
Princeton University Press. 
Nillsen, R. [1999]: “Chaos and One-to-Oneness’, Mathematics Magazine, 72, pp. 14 
21. 
Ott, E. [2002]: Chaos in Dynamical Systems, Cambridge: Cambridge University Press. 
Peitgen, H.-O., Jiirgens, H. and Saupe, D. [1992]: Chaos and Fractals: New Frontiers of 
Science, New York: Springer. 
Robinson, C. [1995]: Dynamical Systems: Stability, Symbol Dynamics, and Chaos, 
Londen: CRC Press. 
Ruelle, D. [1997]: ‘Chaos, Predictability, and Idealizations in Physics’, Complexity, 3, 
pp. 26-8. 
Schurz, G. [1996]: ‘Kinds of Unpredictability in Deterministic Systems’, in P. 
Weingarnter and G. Schurz (eds), Law and Prediction in the Light of Chaos Research, 
Berlin: Springer, pp. 123-41.

--- Page 226 ---

220 Charlotte Werndl 
Schuster, G. and Just, W. [2005]: Deterministic Chaos: An Introduction, Weinheim: Wiley- 
VCH. 
Smith, L. A., Ziehmann, C. and Fraedrich, K. [1999]: ‘Uncertainty Dynamics and 
Predictability in Chaotic Systems’, Quarterly Journal of the Royal Meteorological 
Society, 125, pp. 2855-86. 
Smith, P. [1998]: Explaining Chaos, Cambridge: Cambridge University Press. 
Stone, M. A. [1989]: ‘Chaos, Prediction and Laplacian Determinism’, American Philo- 
sophical Quarterly, 26, pp. 123-31. 
van Lith, J. [2001]: ‘Ergodic Theory, Interpretations of Probability and the Foundations 
of Statistical Mechanics’, Studies in History and Philosophy of Modern Physics, 32, 
pp. 581-95. 
Weingartner, P. [1996]: ‘Under What Transformations Are Laws Invariant?’, in 
P. Weingarnter and G. Schurz (eds), Law and Prediction in the Light of Chaos Re- 
search, Berlin: Springer, pp. 47-88. 
Wiggins, S. [1990]: Introduction to Applied Nonlinear Dynamical Systems and Chaos, 
Berlin: Springer. 
Young, L.-S. [1997]: ‘Ergodic Theory and Chaotic Dynamical Systems,’ X//th Interna- 
tional Congress of Mathematical Physics ( Brisbane), Cambridge, MA: International 
Press, pp. 311-9. 
Young, L.-S. [2002]: ‘What Are SRB Measures, and Which Dynamical Systems Have 
Them?’, Journal of Statistical Physics, 108, pp. 733-54. 
Zaslavsky, G. M. [2005]: Hamiltonian Chaos and Fractional Dynamics, Oxford: Oxford 
University Press. 
Ziehmann, C., Smith, L. A. and Kurths, J. [2000]: ‘Localized Lyapunov Exponents and 
the Prediction of Predictability’, Physics Letters A, 271, pp. 1-15.

--- Page 227 ---

Brit. J. Phil. Sci. 60 (2009), 221-228 
REVIEW 
HASOK CHANG 
Inventing Temperature: Measurement and Scientific Progress. 
New York: Oxford University Press. 2004, cloth 
£38.99/$43.95 ISBN 0-19-517127-6 
Donald Gillies* 
University College London 
Gower Street, London WCIE 6BT, UK 
donald. gillies@ucl.ac.uk 
1 Overview of the Book 
This book develops a general theory of how concepts in science come to be 
measurable through a detailed consideration of the example of temperature. 
The approach is very much that of history and philosophy of science, and in 
each of Chapters | to 4, the history (referred to as ‘narrative’) is given first and 
then some philosophical comments (referred to as ‘analysis’) follow. The his- 
torical episodes chosen are designed to illustrate problems in the measurement 
of temperature. Thus, the narrative of Chapter | is concerned with the ques- 
tion of how the fixed points (boiling and freezing of water) were established. 
Here Chang reveals some surprising and perhaps largely forgotten historical 
facts. For example, (p. 21)' Gay—Lussac reported in 1812 that water boiled 
at different temperatures in glass and metallic containers. Then experiments 
on so-called ‘superheating’ showed that the temperature of water could under 
certain circumstances be raised as high as 200°C without vaporization. There 
were also corresponding results about the supercooling of water. All this raised 
problems about the establishment of fixed points in the temperature scale, and 
in the analysis section of this chapter, Chang introduces his ‘iteration’ account 
of the development of measurement, which will be considered in more detail 
in the next section. Chapter 2 deals with another question. What fluid should 
be used in the construction of thermometers? Should it be alcohol, mercury 
or air? Laplace in 1821 defended the air thermometer using a mathematical 
Review received 26 Jan 2006. 
' Page references will be to the volume under review unless otherwise stated. 
© The Author (2008). Published by Oxford University Press on behalf of British Society for the Philosophy of Science. All rights reserved 
doi:10.1093/bjps/axn050 For Permissions, please email: journals. permissions@oxfordjournals.org 
Advance Access published on December 15, 2008

--- Page 228 ---

LLL Review 
version of the caloric theory. After Laplace, however, there was an empiricist 
movement among French physicists which rejected Laplace’s approach as too 
theoretical. As far as thermometry was concerned, the leader of this movement 
was Regnault, who limited himself to careful experimental investigations using 
the principle of comparability, namely that (p. 77): ‘an instrument that gave 
varying values for one situation could not be trusted, since at least some of 
its indications had to be incorrect’. Regnault’s work along these lines led to 
the result (p. 83) that everything else was worse than the air thermometer, so 
that he reached the same conclusion as Laplace for very different reasons. In 
his philosophical commentary on this, Chang makes the important point that 
(p. 92): ‘when we consider Regnault’s work carefully, what initially seems like 
the purest piece of empiricism turns out to be crucially based on an ontolog- 
ical principle ...’. This ontological principle is that temperature is (p. 77) ‘a 
real physical quantity’ and ‘should have one unique value in a given situation’. 
I am in complete agreement with Chang that Regnault’s work presupposed 
this ‘principle of single value’. However, I cannot agree with his claims that 
this principle (p. 91) ‘is not an empirical hypothesis’ and (p. 92) ‘can only 
have a metaphysical justification’. I will come back to this question later on. 
Chapter 3 ‘To Go Beyond’ takes up the question of how temperature measure- 
ment could be extended to more extreme temperatures at which the existing 
thermometers would freeze or melt. The historical narrative of this chapter 
begins by discussing attempts to measure the freezing point of mercury. It then 
switches to the other end of the temperature scale, and gives a nice example of 
a connection between industrial manufacturing and science. Josiah Wedgwood 
(1730-1795) was a leading manufacturer of porcelain, and, in order to control 
and improve the process, wanted to find a way of measuring the temperature 
of his kilns. This problem led to pyrometry, or the measurement of high tem- 
peratures. Chang’s analysis of this material is of particular interest because it 
contains his critique of Bridgman’s operationalism. I will discuss this in detail 
below, but will remark here that this is indeed an appropriate point to introduce 
Bridgman’s ideas, since, as Chang points out on p. 142, Bridgman carried out 
pioneering work in the physics of high pressures for which he was awarded 
a Nobel Prize in 1946. Bridgman dealt with pressures 100 times higher than 
had been reached before, and naturally came across the problem of measuring 
these pressures in conditions which would cause the break down of all previ- 
ously known pressure gauges. Chapter 4 is concerned with the work of William 
Thomson (later Lord Kelvin) on absolute temperature. Thomson had spent 
some time in Paris working as an assistant to Regnault. However, he concluded 
that to base the definition of temperature on a specific substance such as air in 
the air thermometer was unsatisfactory. What was needed was a definition of 
temperature (p. 182) ‘quite independent of the physical properties of any spe- 
cific substance’. This was what Thomson referred to as ‘absolute temperature’.

--- Page 229 ---

British Journal for the Philosophy of Science 223 
As Chang remarks (p. 203): “What truly set Thomson apart was his dual in- 
sistence: that theoretical concepts should be defined abstractly, and that they 
should be operationalized’. Thomson certainly gave a very abstract; theoretical 
and mathematical account of his absolute temperature, basing his approach on 
a development of Carnot’s account of theoretical heat engines which Thom- 
son had learnt about during his stay in Paris. Back in Britain, he collaborated 
with Joule to produce the Joule-Thomson experiment that enabled existing 
air thermometers to be corrected so that they measured absolute temperature. 
This concludes the historical part of the book, but in Chapter 5, Chang sums 
up and develops his philosophical views, while in Chapter 6, a new theme is 
introduced — complementary science. Chang argues that contemporary science 
presupposes many things that are more dubious than might be supposed. An 
example would be that water at normal atmospheric pressure boils at 100°C. 
The study of history of science shows that the fixity of the boiling point of wa- 
ter was once a matter of considerable discussion, and the debates on this issue 
have now largely been forgotten. Might it not be valuable therefore to carry 
out a ‘complementary science’ which takes up some of these old and forgotten 
problems and investigates them further in a modern context? For example, the 
question of the temperature at which water boils could be investigated in a 
modern laboratory. Investigations of this sort would constitute ‘“complemen- 
tary science’. The proposal to set up such a science is an intriguing one, but I 
will not consider it further in this review, but rather concentrate on the main 
theme of the book — Chang’s theory of measurement in science. 
2 Iteration 
Chang bases his theory of how concepts come to be measurable in science on 
the principle of iteration. Since there is no indubitable foundation to act as a 
starting point, we have to begin (p. 6): ‘by adopting an existing system of knowl- 
edge, with some respect for it but without any firm assurance that it is correct’. 
Then (p. 6) ‘on the basis of that initially affirmed system we launch inquiries 
that result in the refinement and even correction of the original system’. The 
initial steps towards the measurement of temperature are a good illustration of 
this theory. The starting point has to be the qualitative judgements of hot and 
cold made using our unaided senses. On the basis of these, it was possible to 
observe that liquids expand as they get hotter, and this could be the basis of 
constructing what is known as a thermoscope. A thermoscope is an instrument 
that enables one to say that two things are at the same temperature, or that 
one is hotter than another without introducing a numerical scale of tempera- 
ture. To proceed to a numerical scale we have to introduce fixed points, but we 
can use thermoscopes to check whether an alleged fixed point is indeed fixed. 
For example, using a thermoscope (p. 42) ‘we can infer that blood heat is not

--- Page 230 ---

224 Review 
constant and should not be used as a fixed point’. So on the basis of thermo- 
scopes, we can introduce fixed points and thus proceed to thermometers that 
give a numerical scale of temperature. 
This iterative procedure continues as measurements of temperature be- 
come more and more refined. A nice illustration of this is provided by the 
work of Thomson on absolute temperature. As the final result of consider- 
able theoretical work, Joule and Thomson produced the following formula 
(p. 195): 
This was designed to give the correction needed to convert the readings of 
an air thermometer into absolute temperatures. The variables v and p are 
the volume and pressure of the air thermometer as observed, while T is the 
absolute temperature. C, A, J, K are constants whose values can be determined 
experimentally. In particular, J is the mechanical equivalent of heat, and K is 
the specific heat (per unit mass) under constant pressure. Now the interesting 
point here (p. 204) is that to determine the values of J and K experimentally, 
mercury thermometers were used. Thus to correct the air thermometer in 
order to obtain absolute temperatures, Joule and Thomson had to use mercury 
thermometers. Moreover, the mercury thermometers had been adopted because 
of their agreement with the air thermometer. This is a very striking example of 
the complexities of iteration. 
The great advantage of the iterative approach is that it solves one of the 
central puzzles about the introduction of measurement. This is the apparent 
circularity which seems to arise every time a new measuring instrument is con- 
structed. Suppose, for example, mercury thermometers are being constructed 
in order to introduce a numerical way of measuring temperature. These ther- 
mometers depend on the assumption that mercury expands uniformly with 
temperature. But how can we know that this is the case before we have a nu- 
merical method of measuring temperature? It might seem that we could never 
get a numerical method of measurement off the ground. 
The iterative approach shows the way out of this difficulty. The older methods 
provide the basis for testing the assumptions on which the new methods are 
introduced. Thus, one of the important steps from the thermoscope to the 
thermometer was to introduce fixed points, but the thermoscope can be used 
without circularity to test whether the alleged fixed points really are fixed. As 
Chang says (p. 42): ‘ ... a thermoscope does not even need to have any fixed 
points, so that the evaluation of fixed points can be made without a circular 
reliance on fixed points. Employing the thermoscope as a standard allows an 
initial evaluation of fixed points’.

--- Page 231 ---

British Journal for the Philosophy of Science 
3 Operationalism 
Operationalism is defined by Chang (p. 256) as ‘The philosophical view that 
the meaning of a concept is to be found primarily or even solely in the methods 
of its measurement’. This is the view which is normally attributed to Bridgman, 
though Chang has some doubts whether this attribution is entirely fair. Still, in 
his classic 1927 book: The Logic of Modern Physics, Bridgman introduces his 
theory of operationalism using the example of length as follows (p. 5): 
To find the length of an object, we have to perform certain physical op- 
erations. The concept of length is therefore fixed when the operations by 
which length is measured are fixed: that is, the concept of length involves 
as much as and nothing more than the set of operations by which length 
is determined. In general, we mean by any concept nothing more than a 
set of operations; the concept is synonymous with the corresponding set of 
operations. 
This, in effect, is the position that Chang criticizes. One of his key points is the 
following (pp. 151-2): ‘If we accept the most extreme kind of operationalism, 
there is no point in asking whether a measurement method is valid; if the 
measurement method defines the concept and there is nothing more to the 
meaning of the concept, the measurement method is automatically valid, as 
a matter of convention or even tautology’. However, the historical material 
which Chang has presented shows that the validity or otherwise of every new 
measurement method has always been hotly debated. But how is this possible? 
Chang here appeals to Wittgenstein’s view of meaning as use. A scientific 
concept has a variety of uses in different contexts. So, if a new measurement 
method for that concept is to be introduced, it is important to see that it coheres 
with the other uses of the concept. If it fails to do so, it may well be rejected as 
a valid method for measuring the concept. 
4 A Disagreement and a Suggested Modification 
Broadly speaking, I agree both with Chang’s iterative approach to the problem 
of measurement in science, and with his criticisms of operationalism. However, 
I would like to mention one point on which I disagree, and another where I 
think that Chang’s theory needs some modification. The disagreement concerns 
the principle of single value. As already remarked, Chang points out, quite 
correctly in my view, that Regnault presupposed in his work the principle of 
single value, namely that temperature is a real physical quantity and should have 
a single unique value in a given situation. However, he also claims that this is 
not an empirical hypothesis and can only be given a metaphysical justification. 
On the contrary, I would maintain that the principle of single value is an 
empirical, scientific hypothesis. The development of the science of heat could

--- Page 232 ---

226 Review 
have undermined this hypothesis and led to its being abandoned. Scientists 
might have reached the conclusion that what had been thought of as a single 
unified concept of temperature, really consisted of two rather different concepts. 
The possibility of a bifurcation of the concept of temperature is shown by the 
fact that such bifurcations did occur in other sciences. A classic example is 
the notion of ‘quantity of motion’ in mechanics.” In 1644, Descartes defined 
‘quantity of motion’ of a body as (roughly speaking) the mass of the body 
multiplied by its speed. He proposed the law that the total quantity of motion 
of the universe remains constant. This view was attacked by Leibniz in 1686. 
Leibniz proposed that the quantity of motion of a body should be measured by 
the mass of the body multiplied by the square of its speed. The dispute between 
the Cartesians and the Leibnizians on this point lasted for over fifty years, 
before it was resolved by D’Alembert in 1743 with the suggestion, which is still 
accepted today, that there are really two different measures of the quantity of 
motion of a body, namely its momentum, and its kinetic energy. 
My suggested modification concerns Chang’s critique of operationalism. I 
think that this is completely valid as far as the natural sciences are concerned. 
However, I would also maintain that something quite like Bridgman’s opera- 
tionalism gives a good account of how concepts come to be measured in the 
social sciences. Let us take a simple example, which is familiar to lecturers in 
universities. A student at the end of his or her undergraduate days will have 
taken a large number of courses, and the marks for these courses (in the UK at 
least) have to be combined into a single mark on the basis of which the student’s 
degree is given an overall classification. But how are these component course 
marks to be combined to give a single degree mark? The simplest method would 
be to take the average, but this is often rejected on the grounds that it unfairly 
penalizes a student who has done well on nearly all his or her courses, but has 
got a very low mark for one or two courses which he or she particularly disliked. 
This leads to the suggestion that the overall mark should be the average of say 
the best three quarters of the student’s course marks. However, this suggestion 
has a problem. If it became known that this was the algorithm, many students 
would concentrate on just three quarters of their courses — doing very little 
work on the remaining quarter, on the grounds that these will not count any- 
way. To overcome this problem, formulas have been suggested which take some 
account of the course marks in the worst quarter of a student’s courses, but 
not so much account as for those in the best three quarters. In addition to all 
these considerations, there is another complexity. Often courses in a student’s 
first year are weighted as less than those in his or her second year, the courses 
2 An account of this matter is to be found in (Mach [1893], pp. 360-5). 
I have argued for this view in detail in (Gillies [2000], pp. 200-5). Debates about operationalism 
have important implications for the philosophy of probability.

--- Page 233 ---

British Journal for the Philosophy of Science 227 
in the second year are weighted less than those in the third year, and so on. As 
a result of all this, there are a number of competing formulas for calculating 
the degree mark from the course marks of the component courses, and often 
these formulas are quite complicated in character. 
Now the philosophical point I want to make here is that in this situation, 
in contrast to a typical measurement situation in the natural sciences, there is 
no principle of single value. No one believes that there is a single real social 
quantity ‘degree mark’ which the various formulas are trying to measure more 
or less accurately. The question is simply which of the various formulas is 
the most convenient to adopt given the objectives of the university. Once a 
particular formula is adopted by the university, that formula defines what 
is meant by ‘degree mark’. The measurement method defines the concept and 
there is nothing more to the meaning of the concept. The measurement method 
is automatically valid, as a matter of convention. In other words, Bridgman’s 
operationalism, which Chang quite rightly rejected for the natural sciences, 
applies here. Of course, I have only considered here one simple example, but 
a similar account could be given for many important concepts in the social 
sciences. 
5 Conclusions 
Despite these disagreements, I have a very favourable overall view of Chang’s 
book. It deals with a central problem in philosophy of science, the problem of 
how can concepts come to be measured. I would go so far as to say that it is the 
most important book on this subject since Bridgman’s classic work of 1927. 
Bridgman introduced operationalism in his book, but I think that, as far as the 
natural sciences are concerned, this should be superseded by Chang’s ‘iteration’ 
approach. There are, moreover, important reasons why Chang’s book should 
become mandatory reading for anyone who wants to pursue the problem of 
measurement further. Most discussions of the problem consider the example 
of the measurement of temperature, but very often the analysis of this example 
is hypothetical in character and does not take into account the complexities of 
the real historical process by which temperatures came to be measured. Chang 
has expounded these little-known historical details with exemplary clarity and 
accuracy, and now philosophers of science have no excuse for not taking them 
into account in their analyses of temperature. The need to take account of 
these historical details makes the development of a theory of temperature 
measurement much harder, but a theory which can succeed in doing so is 
likely to be a much better theory. This shows the advantage which history and 
philosophy of science has over a purely analytic philosophy of science. The need 
to produce an analysis that fits with actual history imposes extra constraints

--- Page 234 ---

228 Review 
on philosophers of science, and these extra constraints are likely to improve 
their theories. 
References 
Bridgman, P. W. [1927]: The Logic of Modern Physics, Macmillan paperback edition, 
1960, New York, NY: Macmillan. 
Gillies, D. A. [2000]: Philosophical Theories of Probability, London: Routledge. 
Mach, E. [1893]: The Science of Mechanics: A Critical and Historical Account of Its 
Development, Sixth American edition, 1960, Lasalle, IL: Open Court.

--- Page 235 ---

Brit. J. Phil. Sci. 60 (2009), 229-233 
REVIEW 
MARKUS SCHRENK 
The Metaphysics of Ceteris Paribus Laws 
Frankfurt/Main: Ontos, 2007 
192 pp., $79.00 
ISBN: 9783938793428 
Alexander Reutlinger 
Philosophisches Seminar, Westfalische Wilhelms- Universitat Miinster 
Alexander. Reutlinger@uni-muenster.de 
Schrenk’s book is an original contribution to the debate on ceteris paribus laws, 
and focuses on the metaphysics of these laws—to be precise, on the question 
of whether the concepts of natural law are compatible with the metaphysical 
thesis that there possibly are real exceptions to regularities. Addressing this 
topic is unique to the debate on ceteris paribus laws. 
According to Schrenk’s main thesis, those possible exceptions are charac- 
terized straightforwardly: they are ‘real exceptions’ in the world to what the 
law statement says. In other words, ‘[. . .] here laws themselves have exceptions. 
Their consequent property [i.e. the property expressed by the predicate in the 
consequent of a lawful conditional] is indeed not instantiated at all (not even 
partially)’ (p. 26). Metaphorically speaking, laws that are qualified by a ceteris 
paribus clause describe a situation as if ‘God decides to switch off (p. 25) the 
property that the predicate in the consequent refers to. 
The book is divided into three parts. The first part presents two 
examples of real exceptions. The second part argues that real exceptions 
are—in spite of the common opinion that laws are necessarily exceptionless 
regularities—compatible with two important philosophical concepts of laws of 
nature. The two views that Schrenk discusses are a Humean approach on the 
one hand and an anti-Humean view on the other. He chooses David Lewis’ 
best system analysis of laws on the Humean side and David Armstrong’s neces- 
sitarian analysis of law as a relation between (structural) universals as a repre- 
sentative anti-Humean account. In the third and last part, the topic is shifted 
from laws of nature to laws in the special sciences. According to Schrenk, we 
can distinguish two kinds of special science laws that differ in their autonomy 
from laws of nature: (1) grounded and (2) emergent laws. Let me examine these 
three parts in more detail. 
© The Author (2009). Published by Oxford University Press on behalf of British Society for the Philosophy of Science. All rights reserved 
doi:10.1093/bjps/axn044 For Permissions, please email: journals. permissions@oxfordjournals.org 
Advance Access published on January 8, 2009

--- Page 236 ---

230 Review 
In the first part, Schrenk presents two examples of real exceptions to the 
fundamental laws of physics. The examples differ in kind: the first, so-called 
‘index-laws’, are the product of a thought experiment, 1.e., they are construed 
in a philosopher’s armchair as a mere conceptual possibility that need not 
actually be the case according to actual scientific practice (of physics). The 
name ‘index-laws’ stands for ‘individual exceptional space-time region’ (p. 45) 
where a certain regularity does not hold, contrary to the rest of spacetime. 
It does not hold in spite of the fact that the ‘index’ region is alike in all 
intrinsic properties to (at least) one spacetime region where the regularity does 
hold. The second example is taken from one of our best physical theories, 
namely the general theory of relativity. It implies that there are real exceptions 
to the theory: singularities in spacetime such as black and white holes. Schrenk 
considers singularities as spacetime points where physical laws ‘are violated or 
do not make sense’ (p. 55). 
In part two, these examples of real exceptions are confronted with a Humean 
and an anti-Humean account of natural laws. Surprisingly enough, Schrenk has 
strong arguments to convince us that both approaches can be reconciled with 
there being real exceptions. This is astonishing, because the common premise 
of both approaches seems to be that laws of nature are at least exceptionless 
regularities in the world. I will consider only his discussion of Lewis’ theory, 
because real exceptions seem to be more obviously at odds with a Humean 
metaphysics of physics. 
According to the best system analysis, a statement is a law of nature iff 
it is part of a deductive system of statements that maximizes simplicity and 
strength. Schrenk argues that a ceteris paribus clause, which takes a (local) 
exception into account, might weaken the simplicity of the deductive system 
in question. But, nevertheless, it is also possible that the same system remains 
the robustly best system in comparison to other systems. It might still have 
the best balance of simplicity and strength. This is not only due to Schrenk’s 
interpretation of Lewis’ ideas, but Lewis’ own thesis, as Schrenk is able to show 
in a careful exegesis of usually ignored passages in the most frequently cited 
texts on laws of nature by Lewis. Another reason for believing that the best 
system analysis is compatible with the possibility of real exceptions comes from 
Lewis’ thesis of Humean supervenience of laws, i.e. the thesis that laws glob- 
ally supervene on fundamental (supposedly physical), intrinsic and qualitative 
properties, which are instantiated at spacetime points. It follows from these 
features of fundamental properties that there are no necessary connections in 
the world. Contingent instantiations of fundamental properties logically allow 
a Humean mosaic in another possible world where not everything happens in 
a perfectly regular way. It even allows totally messy worlds where there are no 
regularities at all. Schrenk concludes that the actual world might be a messy 
world. But if we believe that what Schrenk has presented so far is true, then he

--- Page 237 ---

The British Journal for the Philosophy of Science 231 
has good reasons to assert that our actual world in fact is messy: One of our best 
physical theories describing the actual world, the general theory of relativity, 
tells us that there are singularities. And if we accept singularities in the sense of 
real exceptions, as Schrenk suggests, we have to conclude that the actual world 
is a member of the set of messy possible worlds. Here Schrenk’s arguments 
clearly imply a thesis that is stronger than merely considering whether the pos- 
sibility of exceptions coheres with our concepts of law: It seems as if Schrenk 
is not aware of the fact that he really argues for the existence of ceteris paribus 
laws in the actual world, not for their mere possibility. However, Schrenk agai: 
draws a provocative consequence: In a messy world, a deductive system that 
contains statements qualified by a ceteris paribus clause might achieve a better 
(and even the robustly best) combination of simplicity and empirical strength 
compared to rival deductive systems containing only strict generalizations (pp. 
85-7). Or, as Schrenk expresses this idea in a slogan: ‘mess might make strictness 
mediocre’ (p. 86). Nevertheless, Humeans (and many anti-Humeans) claim rig- 
orously that laws are at /east exceptionless regularities. Such a statement seems 
to be in blatant contradiction to Schrenk’s real exceptions. But even in this case, 
Schrenk offers a viable solution: Strictly speaking, Lewis’ idea is true that the 
generalizations in the best system hold without an exception. They are even 
true with respect to ceteris paribus laws, because ‘a version of the violated law, 
complicated and weakened by a [ceteris paribus] clause to permit the one excep- 
tion’ (p. 78) does hold without an exception. Hence, in this sense, the ‘violated 
law neither opposes [the Humean thesis that] real laws have no exceptions, nor 
[the Humean thesis that] laws are exceptionless regularities’ (p. 82) and the trick 
is done. 
In part three, Schrenk argues for his metaphysical thesis with regard to laws 
in the special sciences, i.e., non-fundamental, non-physical laws. Two groups 
of special science laws can be distinguished: (1) grounded and (2) emergent 
laws. Both describe the behavior of complex (chemical, biological, mental, so- 
cial etc.) objects whose parts are fundamental objects, but they differ in the 
way they refer to complex objects. While the former describe the behavior 
of a complex object in terms of the interaction of its micro-parts, the latter 
refer to complex objects as wholes. According to Schrenk, grounded laws ‘in- 
herit’ their coherence with the assumption of exceptions by supervening on 
the (fundamental) micro-laws of the parts of a system, because fundamental 
micro-laws themselves allow exceptions. It is promising to draw more atten- 
tion to Schrenk’s theory of ‘emergent laws’ that is independent of any concept 
of natural laws and therefore deals with generalizations which hold—unlike 
natural law statements—uncontroversially with exceptions. 
The treatment of emergent laws is Schrenk’s most original work. Emergent 
laws are supposed to deal with real exceptions with regard to laws in the special 
sciences. ‘Emergent’ refers to the fact that especially in biology, psychology and

--- Page 238 ---

232 Review 
the social sciences we typically do not know a priori whether there is any lawful 
way the special science systems depend on their fundamental components. Even 
if physicalism were true, these laws do not seem to receive their lawlikeness from 
the fact that they can be micro-explained in terms of their parts. In the case 
of special science laws, we have (so far) good reasons to believe that there are 
real exceptions to them. Besides the supervenience argument for real exceptions 
mentioned above, we can present another one: Special science laws can fail to 
hold, because causes—described by more fundamental sciences, e.g., physical 
or biological causes if we take economic laws as an example—can prevent the 
consequent-property of a special science law from occurring. 
It is a novel idea by Schrenk to provide a Humean metaphysics for emergent 
laws. He proposes to apply Lewis’ best system analysis to special science laws 
by ‘running a competition’ for the best system in each special science (and even 
in each sub-discipline of a science). We run this competition in two steps: In a 
first step, the Lewisian best system is constructed from an ideal point of view 
with no epistemic restrictions—we know all the (physical, biological, mental, 
social etc.) facts in spacetime. It not only lists the regularities in the behavior 
of special science systems, but also lists the exceptions to the regularity. In a 
second step, all exceptions are deleted from the ideal deductive system and only 
the statements about regularities remain. What remains is, as Schrenk hopes, 
the robustly best system of a certain special science. Schrenk comments on this 
deletion of listed exceptions: ‘This cosmetic operation is less superficial than it 
might seem: [. . .] it translates the lengthy law statements that have participated 
in the [best system] contest into the law statements typical for the special 
science: law statements with proviso [here synonymously used with ‘ceteris 
paribus’| clauses.’ (p. 166) As Schrenk points out, this provides a metaphysical 
interpretation of a ceteris paribus clause without defining the meaning or the 
epistemic role of ‘ceteris paribus’. In Schrenk’s own words: ‘statements bearing 
provisos are abbreviations of those ideal statements (including the exceptions) 
which are part of the robustly best system. It is the membership in the best 
system that makes the proviso clause acceptable.’ (p. 166f) Nevertheless, it 
seems to be premature for Schrenk to conclude from such an admittedly original 
metaphysical account of ceteris paribus clauses in special science laws that ‘we 
have arrived from the armchair into the lab’ (p. 166). To ‘arrive at the lab’, 
i.e., to reconstruct scientific practice rationally in philosophical terms, requires 
more than successfully arguing that real exceptions are compatible with our 
philosophical concepts of natural and special science laws. What is needed in 
addition is exactly the work that is done in the main debates on ceteris paribus 
laws, causation, explanation etc.: we need to understand the epistemological 
role of such laws and their metaphysics. We achieve these goals by asking 
questions like the following ones: How do explanations and predictions work, 
if they do not contain strict laws? How should we conceive the properties that

--- Page 239 ---

The British Journal for the Philosophy of Science 233 
ceteris paribus laws refer to? As purely categorical properties or as (essential) 
powers of an individual? Note that these questions by no means question the 
value of Schrenk’s arguments. 
I strongly recommend Markus Schrenk’s book to all those philosophers of 
science who are interested in answering these questions. Schrenk’s contribution 
is of great value for the current debate in metaphysics of science, not only 
because he successfully develops original arguments within the debate, which 
connect metaphysics of the natural and the special sciences. More importantly, 
Schrenk tells us something about the debate by revealing unexpected and tacit 
common premises of prima facie opposing philosophical camps: his arguments 
for the possibility of real exceptions remind us that the distinction between 
Humean and anti-Humean accounts might be too simple-minded. He reminds 
us that there is no easy story to tell about the metaphysics of laws, and related 
concepts such as causation and explanation.

--- Page 241 ---

Brit. J. Phil. Sci. 60 (2009), 235-238 
REVIEW 
MICHELA MASSIMI 
Pauli’s Exclusion Principle: The Origin and Validation 
of a Scientific Principle 
Cambridge: Cambridge University Press, 2005, 
price £53/$85 
ISBN: 978-0-521-83911-2, 211 pp., hardback 
Helge Kragh 
Institute of Science Studies, University of Aarhus, Denmark 
One of the fundamental laws of modern particle physics, Pauli’s exclusion prin- 
ciple, is probably best known as the rule that makes the periodic system of the 
chemical elements understandable in terms of quantum mechanics. According 
to this version of the principle, two electrons in the same atom cannot have 
the same four quantum numbers; they cannot be in the same quantum state, 
including the spin quantum state. However, the periodic system is merely one 
area of application of the Pauli principle, which is of a completely general 
and fundamental nature. Although its historical development until the 1940s 
has been detailed by physicists and historians of physics (including Sin-Itiro 
Tomonaga and John Heilbron), it has attracted little attention in the philosoph- 
ical literature. Michela Massimi’s aim is not primarily to describe the history 
of the exclusion principle, or the closely related concept of spin, but to use the 
history for a philosophical analysis of the historical trajectory and validity of 
the principle. This she does in a most interesting way and in the process she 
provides a clear and valuable historical account covering the period from about 
1920 to the 1990s. I think it would have improved the account if she had made 
more use of the research done by historians of science. For example, her section 
on the Dirac equation for the electron would have benefited if this literature 
had been consulted. 
The history of the exclusion principle can be divided into four major phases. 
Its origin can be traced back to the intricate spectroscopic problems that 
plagued the old quantum theory (the so-called atomic core model) and which 
Alfred Landé, Niels Bohr, Werner Heisenberg and others sought to solve each 
in their own way. Realizing the failure of these attempts, at the end of 1924 
Wolfgang Pauli introduced his notion of a non-mechanical Zweideutigkeit 
(double-valuedness), which amounted to a new quantum number for the 
© The Author (2009). Published by Oxford University Press on behalf of British Society for the Philosophy of Science. All rights reserved 
doi: 10.1093/byps/axn056 For Permissions, please email: journals. permissions@oxfordjournals.org

--- Page 242 ---

236 Review 
electron, and he formulated his first version of what he called the exclusion 
rule. Interestingly, this was well before the spectroscopic discovery of the elec- 
tron’s spin, a concept that Pauli initially resisted because it rested on a semi- 
classical mechanical model. As Massimi emphasizes, the exclusion principle 
was originally nothing but a useful rule, lacking foundation in or connection 
with the new quantum mechanics. This changed in the second phase, when 
the principle was incorporated into the quantum statistics of many-particle 
systems, resulting in the important distinction between Fermi-—Dirac statistics 
and Bose-Einstein statistics (or between fermions and bosons). Moreover, Paul 
Dirac proved that the electron’s spin was included in his first-order relativistic 
wave equation. With these theoretical innovations, dating 1926-28, the Pauli 
principle could be understood on a quantum-mechanical basis. This important 
insight was further generalized in the work that about 1940 led Pauli to the fun- 
damental spin-statistics theorem, which provides a vital connection between a 
particle’s spin and its kind of quantum statistics: particles of half-integer spin 
are fermions, those of integer spin are bosons. The Nobel Prize for 1945 was 
awarded to Pauli for ‘the discovery of the exclusion principle, also called the 
Pauli principle.’ At this time Pauli’s rule, originally grounded in experimental 
data only, had developed into a true scientific principle of great nomological 
strength. How this happened and how it can be justified philosophically is a 
central part of Massimi’s book. 
Massimi carries her story of the exclusion principle into its fourth (and last?) 
phase by analyzing the negative evidence that unexpectedly faced the principle 
in connection with the quark theory of hadrons in the 1960s. I consider this 
to be a particularly valuable part of her work because it deals with episodes 
and concepts (such as parastatistics and ‘parons’) that have not been covered 
by historians of physics. To put it briefly, quarks seemed to violate the exclu- 
sion principle, as some elementary particles included two or more quarks in 
the same quantum state. A way ‘to save the phenomena’ was to introduce a 
new quantum number (colour), and this was precisely what happened when 
quantum chromodynamics came into being in 1973. Since then, experimental- 
ists have continued to look for small-scale violation of the Pauli principle, but 
no violation has been detected. Massimi’s account of the history of the exclu- 
sion principle is comprehensive insofar that it includes work from only a few 
years ago. 
Pauli’s Exclusion Principle is organized in five chapters, starting with a philo- 
sophical overview that readers not trained in philosaphy may find quite de- 
manding, not to say frightening. At least I did. Some of the subsequent chap- 
ters are primarily of a historical nature, while others focus on philosophical 
analysis. The structure of the book makes it possible for historians of science to 
skip most of the philosophy, and conversely for philosophers of science to skip 
most of the historical details. But, of course, the book should be read in neither

--- Page 243 ---

The British Journal for the Philosophy of Science 237 
of these restricted ways but in its entirety. It is precisely the integration between 
historical description and philosophical analysis which makes the book so ap- 
pealing and relevant to current science studies. Contrary to many other works 
that use historical case studies as background for philosophical discussion, 
Massimi’s book builds faithfully on a detailed and insightful historical account 
of an important chapter in modern theoretical physics. 
All history involves an element of reconstruction, and Massimi’s account is 
indeed a reconstruction and selection of what happened in past science, but it 
is far from the kind of artificial rational reconstruction that Lakatos advocated 
in the 1960s. At that time it was optimistically believed that history and philos- 
ophy of science were about to establish a ‘marriage’, but the optimism did not 
last. For the last several decades history of science and philosophy of science 
have increasingly developed in separate directions, with many philosophers 
finding the authentic history of science to be of little relevance to their work. 
(Conversely, relatively few historians of science find the works of philosophers 
of science worth following.) However, this is not the place for discussing the 
reasons for the divorce or whether it is good or bad. What matters is that a 
book such as Massimi’s clearly demonstrates that in some cases an integrated 
historico-philosophical approach is not only possible but also eminently fruit- 
ful. Fortunately, it is not the only example of successful integration. Although 
Allan Franklin’s focus is different from Massimi’s, his works on experimen- 
tal particle physics (e.g., Selectivity and Discord of 2002) belong to the same 
genre. 
Massimi’s historical reconstruction of the emergence and development of 
Pauli’s exclusion principle serves primarily as a philosophical framework for 
understanding the dynamical dimensions of how the principle evolved from a 
phenomenological rule to a fundamental principle embedded in quantum field 
theory. More generally, she wants to establish why physicists are justified in 
believing in Pauli’s principle and the rationale for it. This leads her to reconsider 
Kuhn’s view on incommensurability and revolutionary transitions in science, 
which she does in particular in relation to the transition in 1925-26 from the 
old quantum theory to the new quantum mechanics. The incommensurability 
thesis she critically discusses is not so much Kuhn’s old one from 1962 as it 
is the weaker one of the 1980s, where Kuhn understood incommensurability 
as untranslatability between scientific lexicons. Massimi argues convincingly 
that the transition from the old to the new quantum theory does not fit to 
the Kuhnian picture in either of the versions. As to Pauli’s original exclusion 
principle, although it was not deduced from quantum theory, it was solidly 
grounded in the phenomenological rules based on spectroscopic measurements. 
Massimi does not dismiss Kuhn’s idea of lexical taxonomies completely, but 
she argues that they should be understood to have a regulative function and 
not a constitutive one.

--- Page 244 ---

238 Review 
She also deals in some detail with her case in relation to the Duhem-Quine 
thesis, according to which empirical evidence alone is unable to decide between 
rival theories. If this were really the case, it may seem hard to come up with 
a rational justification of the episode in the 1960s, when Pauli’s principle was 
retained in spite of anomalies arising from the quark theory. Massimi argues 
that this is not a serious problem and that there were indeed good scientific 
reasons for maintaining the general validity of the principle, not least because 
the explanatory scope of the quark theory increased with the introduction of 
coloured quarks. 
Among the philosophical positions that Massimi investigates, Kantianism 
appears no less prominently than Kuhnianism. ‘What relevance could Kant’s 
philosophy still have for twenty-first century physics, in the light of the scientific 
revolutions of relativity theory and quantum mechanics?’, she reasonably asks 
(p. 23). Ata first glance the answer might be ‘none’, but not so at a second glance. 
Although Kant was happily unaware of Pauli and his principle, according to 
Massimi a version of ‘dynamic Kantianism’ is well suited as a framework for 
analyzing the exclusion principle. This version, which in part is taken over from 
Ernst Cassirer, emphasizes the regulative principles of systematicity, whereas 
it does not hinge on constitutive principles of an a priori nature. It was, so 
she argues, precisely the systematizing qualities of the early exclusion rule that 
enabled it to transform into a genuine scientific law or principle. Moreover, 
she finds that Kuhn’s lexical taxonomies are best understood by such an ‘inter- 
mediate Kantian reading’ (p. 102). It is a little unclear if she believes that the 
exclusion principle is unique in this respect, or that her dynamic Kantianism 
has a broader range of application. There are other important principles in 
the history of science that started as empirical rules, such as the laws of ther- 
modynamics, and which may or may not fit Massimi’s Kantian explanatory 
pattern. 
To summarize, Pauli’s Exclusion Principle presents a detailed account and 
penetrating philosophical analysis of the historical development of one of the 
basic principles of modern physics. Massimi writes clearly and has an admirable 
command of the technical and conceptual problems of physics that she deals 
with. Although clear, her style is also condensed and she takes much for granted, 
both when it comes to physics and philosophy. The book is far from an easy 
read, but it is rewarding to study it carefully. It will be equally of value to 
historians and philosophers of science; and, not to forget, to physicists with 
an inclination for the philosophical and foundational aspects of their science. 
Fortunately, this species of physicists is not yet extinct.

--- Page 245 ---

CALL FOR PAPERS: Science and Decision, SPS09 
The Third Biennial Congress of the Societe de philosophie des sciences (SPS) will 
be held in Paris, 12-14 November 2009. 
The main theme of the Congress is ‘Science and decision’. 
Proposals on the theme are encouraged. However proposals on any topic in 
philosophy of science are welcome. 
The deadline is 4 May, 2009, with notification by June 30 at the latest. 
The conference will comprise plenary lectures with discussants (by invitation), on 
the congress main theme; contributed symposia (3-4 speakers, 2 hrs.); workshops 
(exploratory, round-table, debate...: flexible format, 1.5-2.5 hrs.); individual papers 
(30’ incl. discussion); posters. The invited speakers will be announced by mid- 
April. 
Three broad areas are understood as falling under the main theme: 
e Decisions within science 
e Decisions based on scientific knowledge 
e Decision as a scientific topic. 
More details can be found here: http://www.sps.ens.fr/activites/2009- 
3econgres.html 
For any question, please send an email to: spsO9@ens.fr 
A selection of papers relating to the main theme will be collected in a volume 
published in French at Editions Vuibert (an English edition is also envisaged). 
Selected papers on other topics will be published online. 
PROGRAM COMMITTEE 
Daniel Andler (Paris, chair); Martin Andler (Versailles Saint-Quentin); Alexander 
Bird (Bristol); Mikael Cozic (Paris); Francoise Gaill (Paris); Max Kistler (Grenoble); 
Catherine Laurent (Paris); Francoise Longy (Strasbourg); Thierry Martin 
(Besancon); Pierre-Michel Menger (Paris); Emmanuel Picavet (Paris); Stephanie 
Ruphy (Aix-en-Provence); Federica Russo (Louvain & Canterbury); Philippe 
Urfalino (Paris) ; Bernard Walliser (Paris). 
ORGANIZING COMMITTEE 
Thomas Pradeu (Paris, chair); |lsabelle Drouet (Leuven); Charles Girard (Paris); 
Brian Hill (Jouy-en-Josas); Florence Hulak (Paris); Gaell Mainguy (Paris); Pascal 
Ludwig (Paris). 
SUBMISSIONS (deadline: 4 May, 2009) 
Format 
* All proposals will be blind-refereed. Please delete all identifying references and 
name from submissions. Submissions by e-mail or by post should comprise all

--- Page 246 ---

nominal information, addresses etc. in a document separate from the proposal. 
« Symposia and workshops: Title; theme (max. 300 words); title and abstract (max. 
250 words) of each contribution. 
* Individual contribution: Title, abstract (max. 250 words), preferred format of 
delivery (oral presentation or poster: the final decision will be made by the 
Program Committee). 
Medium 
* (Recommended). On the Society’s web page, by Easychair (submissions only in 
pdf format) 
* By e-mail: sosO9@ens.fr 
* By post: Dr Stephanie Ruphy, Secretaire generale de la SPS, 11, rue du Vertbois 
75003 Paris, France. 
FEES AND TRAVEL GRANTS 
Registration fee: SPS members: 10€; students and unemployed: 5€; non- 
members, employed: 60€. 
A limited number of travel grants will be awarded to early-career scholars (PhD 
students and postdocs) whose contribution has been accepted. Applications 
should be made at the time of the submission, and include a brief vitae and a 
description of research interests (max. 250 words). The amount of the grants will 
depend on the number of requests and on pending applications to funding 
agencies.

--- Page 247 ---

Philosophy — New from Oxford 
AN Darwinian Populations and Natural Selection 
rommianions | Peter Godfrey-Smith 
The book presents a new way of understanding Darwinism 
* and evolution by natural selection, combining work in biology, 
philosophy, and other fields. It gives new criticisms of gene- 
centered views of evolution, and presents a new framework for 
understanding the evolution of complex organisms and societies. 
224 pages | February 2009 | Hardback | 978-0-19-955204-7| £25.00 
The Law-Governed Universe 
John T. Roberts 
John T. Roberts presents and defends a radical new theory of 
laws of nature. His Measurability Account affirms that there is 
an important sense in which laws govern the universe, rather 
than simply describing it economically. He argues that what is 
essential to laws is that they guarantee the reliability of methods of 
measuring natural quantities. 
424 pages | November 2008 | Hardback | 978-0-19-955770-7 | £50.00 
Grounding Concepts 
An Empirical Basis for Arithmetical Knowledge 
C. S. Jenkins 
Carrie Jenkins presents a new account of arithmetical 
knowledge, which manages to respect three key intuitions: a 
priorism, mind-independence realism, and empiricism. Jenkins 
argues that arithmetic can be known through the examination of 
empirically grounded concepts, non-accidentally accurate representations 
of the mind-independent world. 
304 pages | August 2008 | Hardback | 978-0-19-923157-7 | £35.00 
Being Reduced 
New Essays on Reduction, Explanation, and Causation 
Edited by Jakob Hohwy and Jesper Kallestrup 
Is the mind nothing but neural firings in the brain? Are we just 
a bunch of neurons? If the mind is just the brain, then how can 
we act as genuine, responsible agents in the world? Being 
Reduced attempts to understand these questions. 
336 pages | September 2008 | Hardback | 978-0-19-921153-1 | £45.00 
For more information please contact: OXF O RD 
Rachael Huttly, Humanities Marketing UNIVERSITY PRESS 
Email: rachael.huttly@oup.com www.oup.com

--- Page 248 ---

ACCELERATED ONLINE 
PUBLICATION 
READ PAPERS FROM THIS JOURNAL 
ONLINE, WEEKS IN ADVANCE OF SEEING 
THEM IN PRINT 
The advantage for readers: 
Read the very latest research online ahead of papers 
appearing in a journal issue. 
The advantage for authors: 
See your paper receiving citations faster than ever before. 
Visit Oxford Journal’s website at www.oxfordjournals.org, 
select this journal from the list to reach its homepage, and 
click on Advance Access. Bookmark the page and regularly 
check for the latest accepted papers. 
www.oxfordjournals.org 
OXFORD JOURNALS

--- Page 249 ---

STAY ALERT TO THE VERY 
LATEST RESEARCH 
DESIGN YOUR OWN EMAIL ALERTS TO TRACK 
JOURNAL CONTENT THAT IS IMPORTANT TO 
YOU, USING CITETRACK 
Track topics 
Tell CiteTrack which words or subjects to watch for in new content 
Track authors 
Be alerted whenever key authors you are following publish a new paper 
Track articles 
Know whenever a paper of interest to you is referenced by another 
paper or letter 
CiteTrack checks for content matching your criteria in new issues of 
the journal you've asked it to monitor, and emails you each time new 
content is found. 
With around 1000 journals currently available through this service, you 
can easily monitor a wealth of quality research with one simple service. 
Sign up for CiteTrack today — go to your favourite Oxford 
Journals title, and click on the CiteTrack link under ‘Alerting 
Services’ to set up your custom alerts. 
www.oxfordjournals.org OXFORD JOURNALS

--- Page 250 ---

7p PDA ALERTING 
STAY UP TO DATE WHEREVER YOU ARE 
Are you often away from your desk? 
Do you find it difficult to keep up with the 
latest research? 
If so, this new service could be for you. Simply download 
the tables of contents and abstracts of this journal onto 
your PDA. 
‘To find out how, just click on the ‘Handhelds’ link on the 
journal home page. 
www.oxfordjournals.org 
OXFORD JOURNALS

--- Page 251 ---

NOTES TO CONTRIBUTORS 
All submissions of articles and discussion notes should be sent to the editors, at the following address: Professor 
Alexander Bird and Professor James Ladyman, Department of Philosophy, University of Bristol, 9 Woodland 
Road, Bristol BS8 1TB, UK. (Tel. +44 (0) 117 928 7825. Fax +44 (0) 117 928 8626. E-mail bjps-editors@bris- 
tol.ac.uk) 
Articles on any aspect of the philosophy of science will be considered. Discussion notes related to articles 
previously published are welcome 
Contributors should send their submissions either in hard copy or by email. If submitting in hard copy, please 
send three hard copies, which will not normally be returned, to the editors at the address above. If submitting by 
email, please send your submission to bjps-editors@bristol.ac.uk, preferably in PDF with wide margins 
Submissions should be prepared for blind refereeing, with identifying references removed, but should be accom- 
panied by the telephone number, fax number, and e-mail address of the corresponding author. They need not be 
in any particular style on initial submission, but if they are accepted for publication we will require a version in 
the Journal’s style 
Articles must be prefaced by an abstract of about 100 words 
Every effort will be made to ensure that decisions on all submissions take place with 
SUBSCRIPTIONS 
A subscription to The British Journal for the Philosophy of Science comprises 4 issues. Prices include 
postage by surface mail; or for subscribers in the USA and Canada, by airfreight; or in India, Japan, Australia 
and New Zealand, by standard air 
Annual Subscription Rate (Volume 60, 4 issues, 2009) 
Institutional 
Print edition and site-wide online access: £123/US$240/€185 
Print edition only: £117/US$228/€176 
Site-wide online access only: £117/US$228/€176 
Personal 
Print edition £101/US$197/€152 
Please note: US$ rate applies to US & Canada, Euros applies to Europe, UK£ applies to UK and Rest of World 
There are special subscription rates available to society members; For a complete listing, please visit 
http://www.oxfordjournals.org/phisci/access_purchase/price_list.htm] 
Full prepayment, in the correct currency, is required for all orders. Orders are regarded as firm and payments are 
not refundable. Subscriptions are accepted and entered on a complete volume basis. Claims cannot be consid- 
ered more than FOUR months after publication or date of order, whichever is later. All subscriptions in Canada 
are subject to GST. Subscriptions in the EU may be subject to European VAT. If registered, please supply details 
to avoid unnecessary charges. For subscriptions that include online versions, a proportion of the subscription 
price may be subject to UK VAT. Personal rate subscriptions are only available if payment is made by personal 
cheque or credit card and delivery is to a private address 
The current year and two previous years’ issues are available from Oxford University Press. Previous volumes 
can be obtained from the Periodicals Service Company, |! Main Street, Germantown, NY 12526, USA. Email 
psc@periodicals.com. Tel: +1 (518) 537 4700. Fax: +1 (518) 537 5899 
For further information, please contact: Journals Customer Service Department, Oxford University Press, 
Great Clarendon Street, Oxford OX2 6DP, UK. Email: jnls.cust.serv@oxfordjournals.org. Tel (and answer 
phone outside normal working hours): +44 (0)1865 353907. Fax: +44 (0)1865 353485. In the US please con- 
tact: Journals Customer Service Department, Oxford University Press, 2001 Evans Road, Cary, NC 27513, 
USA. Email: jnlorders@oxfordjournals.org. Tel (and answerphone outside normal working hours): 800 852 
7323 (tollfree in USA/Canada). Fax: 919 677 1714. In Japan, please contact: Journals Customer Services, 
Oxford Journals, Oxford University Press, Tokyo, 4-5-10-8F Shiba, Minato-ku, Tokyo 108-8386, Japan 
Email: custserv.jp@oxfordjournals.org. Tel: +81 3 5444 5858. Fax: +81 3 3454 2929 
Methods of payment. (i) Cheque (payable to Oxford University Press, Cashiers Office, Great Clarendon Street, 
Oxford OX2 6DP, UK) in GBE Sterling (drawn on a UKbank), US$ Dollars (drawn on a US bank), or EU€ Euros 
(ii) Bank transfer to Barclays Bank plc, Oxford Group Office, Oxford (bank sort code 20-65-18) (UK), overseas 
only Swift code BARC GB 22 (GBE Sterling to account no. 70299332, IBAN GB89BARC2065 1870299332, US$ 
Dollars to account no. 66014600, IBAN GB27BARC20651866014600; EU€ Euros to account no. 78923655, 
IBAN GB16BARC2065 1878923655). (iii) Credit card (Mastercard, Visa, Switch or American Express) 
Oxford Journals Environmental and Ethical Policies 
Oxford Journals is committed to working with the global community to bring the highest quality research to the 
widest possible audience. Oxford Journals will protect the environment by implementing environmentally 
friendly policies and practices wherever possible. Please see http://www.oxfordjournals.org/ethicalpolicies.html 
for further information on Oxford Journals’ environmental and ethical policies 
USA DISTRIBUTOR The British Journal for the Philosophy of Science (ISSN 0007 0882) is published quarterly in March, June, 
September and December by Oxford University Press, Oxford, UK. Annual subscription price is 
£123/US$240/€185. The British Journal for the Philosophy of Science is distributed by Mercury International, 
365 Blair Road, Avenel, New Jersey, NJO7001, USA. Periodical postage paid at Rahway, New Jersey, USA and 
at additional entry points. 
US POSTMASTER: send address changes to The British Journal for the Philosophy of Science, c/o Mercury 
International, 365 Blair Road, Avenel, NJ 07001, USA.
