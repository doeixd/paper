--- Page 1 ---

r:¥&& m 
iUS&ix I... 
• •' — ^ 
. »p;stt:xUTc»:K*:Ui!>:»; 
:Jr{j;«M*2H:-:fhH?:i 
: xisi tr:?; >• Ut^V. mH H 
: v : : 
IfflH^gggHSSgSSgS 
^gi^gggta 
;iH'”/jn;i.-;;:' :i3K: 
rfc:.-:*ttl>;5T«pt4;iiiitK:t»fniirl*t3£?:5;Hni' :r *»- tni.-tetnii&iS” tf::: -. l^Ur, przr.^-. 
■tenMZKgsgggfe&feS~ggggggg tSrrZ L;. gj.;: £$=*£ pj 
gj§ .’.' 
< iyT;5£.;:Kri»s ;£i5ijnr.T 
^^PUi^snsfiit 
££££3^ 
iB'!:Sgmi!iSii!;3a;HaiiBHi?!!iK!gffi;iHi:ig®m<s35SS;r 
... - s

--- Page 5 ---

FINITE MARKOV CHAINS

--- Page 6 ---

THE UNIVERSITY SERIES IN 
UNDERGRADUATE MATHEMATICS 
Editors 
John L. Kelley, University of California 
Paul R. Halmos, University of Chicago 
Patrick Suppes—Introduction to Logic 
Paul R. Halmos—Finite-Dimensional Vector Spaces, 2nd Ed. 
Edward J. McShane and Truman A. Botts—Real Analysis 
John G. Kemeny and J. Laurie Snell—Finite Markov Chains 
A series of distinguished texts for undergraduate mathematics. 
Additional titles will be listed and announced as published.

--- Page 7 ---

FINITE MARKOV CHAINS 
JOHN G. KEMENY 
Professor of Mathematics 
Dartmouth College 
AND 
J. LAURIE SNELL 
Associate Professor of Mathematics 
Dartmouth College 
D. VAN NOSTRAND COMPANY, INC. 
PRINCETON, NEW JERSEY 
TORONTO LONDON 
NEW YORK

--- Page 8 ---

D. VAN NOSTRAND COMPANY, INC. 
120 Alexander St., Princeton, New Jersey (Principal office) 
257 Fourth Avenue, New York 10, New York 
D. Van Nostrand Company, Ltd. 
358, Kensington High Street, London, W.14, England 
D. Van Nostrand Company (Canada), Ltd. 
25 Hollinger Road, Toronto 16, Canada 
Copyright © i960 by 
D. VAN NOSTRAND COMPANY, Inc. 
Published simultaneously in Canada by 
D. Van Nostrand Company (Canada), Ltd. 
Library of Congress Catalogue Card No. 59-15644 
No reproduction in any form of this book, in whole or in 
part (except for brief quotation in critical articles or reviews), 
may be made without written authorization from the publishers. 
PRINTED IN THE UNITED STATES OF AMERICA

--- Page 9 ---

PREFACE 
The basic concepts of Markov chains were introduced by A. A. Markov 
in 1907. Since that time Markov chain theory has been developed by 
a number of leading mathematicians. It is only in very recent times 
that the importance of Markov chain theory to the social and biological 
sciences has become recognized. This new interest has, we believe, 
produced a real need for a treatment, in English, of the basic ideas 
of finite Markov chains. 
By restricting our attention to finite chains, we are able to give 
quite a complete treatment and in such a way that a minimum amount 
of mathematical background is needed. For example, we have written 
the book in such a way that it can be used in an undergraduate prob¬ 
ability course, as well as a reference book for workers in fields out¬ 
side of mathematics. 
The restriction of this book to finite chains has made it possible to 
give simple, closed-form matrix expressions for many quantities usually 
given as series. It is shown that it suffices for all types of problems to 
consider just two types of Markov chains, namely absorbing and ergodic 
chains. A “fundamental matrix” is developed for each type of chain, 
and the other interesting quantities are obtained from the fundamental 
matrices by elementary matrix operations. 
One of the practical advantages of this new treatment of the sub¬ 
ject is that these elementary matrix operations can easily be programed 
for a high-speed computer. The authors have developed a pair of pro¬ 
grams for the IBM 704, one for each type of chain, which will find a 
number of interesting quantities for a given process directly from the 
transition matrix. These programs were invaluable in the computation 
of examples and in the checking of conjectures for theorems. 
A significant feature of the new approach is that it makes no use of 
the theory of eigen-values. The authors found, in each case, that the 
expressions in matrix form are simpler than the corresponding expres¬ 
sions usually given in terms of eigen-values. This is presumably due 
to the fact that the fundamental matrices have direct probabilistic in¬ 
terpretations, while the eigen-values do not. 
The book falls into three parts. Chapter I is a very brief summary 
of prerequisites. Chapters II-VI develop the theory of Markov chains. 
Chapter VII contains applications of this theory to problems in a variety 
of fields. A summary of the symbols used and of the principal defi¬ 
nitions and formulas can be found in the appendices together with page 
references. Therefore, there is no index, but it is hoped that the de-

--- Page 10 ---

VI PREFACE 
tailed table of contents and the appendices will serve a more useful 
purpose. 
It was not intended that Chapter I be read as a unit. The book can 
be started in Chapter II, and the reader has the option of looking up the 
brief summary of any prerequisite topic not familiar to him, when he 
needs it in a later chapter.* 
The book was designed so that it can be used as a text for an under¬ 
graduate mathematics course. For this reason the proofs were carried 
out by the most elementary methods possible. The book is suitable 
for a one-semester course in Markov chains and their applications. 
Selections from the book (presumably from Chapters II, III, IV, and 
possibly VII) could also be used as part of an upper-class course in 
probability theory. For this use, exercises have been given at the end 
of Chapters II-VI. v 
The following system of notation has been used in the book: Num¬ 
bers are denoted by small italic letters, matrices by capital italics, vectors 
by Greek letters. Functions, sets, and other abstract objects are de¬ 
noted by boldface letters. 
The authors gratefully acknowledge support by the National Science 
Foundation to the Dartmouth Mathematics Project. Many of the origi¬ 
nal results in this book were found by the authors while working on 
this project. The authors are also grateful for computing time made 
available by the M.I.T. and Dartmouth Computation Center for the 
development of the above-mentioned programs and for the use of these 
programs. 
The authors wish to express their thanks to two research assistants, 
P. Perkins and B. Barnes, for many valuable suggestions as well as for 
their careful reading of the manuscript. Thanks are due to Mrs. M. 
Andrews and Mrs. H. Hanchett for typing the manuscript. 
The Authors 
Hanover, New Hampshire 
September, 1959 
* A more detailed treatment of most of these topics may be found in one of the 
following books: (1) Modern Mathematical Methods and Models, Volumes 1 and 2, 
by the Dartmouth Writing Group, published by the Mathematical Association of 
America, 1958. [Referred to as M4.] (2) Introduction to Finite Mathematics, by 
Kemeny, Snell, and Thompson, Prentice-Hall, 1957. [Referred to as FM.] (3) 
Finite Mathematical Structures, by Kemeny, Mirkil, Snell, and Thompson, Prentice- 
Hall, 1959. [Referred to as FMS.] For the prerequisites in probability theory, as 
well as a treatment of Markov chains from a different point of view, the reader may 
also wish to consult Introduction to Probability Theory and Its Applications, by W. 
Feller, Wiley, 1957.

--- Page 11 ---

TABLE OF CONTENTS 
CHAPTER I—PREREQUISITES 
SECTION PAGE 
1.1 Sets 1 
1.2 Statements 
A- 
2 
1.3 Order Relations 3 
1.4 Communication Relations 5 
1.5 Probability Measures 7 
1.6 Conditional Probability 9 
1.7 Functions on a7 Possibility Space 10 
1.8 Mean and Variance of a Function 12 
1.9 Stochastic Processes 14 
1.1C 1 Summability of Sequences and Series 18 
1.11 Matrices 19 
CHAPTER II—BASIC CONCEPTS OF MARKOV CHAINS 
2.1 Definition of a Markov Process and a Markov Chain 24 
2.2 Examples 26 
2.3 Connection with Matrix Theory 32 
2.4 Classification of States and Chains 35 
2.5 Problems to be Studied 
CHAPTER III—ABSORBING MARKOV CHAINS 
38 
3.1 Introduction 43 
3.2 The Fundamental Matrix 45 
3.3 Applications of the Fundamental Matrix 49 
3.4 Examples 55 
3.5 Extension of Results 
CHAPTER IV—REGULAR MARKOV CHAINS 
58 
4.1 Basic Theorems 69 
4.2 Law of Large Numbers for Regular Markov Chains 73 
4.3 The Fundamental Matrix for Regular Chains 
vii 
75

--- Page 12 ---

viii _ TABLE OF CONTENTS 
SECTION . PAGE 
4.4 First Passage Times , 78 
4.5 Variance of the First Passage Time 82 
4.6 Limiting Covariance 84 
4.7 Comparison of Two Examples 90 
4.8 The General Two-State Case 94 
CHAPTER V—ERGODIC MARKOV CHAINS 
• 
5.1 Fundamental Matrix 99 
5.2 Examples of Cyclic Chains 102 
5.3 Reverse Markov Chains 105 
CHAPTER VI—FURTHER RESULTS 
6.1 Application of Absorbing Chain Theory to Ergodic Chains 112 
6.2 Application of Ergodic Chain Theory to Absorbing 
Markov Chains 117 
6.3 Combining States 123 
6.4 Weak Lumpability 132 
6.5 Expanding a Markov Chain 140 
CHAPTER VII—APPLICATIONS OF MARKOV CHAINS 
7.1 Random Walks 149 
7.2 Applications to Sports 161 
7.3 Ehrenfest Model for Diffusion 167 
7.4 Applications to Genetics 176 
7.5 Learning Theory 182 
7.6 Applications to Mobility Theory 191 
7.7 The Open Leontief Model 200 
APPENDIX I—SUMMARY OF BASIC NOTATION 207 
APPENDIX II—BASIC DEFINITIONS 207 
APPENDIX III—BASIC QUANTITIES FOR 
ABSORBING CHAINS 208 
APPENDIX IV—BASIC FORMULAS FOR 
ABSORBING CHAINS 209 
APPENDIX V—BASIC QUANTITIES FOR 
ERGODIC CHAINS 209 
APPENDIX VI—BASIC FORMULAS FOR 
ERGODIC CHAINS 210

--- Page 13 ---

CHAPTER I 
» 
PREREQUISITES 
§ 1.1 Sets. By a set a mathematician means an arbitrary but well- 
defined collection of objects. Sets will be denoted by bold capital 
letters. The objects in the collection are called elements. 
If A is a set, and B is a set whose elements are some (but not neces¬ 
sarily all) of the elements of A, then we say that B is a subset of A, 
symbolized asB^A. If the two sets have exactly the same elements, 
then we say that they are equal, i.e. A = B. Thus A = B if and only 
if A c= B and BcA. If B is a subset of A and is not equal to A, then we 
say that it is a proper subset, and write BcA. If A and B have no 
element in common, we say that they are disjoint. 
Very frequently we will deal with a given set of objects, and discuss 
various subsets of it. The entire set will be called the universe, U. 
A particularly interesting subset is the set with no elements, the 
empty set E. 
Given a set, there are a number of ways of getting new subsets from 
old ones. If A and B are both subsets of U, then we define the follow¬ 
ing operations: 
(1) The complement of A, A, has as elements all the elements of U 
which are not in A. 
(2) The union of A and B, A U B, has as elements all the elements of 
A and all the elements of B. 
(3) The intersection of A and B, A n B, has as elements all the 
elements that A and B have in common. 
(4) The difference of A and B, A — B has as elements all the elements 
of A that are not in B. 
list some easily provable 
A n B = B n A 
A u E = A 
A n E = E 
l 
To illustrate these operations, we will 
relations between these sets: 
U = E aTTb = A n B 
A = A A n B = A u B 
A-B = A n B A u B = B u A

--- Page 14 ---

2_FINITE MARKOV CHAINS Chap. I 
If Ai, A2, . . . , Ar are subsets of U, and every element of U is in one 
and only one set Aj, then we say that A = {Ai, A2, . . . , Ar| is a 'par¬ 
tition of U. 
If we wish to specify a set by listing its elements, we write the 
elements inside curly brackets. Thus, for example, the set of the 
first five positive integers is {1, 2, 3, 4, 5}. The set {1, 3, 5} is a proper 
subset of it. The set {2}, which is also a subset of the five-element set, 
is called a unit set, since it has only one element. 
In the course of this book we will have to deal with both finite and 
infinite sets, i.e. with sets having a finite number or an infinite number 
of elements. The only infinite sets that are used repeatedly are the 
set of integers {1, 2, 3, . . .} and certain simple subsets of this set. 
For a more detailed account of the theory of sets see FM Chanter II 
or FMS Chapter II. j- 
§ 1.2 Statements. We are concerned with a process which will 
frequently be a scientific experiment or a game of chance. There are 
a number of different possible outcomes, and we will consider various 
statements about the outcome. 
We form the set U of all logically possible outcomes. These must 
be so chosen that we are assured that exactly one of these will take 
place. The set U is called the possibility space. If p is any statement 
about the outcome, then it will (in general) be true according to some 
possibilities, and false according to others. The set P of all possibilities 
which would make p true is called the truth set of p. Thus to each 
statement about the outcome we assign a subset of U as a truth set. 
The choice of U for a given experiment is not unique. For example, 
for two tosses of a coin we may analyze the possibilities as 
— { H, HT, TH, TT} or U = {0H, 1H, 2H}. In the first case we give 
the outcome of each toss and in the second only the number of heads 
which turn up. (For a more detailed discussion of this concept see 
FM Chapter II or FMS Chapter II.) 
Given two statements p and q having the same subject matter (i.e. 
the same U), we have a number of ways of forming new statements 
from them. (We will assume that the statements have P and 0 as 
truth sets:) 
(1) The statement ~p (read “not p”) is true if and only if p is false. 
Hence it has P as truth set. 
(2) The statement pV q (read “p or q”) is true if either p is true or 
q is true or both. Hence it has P U Q as truth set. 
^ ,i'rrffKvTy!,SnCli’ and ThomPson> Introduction to Finite Mathematics, Engle¬ wood Cliffs, N.J., Prentice-Hall, Inc., 1957. g 
SneU’ and ThomPson. Finite Mathematical Structures, Engle¬ wood Cliffs, N.J., Prentice-Hall, Inc., 1959. B

--- Page 15 ---

Sec. 3 PREREQUISITES 3 
(3) The statement p /\q (read “p and q”) is true if both p and q are 
true. Hence it has P n Q as truth set. 
Two special kinds of statements are among the principal concerns 
of logic. A statement that is true for each logically possible outcome, 
that is, a statement having U as its truth set, is said to be logically 
true (such a statement is sometimes called a tautology). A statement 
that is false for each logically possible outcome, that is a statement 
having E as its truth set, is logically false or self-contradictory. 
Two statements are said to be equivalent if they have the same truth 
set. That means that one is true if and only if the other is true. 
The statements pi, P2, . . . , p* are inconsistent if the intersection of 
their truth sets is empty, i.e., Pi nP2 n • • ■ n Pj- = E. Otherwise 
they are said to be consistent. If the statements are inconsistent, then 
they cannot all be true. If they are consistent, then they could all be 
true. 
The statements pi, P2, . . . , p* are said to form a complete set of 
alternatives if for every element of U exactly one of them is true. This 
means that the intersection of any two truth sets is empty, and the 
union of all the truth sets is U. Thus the truth sets of a complete set 
of alternatives form a partition of U. A complete set of alternatives 
provides a new way (and normally a less detailed way) of analyzing 
the possible outcomes. 
§ 1.3 Order relations. We will need some simple ideas from the 
theory of order relations. A complete treatment of this theory will 
be found in M4, Vol. II, Unit 2. j We will take only a few concepts 
from that treatment. 
Let R be a relation between two objects (selected from a specified 
set U). We denote by aRb the fact that a holds the relation R to b. 
Some special properties of such relations are of interest to us. 
1.3.1 Definition. The relation R is reflexive if xRx holds for all 
x in U. 
1.3.2 Definition. The relation R is symmetric if whenever xRy 
holds, then yRx also holds, for all x, y in U. 
1.3.3 Definition. The relation R is transitive if whenever 
xRy AyRz holds, then xRz also holds, for all x, y, z in U. 
1.3.4 Definition. A relation that is reflexive, symmetric, and 
transitive is an equivalence relation. 
The fundamental property of an equivalence relation is that it 
partitions the set U. More specifically, let us suppose that R is an 
f MModern Mathematical Methods and Models, by the Dartmouth Writing Group. 
Mathematical Association of America, 1958.

--- Page 16 ---

4 FINITE MARKOV CHAINS Chap. I 
equivalence relation defined on U. We put elements of U into classes 
in such a manner that two elements a and b are in the same class if 
aRb. It can be shown that the resulting classes are well defined and 
mutually exclusive, giving us a partition of U. These classes are the 
equivalence classes of R. 
For example, let xRy express that “x is the same height as y,” where 
U is a set of human beings. Then the resulting partition divides these 
people according to their heights. Two men are in the same equiva¬ 
lence class if and only if they are the same height. 
1.3.5 Definition. A relation T is said to be consistent with the 
equivalence relation R if, given that xRy, then if xTz holds so does 
yTz, and if zTx holds so does zTy. 
1.3.6 Definition. A relation that is reflexive and transitive is 
known as a weak ordering relation. 
A weak ordering relation can be used to order the elements of U. 
Given a weak ordering T, and given any two elements a and b of U, 
there are four possibilities: (1) aTbAbTa; then the two elements are 
“alike” according to T. (2) aTbA-(bTa); then a is “ahead” of b. 
(3) ~ (aTb) AbTa; then b is “ahead.” (4) ~ (aTb) A~ (bTa); then we 
are unable to compare the two objects. 
For example, if xTy expresses that “I like x at least as well as y,” 
then the four cases correspond to “I like them equally,” “I prefer x,” 
I prefer y,” and “I cannot choose,” respectively. 
The relation of being alike acts as an equivalence relation. Indeed, 
it can be shown that if T is a weak ordering, then the relation xRy that 
expresses that xTy AyTx is an equivalence relation consistent with T. 
Thus T serves both to classify and to order. Consistency assures us 
that equivalent elements of U have the same place in the ordering. 
For example, if we choose ‘is at least as tall” as our weak ordering, 
this determines the equivalence relation “is the same height,” which is 
consistent with the original relation. 
1.3.7 Definition. If T is a weak ordering, then the relation 
xTyAyTx is the equivalence relation determined by it. 
1.3.8 Definition. If T is a weak ordering, and the equivalence 
relation determined by it is the identity relation (x = y) then T is a 
partial ordering. 
The significance of a partial ordering is that no two distinct elements 
are alike according to it. One simple way of getting a partial ordering 
is as follows: Let T be a weak ordering defined on U. Define a new 
relation T* on the set of equivalence classes by saying that uT*v 
holds if every element of u bears the relation T to every element of v

--- Page 17 ---

Sec. 4 PREREQUISITES 
This is a partial ordering of the equivalence classes, and we call it the 
partial ordering induced by T. 
1.3.9 Definition. An element a of U is called a minimal element 
if aTx implies xTa for all x e U. If a minimal element is unique, we 
call it a minimum. 
We can define “maximal element” and “maximum” similarly. If 
U is a finite set, then it is easily shown that for any weak ordering there 
must be at least one minimal element. However, this minimal element 
need not be unique. Similarly, the weak ordering must have a maxi¬ 
mal element, but not necessarily a maximum. 
§ 1.4 Communication relations. An important application of order 
relations is the study of communication networks. Let us suppose 
that r individuals are connected through a complex network. Each 
individual can pass a message on to a subset of the individuals. This 
we will call direct contact. These messages may be relayed, and 
relayed again, etc. This will be indirect contact. It will not be assumed 
that a member can contact himself directly. Let aTb express that the 
individual a can contact b (directly or indirectly) or that a = b. It is 
easy to verify that T is a weak ordering of the set of individuals. It 
determines the equivalence relation xTy /\yTx, which may be read as 
“x and y can communicate with each other, or x = y.” 
This equivalence relation may be used to classify the individuals. 
Two men will be in the same equivalence class if they can communicate, 
that is, if each can contact the other one. The induced partial ordering 
T* has a very intuitive meaning : The relation uT*v holds if all members 
of the class u can contact all members of the class v, but not con¬ 
versely unless u = v. Thus the partial ordering shows us the possible 
flow of information. 
In particular, u is a maximal element of the partial ordering if its 
members cannot be contacted by members of any other class, and u 
is a minimal element if its members cannot contact members of other 
classes. Thus the maximal sets are message initiators, while the 
minimal sets are terminals for messages. (See M4 Vol. II, Unit 2.) 
It is interesting to study a given equivalence class. Any two 
members of such a class can communicate with each other. Hence 
any member can contact any other member. But how long does it 
take to contact other members ? As a unit of time we will take the 
time needed to send a message from any one member to any member 
he can contact directly. We call this one step. We will assume that 
member i sends out a message, and we will be interested to know where 
the message could possibly be after n steps.

--- Page 18 ---

6 FINITE MARKOV CHAINS Chap. I 
Let Na be the set of n such that a message starting from member i 
can be in member j’s hands at the end of n steps. We will first con¬ 
sider Njj, the possible times at which a message can return to its 
originator. It is clear that if a e N« and b e Ny, then a + b e Ny after 
all the message can return in a steps and can be sent out again and be 
received back after b more steps. So the set Ny is closed under addition. 
The following number-theoretic result will be useful. Its proof is 
given at the end of the section. 
1.4.1 Theorem. A set of positive integers that is dosed under 
addition contains all but a finite number of multiples of its greatest 
common divisor. 
If the greatest common divisor of the elements of Ny is designated 
di, it is clear that the elements of Ny are all multiples of dt. But 
Theorem 1.4.1 tells us in addition that all sufficiently high multiples 
of di are in the set. 
Since each member can contact every other member in its equiva¬ 
lence class, the Ny are non-empty. We next prove that for i and j 
in the same equivalence class, di = dj = d, and that the elements of 
a given Ny are congruent to each other modulo d (their difference is a 
multiple of d). Suppose that a e Ny, b e Ny, and c e Ny. 
First of all, member i can contact himself by sending a message to 
member j and getting a message back. Hence a + c e Ny. The mes¬ 
sage could also go to member j, come back to member j, and then go 
to member i. This could be done in a + Jcdj -f c steps, where k is 
sufficiently large. Hence dj must be a multiple of di. But inexactly 
the same way we can prove that dt is a multiple of dp Hence 
di dj — d. 
Or again, the message could go to member j in b steps, and then back to 
member i. Hence b + c e Ny. Hence a + c and b + c are both divisible 
by d, and thus we see that a = b (mod d). Thus the elements of a 
given Ny are congruent to each other modulo d. We can thus intro¬ 
duce numbers fy, with 0 <fy<d, so that any element of Ny is con¬ 
gruent to fy, modulo d. It is also easy to see that Ny contains all but 
a finite number of the numbers fy + led. 
In particular we see that tu = 0 in each case, and hence fy + fy = 0 
(mod d). Also tij + tjm = tim (mod d). From this it is easily seen that 
tij 0 is an equivalence relation. Let us call such an equivalence 
class a cyclic class. 
Since fy + tjm = tim (mod d), we see that t^ = tim if and only if tjm = 0, 
hence if and only if members j and m are in the same cyclic class. 
Let n be any integer. If n = tij (mod d), then the message originating 
from member i can only be in this one cyclic class after n steps. From

--- Page 19 ---

Sec. 5 PREREQUISITES 7 
this it immediately follows that there are exactly d cyclic classes, and 
that the message moves cyclically from class to class, with cycle of 
length d. It is also easily seen that after sufficient time has elapsed, 
it can be in the hands of any member of the one cyclic class appropriate 
for n. 
While this description of an equivalence class of the communication 
network holds in complete generality, the cycle degenerates when 
d—\. In this case there is a single “cyclic class,” and after sufficient 
time has elapsed the message can be in the hands of any member at 
any time. 
In particular, it is worth noting that if any member of the equivalence 
class can contact himself directly, then d — 1. This is immediately 
seen from the fact that d is a divisor of any time in which a member 
can contact himself, and here d has to divide 1. 
The number-theoretic result, § 1.4.1, is of such interest that its 
proof will be given here. 
First of all we note that if the greatest common divisor d of the set 
is not 1, then we can divide all elements by d, and reduce the problem 
to the case d= 1. Hence it suffices to treat this case. Here we have 
a set of numbers whose greatest common divisor is 1, and we must 
have a finite subset with this property. Hence, by a well-known 
result, there is a linear combination. aim + a2w2 + • • • + aknk of the 
elements (with positive or negative integers at) which is equal to 1. 
If we collect all the positive and all the negative terms separately, 
and remember that the set is closed under addition, we note that there 
must be elements m and n in the set, such that m — n= 1 (m being the 
sum of the positive terms, and -n the sum of the negative terms). 
Let q be any sufficiently large number, or more precisely q^n(n- 1). 
We can write q = an + b, where a^(n-1) and o^b^(n-1). Then 
we see that q = (a — b)n + bm, and hence q must be in the set. 
§1.5 Probability measures. In making a probability analysis of an 
experiment there are two basic steps. First, a set of logical possibili¬ 
ties is chosen. This problem was discussed in § 1.2. Second a proba¬ 
bility measure is assigned. The way that this second step is carried 
out will be discussed in this section. We consider first a finite possi¬ 
bility space. (For a more detailed discussion see FM Chapter IV or 
FMS Chapter III.) 
1.5.1 Definition. Let U = {ai, a2, /. . , ar} be a set of logical 'possi¬ 
bilities. A probability measure for U is obtained by assigning to each 
element aj a positive number w(a?-), called a weight, in such a way that 
the weights assigned have sum 1. The measure of a subset A of U, 
denoted by m(d), is the sum of the weights assigned to elements of A.

--- Page 20 ---

8 FINITE MARKOV CHAINS Chap. I 
1.5.2 Theorem. A probability measure m assigned to a possibility 
set U has the following properties: 
(1) For any subset P of U, 0 < m(P) ^ 1. 
(2) If P and Q are disjoint subsets of U, then m(P U Q) = m(P)+m(Q). 
(3) For any subsets P and Q of U, m(P U Q) = m(P)+ m(Q)- 
m(P n Q). 
(4) For any set P in U, m(P) = 1 — m(P). 
1.5.3 Definition. Let \)bea statement relative to a set U having truth 
set P. The probability of p relative to the probability measure m 
is defined as m(P). 
In any discussion where there is a fixed probability measure we shall 
refer simply to the probability of p without mentioning each time the 
measure. From Theorem 1.5.2 and the relation of the connectives to 
the set operations, we have the following theorem: 
1.5.4 Theorem. Let U be a set of possibilities for which a probability 
measure has been assigned. The probabilities of statements determined 
by this measure have the following properties: 
(1) For any statement p, 0 ^ Pr[p] ^ 1. 
(2) If p and q are inconsistent then Pr[p V q] = Pr[p] + Pr[q], 
(3) For any two statements p and q, Pr[pV q] = Pr[p] + Pr[q]- 
Pr[p Aq]- 
(4) For any statement p, Pr[~ p] = 1 — Pr[p]. 
1.5.5 Example. Given any finite set having s elements we can 
determine a probability measure by assigning weight l/s to each 
element of U. This measure is called the equiprobable measure. For 
any set A with r elements, m(A) = rjs. For example, this is the mea¬ 
sure which would normally be assigned to the outcomes for the roll of 
a die. In this case U = {1, 2, 3, 4, 5, 6} and a weight of is assigned 
to each. 
1.5.6 Example. As an example of a situation where different 
weights would be assigned consider the following: A man observes a 
race between three horses a, b, and c. He feels that a and b have the 
same chance of winning but that c is twice as likely to win as a. We 
take the possibility set to be U = {a, b, c} and assign weights w(a) = 1/,i 
w(b) = i/4andw(c) = i/2. 7 
It is occasionally necessary to extend the above concepts to include 
the case of an experiment with an infinite sequence of possible outcomes. 
For example, consider the experiment of tossing a coin until the first

--- Page 21 ---

Sec. 6 PREREQUISITES 9 
time that a head turns up. The possible outcomes would be 
U = {1, 2, 3, . . .}. The above definitions and theorems apptyequally 
well to this possibility set. We will have an infinite number of weights 
assigned but we still must require that they have sum 1. In the 
example just mentioned we would assign weights (1/2, 1/a, 1/8, ■ ■ .). 
These weights form a geometric progression having sum 1. 
§ 1.6 Conditional probability. It often happens that a probability 
measure has been assigned to a set U and then we learn that a certain 
statement q relative to U is true. With this new information we change 
the possibility set to the truth set Q of q. We wish to determine a 
probability measure on this new set from our original measure m. We 
do this by requiring that elements of Q should have the same relative 
weights as they had under the original assignment of weights. This 
means that our new weights must be the old weights multiplied by a 
constant to give them sum 1. This constant will be the reciprocal of 
the sum of the weights of all elements in Q, i.e. l/m(Q). (See FM 
Chapter IV or FMS Chapter III.) 
1.6.1 Definition. Let U = {ai, a2, . . . , ar} be a possibility set for 
which a measure has been assigned, determined by weights w(aj). Let 
q be a statement relative to U (not a self-contradiction). The con¬ 
ditional probability measure given q is a probability measure defined 
on Q the truth set of q, determined by weights 
w(aj) 
w(ai) 
m(Q) 
1.6.2 Definition. Let p and q be two statements relative to a set 
U (q not a self-contradiction). The conditional probability of p given q, 
denoted by Pr[p|q] is the probability of p computed from the conditional 
probability measure given q. 
1.6.3 Theorem. Let p and q be two statements relative to U (q not 
a self-contradiction). Assume that a probability measure m has been 
assigned to U. Then 
Pr[p Aq] 
Pr[q] 
where Pr[p Aq] and Pr[q] are found from the measure m. 
1.6.4 Example. In Example 1.5.6 assume that the man learns 
that horse b is not going to run. This causes him to consider the new 
possibility space Q = {a, c}. The new weights which determine the 
conditional measure are w(a) = ——=ljz and w(c) = ——= 2/3. 
1/4+1/2 1/4 + 1/2

--- Page 22 ---

10 FINITE MARKOV CHAINS Chap. I 
We observe that it is still twice as likely that c will win than it is that 
a will win. 
1.6.5 Definition. Two statements p and q (neither of which is 
a self-contradiction) are independent if Pr[p /\q] = Pr[p] ■ Pr[q], 
It follows from Theorem 1.6.3 that p and q are independent if and 
only if Pr[p|q] = Pr[p] and Pr[q|p] = Pr[q], Thus to say that p and q 
are independent is to say that the knowledge that one is true does not 
effect the probability assigned to the other. 
1.6.6 Example. Consider two tosses of a coin. We describe the 
outcomes by U = {HH, HT, TH, TT}. We assign the equiprobable 
measure. Let p be the statement “a head turns up on the first toss” 
and q the statement “a head turns up on the second toss.” Then 
Pr[P Aq] = 1/4, Pr[p] = Pr[q] = 1j2. Thus p and q are independent. 
§ 1.7 Functions on a possibility space. Let U = {ai, a2, . . . , ar} be a 
possibility space. Let f be a function with domain U and range 
R = {ri> r2, . . . , r5}. That is, f assigns to each element U a unique 
element of R. If f assigns rk to aj, we write f(aj) = rk. We write 
f— r& for the statement “the value of the function is r*.” This is a 
statement relative to U, since its truth value is known when the 
outcome aj is known. Hence it has a truth set which is a subset of U. 
(See FMS Chapters II, III, or M* Vol. II, Unit 1.) 
1.7.1 Definition. Let f be a function with domain U and range R. 
Assume that a measure has been assigned to U. For each rk in R 
let w(r*) = Pr[f = r&]. The weights w(r^) determine a probability 
measure on the set R, called the induced measure for f. The weights 
are called the induced weights. 
We shall normally indicate the induced measure by giving both the 
range values and the weights in the form: 
£. f r2) • • • > fsl 
(w(ri), w(r2), ..., w(r,)J 
Thus the induced weight of rk in R is the measure of the truth set 
of f = r^ in U. 
1.7.2 Example. In Example 1.6.6 let f be the function which gives 
the number of heads which turn up. The range of f is R = {0, 1, 2}. 
The Pr[f = 0] = i/4, Pr[f = 1] = i/2, and Pr[f = 2] = i/4. Hence the’range 
and induced measure is : 
f: 0 1 2

--- Page 23 ---

Sec- 7 PREREQUISITES_U 
1.7.3 Definition. Let U be a possibility space, and f and g be two 
functions with domain U, each having as range a set of numbers. The 
function f + g is the function with domain U which assigns to &j the 
number f(a;)+ g(a;). The function fg is the function with domain U 
which assigns to a?- the number f(a^)-g(a;). For any number c the 
constant function c is the function which assigns the number c to every 
element of U. 
Let U be a possibility space for which a measure has been assigned. 
Then if f and g are two numerical functions with domain U, f + g and 
f • g will be functions with domain U, and as such have induced measures. 
In general there is no simple connection between the induced measures 
of these functions and the induced measure for f and g. 
1.7.4 Example. In Example 1.6.6 let g be a function having the 
value 1 if a head turns up on the first toss and 0 otherwise. Let h be 
a function having the value 1 if a head turns up on the second toss 
and 0 if a tail turns up. Then the range and induced measures for 
g, h, g + h, and gh are 
1° ’} V/2 vJ 
g + h: 
gh: 
0 
-V 4 
' 0 
-3/4 
1 2 
V2 1/i 
.;}■ 1.7.5 Definition. Let f be a function defined on U. Let p be a 
statement relative to U having truth set P. Assume that a measure m 
has been assigned to U. Let f' be the function f considered only on the 
set P. Then the induced measure for f' calculated from the conditional 
measure given p is called the conditional induced measure for f 
given p. 
1.7.6 Definition. Let f and g be t wo functions defined on a space U 
for which a probability measure has been assigned. Then f and g are 
independent if, for any rk in the range of f and sj in the range of g, 
the statements f=r^ and g = sj are independent statements. 
An equivalent way to state the condition for independence of two

--- Page 24 ---

FINITE MARKOV CHAINS 12 Chap. I 
functions is to say that the induced measure for one function is not 
changed by the knowledge of the value of the other. 
§ 1.8 Mean and variance of a function. Throughout this section we 
shall assume that the functions considered are functions whose range 
set is a set of numbers. (A detailed discussion of the concepts intro¬ 
duced in this section is given in FMS Chapter III, or M4 Vol. II, Unit 1.) 
1.8.1 Definition. Let f be a function defined on a possibility space 
U = {ai, a2, . . . , ar}, for which a measure determined by weights 
w(%) has been assigned. Then the mean value of f denoted by M[f] is 
M[f] = 2 f(a;)-w(a*). 
j 
The term expected value is often used in place of mean value. 
1.8.2 Theorem. Let f be a function defined on U. Assume that for 
a probability measure m defined on U, the function f has induced 
measure 
Then 
r2, • • • , rs 
w(r2), ..., w(rs) 
M[f] = 2D-w(r,). 
i 
1.8.3 Example. In Example 1.6.6 let f be the number of heads 
which turn up. From the definition of mean value we have 
M[f] = f(HH) • i/4 + f(HT) • 1/4 + f(TH) • 1/4 + f(TT) ■ 1/4 
— 2 • ^4 -f- 1 • ^ + 1. i/4 4- 0 •1/^ 
= 1. 
We can also calculate the mean of f by making use of Theorem 1.8.2. 
The range and induced measure for f is 
0 1 2 
V4 V2 V 4 
Thus by Theorem 1.8.2, 
M[f] = 0-1/4+l-1/2 + 2-1y,4 = 1. 
1.8.4 Definition. Let f be a function defined on a possibility space 
U for which a measure has been assigned. Let M[f] = m be the mean of 
this function. Then the variance of f, denoted by Var[f], is the mean 
of the function (f-m)2. The standard deviation denoted by sd[f], is 
the square root of the variance.

--- Page 25 ---

Sec. 8 PREREQUISITES 13 
1.8.5 Theorem. Let f be a function having mean value m. Then 
Var[f] = M[f2] - m2. 
1.8.6 Example. Let f be the function in Example 1.8.3. We 
found that M[f] = 1. Thus 
Var[f] = (2— 1)2. i/4+ (i _ i)2. i/4 + (i _ i)2. i/4H_ (0_ i)2. i/4 
= X/2- 
An alternative way to compute the variance is to make use of 
Theorem 1.8.5. Using this result we find 
M[f2] = 4.i/4+1-V4+1-1/4 + 0-1/4 = 3/2. 
Since M[f] = 1, we have Var[f] = 3/2 — 1 = 1/2. 
1.8.7 Theorem. Letf and g he any two functions for which means and 
variances have been defined. Then 
(1) M[c] = c. (4) Var[c.f] = c2-Var[f]. 
(2) M[f+g] = M[f] + M[g]. (5) Var[c + f] = Var[f], 
(3) M[c-f] = c-M[f], (6) Var[c] = 0. 
If f and g are independent functions then 
(7) M[f-g] = M[f]-M[g]. 
(8) Var[f+g] = Var[f] + Var[g], 
1.8.8 Definition. Let p be a statement relative to a possibility set U 
for which a measure has been assigned. Let f be a function with 
domain U. The conditional mean and variance of f given p are the 
mean and variance of f computed from the conditional measure given p. 
We denote these by M[f|p] and Var[f|p], 
1.8.9 Theorem. Let pi, p2, . . . , pr be a complete set of alternatives 
relative to a set U. Let f be a function with domain U. Then 
M[f] = 2M[f|p,]-Prfe]. 
j 
1.8.10 Theorem. If f1; f2, . . . is a sequence of functions such that 
for some constant c,

--- Page 26 ---

14 FINITE MARKOV CHAINS Chap. I 
1.8.11 Definition. Let fi and £2 be two functions with M[f$] = aj 
and sd[fi] = 6j. Then the covariance of fi and f2 is defined by 
Cov[fi, f2] = M[(fi — «i)(f2 — a2)], 
and the correlation of fi and f2 is 
Corr[f1; f2] = -OV[fl) fa], 
b\-o2 
§ 1.9 Stochastic processes. In this section we shall briefly describe 
the concept of a stochastic process. A more complete treatment may 
be found in FM Chapter IV or FMS Chapter III. 
We wish to give a probability measure to describe an experiment 
which takes place in stages. The outcome at the n-th stage is allowed 
to depend on the outcomes of the previous stages. It is assumed, 
however, that the probability for each possible outcome at a particular 
stage is known when the outcomes of all previous stages are known. 
From this knowledge we shall construct a possibility space and measure 
for the over-all experiment. 
We shall illustrate the construction of the possibility space and 
measure by a particular example. The general procedure will be clear 
from this. 
1.9.1 Example. We choose at random one of two coins A or B. 
Coin A is a fair coin and coin B has heads on both sides. The coin 
chosen is tossed. If a tail comes up a die is rolled. If a head turns up 
the coin is thrown again. The first stage of the experiment is the 
choice of a coin. At the second stage, a coin is tossed. At the third 
stage a coin is tossed or a die is rolled, depending on the outcome of 
the first two stages. 
We indicate the possible outcomes of the experiment by a tree as 
shown in Figure 1-1. 
The possibilities for the experiment are ti = (A, H, H), t2 = (A, H, T), 
t3 = (A, T, 1), t4 = (A, T, 2), etc. Each possibility may be identified 
with a path through the trees. Each path is made up of line segments 
called branches. In the tree we have just given, there are nine paths 
each having three branches. 
We know the probability for each outcome at a given stage when the 
previous stages are known. For example, if outcome A occurs on the 
first stage and T on the second stage, then the probability of a 1 for 
the third stage is 1/6. We assign these known probabilities to the 
branches and call them branch probabilities. 
We next assign weights to the paths equal to the product of the 
probabilities assigned to the components of the path. For example

--- Page 27 ---

Sec. 9 PREREQUISITES 15 
the path t? corresponds to outcome A on the first stage, T on the second, 
and 5 on the third. The weight assigned to this path is 
1/2'1/2-1/6 = 1/24- 
This procedure assigns a weight to each path of the tree and the sum 
of the weights assigned is 1. The set U of all paths may be considered 
a suitable possibility space for the consideration of any statement 
whose truth value depends on the outcome of the total experiment. 
The measure assigned by the path weights is the appropriate prob¬ 
ability measure. 
t 
ti 
t2 
t3 
U 
t5 
te 
t7 
ts 
tg 
w(t) fl(t) f2(t) fs(t) 
Vs A H H 
Vs A H T 
V24 A T 1 
V24 A T 2 
V 24 A T 3 
V24 A T 4 
V 24 A T 5 
V24 A T 6 
V2 B H H 
The above procedure can be carried out for any experiment that 
takes place in stages. We require only that there be a finite number 
of possible outcomes at each stage and that we know the probabilities 
for any particular outcome at the j-th stage, given the knowledge of 
the outcome for the first j - 1 stages. For each j we obtain a tree Ur. 
The set of paths of this tree serves as a possibility space for any state¬ 
ment relating to the first j experiments. On this tree we assign a 
measure to the set of all paths. We first assign branch probabilities. 
Then the weight assigned to a path is the product of all branch proba¬ 
bilities on the path. The tree measures are consistent in the following 
sense. A statement whose truth value depends only on the first j 
stages may be considered a statement relative to any tree Ut- for i ^ j.

--- Page 28 ---

16 FINITE MARKOV CHAINS Chap. I 
Each of these trees has its own tree measure and the probability of the 
statement could be found from any one of these measures. However, 
in every case the same probability would be assigned. 
Assume that we have a tree for an n stage experiment. Let f) be 
a function with domain the set of paths Uw and value the outcome 
at thej-th stage. Then the functions fi, f2, . . . , fn are called outcome 
functions. The set of functions fi, f2, . . . , in is called a stochastic 
process. (In Markov chain theory it is convenient to denote the first 
outcome by f0 instead of fi.) 
In our example there are three outcome functions. We have indi¬ 
cated in Figure 1-1 the value of each function on each path. 
There is a simple connection between the branch probabilities and 
the outcome functions. The branch probabilities at the first stage are, 
Pr[fi = r<] 
at the second stage 
Pr[f2 = r? | f i = r*] 
at the third stage 
Pr[f3 = r*|f2 = Tj Afi = r*] 
etc. 
In our example, 
Pr[f: = A] = w(ti) + 
Pr[f2 T|fi = A] = 
w(t3)+ • 
w(ti) + 
+ w(t8) = -*-/2 
Pr[f2 = T Afi = A] 
A] 
= Vs 
Pr[fi 
+ W(t8) X/4 
w(t8) 1h 
Pr[f3 l|f2 = T Afi = A] = pVf3 - 1 Af2 - T Afi - A] 
1 J Pr[f2 = TAfi = A] 
w(t3) V 24 
w(t3) + •W(t8) V4 
= 1/ 
1.9.2 Example. We shall often deal with experiments where we 
allow an arbitrary number of stages. For example, in considering 
the tosses of a coin, we can envision any number of tosses. The tree 
for three tosses and the path measure is shown in Fig. 1.2. 
For any number of tosses we can construct a tree. It is even possible 
to consider continuing the tree indefinitely to obtain a tree with 
infinite paths. Our procedure for assigning a measure would not in 
this case be adequate since it would assign weight 0 to every path. 
We shall not, however, have to assign a measure to the infinite tree. 
This is the case because the statements about the process that interest 
us will depend only on a finite part of the tree, and for any finite number

--- Page 29 ---

Sec. 9 PREREQUISITES 17 
of stages we have a method of assigning a measure. We shall, how¬ 
ever, consider functions whose definition requires the infinite tree. 
For example, in Example 1.9.2 let the value of f be the stage at 
which the first head occurs. Then f is defined for all paths with at 
least one head. This is a subset of paths in the infinite tree. WQ shall 
Figure 1-2 
t w(t) 
tl Vs 
t2 V 8 
Q V 8 
t4 Vs 
t5 1ls 
Q Vs 
t7 V.8 
*8 V 8 
speak of the mean value of such a function when the following con¬ 
ditions are satisfied: 
(a) There is a sequence of numerical range values n, r2, . . . such 
that the truth value of the statement f = r; depends only on the 
outcomes of a finite number of stages and 2 Pr[f=r;-] = 1. 
j 
(b) 2 ryPr[f = r;-] < oo. 
j 
In case (a) and (b) hold, we say that f has a mean value given by 
M[f] = 2 UPrLf = Tj] 
j 
When f has a mean a, we shall say that f has a variance if (f-a)2 has 
a mean. If so, Var[f] = M[(f—a)2]. 
All properties of means and variances given in § 8 hold for these

--- Page 30 ---

18 FINITE MARKOV CHAINS Chap. I 
extended mean values. In addition we shall need the following 
theorem. 
1.9,3 Theorem. Let fi, f2, . . . be functions such that the range of 
each ij is a subset of the same finite set of numbers. Let s = fi + f2 + 
• • • . Then if the mean of s exists, 
MW = 2 M[f,]. 
} 
A stochastic process for which the outcome functions all have ranges 
which are subsets of a given finite set is called a finite stochastic -process. 
Thus Theorem 1.9.3 states that in a finite stochastic process the mean 
of the sum of the functions (if this mean exists) is the sum of the means 
of the functions. 
§ 1.10 Summability of sequences and series. It may occur that for a 
divergent sequence so, si, s2, . . . we can form a sequence of averages 
of the terms, and that this new sequence converges. In this case 
we say that the original sequence is summable by means of the 
averaging process. We will be concerned with only two methods of 
averaging. 
n—1 n / \ 
Let tn= (1/n) ^ Si and let un— ^ l^)^”-i(l — k)f8i for some Jc such 
i=0 i=0 'l' 
that 0 <k< 1. Each of these is an average of terms of the sequence, 
with non-negative coefficients whose sum is 1. If the sequence 
ti, f2, . . . converges to a limit t, then we say that the original sequence is 
Cesaro-summable to t. If the sequence u\, u2, . . . converges to u, then 
we say that the original sequence is Euler-summable to u. 
For example, consider the sequence 1, 0, 1, 0, 1, 0, ... . We find 
that tn = ll2 if n is even, l/2+ l/n if n > 1 is odd. This sequence con¬ 
verges to x/2, and hence the original sequence is Cesaro-summable 
to x/2. It is easy to verify that lim%w = 1/2 and hence the original 
n—>co 
sequence is also Euler-summable to 1/2. But the original sequence 
diverges. 
We will need the following two simple facts concerning summa¬ 
bility : (1) If a sequence converges, then it is summable by each method 
to its limit. (2) If a sequence is summable by both methods, the two 
sums must be the same. 
Summability may also be applied to a series. To say that the 
00 
series ^ ak is summable by a given method means that its sequence of 
k—0

--- Page 31 ---

Sec. 11 PREREQUISITES 19 
partial sums St — Z, ak is summable by that method. For example 
k=0 
if we apply Cesaro-summability to the partial 
# n^n-k 
tn~ Z ak- i=0 n 
sums, we obtain 
§ 1.11 Matrices. A matrix is a rectangular array of numbers. An 
r x s matrix has r rows and s columns, a total of rs entries (or com¬ 
ponents). Three special kinds of matrices will be especially important 
in this book. A matrix having the same number of rows as columns 
is called a square matrix. That is, a square matrix is r x r. If r=l, 
that is, the matrix consists of a single row, then we call it a row vector. 
ff ^ I? be. the matrix has a single column, we call it a column vector. 
Matrices will be denoted by capitals and vectors by small Greek 
letters. 
Let the rxs matrix A have components ai}, and the r' xs' matrix B 
have components 6y. Then we define the following operations and 
relations: 
(1) The matrix kA has components kay. That is, a multiplication 
of the matrix by a number means multiplying each component 
by this number. The matrix - A is ( - 1 )A. 
(2) If r = r' and s = s', then the matrix sum A + B has components 
aij + bij. That is, addition is carried out componentwise. 
S 
(3) If s = r', we define the product AB to have components J aahj- 
Note that the product of an rxs and s x t matrix is an r xt 
matrix. This definition also applies to the product of a row 
vector and a matrix, ccA, or to a matrix times a column vector, 
A/3. In the former case the product of a 1 x r and an rxs 
matrix is a 1 x s matrix, or a row vector. If the matrix A is 
square, the resulting row vector has the same number of 
components as a. Thus a square matrix may be thought of 
as a transformation of row vectors. Similarly we can think 
of it as a transformation of column vectors. This will be our 
principal use of the product of a vector and a matrix. 
(4) We say that A ^ B (or that A = B) if ai}- ^ bi} (or atj = bi}) for all 
i and j. That is, matrix relations must hold componentwise—- 
for all corresponding components. 
(5) Some special matrices play an important role. The rxs matrix 
having all components equal to 0 is denoted by 0rXs■ The 
subscripts are omitted whenever there is no danger of con¬ 
fusion. The rxr matrix having l’s as components au (“on

--- Page 32 ---

20 FINITE MARKOV CHAINS Chap. I 
the main diagonal”) and 0’s elsewhere is denoted by Ir. The 
subscript is often omitted. The role that these matrices play 
can be seen as follows. Let A, I, and 0 be rxr, let a be an 
r-component row vector, and ft an r-component column vector. 
Then: 
A + O = O + A = A 
A + {-A) = {-A) + A = 0 
AI — IA = A 
al = a 
I ft =ft 
AO = OA = 0 
Oft =0 
aO — 0. 
Thus the matrices 0 and I play somewhat the same role as the 
numbers 0 and 1. 
(6) In analogy to the reciprocal of a number we define the inverse 
of a matrix. The rxr matrix B is said to be the inverse of the 
rxr matrix A if AB = I. If such an inverse exists, it is denoted 
by A~x. The inverse can be found by solving r2 simultaneous 
equations. Of course, these equations may fail to have a 
solution. But when they do have a solution, the solution is 
unique, and we can show that AA~1 = A~1A =/. 
The various arithmetical operations on matrices, whenever they are 
defined, obey the usual laws of arithmetic. The one major exception 
to this is that matrix multiplication is not commutative, i.e. that 
AB need not equal BA. One important case where matrices commute 
is the case of powers of a given matrix. Let An be A multiplied by 
itself n times. Then An-Am = Am-An for every n and m. We define 
A* = I. 
It is convenient to introduce row vector rjr and the column vector ftr 
having all components equal to 1. The subscript is again omitted 
when possible. These vectors are convenient for summing vectors 
or rows and columns of matrices. The product ag is a number (or 
more precisely a matrix with a single entry) which is the sum of the 
components of a. Similarly for rjft. The product A£ is a column 
vector whose Lth component gives the sum of the components in the 
Lth row of A (or the i-th row sum of A). Similarly r]A gives the 
column sums of A. We shall denote by E a square matrix with all 
entries 1. Note that E = £r).

--- Page 33 ---

Sec. 11 PREREQUISITES 21 
Let us give some examples of these operations and relations. 
Therefore, 
(1,2, 3) + (2, 1,0) = (3, 3, 3) 
For a square matrix A we introduce its transpose AT. The ij-tla 
entry of AT is the ji-th entry of A. We also define the matrix Adg 
which agrees with A on the main diagonal, but is 0 elsewhere. The 
matrix ^4sq is formed from A by squaring each entry. This, of course, 
will not normally be the same as A2. (But D2 = Dsq for a diagonal 
matrix D, i.e. a matrix whose only non-zero entries are on the main 
diagonal.) Similarly we define asq for a vector a. 
It is often convenient to give a matrix or a vector in terms of its 
components. We thus write {an} for the matrix whose ij component

--- Page 34 ---

22 FINITE MARKOV CHAINS Chap. I 
is aij. Similarly we write [af\ for a row-vector, and {a*} for a column 
vector. The following relations will illustrate this notation. 
{aij} + {bij} = {dij + bij}, 
o = {0}, 
ft = E = {1}, 
{aij} sq — 
{aO'}T = {%«}, 
3{«j} = {3cq}, 
KHM = {aibj}. 
The last example shows that the product of a column vector and a 
row vector (each with r components) is a matrix (with r x r components). 
This must be contrasted with the product in the reverse order, which 
is a single component. For example, if a is a row vector, then ag 
gives the sum of its components. However, ga gives an rxr matrix 
with a for each row. 
Suppose that we have a sequence of matrices A&, with entries aW^-. 
We will say that the series A0 + Ai + A% + ■ • • converges if each series 
of entries converges, i.e. if a<°>y + aWi} + a<2>w + • • • converges for every 
i and j. And if the sum of this series of components is aih for each 
i andj', and if A is the matrix with these entries as components, then 
we say that A is the sum of the infinite series of matrices. In brief, 
we define an infinite sum of matrices by forming the sum for each 
component. 
1.11.1 Theorem. If An tends to 0 (zero matrix) as n tends to infinity, 
then (I — A) has an inverse, and 
co 
(/-M)-1 = 1 + A +AZ + M3+ ■ • • = 
k=o 
proof. Consider the identity 
(.I-A)-(I + A+A*+ ■ ■ ■ +A»~ i) = I-A», 
which is easily verified by multiplying out the left side. By hypothesis 
we know that the right side tends to I. This matrix had determinant 1. 
Hence for sufficiently large n, I — An must have a non-zero determinant. 
But the determinant of a product of two matrices is the product of the 
determinants, hence I —A cannot have a zero determinant. The 
determinant not being equal to zero is a sufficient condition for a 
matrix to have an inverse. Hence I —A has an inverse. Since this 
inverse exists, we may multiply both sides of the identity by it: 
I + A+A*+ • • • +4»-i =

--- Page 35 ---

Sec. 11 PREREQUISITES 23 
But the right side of this new identity clearly tends to (/ — A)~l, 
which completes the proof. 
One can define the summability of matrix sequences and series 
exactly as in § 1.10, applying the averaging method to each component 
of the matrix. Then there is a generalization of the previous theorem: 
If the sequence An is summable to 0 by some averaging method, then 
the matrix I —A has an inverse, and the series I + A+A‘l+ ••• is 
summable by the same method to (I — A)~x. 
1.11.2 Definition. A square matrix A is positive semi-definite if 
for any column vector y, yTAy ^ 0. 
1.11.3 Theorem. For any positive semi-definite matrix A there 
is a matrix B such that A — BT B.

--- Page 36 ---

CHAPTER II 
BASIC CONCEPTS OF MARKOV CHAINS 
§ 2.1 Definition of a Markov process and a Markov chain. We 
recall that for a finite stochastic process we have a tree and a tree 
measure and a sequence of outcome functions fw, n — 0, 1, 2, . . . . 
The domain of fn is the tree Tn and the range is the set UM of possible 
outcomes for the n-th experiment. The value of in is sj if the outcome 
of the w-th experiment is S; (see § 1.9). In the following definitions, 
whenever a conditional probability Pr[q|p] occurs, it is assumed that 
p is not logically false. The reader may find it convenient from time 
to time to refer to the summary of basic notations and quantities at 
the end of the book. 
A finite stochastic process is an independent process if 
(I) For any statement p whose truth value depends only on the outcomes 
before the n-th, 
Pr[f„ = s*|p] = Pr[fw = Sj]. 
For such a process the knowledge of the outcome of any preceding 
experiment does not affect our predictions for the next experiment. 
For a Markov process we weaken this to allow the knowledge of the 
immediate past to influence these predictions. 
2.1.1 Definition. A finite Markov process is a finite stochastic 
process such that 
(II) For any statement p whose truth value depends only on the outcomes 
before the n-th. 
Pr[fn = sj|(f„_i = s<) Ap] = Pr[fM = s;-|fK_i = Si]. 
{It is assumed that fn-i = s» and p are consistent.) 
We shall refer to condition II as the Markov property. For a 
Markov process, knowing the outcome of the last experiment we can 
neglect any other information we have about the past in predicting 
the future. It is important to realize that this is the case only if we 
24

--- Page 37 ---

Sec. 1 BASIC CONCEPTS OF MARKOV CHAINS 25 
know exactly the outcome of the last experiment. For example, if 
we know only that the outcome of the last experiment was either s< 
or Sfc then knowledge of the truth value of a statement p relating to 
earlier experiments may affect our future predictions. 
2.1.2 Definition. The n-th step transition probabilities for a 
Markov process, denoted by Pifin) are 
Pijin) = Pr[f» = sj|fw_i = si]. 
2.1.3 Definition. A finite Markov chain is a finite Markov process 
such that the transition probabilities ptfin) do not depend on n. In 
this case they are denoted by ptj. The elements of U are called states. 
2.1.4 Definition. The transition matrix for a Markov chain is the 
matrix P with entries pij. The initial probability vector is the 
vector 770 = {iV0)} = {Pr[f0 = s;]}. 
For a Markov chain ve may visualize a process which moves from 
state to state. It starts in sj with probability p^f. If at any time it 
is in state s*, then it moves on the next “step” to sj with probability py. 
The initial probabilities are thought of as giving the probabilities for 
the various possible starting states. The initial probabilitj^ vector and 
the transition matrix completely determine the Markov chain process, 
since they are sufficient to build the entire tree measure. Thus, given 
any probability vector 770 and any probability matrix P, there is a 
unique Markov chain (except possibly for renaming the states) which 
will have the 770 as initial probability vector and P as transition matrix. 
In most of our discussions we will consider a fixed transition matrix 
P, but we will wish to vary the initial vector 77. The tree measure 
assigned will depend on the initial vector 77 that is chosen. Hence if 
p is any statement relative to the tree, or f is a function with domain 
the tree, Pr[p], M[f], and Var[f] all depend on 77. We indicate this by 
writing Pr„[p], M„[f] and Varw[f]. The special case where 77 has a 1 
in the Tth component (process is started in state s«) is denoted Prj[p], 
Mj[f], Var<[f]. 
We shall give several examples of Markov chains in the next section. 
We conclude this section with a few brief remarks about the Markov 
property. 
It can be easily proved that the Markov property is equivalent to 
the following property more symmetric with respect to time. 
(IF) Let p be any statement whose truth value depends only on outcomes 
after the n-th experiment and q be any statement whose truth value 
depends only on outcomes before the n-th experiment. Then 
Pr[p Aq|fn = Sj] = Pr[p|fw = s;]-Pr[q|fw = sy].

--- Page 38 ---

26 FINITE MARKOV CHAINS Chap. II 
This condition says essentially that, given the present, the past 
and future are independent of each other. This more' symmetric 
definition suggests in turn that a Markov process should remain a 
Markov process if it is observed in reverse order. That the latter is 
true is seen from the following theorem. (We shall not prove this 
theorem.) 
2.1.5 Theorem. Given a Markov process let p be any statement 
whose truth value depends only on experiments after the n-th experiment. 
Then 
Pr[fM = Sj| (f„+i = at) Ap] = Pr[f„ = Sj\in+1 = s,]. 
Since a Markov process observed in reverse order remains a Markov 
process, it might be suspected that the same is true for a Markov 
chain. This would be the case if the “backward transition proba¬ 
bilities, p*ij(n) = Pr[fm = Sj|fw+i = Sj], were independent of n. These 
probabilities may be found as follows : 
p*ii(n) Prl fi^s,- /\f?H-i st-] 
Pr[f«+i = Sj] 
Pr[f«+i= Sj j fn = Sj] • Pr[fw = Sj] 
Pr[f»»+i = Si] 
= Pn • Pr[fw = Sj] 
Pr[f»+i = Si] 
These transition probabilities would be independent of n only if the 
probability of being in a particular state at time w was independent of n. 
This is certainly not the case in general. For example, if the system 
is started in state «i with probability 1, then the probability that it is 
there on the next step is pxl. Thus, in general, Pr[f0 = Sj] A Pr[fx = Sl], 
Thus a Markov chain looked at in reverse order will be a Markov 
process, but in general its transition probabilities will depend on time 
and hence it will not be a Markov chain. We will return to this 
problem in § 5.5. 
§2.2 Examples. In this section we shall give several simple 
examples of Markov chains which will be used in future work for 
illustrative purposes. The first five examples relate to what is normally 
called a random walk.” We imagine a particle which moves in a 
straight line in unit steps. Each step is one unit to the right with 
probability p or one unit to the left with probability q. It moves 
until it reaches one of two extreme points which are called “boundary 
points.” The possibilities for its behavior at these points determine

--- Page 39 ---

Sec. 2 BASIC CONCEPTS OF MARKOV CHAINS 27 
several different kinds of Markov chains. The states are the possible 
positions. We take the case of 5 states, states si and S5 being the 
boundary states, and S2, S3, S4 the “interior states.” 
S1 S2 S3 S4 S5 
EXAMPLE 1 
Assume that if the process reaches state si or S5 it remains there 
from that time on. In this case the transition matrix is given by 
si 
s2 
P = S3 
54 
55 
S2 S3 
0 0 
0 p 
q 0 
0 q 
0 0 
s4 
0 
0 
P 
0 
0 
(1) 
EXAMPLE 2 
Assume now that the particle is “reflected” when it reaches a 
boundary point and returns to the point from which it came. Thus if 
it ever hits Si it goes on the next step back to S2. If it hits S5 it goes 
on the next step back to S4. The matrix of transition probabilities 
becomes in this case 
P 
s2 s3 
1 0 
0 p 
q 0 
0 q 
0 0 
s4 
0 
0 
P 
0 
1 
(2) 
EXAMPLE 3 
As a third possibility we assume that whenever the particle hits 
one of the boundary states, it goes directly to the center state s3. We 
may think of this as the process of Example 1 started at state s3 and

--- Page 40 ---

28 FINITE MARKOV CHAINS Chap. II 
repeated each time the boundary is reached. The transition matrix is 
51 
52 
P = S3 
54 
55 
Si S2 S3 S4 S5 
(0 0 1 
q o P 
0 q 0 
0 0 q 
0 0 1 
(3) 
EXAMPLE 4 
Assume now that once a boundary state is reached the particle 
stays at this state with probability 1/2 and moves to the other boundary 
state with probability 1/2. In this case the transition matrix is 
Sl 
51 
52 
P = S3 
54 
55 
S2 
0 
0 
q 
o 
0 
S3 s4 S5 
0 
V 
0 
q 
0 
(4) 
EXAMPLE 5 
As the final choice for the behavior at the boundary, let us assume 
that when the particle reaches one boundary it moves directly to the 
other. The transition matrix is 
Sl 
s2 
P = S3 
54 
55 
S2 
0 
0 
q 
0 
0 
S3 S4 
0 0 
p 0 
0 p 
q 0 
0 0 
EXAMPLE 6 
(5) 
We next consider a modified version of the random walk. If the 
process is in one of the three interior states, it has equal probability 
of moving right, moving left, or staying in its present state. If it is

--- Page 41 ---

Sec. 2 BASIC CONCEPTS OF MARKOV CHAINS 29 
on the boundary, it cannot stay, but has equal probability of moving 
to any of the four other states. The transition matrix is : 
Si S2 S3 
Si yo 1/4 x/4 
§2 / V3 V3 1/3 
P = S3 I 0 1/3 1/3 
S4 \ 0 0 1/3 
s5 V/4 1/4 1/i 
EXAMPLE 7 
S4 
V4 
0 
Vs 
Vs 
V 4 
(6) 
A sequence of digits is generated at random. We take as states 
the following: Si if a 0 occurs, s2 if a 1 or 2 occurs, s3 if a 3, 4, 5, or 6 
occurs, s4 if a 7 or 8 occurs, S5 if a 9 occurs. This process is an indepen¬ 
dent trials process, but we shall see that Markov chain theory gives 
us information even about this special case. The transition matrix is 
Si S2 S3 S4 S5 
EXAMPLE 8 
According to Finite Mathematics (Chapter V, Section 8), in the Land 
of Oz they never have two nice days in a row. If they have a nice day 
they are just as likely to have snow as rain the next day. If they 
have snow (or rain) they have an even chance of having the same the 
next day. If there is a change from snow or rain, only half of the 
time is this a change to a nice day. We form a three-state Markov 
chain with states R, N, and S for rain, nice, and snow, respectively. 
The transition matrix is then 
R N S 
R lxf% V4 V4\ 
P = N ( 1/2 0 1/2 • (8) 
S V/4 V 4 V* /

--- Page 42 ---

30 FINITE MARKOV CHAINS Chap. II 
In our previous examples the Markov property clearly held. In 
this case it could only be regarded as an approximation since the 
knowledge of the weather the last two days, for example, might lead 
us to different predictions than knowing the weather only on the 
previous day. One way to improve this approximation is to take as 
states the weather for two successive days. The states would then be 
NN, NR, NS, RN, RR, RS, SN, SR, SS. New transition probabilities 
would have to be estimated. A single step would still be one day, so 
that from NR, for example, we could move only to states RN, RR, RS. 
In examples of this kind it is possible to improve the approximation, 
still using the Markov chain theory, but at the expense of increasing 
the number of states. 
example 9 
An urn contains two unpainted balls. At a sequence of times a 
ball is chosen at random, painted either red or black, and put back. 
If the ball was unpainted, the choice of color is made at random. 
If it is painted, its color is changed. We form a Markov chain by 
taking as a state three numbers (x, y, z) where x is the number of 
unpainted balls, y the number of red balls, and 2 the number of black 
balls. The transition matrix is then 
(0,1,1) 
(0,2,0) 
(0,0,2) 
(2,0,0) 
(1,1,0) 
(1,0,1) 
(0,1,1) (0,2,0) 
V2 
0 
0 
0 
V 4 
0 
(0,0,2) 
V2 
0 
0 
0 
0 
V 4 
(2,0,0) 
0 
0 
0 
0 
0 
0 
(1,1,0) 
0 
0 
0 
X/2 
0 
V 2 
(1,0,1) 
(9) 
EXAMPLE 10 
Assume that a student going to a certain college has each year a 
probability p of flunking out, a probability q of having to repeat the 
year, and a probability r of passing on to the next year. We form a 
Maikov chain, taking as states Si—has flunked out, S2—has graduated, 
s3 is a senior, S4 is a junior, S5—is a sophomore, S6—is a freshman.

--- Page 43 ---

Sec. 2 BASIC CONCEPTS OF MARKOV CHAINS 31 
The transition matrix is then 
P = 
51 
52 
53 
54 
55 
56 
Sl S2 S3 
0 0 
1 0 
r q 
0 r 
0 0 
0 0 
s4 s5 
0 0 
0 0 
0 0 
q 0 
r q 
0 r 
s6 
(10) 
EXAMPLE 11 
A man is playing two slot-machines. The first machine pays off 
with probability c, the second with probability d. If he loses, he plays 
the same machine again; if he wins, he switches to the other machine. 
Let S{ be the state of playing the i-th machine. The transition matrix is 
Si s2 
(11) 
As c and d take on all permissible values (0 < c ^ 1, O^d^l) we get all 
2x2 Markov chains. 
example 12 
Consider the special two-state Markov chain (Example 11) with 
transition matrix 
Sl s2 
si Z1/2 Vs\ 
s2 \x/4 3/J 
(11a) 
(This can be called Example 11a.) 
From this Markov chain we form a new Markov chain as follows. 
A state in the new chain will be a pair of states in the old chain. That 
is, the states are SiSi, sis2, s2Si, s2s2. The new chain is in state SjS3- 
on the n-th step if the old chain was in state Si on the n-ih step and Sj 
on the (n+ l)-th step.

--- Page 44 ---

32 FINITE MARKOV CHAINS Chap. II 
The transition matrix for the new chain (Example 12) is 
SlSi S1S2 S2S1 S2S2 
We shall see in § 6.5 that the study of this new chain gives us more 
detailed information about the original process than could be obtained 
directly from the two-state chain. 
§ 2.3 Connection with matrix theory. In this section we shall show 
the connection between Markov chain theory and matrix theory. We 
shall start with the general finite Markov process and then specialize 
our results to the finite Markov chain. 
2.3.1 Theorem. Let fn be the outcome function at time n for a finite 
Markov process with transition probabilities Pij(n), then 
Pr[fw = sv] = = 
U 
proof. The statement f„ = is a statement relative to the tree T„. 
To find its probability, we add the weights of all paths in its truth set. 
That is, all possible paths which end in outcome sv. Thus if j, k, . . . , u 
is a possible sequence of states 
Pr[f» = s„] 
= 2 Prtfo = S; A • • • Afw-1 - su Ain = sj. 
j,k,...,u 
~ 2 Pr[f° = A • ' ' ATi-l = SM] ■ Pr[fM = S„|f0 = S; A • • • Afra-1 = SM]. A/, ... j u 
By the Markov property this is 
2 Pr[f0 Sy A A fra— 1 ~ s%i\puv(n)‘ 
j,k, ... ,u 
If in this last sum we keep u fixed and sum over the remaining indices 
we obtain 
Pr[fw = sv] = 2 Pr[fra-i = su]puv(n). 
U 
This completes the proof. 
We can write the result of this theorem in matrix form. Let nn

--- Page 45 ---

Sec. 3 BASIC CONCEPTS OF MARKOV CHAINS 33 
be a row vector which gives the induced measure for the outcome 
function fn. That is 
?rn = [p(n)l, V(n)2, • • • , P(n)r}, 
where phnf = Pr[fM = s^]. Thus, p^nf is the probability that the process 
will after n steps be in state s;-. The vector -ttq is the initial probabilities 
vector. Let P(n) be the matrix with entries Pu{n). Then the result 
of Theorem 2.3.1 may be written in the form 
TTn = 7Tn-l-P(n) 
for n ^ 1. By successive application of this result we have 
TTn = 7T0 • P(l) • -P(2) • . . . •P(n). 
In the case of a Markov chain process, all the P\n)’s are the same and 
we obtain the following fundamental theorem. 
2.3.2 Theorem. Lei Ttn be the induced measure for the outcome 
function in for a finite Markov chain with initial probability vector ttq 
and transition matrix P. Then 
TTn = tt o • Pn. 
This theorem shows that the key to the study of the induced measures 
for the outcome functions of a finite Markov chain is the study of the 
powers of the transition matrix. The entries of these powers have 
themselves an interesting probabilistic interpretation. To see this, 
Path 
Weights 
V4 
Vs 
Vs 
Vs 
Vs 
V 4 
Figure 2-1

--- Page 46 ---

34 FINITE MARKOV CHAINS Chap. II 
take as initial vector 770 the vector with 1 in the i-th component and 
0 otherwise. Then by Theorem 3.2, nn = 7T0Pn. But 770PW is the Pth 
row of the matrix Pn. Thus the Pth row of the n-th. power of the 
transition matrix gives the probability of being in each of the various 
states under the assumption that the process started in state s*. 
In Example 1, let us assume that the process starts in state S3. 
Then 77"0 — {0, 0, 1, 0, 0}. We can find the induced measures (see § 1.7) 
for the first three outcome functions by constructing a tree and tree 
measure for the first three experiments. This tree is given in Figure 2-1. 
From this tree and tree measure we easily compute the induced 
measures for the functions fi, f2, f3. They are 
77"1 = {0, 1/2j 0, 1/2, 0} 
772 = {l/4> 0, l/a, 0, 1/4 
77"3 = {1/4, 1/4, 0, V4, 1/4}- 
By Theorem 2.3.2 these induced measures should also be the third 
row in the matrices P, P2, and P3, since the starting state was s3. 
These matrices are 
P - 
0 
0 
V« 
0 
0 
0 0 
0 0 
0 
0 
p 2 
V 4 
0 
p3 = 
000 
0 V4 0 
v4 0 1/4 
8 0 !/4 0 
0 0 0 0

--- Page 47 ---

Sec. 4 BASIC CONCEPTS OF MARKOV CHAINS_35 
We thus see that these matrices furnish us several tree measures 
simultaneously. 
§ 2.4 Classification of states and chains. We wish to classify the 
states of a Markov chain according to whether it is possible to go 
from a given state to another given state. This problem is exactly 
like the one treated in § 1.4. If we interpret iTj to mean that the pro¬ 
cess can go from state sto state Sy (not necessarily in one step), then 
all the results of that section are applicable. 
In particular, the states are divided into equivalence classes. Two 
states are in the same equivalence class if they “communicate,” i.e. if 
one can go from either state to the other one. The resulting partial 
ordering shows us the possible directions in which the process can 
proceed. 
The minimal elements of the partial ordering are of particular 
interest. 
2.4.1 Definition. The minimal elements of the partial ordering of 
equivalence classes are called ergodic sets. The remaining elements 
are called transient sets. The elements of a transient set are called 
transient states. The elements of an ergodic set are called ergodic 
{or non-transient) states. 
Since every finite partial ordering must have at least one minimal 
element, there must be at least one ergodic set for every Markov chain. 
However, there need be no transient set. The latter will occur if the 
entire chain consists of a single ergodic set, or if there are several 
ergodic sets which do not communicate with others. 
From the results of § 1.4 we see that if a process leaves a transient 
set it can never return to this set, while if it once enters an ergodic set, 
it can never leave it. In particular, if an ergodic set contains only 
one element, then we have a state which once entered cannot be left. 
Such a state is called absorbing. Since from such a state we cannot 
go to another state, the following theorem characterizes absorbing 
states. 
2.4.2 Theorem. A state s* is absorbing if and only if pa = 1. 
It is convenient to use our classification to arrive at a canonical 
form for the transition matrix. We renumber the states as follows: 
The elements of a given equivalence class will receive consecutive 
numbers. The minimal sets will come first, then sets that are one 
level above the minimal sets, then sets two levels above the minimal 
sets, etc. This will assure us that we can go from a given state to 
another in the same class, or to a state in an earlier class, but not to 
a state in a later class. If the equivalence classes arranged as here

--- Page 48 ---

36_FINITE MARKOV CHAINS Chap. II 
described are ui, U2, . . . , ufc, then our matrix will appear, as follows 
(where 1c is taken as 5, for the sake of illustration): 
Here the P* represent transition matrices within a given equivalence 
class. The region 0 consists entirely of 0’s. The matrix PH will be 
entirely 0 if Pt is an ergodic set, but will have non-zero elements 
otherwise. 
In this form it is easy to see what happens as P is raised to powers. 
Each power will be a matrix of the same form ; in Pn we still have zeros 
in the upper region, and we simply have Pni in the diagonal regions. 
This shows that a given equivalence class can be studied in isolation, 
by treating the submatrix Pt. This will be considered in detail later. 
We can also apply the subdivision of an equivalence class considered 
in the previous chapter. We saw there that each equivalence class 
can be partitioned into cyclic classes. If there is only one cyclic 
class, then we say that the equivalence class is regular, otherwise we 
say that it is cyclic. 
If an equivalence class is regular, then after sufficient time has 
elapsed the process can be in any state of the class, no matter which 
of the equivalent states it started in (see § 1.4). This means that all 
sufficiently high powers of its P, must be positive (i.e. have only 
positive entries). If the equivalence class is cyclic, then no power of 
Pi can be positive. 
From this classification of states we can arrive at a classification 
of Markov chains. We have noted that there must be an ergodic set, 
but there need be no transient set. This will lead to our primary 
subdivision. Within this we can subdivide according to the number 
and type of ergodic sets. 
I. Chains Without Transient Sets 
If such a chain has more than one ergodic set, then there is abso¬ 
lutely no interaction between these sets. Hence we have two or more 
unrelated Markov chains lumped together. These chains may be 
studied separately, and hence without loss of generality we may

--- Page 49 ---

Sec. 4 BASIC CONCEPTS OF MARKOV CHAINS 37 
assume that the entire chain is a single ergodic set. A chain consisting 
of a single ergodic set is called an ergodic chain. 
I-A. The ergodic set is regular. In this case the chain is called a 
regular Markov chain. As we see from previous considerations, all 
sufficiently high powers of P must be positive in this case. Thus no 
matter where the process starts, after sufficient lapse of time it 
could be in any state. 
I- B. The ergodic set is cyclic. In this case the chain is called a 
cyclic Markov chain. Such a chain has a period d, and its states 
are subdivided into d cyclic sets (d>l). For a given starting 
position it will move through the cyclic sets in a definite order, 
returning to the set of the starting state after d steps. We also 
know that after sufficient time has elapsed, the process can be in 
any state of the cyclic set appropriate for the moment. 
II. Chains With Transient Sets 
In such a chain the process moves towards the ergodic sets. As will 
be seen in the next chapter, the probability that the process is in an 
ergodic set tends to 1; and it cannot escape from an ergodic set once it 
enters it. Hence it is fruitful to classify such chains by their ergodic sets. 
II- A. All ergodic sets are unit sets. Such a chain is called an 
absorbing chain. In this case the process is eventually trapped in a 
single (absorbing) state. This type of process can also be character¬ 
ized by the fact that all the ergodic states are absorbing states. 
II-B. All ergodic sets are regular, but not all are unit sets. 
II-C. All ergodic sets are cyclic. 
II-D. There are both cyclic and regular ergodic sets. 
Naturally, in each of these classes we can further classify chains 
according to how many ergodic sets there are. Of particular interest 
is the question whether there are one or more ergodic sets. 
We can illustrate all of these types except II-D by the random walk 
examples. 
For Example 1 : The states Si and s5 are absorbing states. The 
states §2, S3, S4 are transient states. It is possible to go between any 
two of these states. Hence they form a single transient set. We have 
an absorbing Markov chain—that is, case II-A. 
For Example 2: I11 this example it is possible to go from any state 
to any other state. Hence there are no transient states and there is a 
single ergodic set. Thus we have an ergodic chain. It is possible to 
return to a state only in an even number of steps. Thus the period

--- Page 50 ---

38 FINITE MARKOV CHAINS Chap. II 
of the states is 2. The two cyclic sets are {si, s3, S5} and {s2, S4}. 
This is type I-B. 
For Example 3 : Again we can go from any state to any other state. 
Hence we again have an ergodic chain. It is possible to return to 
state s3 from s3 in either two or three steps. Hence the greatest 
common divisor d= 1, and the period is 1. This is type I-A. 
For Example 4 : In this example {si, s5} is an ergodic set which is clearly 
regular. The set {s2, s3, s4} is the single transient set. This is type II-B. 
For Example 5: Here we have a single ergodic set {si, s5} which has 
period 2. The set {S2, s3, s4} is again a transient set. This is type II-C. 
§ 2.5 Problems to be studied. Let us consider our various types of 
chains, and ask what types of problems we would like to answer in the 
following chapters. 
First of all we may wish to study a regular Markov chain. In such 
a chain the process keeps moving through all the states, no matter 
where it starts. Some of the questions of interest are : 
(1) If a chain starts in sf, what is the probability after n steps that 
it will be in s, ? 
(2) Can we predict the average number of times that the process 
is in si ? And if so, how does this depend on where the process starts ? 
(3) We may wish to consider the process as it goes from si to sj. 
What is the mean and variance of the number of steps needed ? What 
are the mean and variance of the number of states passed ? What is 
the probability that the process passes through s* ? 
(4) We may wish to study a certain subset of states, and observe 
the process only when it is in these states. How does this modify 
our previous results ? These questions are treated in Chapter IV. 
Next we may wish to study a cyclic chain. Here the same kinds of 
questions are of interest as for a regular chain. Naturally, a regular 
chain is easier to study; and we will find that, once we have the 
answeis for regular chains, it is not hard to find the corresponding 
answers for all ergodic chains. This extension of regular chain theory 
to the theory of ergodic chains is carried out in Chapter V. 
Next we may wish to consider a Markov chain with transient states. 
There are two kinds of questions to be asked here. One will concern 
the behavior of the chain before it enters an ergodic set, while the other 
kind will apply after the chain has entered an ergodic set. The latter 
questions are no different from the ones considered above. Once a 
chain enters an ergodic set it can never leave it, and hence the existence 
of states outside the set is irrelevant. Thus questions of the second 
kind can be answered by considering a chain consisting of a single 
ergodic set, i.e. an ergodic chain.

--- Page 51 ---

Sec. 5 BASIC CONCEPTS OF MARKOV CHAINS 39 
The really new questions concern the behavior of the chain up to 
the moment that it enters an ergodic set. However, for these questions 
the nature of the ergodic states is irrelevant, and we may make them 
all into absorbing states if we wish. More generally, if we wish to 
study the process while it is in a set of transient states, we may make 
all other states absorbing. This modified process will serve to find all 
the answers we desire. Hence the only new questions concern the 
behavior of an absorbing chain. 
Some of the questions that are of interest concerning a transient 
state Si are: 
(1) The probability of entering a given ergodic set, starting from Sj. 
(2) The mean and variance of the number of times that the process 
is in Si before entering an ergodic set, and how this number depends on 
the starting position. 
(3) The mean and variance of the number of steps needed before 
entering an ergodic set starting at s*. 
(4) The mean number of states passed before entering an ergodic 
set, starting at si. 
Chapter III will deal with absorbing chains, and all these questions 
will be answered. Thus we will find the most interesting questions 
about finite Markov chains answered in Chapters III, IV, and V. 
Exercises for Chapter II 
For § 2.1 
1. Five points are marked on a circle. A process moves from a given 
point to one of its neighbors, with probability ^ for each neighbor. Find 
the transition matrix of the resulting Markov chain. 
2. Three tanks fight a duel. Tank A hits its target with probability 2/3, 
tank B with probability 1/2, and tank C with probability ljz. Shots are fired 
simultaneously, and once a tank is hit it is out of action. As a state we 
choose the set of tanks still in action. If on each step each tank fires at its 
strongest opponent, verify that the following transition matrix is correct: 
E A B C AC BC ABC

--- Page 52 ---

40 FINITE MARKOV CHAINS Chap. II 
3. Modify the transition matrix in the previous exercise, assuming that 
when all tanks are in action, A fires at B, B at C, and C at A. 
4. We carry out a sequence of experiments as follows: At first a fair coin 
is tossed. Then, if experiment n — 1 comes out heads, we toss a fair coin; 
if it comes out tails, we toss a coin which has probability 1 jn of coming up 
heads. What are the transition probabilities? What kind of process is 
this ? 
For § 2.2 
5. Modify Example 1 by assuming that when the process reaches si it 
goes on the next step to state S2. Form the new transition matrix. 
6. Modify the process described in Example 2 by assuming that when the 
process reaches si it stays there for the next two steps and on the third step 
moves to state S2. Show that the resulting process is not a Markov chain 
(with the five given states). 
7. In Exercise 6 show that we can treat the process as a Markov chain, by 
allowing a larger number of states. Write down the transition matrix. 
8. Modify the transition matrix of Example 7, assuming that the digit 0 is 
twice as likely to be generated than any other digit. 
9. Modify Example 7, assuming that the same digit is never generated 
twice in a row, but otherwise digits are equally likely to occur. 
10. In Example 8 allow only two states: Nice and not nice. Show that 
the process is still a Markov chain, and find its transition matrix. 
For § 2.3 
11. In Example 11a compute P2, P4, P8, and Plf5, and write the entries 
as decimal fractions. Note the trend, and interpret your results. 
12. Show that, no matter how Example 7 is started, the probabilities for 
being in each of the states after 1 step agree with the common row for the 
transition matrix. What are the probabilities after n steps? 
13. Assume that Example 8 is started with initial vector tt0 = (2/5,1U, 2/5). 
Find 771, tt2. What is 77 w ? 
14. The weather is nice today in the Land of Oz. What kind of weather 
is most likely to occur day after tomorrow ? 
15. In Example 11, assume that c = 4/2 and d = 4. The man randomly 
chooses the machine to play first. What is the probability that he plays the 
better machine (a) on the second play, (b) on the third play, and (c) on the 
fourthplay? 
16. In Example 2 assume that the process is started in state s3. Construct 
a tree and tree measure for the first three experiments. Use this to find the 
induced measure for the first three outcome functions. Verify that your 
results agree with the probabilities found from P, P2, and P3. 
For § 2.4 
17. For the following Markov chain, give a complete classification of the 
states and put the transition matrix in canonical form.

--- Page 53 ---

Sec. 5 BASIC CONCEPTS OF MARKOV CHAINS 41 
18. *A Markov chain has the following transition matrix, with non-zero 
entries marked by x. Give a complete classification of the states and put the 
transition matrix in canonical form. 
P 
Si S2 S3 s4 S5 S6 S7 S8 
a? X 0 0 0 0 0 0 
a; X X 0 0 0 X 0 
0 0 0 0 0 0 X 0 
0 0 0 X 0 0 0 0 
0 0 0 0 X 0 0 0 
0 0 X 0 0 X 0 0 
0 0 X 0 0 0 0 0 
0 X 0 0 0 X 0 X 
0 0 0 X 0 0 0 0 
S9 
0 
0 
0 
x 
0 
0 
0 
0 
x 
I 
\ 
19. Classify the following chains as ergodic or absorbing. Which of the 
ergodic chains is regular ? 
(a) P = f1/2 
i V 2 
Va' 
Vay (b) P = 
0 
1 
V 3 
(C) P = 
(e) 
/° V 2 V a\ /* 0 0 °\ 
1 0 0 1 f 0 1h X/3 Vs (d) P = 
l1 0 0 lh 1/3 Vs 1 
Vo lh V3 Vs/ 
/° 
1 °\ 
0 
0 A VI* 0/

--- Page 54 ---

42 FINITE MARKOV CHAINS Chap. II 
20. In Example 9 classify the states. Put the transition matrix in 
canonical form. What type of chain is this ? 
21. For an ergodic chain the i-th state is made absorbing by replacing the 
i-th row in the transition matrix by a row with a 1 in the i-th component. 
Prove that the resulting chain is absorbing. 
22. In Example 11, give conditions on c and d so that the resulting chain 
is 
(a) ergodic (b) regular (c) cyclic (d) absorbing 
For the entire chapter 
23. In a certain state a voter is allowed to change his party affiliation (for 
primary elections) only by abstaining from the primary for one year. Let 
si indicate that a man votes Democratic, s2 that he votes Republican,-and s3 
that he abstains, in the given year. Experience shows that a Democrat will 
abstain 1j2 the time in the following primary, a Republican will abstain 1 /4 
time, while a voter who abstained for a year is equally likely to vote for 
either party in the next election. [We will refer to this as Example 13.] 
(a) Find the transition matrix. 
(b) Find the probability that a man who votes Democratic this year will 
abstain three years from now. 
(c) Classify the states. 
(d) In a gi ven year */4 of the population votes Democratic, !/2 Republican, 
the rest abstain. What proportions do you expect in the next primary 
election ? 
24. A sequence of experiments is performed, in each of which two fair coins 
are tossed. Let si indicate that two heads come up, s2 that a head and a tail 
come up, and s3 that two tails turn up. [We will refer to this as Example 14.] 
(a) Find the transition matrix. 
(b) If two heads turn up on a given toss, what is the probability of two 
heads turning up three tosses later ? 
(c) Classify the states.

--- Page 55 ---

CHAPTER III 
ABSORBING MARKOV CHAINS 
§ 3.1 Introduction. Let us recall the basic definitions relevant to 
an absorbing chain. In the classification of states, the equivalence 
classes were divided into transient and ergodic sets. The former, once 
left, are never again entered; while the latter, once entered, are never 
again left. If a state is the only element of an ergodic set, then it is 
called an absorbing state. For such a state s* the entry pa must be 1, 
and hence all other entries in this row of the transition matrix are 0. 
A chain, all of whose non-transient states are absorbing, is called an 
absorbing chain. These chains will occupy us in the present chapter. 
3.1.1 Theorem. In any finite Markov chain, no matter where the 
process starts, the probability after n steps that the process is in an ergodic 
state tends to 1 as n tends to infinity. 
proof. If the process once reaches an ergodic state, then it can 
never leave its equivalence class, and hence it will at all future steps be 
in an ergodic state. Suppose that it starts in a transient state. Its 
equi valence class is not minimal; hence there is a minimal element 
below it. This means that it must be possible to reach some ergodic set. 
Let us suppose that from any transient state it is possible to reach an 
ergodic state in not more than n steps. (Since there are only a finite num¬ 
ber of states, n is simply the maximum of the number of steps required 
from each state.) Hence there is a positive number p such that the prob¬ 
ability of entering an ergodic state in at most n steps is at least p, from 
any transient state. Hence the probability of not reaching an ergodic 
state in n steps is at most (1 — p), which is less than 1. The probability 
of not reaching an ergodic state in kn steps islessthan orequal to (1 — p)k, 
and this probability tends to 0 as k increases. Hence the theorem follows. 
3.1.2 Corollary. There are numbers b> 0, 0<c<l such that 
p(n)ii ^b-cn, for any transient states s*, sj. 
This is a direct consequence of the above proof. It shows the rate 
at which p^fj tends to 0. 
43

--- Page 56 ---

44 FINITE MARKOV CHAINS Chap. Ill 
It is convenient to consider the canonical form of the matrix P in an 
aggregated version. We unite all the ergodic sets, and all the tran¬ 
sient sets. (Let us say that there are s transient states, and r — s 
ergodic states.) The form then becomes 
r — s s 
Here again the region 0 consists entirely of 0’s. The s x s submatrix Q 
concerns the process as long as it stays in transient states, the 
s x (r — s) matrix E concerns the transition from transient to ergodic 
states, and the (r — s) x (r — s) matrix deals with the process after it has 
reached an ergodic set. From Theorem 3.1.1 we see that the powers 
of Q tend to 0. Hence as we raise P to higher and higher powers, the 
matrices approach a matrix whose last s columns are all 0. This is the 
matrix version of Theorem 3.1.1. 
Let us now consider an absorbing chain. By its definition we see 
that S is I(r—s)x(r—s)} i.e. an identity matrix of the appropriate dimension. 
Thus its canonical form is 
r — s s 
And by the nature of the powers of P we know that the region 1 remains 
I. This corresponds to the fact that once an absorbing state is entered, 
it cannot be left. From Theorem 3.1.1 we know that the probability 
that such a state is entered, in an absorbing chain, tends to 1. Hence 
we may say that with probability 1 the chain will enter an absorbing 
state and stay there, i.e. that it will be “absorbed.” 
Let us write some of our examples from Chapter II, § 2.2, in the new 
canonical form. In Example 1 the states Si and S5 are absorbing, 
hence these must be written first. We thus have 
P = 
Si ss S2 S3 S4 
1 0 0 0 0 
0 1 0 0 0 
q 0 0 p 0 
0 0 q 0 p 
0 p 0 q 0 
where the regions I, 0, R, and Q have been marked off.

--- Page 57 ---

Sec. 2 ABSORBING MARKOV CHAINS 45 
The matrix for Example 10 is already in canonical form in § 2.2. 
The first two states are absorbing. Hence R is 4 x 2 and Q is 4 x 4 in 
this example. 
Example 9 is not an absorbing chain. It has a single ergodic set, 
consisting of the first three states. The matrix appears in canonical 
form in § 2.2. If we want to study this process only until it enters the 
ergodic set, then we may make the ergodic states absorbing. The 
resulting transition matrix is 
P = 
1 0 0 0 0 0 
0 i 0 0 0 0 
0 0 1 0 0 0 
0 0 0 0 Vs Vs 
X/4 V 4 0 0 0 Vs 
V 4 0 V 4 0 Vs 0 
If we do not even care at which state the ergodic set is entered, we may 
lump the three ergodic states into a single one, obtaining the much 
simpler matrix 
P = 
1 0 0 0 
0 0 Vs Vs 
Vs 0 0 Vs 
Vs 0 Vs 0 
The former matrix preserves Q and R, while it modifies S; the latter 
preserves only Q. This is in good agreement with the interpretation 
given for Q, R, and S earlier in this section. 
It should now be clear that absorbing chains serve to answer all 
questions of the second type (concerning transient states) raised in 
§ 2.5. But absorbing chains are also important in the study of an 
ergodic set. Suppose that we wish to ask a question about what happens 
as the process goes from s< to sj Then we may wish to “stop” the 
process as soon as it reaches sj, which we accomplish by making sj 
an absorbing state. And since sj can be reached from all states of its 
equivalence class, the resulting chain will be an absorbing Markov 
chain. This trick will be developed in § 6.1. 
§ 3.2 The fundamental matrix. The following basic theorem is a 
direct consequence of the matrix theorem we proved in § 1.11.1, if we 
recall that Qk tends to 0.

--- Page 58 ---

46 FINITE MARKOV CHAINS Chap. Ill 
3.2.1 Theorem. For any absorbing Markov chain, I — Q has an 
inverse, and 
00 
(I-Q)-1 = l+Q+Q2+ ••• = 2*2* 
k = 0 
3.2.2 Definition. For an absorbing Markov chain we define the 
fundamental matrix to be N = (I — Q)-1. 
3.2.3 Definition. We define n-,- to be the function giving the total 
number of times that the process is in Sj. (This is defined only for 
transient state s,.) ukj is defined as the function that is 1 if the process 
is in state Sj after k steps, and is 0 otherwise. (See §§1.7 and 1.8 for 
the notation used in this section.) 
We will now give a probabilistic interpretation to N. We let T be 
the set of transient states. 
3.2.4 Theorem. {Mf[n^]} = JV, where s*, sj e T. 
proof. It is easily seen that ip = 2 u*y. 
Hence 
* = 0 
mw)} = m. 
2u* -k= 0 
= { f M,[u»;] 
U=o 
2 ((i-^j-o+^d) ;= 0 
2 k = 0 
co 
= 2 Qk since si, sj are transient 
* = u 
= N by 3.2.1, 3.2.2. 
This completes the proof. 
This theorem establishes the fact that the mean of the total number 
of times the process is in a given transient state is always finite, and 
that these means are simply given by N. 
There is an interesting alternative proof for this result. To compute 
Mi[n7-], we may add up the original position’s contribution, plus each 
of the steps’ contribution. The original position contributes 1 if and 
only if i =j. It is convenient to define dtJ, the constant function that is 
1 if i=j, 0 otherwise. Then we can say that the original position

--- Page 59 ---

Sec. 2 ABSORBING MARKOV CHAINS 47 
contributes dy. After one step we move to s* with probability p^. 
If the new state is absorbing, it contributes nothing to our mean, but 
if it is transient, then it contributes M*[n^]. Hence we have 
Hence 
s*eT 
(Mf[n?-]} = I+ Q • (Mifny]}. 
{Mi[n,-]} = (I-Q)-i = N. 
We will apply these results to the examples of the last section. In 
the random walk, Example 1 of § 2.2, 
and hence 
S2 
N = (I — Q)-1 = s3 
S4 
S2 S3 s4 
,p+q2 p p2 
p2 + q2 p2 + q2 p2 + q2 
q 1 V 
p2 + q2 p2 + q2 p2 + q2 
q2 q q + p2 
p2 + q2 p2 + q2 p2 + q2' 
[Since p + g = 1, and hence (p + q)2=l, we have that 1 — 2pq = p2 + q2.] 
We see that, for example, if the process starts in S3 (the middle state), 
then it will be in the middle state an average of 1 l(p2 + q2) times. This 
quantity is always between 1 and 2. The minimum of 1 is achieved if 
p = 0 or 1, the maximum of 2 if p = 1/2. In the former case the process 
starts at S3 and goes directly to one of the boundaries, hence it will be 
in state s3 only at the beginning. But even in the case p=1l2 we 
expect the process to return only once on the average. 
3.2.5 Example la. As an illustration we give the fundamental 
matrix for the case p = 2jz, i.e. when it is twice as likely to move to the 
right as to the left. 
S2 S3 s4 
s2 r15 6/5 
V5\ 
CO 
Tfl 
II 
1 3/5 9/5 6/5 
s4 VI* 3/5 7/5/

--- Page 60 ---

48 FINITE MARKOV CHAINS Chap. Ill 
In the college example, Example 10 of § 2.2, remembering that 
p + q + r— 1, we have 
(I-Q) = 
— r 
0 
p + r 
— r 
0 
0 
0 
p + r 
— r 
0 
0 
0 
p + r 
\ 
J 
S3 
s4 
N = (I-Q)-' = 
S5 
S6 
S4 
0 
1 
p + r 
r 
(p + r)2 
2 
(p + r)3 
s5 
0 
0 
1 
p + r 
r 
(p + r)2 
s6 
SENIOR 
JUNIOR 
SOPHOMORE 
FRESHMAN 
The zeros in N indicate that no one is demoted in the college. Thus, 
for example, a junior cannot spend any time as a sophomore or fresh¬ 
man in the future. As an illustration we compute N (approximately) 
for the case we will call Example 10a, where the probabilities of flunking 
out, repeating, and being promoted arep = .2, q — .1, r= .7, respectively. 
1.11 0 0 
° \ 
SENIOR 
.86 1.11 0 0 ’ JUNIOR 
.67 .86 1.11 
° 1 
SOPHOMORE 
.52 .67 .86 1.11/ FRESHMAN 
In the urn example, Example 9 of § 2.2, we have 
/I -i/2 -i/2N 
(I-Q) = I 0 1 -i/2 j 
\0 -Va 1/ 
s4 fl 1 1 
- (I-Q)-' = S5 j 0 4/3 2/3 
S6 \0 2/3 4/3/ 
(2,0,0) 
(1,1,0). 
(1,0,1)

--- Page 61 ---

Sec. 3 ABSORBING MARKOV CHAINS_49 
If the process reaches (1,1,0) or (1,0,1), then from then on it is expected 
to be in that state 4/3 times, and in the other state 2/3 times. (The 4/3 
includes the original position.) From neither of these states can the 
state (2,0,0) be reached, since a painted ball always remains painted. 
If the process starts in (2,0,0), which is its natural starting position, it 
will be in this position only once. It is expected to be in each of the 
other two states once, which is the average of 4/3 and 2/3. 
These fundamental matrices will be used throughout this chapter 
for illustrations. 
§ 3.3 Applications of the fundamental matrix. We will show that a 
number of interesting quantities can be expressed in terms of the 
fundamental matrix. These results will here be illustrated in terms of 
the random walk Example la (see § 3.2.5), and all the absorbing chains 
will be worked out in the next section. 
3.3.1 Definition. We define the following new matrices and vectors: 
N2 = N(2Nds — I) — 2Vsq 
B = NR 
r = 
T2 = (22V — I)t — Tgq 
s x s matrix 
s x (r — s) matrix 
s component column vector 
s component column vector 
3.3.2 Theorem. QN = NQ = N — I. 
proof. From 3.2.1, 3.2.2, 
N = I + Q + Q 2+ .... 
Hence, 
QN = NQ = Q + Q2 + Q3 + 
which is the original series without I. 
3.3.3 Theorem. {Varj[n,]} = 2V2, where e T. 
proof. We recall that Var^n^] = Mj[n2j] — Mj[n;-]2. From Theorem 
3.2.4 we see that 
{M,[n,p} = Nk, 
hence we need only show that 
{M*[n2,-]} = 2V(22Vdg —/). 
We will assume that these means are finite. A proof of this fact will 
be given at the end of this section. To compute these means we again 
ask where the process can go in one step, from its starting position Sj. 
It can go to s* with probability pik. If the new state is absorbing, then 
we can never reach again, and the only possible contribution is from 
the initial state, which is dy. If the new state is transient, we will be

--- Page 62 ---

50 FINITE MARKOV CHAINS Chap. Ill 
in Sj dij times from the original position, and nj times from the later steps. 
Hence, remembering that dy is a constant function and dy = d2^, 
{Mi[n2d} = / 2 PncA2ij+ ^ ^M*[(n; + d0)2]l 
UsT SiET / 
_ / 2 + 2M*[ni] • dw) + dy\ 
UsT f 
= §{Mj[n2^]} + 2(QN)dg +1. 
Hence 
{Mj[n2j]} =(I-Q)-H2(QN)ig + I) 
= N(2(N-I)ig + I) = i\^(2iVdg —/). 
The matrix «?iV)dg appeared above since the factor dy has the effect 
of setting all elements off the main diagonal equal to 0. 
In our Example la, we have already computed N, and we now 
compute A^2 as well. 
N = 
Ndg = 
NS(l = 
2N^-I= 
3/25 78/25 36/25 
N(2Ndg-I) = 117/25 54/25 125 “'/25 
9/„„ 39 /„„ 63 7 25 39/25 63/25/ S4 
'25 36/25 16/25\ 
'25 81/25 33 j 25 
25 *7 25 49/m/ 
75 0 
°\ 
0 13/5 
° 
0 0 9lJ 
s2 s3 S4 
V25 42/25 2°/25\ 
8/25 33 / 25 18/25 
3j 25 30/25 14/25/ 
Thus we see that for any state as initial state the variance is largest for 
the middle state. We also note that N2 is quite large compared to 
Asq; hence the means are fairly unreliable estimates for this Markov 
chain. This will often be the case. 
3.3.4 Definition. Let t be the function giving the number of steps 
(■including the original position) in which the process is in a transient 
state. 
If the process starts in an ergodic state, then t = 0. If the process 
starts m a transient state, then t gives the total number of steps needed 
to reach an ergodic set. In an absorbing chain this is the time to 
absorption.

--- Page 63 ---

Sec. 3 ABSORBING MARKOV CHAINS 51 
3.3.5 Theorem. {M«[t]} = r; {Var,[t]} = T2, where Si e T. 
proof. It is easily seen that t = ^ ni- 
Sj e T 
Hence 
- |2t 
= N{, 
since this gives the row sums of N. 
For the variance we carry out an argument similar to that in §3.3.3, 
but here the first step always counts. 
/ 
Hence 
Thus 
= / 2 ^(M*[t2] + 2M/fc[t]) + l 
\S(t6T 
- Q{Mi[t2]} + 2Qr + i. 
{M^t2]} - (I-Q)~H2Qr + i) 
= 2NQt + N£ 
— 2(N — I)t + t 
= (2 N-I)t. 
(Vari[t]} = {Mi[t2] — Mi[t]2} = (2N-I)r-rsq. 
In our example, 
(2N-I) = «/5 18/6 
(2N-I)t =

--- Page 64 ---

52 FINITE MARKOV CHAINS Chap. Ill 
We see that one expects to reach the boundary most quickly from 
S4. This is not surprising, since it is easier to reach the boundary from 
an outside state than from the middle, and it is more probable that the 
process moves to the right. But we again note that the variance is 
sizable. 
We have computed means only for measures in which the process 
starts in a given state s«. But it is easy to obtain from this the means 
and variances for an arbitrary initial probabilitjr vector. 
3.3.6 Corollary. If -n is the initial probability vector for an 
absorbing chain, and v' consists of the last s components of v, i.e. 7t' 
gives the initial probabilities for the transient states, then 
{Mw[ny]} = tt'N 
{Var„[ny]} = 7r'N(2Nag-I) - (tt'N)s(1 
{M„[t]} =7T'T 
{Var„[t]} = — I)t— 
proof. This is an immediate consequence of the fact that for any 
function f, M„[f] = 77Mj[f], which follows from the nature of the tree 
measure. The right sides contain 77 rather than 77, since the various 
means are 0 if the initial state is absorbing. 
Our remaining applications will concern the question of which 
absorbing state is likely to capture the process. 
3.3.7 Theorem. If bi} is the probability that the process starting in 
transient state s* ends up in absorbing state sy, then 
{bij} = B = NB, Si e T, sy e T. 
proof. Starting in s*, the process may be captured in sy in one or 
more steps. The probability of capture on a single step is ptj. If this 
does not happen, the process may move either to another absorbing 
state (in which case it is impossible to reach sy), or to a transient state 
Sk- In the latter case there is probability bkj of being captured in the 
right state. Hence we have 
btj = Pij+ 2 Plkb/cj> 
skeT 
which can be written in matrix form as 
Thus 
B = B + QB. 
B = (I-Q)~iR - NB. 
An alternative proof is based on the following observation: Every

--- Page 65 ---

Sec. 3 ABSORBING MARKOV CHAINS 53 
time that the process is in transient state s*, it has probability pkj of 
going to sj. Hence it is possible to show that 
bU = 2 'Pkj- 
steT 
This gives directly that 
B = NR. 
In our example 
Si s5 
Z1/3 0 \ s2 /7/l5 8/l5\ 
B = 0 0 B = NR = S3 I l/5 4/5 . 
' 0 W «4 \Vl5 14/l5/ 
It is worth noting that for each starting state the sum of the two 
absorption probabilities is 1. By Theorem 3.1.1 it will always be true 
that NRgrs = ^r-s. It is also easy to verify this directly. 
The further to the right we start, the more probable it is, of course, 
that the process will end up at the right end. It is interesting to see 
that even in the leftmost transient state the probability is somewhat 
greater for capture on the right. 
3.3.8 Corollary. If pa is the a-th column of R, i.e. pa=pia for 
Si in T and for fixed a, then Npa gives the probabilities of absorption in 
the given absorbing state sa, for any transient state as initial state. 
This corollary is useful if we are interested in a single absorbing state. 
3.3.9 Theorem. If B* is the r x r matrix whose entry b*ij gives the 
probability of being absorbed in sj, starting in Si, for all states sj and Sj, 
then 
PB* = B*. 
proof. If Sj e T, then b*ij = 0. Hence the last s columns of B* are 0. 
Consider Sj absorbing. If s< e T, then b*tJ = btf, as in § 3.3.7. If s« is 
also absorbing, then b*i} = dij. Hence we have 
PB* I1 0 
\ R Q B 
But R + QB = R + QNR = R+(N-I)R = NR=B. 
i 0 
R + QB 0 
--B.

--- Page 66 ---

54_FINITE MARKOV CHAINS Chap. Ill 
Hence PB* — B*. 
We thus see that the r-component column vector giving the proba¬ 
bilities of absorption in an absorbing state sj is a fixed vector of P, and 
its first r-s components are 0, except the j-th, which is 1. This deter¬ 
mines the vector. This method of finding the absorption probabilities 
is useful if we are not interested in finding N. 
In our example it is easily verified that 
are fixed vectors of P. 
We will now supply the missing step for Theorem 3.3.3. 
3.3.10 Theorem. Mj[n2^] is finite for any absorbing chain, and any 
ssj e T. 
PROOF. Mi[n2;-] = M, 
= M 
co 
2 u*4 U = o 
to oo 
2 2 k = 0 i = 0 
oo oo 
= 22 Mf[u*M]. k = 0 1 = 0 
M<[u*,uy is the probability that the process is in s, both on step k and 
on l, starting in s*. If we let ra = min(fc, l), d= \k-l\, then this is the 
probability of being in s} after m steps, and of returning d steps later. 
Hence M^u^u^] =p(m)i.p(d).j% 
CO 00 
Mj[n2j] =22 P{m)uPwjj 
k = 0 1 = 0 
oo oo 
<22 (b-c™)(b-cd) 
k = 0 1 = 0 
00 00 
= 62 2 2 c” where n = max (k, l) 
k = 0 / = 0 
00 
= 62 2 (2ti + 1 )cn, which is finite. 
n = 0

--- Page 67 ---

Sec. 4 ABSORBING MARKOV CHAINS 55 
§ 3.4 Examples 
Example 3.4.1 (Example 1 of § 2.2 continued). In the random 
walk we find: 
N = 
p2 + q2 
'p + q2 p p2 
q 1 p 
q2 q q+ps 
N» = pq 
T = 
1 
p2 + q2 
fp + q2 1 + 2p p+p^ 
1 2 1 
\q + q2 1 + 2 q q+p2, 
'l + 2p2\ /1 + 2p\ 
(p2 + q2)2 
B = 
2 
1 + 2q2 ‘ 
1 
T2 = 4pq 
(p2 + q2)2 
fpq + q3 p3 
p2 + q2 
In particular, ifp = i/2 (Example lb), then 
qi pi 
qz pq + pzj 
'312 1 V 2’ 
iV = 
X/2 1 3/2/ 
/3/4 2 »/4 
IV2 = [ 1 2 1 
\3/4 2 3/4i 
And ifp=l (Example lc), then 
5 
and the variances are all 0. 
This last case is easily interpreted if we remember that the process 
in this case must move to the right.

--- Page 68 ---

56 FINITE MARKOV CHAINS Chap. Ill 
Example 3.4.2 (Example 10 of § 2.2 continued). In the college 
process we have, letting t = p + r 
N = 
No = 
(p + r)‘‘ 
qt + t — t2 
qt2 + t2-t4 
\qt3 + t3 — t+ 
0 
0 
(1 
t T = 
P 
qt2 + t2-t4 qt + t-t' 
1 -t 
1 
1 
\l —i 
1 \ T2 = 
p(p + r) 
q(l-t) 
q(l-t2) + t-2t2 + t3 
q(l-t3) + t + t2-U3 + til + t5 
\q{\-t*) + t + t2 + t3-M* + t3 + t* + ty 
('-' ' \ 
]> -- 
\-t3 t3 
\l-£4 «V 
The piobability of graduating from each class depends only on the 
r 
ratl° 1 = This ratio is the conditional probability that the man 
is promoted rather than flunked out, given that he leaves his present 
class. Having successive powers of this ratio can be interpreted as 
saying that each time he leaves his class he must be promoted rather 
than flunked out, but it does not matter how long he stays in his present 
class. The formulas simplify greatly if we eliminate the possibility of 
having a man repeat the class, that is if q = 0. In that case, 
r t = - = r, and p + r

--- Page 69 ---

Sec. 4 ABSORBING MARKOV CHAINS 
N = 
0 
1 
r 
r2 
0 
0 
1 
r 
T = r 2 = pr 
1 + 3 r + r2 
> 1 + Sr + 6 r2 + 3rz + r4/ 
B is unchanged. 
In the numerical Example 10a (cf. § 3.2.5) we have: 
N = 
N2 
/Ill 0 0 
.86 1.11 0 
.67 , 
.86 1 11 
\ .52 .67 86 
/.!2 0 0 
.31 .12 0 
.37 .31 .12 
\.37 .37 .31 
° \ 0 
° / 1.11/ 
°\ 0 
0 
J 
T = 
FLUNK 
OUT 
(.22 
.40 
.53 
.63 
GRADUATE 
.78 
.60 
\ 
■47/ 
.37/ 
SENIOR 
JUNIOR 
SOPHOMORE 
FRESHMAN 
Thus a student must reach the junior year before he has a better than 
even chance of graduating.

--- Page 70 ---

58 FINITE MARKOV CHAINS Chap. Ill 
Example 3.4.3 (Example 9 of § 2.2). In the urn example the five 
vectors and matrices are : 
N = N2 = 
'2\ 
T2 = I 2 
*/» 
4/s 
2/3 
2/3 ' 
2/3 
4/9, 
Si S2 s3 
s4 /V 2 V4 
V4\ B = s5 J V* V3 
V/ 
S6 W2 Ve w 
Since the process must leave s4 immediately and cannot return, there is 
0 variance for the number of times in this state. Of the remaining 
variances the diagonal elements are smallest—this is due to the 
stabilizing effect of having to count the original position. 
The B matrix needs special interpretation in this case. Since the 
states si, s2, and S3 were not absorbing in the original process, the 
“absorption probabilities” must be interpreted as probabilities of 
entering the ergodic set at the given state. Thus, for example, if the 
process starts with both balls unpainted (state s4), then there is pro¬ 
bability x/2 that the first time both balls are painted there will be one 
of each color, J/4 that they will both be red, and i/4 that they will both 
be black. It should be noted that these probabilities are the same 
as if we had assigned the two balls colors independently and at 
random. 
§ 3.5 Extension of results. We will see that results obtained in 
§ 3.3 can be applied to a wider variety of problems. 
3.5.1 Definition. A set S of states is an open set if from every state 
in S it is possible to go to a state in S. 
It is easy to think of examples of open sets : A set consisting of a single 
state is open (unless the state is absorbing), so is a set of transient 
states, so is a proper subset of an ergodic set, etc. The following 
theorem characterizes these sets.

--- Page 71 ---

Sec. 5 ABSORBING MARKOV CHAINS 59 
3.5.2 Theorem. A set S of states is open if and only if no ergodic set 
is a subset of S. 
proof. If an ergodic set is contained in S, then there is no escape 
from this set once it is entered; hence S is not open. 
On the other hand we know that from every state we can reach an 
ergodic state. And from an ergodic state we can reach all the elements 
of its ergodic set. Hence if there is no ergodic set contained in S, 
then for every element of S we can find an ergodic state in S which can 
be reached from the given state. Hence S is open. 
3.5.3 Theorem. If S is an open set of states, and all the states in S 
are made absorbing states, then the resulting Markov chain is absorbing, 
and its transient states are the elements of S. 
proof. Since S is open, from every state of it we can reach a state 
S which must be an absorbing state. Hence the chain is absorbing. 
And since from each element of S we can reach an absorbing state, the 
elements of S must all be transient states in the new process. 
3.5.4 Theorem. Let S be an open set of s states. Let Q be the 
sxs submatrix of P corresponding to these states. Let pa be the s- 
component column vector with components pia, where the s« are the 
elements of S and sffl £ S. Let the process start in S{. Then: 
(1) The ij-component of N = (I — Q)~1 is the mean number of times 
the process is in s}- before leaving S. 
(2) The ij-components of N2 = N(2Nag-I)-Nsq is the variance of 
the same function. 
(3) The i-th component of t = is the mean number of steps needed 
to leave S. 
(4) The i-th component of t2 = (2N- — I)t — TSq is the variance of the 
same function. 
(5) The i-th component of Npa is the probability that the process goes 
to sa when it leaves S. 
proof. The various parts of this theorem are a direct consequence 
of the corresponding results in § 3.3, due to Theorem 3.5.3. 
As an application of this theorem consider the following problem. 
Let Sj and s/c be any two states in a regular Markov chain. Assume 
that the process is started at a third state. What is the probability 
of reaching s/c before Sj ? This probability may be found from 
5.4(5) by choosing S to be the set of all states in the ergodic set except 
sj and s/c.

--- Page 72 ---

60 FINITE MARKOV CHAINS Chap. Ill 
3.5.5 Example. Consider the random walk Example 6_of Chapter 
II. The transition matrix is 
P = 
V3 
V 3 
V 4 
and since from any state we can move to any other state in two steps, 
the Markov chain is regular. Hence any proper subset of the states is 
open. Let S consist of the last three states. 
Q = S4 V 3 Vs V 
The N matrix tells us the mean number of times that the process is in 
each of the last three states, before it goes to one of the first two states. 
We see that the numbers are small if the process starts in the last state. 
But this is intuitively clear, since in this case it has a x/2 probability 
of “escaping” on the first step. For the same reason, the mean number 
of times that it is in the last state is small, no matter where the process 
starts. However, from N2 we see that the former numbers have much 
greater variances than the latter.

--- Page 73 ---

Sec. 5 ABSORBING MARKOV CHAINS 61 
From r we see that it takes longest to escape from 54, which has no 
connection to S. Indeed, the differences in mean number of steps to 
escape can be accounted for by the number of connections the three 
states have with outside states. Note that while the means differ 
considerably, the variances are roughly the same. 
Finally, the vector Npi gives us the “exit probabilities” for state si, 
i.e. the probabilities (depending on starting state) of going to si when 
the process leaves S; or, stated otherwise, the probability of hitting si 
before hitting s2. These probabilities seem to depend very simply on 
the. number of steps necessary to reach Si from the starting state 
(going through S). 
3.5.6 Theorem. Let r* be the function giving the number of times that 
the process remains in the non-absorbing state s< once the state is entered 
(•including the entering step). Then 
Min] = ll(l-pu), (a) 
Vari[r$] = pu/(l—pu)2. (b) 
And the conditional probability of the process going to s}, given that it 
leaves s*, is 
Pij/(l~Pii)- (c) 
proof. The set whose only element is s* is an open set. We apply 
Theorem 3.5.4 to this set. In this case IV is a 1 x 1 matrix, and hence 
identical with r; its only component is lj(l—pu). Hence (a) is a 
consequence of either (1) or (3) of Theorem 3.5.4. Similarly, IV2 = t2, 
and (b) is a consequence of either (2) or (4) of Theorem 3.5.4. We 
obtain (c) from 3.5.4(5) by choosing the vector pj whose only com¬ 
ponent is pij. Since s* is not absorbing, pu< 1, hence our quantities 
are well defined. 
One type of concept that we have not investigated as yet is illustrated 
by the question of whether the process ever enters a given transient 
state. This and related questions are taken up in Theorems 3.5.7, 
3.5.8, and 3.5.9. For these theorems we will let xij be the number of 
times that the process is in transient state s3-, m be the total number 
of transient states it will ever be in, and hij be the probability 
that the process will ever go to transient state sj, starting in transient 
state Si- 
3.5.7 Theorem. 
H = {hif = (N-I)Ndg~ 1.

--- Page 74 ---

FINITE MARKOV CHAINS 
/ 
Chap. Ill 62 
PROOF. 
or 
or 
{M«[n/]} = {dij} + {hi)Mj[nj'\} 
{nn} = I+ {hiJnJi} 
N = I + HNAg. 
Hence 
II = (N-I)NAg~K 
3.5.8 Theorem. {Prj[n3- — dj;- = &]} = 
(E — H if Jc = 0 
{H-H^ll-Hag] = (N-I)NAg-2(I-Nag-i)k-i if k > 0 
This theorem determines the probability of going to a given transient 
state exactly k times. The theorem is an immediate consequence of 
the following consideration: To go to a given state k times one must go 
there at least once, then one must return k—1 times, and one must not 
return again. 
3.5.9 Theorem. 
/* = = [H + (I-Hag)]£ = NNag~H. 
proof. The mean number of transient states occupied is equal to 
the sum of the probabilities of ever being in the various states. If the 
process starts in s*, the probability of ever being in s;- is htj if i^j, 
and is 1 if i =j. 
If we apply Theorem 3.5.7 to Example 1, we obtain 
H = 
We see, for example, that if q = 0, then all entries on and below the main 
diagonal are 0. This means that if the process is sure to move to the 
right, then it can never re-enter the starting state, nor can it enter a 
state to the left of the starting state.

--- Page 75 ---

Sec. 5 ABSORBING MARKOV CHAINS 63 
If q — 0 the vector y — I 2 ), which is obvious in this case since it moves 
directly to the right boundary, passing through the intermediate states 
only once. 
3.5.10 Theorem. The mean and variance of the number of changes of 
state in an absorbing chain can be calculated by setting pti = 0 for all 
transient states, and dividing each row by its row-sum. The i-th 
component of the new r gives the mean number of changes of state for the 
oiiginal process. The variance of the same function is given by the 
new T2. 
proof. Assume that the Markov chain is started in a non-absorbing 
state. We form a new process in which the n-th outcome function is 
defined as follows: If the original chain is absorbed at state s* before 
making n changes of state, then = s*. If not, is the state to which 
the process moved on the n-th change of state. The new process is 
clearly a Markov chain. The transition probabilities are the same as 
P for Si absorbing. For s* non-absorbing 
pu = Prfif^i] = 0 
pu = Prfifi =j] = J pin)upu = -j————• 
n = 0 1 ~Pii 
From this new transition matrix we can obtain the mean and variance 
of the time to absorption for the process h, h, ... . This time repre¬ 
sents the number of changes of state in the original chain started in 
state Si. 
We can also find the mean number of times that the process does not 
change its state while it is among the transient states. This is found 
by taking the mean number of times in the absorbing states and 
subtracting the mean number of changes of state. 
If we want to illustrate Theorem 3.5.10 by the college example, 
Example 10 of § 2.2, we set pu = 0, i = 3, 4, 5, 6, and renormalize :

--- Page 76 ---

64 FINITE MARKOV CHAINS Chap. Ill 
By comparing these resuits with Example 3.4.2, we note that the 
mean number of steps to absorption is somewhat higher than the mean 
number of changes of state (but not by much, since repetition of a 
state is rare), and that the variance of the former is considerably higher 
than that of the latter. 
Another interesting use of conditional probabilities for absorbing 
chains is the following. Assume that for an absorbing chain we start 
in a non-absorbing state and compute all probabilities relative to the 
hypothesis that the process ends up in a given absorbing state, say si. 
Then we obtain a new absorbing chain with a single absorbing state Si. 
The non-absorbing states will be as before, except that we have new 
transition probabilities. We compute these as follows. Let p be the 
statement “the original process is absorbed in state si.” Then if s4- 
is a non-absorbing state, the transition probabilities for the new 
process are 
Pr<[fi = Sj|p] Pri[fi = sMp] = Prf[plfi = sf]-Pri[f1 = sj] 
pH[p] Pr«[p] 
Pa bjiPa 
bn 
This formula applies for j = 1 if we interpret 6n= 1. The standard 
form for P may be obtained as follows. The matrix JR is a column 
vector with R = Let D0 be a diagonal matrix with diagonal 
entries bji, for S; non-absorbing. Then 
Q = D~\QDq. 
From this we see that 
Qn = D~1oQnD0

--- Page 77 ---

Sec. 5 ABSORBING MARKOV CHAINS 65 
and 
ft = D~i0[I+Q + Q2+ • • • ]Z>0 
= D-i0ND0. 
B = € and t may be obtained from ft. 
Example. Consider Example la, § 3.2.5. Let us consider the 
process obtained by assuming that the original chain is absorbed in 
state si. Then the new matrix Q is 
Q = 
so that 
Sl 
ft = 
Sl 
p S2 
S3 
S4 
0 
°\ 
15h 
° 
0 15/ 
to CO 
00 
4/35N 
75 75 
75 75/ 
S2 
0 
0 
79 
75 
75 
75 
7 15

--- Page 78 ---

66 FINITE MARKOV CHAINS Chap. Ill 
Exercises for Chapter III 
For § 3.1 
1. Put the following matrices in the canonical form for absorbing chains. 
(a) 
(b) P = 
Si S2 S3 
Si X/3 
S2 
° 1 0 
S3 V/» V 2 Vey 
Si S2 S3 
Si /' 0 0 
S2 1 0 0 
S3 lU V 4 V 4 
S4 \o 0 0 
2. Apply Theorem 3.1.1 to an absorbing chain with a single absorbing 
3. Apply the result of the previous exercise to an ergodic chain in which 
one state has been made absorbing. (See Chapter II, Exercise 21.) 
4. In Example 8 of § 2.2 make state R into an absorbing state. What 
does Theorem 3.1.1, applied to the resulting absorbing chain, say about the 
weather in the Land of Oz ? (That is, what do we learn about the original 
chain?) 
For § 3.2 
5. Compute the fundamental matrix for the absorbing chain with transition 
matrix. 
Si S2 S3 
Si c 0 0 
= s2 
0 
V 2 V 2 
S3 Vk 0 V 2 
6. Compute the fundamental matrix for Example 11 of Chapter II when 
c = 0, and 0. 
7. Make Example 9 of § 2.2 into an absorbing chain by making all of the 
ergodic states absorbing. Find the fundamental matrix and interpret the 
entries of the first row of this matrix. 
8. Show that if the fundamental matrix N is given for an absorbing chain 
then A-1 exists and Q = I — N~l. 
9. Prove that NQ — N — I. 
10. Check the results of Exercise 9, above, in Example 9 of § 2.2.

--- Page 79 ---

Sec. 5 ABSORBING MARKOV CHAINS 67 
For § 3.3 
11. If an absorbing chain has only one absorbing state, what can be said 
about the matrix _B? In Example 8 of § 2.2 make R an absorbing state, 
compute N and B, and verify your statement. 
12. Change Example 7 of § 2.2 into an absorbing chain by assuming that 
the process is stopped if a 0 or 9 is reached. Construct the new transition 
matrix, in canonical form. 
13. In the example of Exercise 12, above, compute N, N%, B, r, T2- 
14. In Example 8 of § 2.2 make N into an absorbing state. Compute the 
fundamental matrix for the resulting Markov chain. Find N2, B, T, t2. 
Interpret the results in terms of the original chain. 
15. Compute N for the tank duel (Exercise 2 of Chapter II). From this 
find the mean length of the duel and the probability of each possible ending. 
16. Carry out the computations of Exercise 15, above, for the modified 
tank duel (Exercise 3 of Chapter II). Which duel is more favorable to 
tank A ? 
17. In Example 10a (of § 3.2.5) find the probabilities of graduation by the 
method resulting from Theorem 3.3.9, that is, by finding a certain fixed 
column vector for the transition matrix. 
18. The chain of Example la (cf. § 3.2.5) is started by means of a random 
device which make all five states equally likely as starting states. Find the 
means and variances of the number of times in the various transient states, 
and of the number of steps to absorption. 
For § 3.5 
19. In Example 9 of §2.2, assume that initially both balls are unpainted. 
Find the mean number of draws before the first time that both balls are 
painted. When this occurs, what is the probability that both balls are red? 
20. It is snowing in the Land of Oz today. Find the mean number of 
different kinds of weather that will occur before the next rainy day. Find 
the probability that there is at least one nice day before a rainy day. 
21. For Example 1 of § 2.2 with p = 1/2, assume that it is known that the 
process is absorbed in state si. Find the transition matrix for the new 
conditional process. Find the mean time to absorption. 
22. Compute the following quantities for the tank duel (see Exercise 2 of 
Chapter II). 
(a) The mean and variance of the number of rounds for which all three 
tanks remain active. 
(b) The probability that at some stage A and C will still be active, but B is 
no longer active. 
(c) The probability that at some stage A and B will still be active, but C is 
no longer active. 
(d) The probability that A and B will be eliminated on the same round. 
(e) P, N, r, assuming that C wins the duel. 
(f) P, N, r, assuming that no tank survives.

--- Page 80 ---

68 FINITE MARKOV CHAINS Chap. Ill 
23. In the tank duel (Exercise 2 of Chapter II) let tank A have probability 
3/4 of hitting, tank B probability 3/s, and tank C an unspecified probability p 
(with p<3/5). 
(a) Set up the transition matrix. 
(b) Find the probability that tank C is the survivor. 
(c) In the answer obtained in (b), let p tend to 0. 
Interpret your result. 
For the entire chapter 
24. Seven boys are playing with a ball. 
The first boy always throws it to the second boy. 
The second boy is equally likely to throw it to the third or the seventh. 
The third boy keeps the ball if he gets it. 
The fourth boy always throws it to the sixth. 
The fifth boy is equally likely to throw it to the fourth, sixth, or seventh 
boy. 
The sixth boy always throws it to the fourth. 
The seventh boy is equally likely to throw it to the first or fourth boy. 
(a) Set up the transition matrix P. 
(b) Classify the states. 
(c) Put P into canonical form. 
(d) Give an interpretation for the chain ending up in one of the ergodic sets. 
(e) The ball is given to the fifth boy. Find the mean and variance of the 
number of times that the seventh boy has the ball, and find the mean 
and variance of the time to reach an ergodic set. 
25. Given an absorbing Markov chain, we play a game as follows: 
We start in a specified state, and carry the chain out till it reaches an absorb¬ 
ing state. If we reach sa, we receive a payment of ca. Form the column 
vector y whose i-th component is the mean of the payment if we start in s4. 
(a) Prove that Py — y. 
(b) Prove that for absorbing state sa the a-th component of y is ca. 
(c) Prove that these two conditions determine y. (Hint • Consider the 
limit of Pny.) 
(d) Let ya be the vector giving the probabilities of absorption in sa. 
Show that y can be expressed in terms of the ya.

--- Page 81 ---

CHAPTER IV 
REGULAR MARKOV CHAINS 
§4.1 Basic theorems. In this section we shall study the behavior of 
a regular Markov chain. We recall that a regular Markov chain is one 
that has no transient sets, and has a single ergodic set with only one 
cyclic class. 
411 Definition. The transition matrix for a regular Markov 
chain is called a regular transition matrix. 
4.1.2 Theorem. A transition matrix is regular if and only if for 
some N, PN has no zero entries. 
It was shown in Chapter II that a Markov chain was regular if and 
only if it is possible to be in any state after some number N of steps, no 
matter what the starting state. That is, if and only if P* has no zero 
entries for some N. 
4.1.3 Theorem. Let P be an r x r transition matrix having no zero 
entries. Let e be the smallest entry of P. Let x be any r-component 
column vector, having maximum component M$ and minimum com¬ 
ponent mo, and let M\ and m\ be the maximum and minimum com¬ 
ponents for the vector Px. Then Mi ^ M0} mi > m0, and 
Mi —mi ^ (1 — 2e)(M0 — mo). 
proof. Let x' be the vector obtained from x by replacing all com¬ 
ponents, except one m0 component, by M0. Then x^x'. Each 
component of Px' is of the form 
a-m0+(l~a)-M0 = M0-a(M0-m0) 
where a ^ e. Thus each such component is ^M0—e(M0 — mo). But 
since x < x', we have 
Mi ^ M0 — e(M0 — mo). 
69 
(1)

--- Page 82 ---

70 FINITE MARKOV CHAINS Chap. IV 
If we apply this result to the vector — x we obtain 
-mi ^ -m0- e(-m0 + M0). (2) 
Adding (1) and (2) we have 
Mi —mi V M0 — mo — 2e(M0 — ra0) 
= (1-2 e)(M0-m0). 
This theorem gives us a simple proof of the following fundamental 
theorem for regular Markov chains. 
4.1.4 Theorem. If P is a regular transition matrix then 
(i) The powers Pn approach a probability matrix A. 
(ii) Each row of A is the same probability vector a = {ai, a2, . . . , an}, 
that is A — £a. 
(iii) The components of a are positive. 
proof. We shall first assume that P has no zeros. Let e be the 
minimum entry. Let pj be a column vector with a 1 in the J-th 
component and 0 otherwise. Let Mn and mn be the maximum and 
minimum components of the vector PnPj. Since PnPj = P- Pn-iph 
we have, from Theorem 4.1.3, that M1^M2^M3^ •••and 
mi < m2 m3 < • • • and 
s' 
Mn — mn C (1 — 2e)(Afw_i — mw_i) 
for n ^ 1. If we let dn = Mn — mn, this tells us that 
dn < (l-2e)»-d0 = (1 — 2e)w. 
Thus as n tends to infinity dn goes to 0, Mn and mn approach a common 
limit, and therefore PnPj tends to a vector with all components the 
same. Let oq be this common value. It is clear that, for all n, 
mn^a^Mn. In particular, since 0<mi and Mi<\, we have that 
0< aj< 1. Now PnPj is the j-th column of Pn. Thus the j-th column 
of Pn tends to a vector with all components the same value a}. That is, 
Pn tends to a matrix A with all rows the same vector a = [ax, a2, . . ., ar}. 
Since the row-sums of P» are always 1, the same must be true of the 
limit. This completes the proof for the case where the matrix has all 
positive entries. 
Consider next the case that P is only assumed to be regular. Let N 
be such that PN has no zero entries. Let e' be the smallest entry of 
PN. Applying the first part of the proof to the matrix P*r, we have 
dkN < (1 — 2e')k (3) 
Therefore the sequence dn, which is non-increasing, has a subsequence

--- Page 83 ---

Sec. 1 REGULAR MARKOV CHAINS 71 
tending to 0. Thus dn tends to zero and the rest of the proof is the 
same as in the proof for all positive entries. 
4.1.5 Corollary. Let P be a regular transition matrix. Let 
ai = limpAnhj. Then there are constants b and r with 0<r< 1 such that n—>oo 
P{n)H = aj + e^Uj 
with |e<wh?-| ^ brn. 
proof. We know that |e<»>y| ^dn. Let N be such that P^ has no 
zero entries. Let e be the smallest entry of PN. Choose r=(l — 2e)1/N 
and b= 1/(1 — 2e) = r~N. If n — kN, then from (3), dn^rn. If 
n = kN + n±, where 0^n\^N, then since dn is non-increasing, 
dn ^ rn-ni ^ rn ■ r~N = brn. The bound here obtained for e<»>w is useful 
for proving theorems, but it is very conservative as an estimate for 
the rate of convergence of p(nfj. 
4.1.6 Theorem. If P is a regular transition matrix and A and a 
are as given in Theorem 4.1.4, then 
(a) For any probability vector rr, tt ■ Pn approaches the vector a as 
as n tends to infinity. 
(b) The vector a is the unique probability vector such that aP = a 
(c) PA—AP — A. 
proof. If 77 is a probability vector, then 77^ = 1; hence nA - 77£<x = a. 
But 77■ Pn approaches tt-A. Hence it approaches a. This proves 
part (a). 
Since the powers of P approach A, pn+i = pn.p approaches A, but 
it also approaches AP ; hence AP = A. Similarly PA = A, proving (c). 
Any one row of this matrix equation states that aP = a. We now show 
that a is unique. Let be any probability vector such that j3P = fi. 
By (a), fi-Pn approaches a. But since fiP = fi, fiPn = p. Hence a = /?. 
Thus we have proved (b). 
The matrix A and vector a will be referred to as the limiting matrix 
and limiting vector for the Markov chain determined by P. 
Theorem 4.1.6 shows that for a regular transition matrix there is a 
row vector a which remains “fixed” when multiplied by P. Any 
other vector a such that a'P = a' is proportional to the probability 
vector a. The following theorem shows that any fixed column vector 
for P is proportional to £. 
4,1.7 Theorem. If P is a regular transition matrix and p = {rf is a 
column vector such that 
Pp = P 
then p = c-£ for some constant c.

--- Page 84 ---

72 FINITE MARKOV CHAINS Chap. IV 
proof. Since Pp = p, P^p — Pp = p and in general Pnp = p. Hence 
also Ap = p. Thus r* = ap. But this states that all components of p 
have the same value. That is p = cg for some constant c. 
In Chapter II we saw that if the process is started in each of the 
states with probabilities given by rr, then the probabilities for being in 
each of the states after n steps are given by 7rPn. For large n Theorem 
4.1.6 states that ttP” is approximately a. Since a depends only on P 
and not on n, this may be described by saying that, for a regular 
Markov chain, the long range predictions are independent of the initial 
vector. Let us illustrate this in terms of Example 8 of Chapter II. 
The transition matrix for this example is 
R N S 
*/Va V 4 V4\ 
Nj V* 0 V2J. 
SV/4 V 4 V2 / 
To find the vector a = (aq, a2, a3), we must find a probability vector 
such that aP — a. That is, we must satisfy the following set of equa¬ 
tions : 
1 — d\ + d2 + a3 
al — 112alP 2d2-{-1/4d3 
a2 = 1/4«1 + 1/4®3 
a2 — 1/4®1 + ■*■/2<^2 + 1/2®3- 
The unique solution to these equations is 
« = (2/s, Vs, 2/s). 
The limit matrix A is then 
iA 2 V A = .4 .2 .4 . 
\-4 .2 .4/ 
Corollary 4.1.5 states that this limit is reached geometrically. This 
being a very fast kind of convergence, we would expect that, even for 
moderately large values of n, P« should be approximated by A. The 
matrix P5 is 
R N 
R /.4004 .2002 
P5 = N| .4004 .1992 
S\.3994 .2002 
S 
.3994 
.4004 
.4004,

--- Page 85 ---

Sec. 2 REGULAR MARKOV CHAINS 73 
Each row of P* gives the probability of each kind of weather five days 
after a particular kind of day. For example, the first row gives the 
probabilities for each kind of weather five days after a rainy day. The 
fact that the rows are so nearly equal means that today’s weather in 
the Land of Oz may be considered to have very little effect on our pre¬ 
dictions for five days from now. 
§ 4.2 Law of large numbers for regular Markov chains. As we have 
seen in § 4.1, for a regular Markov chain there is a limiting probability 
a,j of being in state sj independent of the starting state. In this section 
we shall prove that aj also represents the fraction of the time that the 
process can be expected to be in state s^ for a large number of steps. 
This result will also be independent of the starting state. 
To state the above result precisely, we shall need to introduce some 
new functions. Let u<M^ be a function with domain the tree Uw and 
with value 1 if the n-th step was to state s; and 0 otherwise. We 
n 
define y^ Then y<w>^ is again a function with domain 
k=l 
the tree Un and value the number of times (not counting the initial 
position) that the process is in state S; during the first n steps. The 
function v<»>j = yWj/n gives the fraction of times in the first n steps that 
the process moves to state sj. 
4.2.1 Theorem (The Law of Large Numbers). Consider a regular 
Markov chain with limiting vector a — (ai, a2, • . . , ar). For any 
initial vector -n, 
Mw[v(»)J-] a, (a) 
and for any <r > 0 
Prw[|v(«)> — af\ > e]-^0 (b) 
as n tends to infinity. 
proof. According to Theorem 1.8.10, to prove this theorem it is 
sufficient to prove that M„[(v<”>j-<q)2]->0 as n tends to infinity. To 
prove this it is sufficient to prove that, for every i, Mj[(v(«),- -%)2]^0. 
uWj/nj-aA Mi[(v(^--%)2] = Mi 
/ c- 
I i-=l 
= -vMi 
71 ‘ 2(uW;- a7- 
. i'=l 
Let mic,i = Mi[(uG)y — a;-)(uO)y — a^)]. Then we must prove that

--- Page 86 ---

74 FINITE MARKOV CHAINS Chap. IV 
as n tends to infinity. Multiplying out the expression for we have 
mk,i = — ajMi[u<z>j] + a2,. 
Let m = min (k, l) and d= \k-l\. Then 
mk,i = P(m)i)P{d)jj — ajP(k)ij — ajP(l}ij + a2)- 
Using Corollary 4.1.5, we have 
m.i = - e«)«) + e(»)wetf)w 
where | < brn with 0< r< 1. Hence for a suitably chosen constant 
c, 
\mk,i\ < c(rm + rd + rk + rl). 
Each value of m, d, k, and l occurs < 2n times in the sum in (1). Hence, 
using (2), we have 
1 
n2 
It It 
2 2 b*.*i 1=1 k= 1 
4c 2 n 8c 
n2 1— r n(l — r) 
The right side of this inequality tends to 0 as n tends to infinity; 
hence, also the left side, as was to be proved. 
Let us apply this theorem to the Land of Oz example. We found in 
§4.1.5 that a = (2/5, i/5, 2/5y Thus we can now say that for a large 
number of days we can expect about 2/5 of the days to be rainy, i/5 
of the days to be nice, and 2/5 of the days to be snowy. 
Consider the special case of an independent trials process. Such a 
process is a Markov chain with transition matrix having all rows the 
same vector a and with initial probability vector chosen to be a. The 
law of large numbers for an independent trials process is thus a special 
case of the theorem just proved. The proof for this case is very much 
simpler. In fact, in this case Ma[u<»>j] = as for all n\ hence also 
Ma[v(w^] = aj. Also rri/cj — 0 for k^l and m*,* = a2, a constant for all k. 
Hence 
Ma[(v<n>y-Qq)2] = Vara[v(»VJ = —. 
n 
This tends to 0 as n tends to infinity. 
Another special case of interest is a general Markov chain process 
which is started by an initial probability vector tt = a. In this case also 
Ma[u<w)^] = Ma[v(w);-] = a,j for all n. Hence 
Ma[(v(»)i-aJ-)2] = Var„[v<»>,]. 
However, it is not possible, in this case, to give a simple expression for 
this variance as a function of n, as was possible in the independent case.

--- Page 87 ---

Sec. 3 REGULAR MARKOV CHAINS 75 
We shall consider this variance in § 4.6, where we shall give an 
asymptotic expression for it. 
§ 4.3 The fundamental matrix for regular chains. In Chapter III 
we found that, for an absorbing chain, the matrix (I-Q)~1 played a 
fundamental role. (Q was the matrix obtained by truncating the 
transition matrix to include only the non-absorbing states.) We shall 
see that there is a corresponding fundamental matrix for regular chains. 
4.3.1 Theorem. Let P be the transition matrix for a regular Markov 
chain. Let A be the limiting matrix. Then Z = (/ — (P — A))~l 
exists and 
Z = 1+2 (pn~A)- n— 1 
proof. We shall prove that (P — A)n = Pn — A. Since Pn — A-+ 0, 
our theorem will then follow from the matrix theorem proved in 
§ 1.11.1. We have A2 = gaga = ga = A, hence Ak = A, and 
(p-a)« = y (fji- 
= Pn-A. 
4.3.2 Definition. Let P be a regular transition matrix. The 
matrix Z — {I — (P — A))~l is called the fundamental matrix for the 
Markov chain determined by P. 
We shall see that the matrix Z is the basic quantity used to compute 
most of the interesting descriptive quantities for the behavior of a 
regular Markov chain. We shall first establish certain important 
properties of the matrix which will be useful in later work. 
4.3.3 Theorem. Let Z be the fundamental matrix for a regular 
Markov chain with transition matrix P, limiting vector a, and limiting 
matrix A. Then 
(a) PZ = ZP 
(b) Zg= g 
(c) aZ = a 
(d) I-Z = A-PZ. 
proof. Part (a) follows from the infinite series representation for Z 
and the fact that P commutes with each term in this infinite series.

--- Page 88 ---

76 FINITE MARKOV CHAINS Chap. IV 
Part (b) states that Z has row-sums 1. This again follows from the 
infinite series representation for Z since the first matrix / has row- 
sums 1 and each of the matrices Pn — A have row-sums 0. Part (c) 
follows from the infinite series representation for Z, since a I — a and 
00 
a(Pn — A) = 0. To prove (d) we multiply Z = I+ ^ (Pn — A) by 
n=l 
I — P obtaining 
(I-P)Z = (I — P) + {P — A) 
= I-A. 
We shall use the Land of Oz example as our standard example of the 
applications of the Z matrix. For this example P and A are 
R N S 
R /Va V4 VA 
p = N Vs 0 1/s 
SV/4 V 4 V 
R N S 
/a/« Vs 2/s\ R 
A = 2/5 Vs 2/s N. 
\2/5 Vs 2/s/ s 
To find the matrix Z we must find the inverse of the matrix 
I-P + A 
Doing this we obtain 
-.05 
1.2 
-.05 
N S 
3 -14\ R 
63 6 N. 
3 86/ S 
While the fundamental matrix Z has several properties in common 
with a transition matrix, we see from this example that it does not 
necessarily have non-negative entries. 
An example where the Z matrix turns out to be a very simple matrix 
is the case of an independent trials process. In this case P = A so that 
Z = {I-(P-A))~1 = I. Thus for an independent trials process the 
Z matrix is the identity matrix. 
Let y(»V be the number of times that the process is in state Sj in the 
first n stages, i.e. the initial position plus n— 1 stages.

--- Page 89 ---

Sec. 3 REGULAR MARKOV CHAINS 77 
4.3.4 Theorem. For any regular Markov chain, and any initial 
vector 77*, 
{MOT[y(«)J-]}-na-^7r(Z-H) = ttZ-cc. 
proof. For any i, 
Mi[y<w>;] = J M([u<*>,] = 2 P{k)a- 
k=0 *=0 
Thus 
ra — 1 
{M^yW^-na;} = 2 {P*-A)-+ Z-A. 
k=0 
Therefore 
ir{Mi[y<n)j]-naj}->7r(Z-A) = nZ-a. 
An immediate consequence of this theorem is the following: 
4.3.5 Corollary. For any two initial distributions n and n' 
Mw[y(^]-Mw{y(^-] -> (tt-tt')Z. 
If we choose a particular starting state, say i, then Theorem 4.3.4 
states that 
Mi[y(w)j] - na-j -> (zy - a}). 
Thus we see that for large n the mean number of times in state s;*, 
starting at state Sy differs from naj by approximately zy — a;* We recall 
that by Theorem 4.2.1 the mean of the fraction of times in state s} 
approaches % independent of the starting state. Thus the entries of 
(Z — A) give us an interesting quantity for regular chains for which the 
initial state does have an influence. We can compare two starting 
states, since by Corollary 4.3.5 
Mi[y(w);] - M*[y(»»j] -> Zy - zkJ. 
Another interesting corollary to Theorem 4.3.4 is the following. 
4.3.6 Corollary. Let c = 2 zn- Then 
2 (M^[y(w)?] - M-[y(ra);]) -> c -1 
j 
as n approaches infinity, independent of tt. 
proof. By Corollary 4.3.5 
Mj[y(n)j] - M„[y(«);] -> zjj - (irZ)j. 
Therefore the sum approaches 
2 Z)j — nZt; — C— 1.

--- Page 90 ---

78 FINITE MARKOV CHAINS Chap. IV 
This corollary has the following interpretation. For any 77, 
M3-[y(w)/] ^ Hence M^[y(”);] gives the largest possible mean 
number of times in sj. The corollary states that the deviations from 
this maximum, summed over all states, approach a limit which is 
independent of the choice of 77. 
§ 4.4 First passage times. In this section we shall study the length of 
time to go from a state s* to a state Sj for the first time. We shall see that 
the mean of this time is easily obtained from the fundamental matrix. 
4.4.1 Definition. For a regular Markov chain, the first passage 
time f* is a function whose value is the number of steps before entering 
sk.for the first time after the initial position. 
4.4.2 Theorem. For any i, M,[f*] is finite. 
proof. Assume first that i^k. Form a new Markov chain by 
making state s* into an absorbing state. The resulting Markov chain 
is an absorbing Markov chain with a single absorbing state, s*. The 
mean time to go from Sj to sj in the given chain is the same as the mean 
time before absorption in the new chain. The mean time before 
absorption is finite by Theorem 3.2.4. 
If i = k, then 
Mi[fj] = pu + 2 PikM-k\fi] 
k=£i 
which is finite by the first part of the proof. 
4.4.3 Definition. The mean first passage matrix, denoted by M, 
is the matrix with entries my = M<[f}]. 
^ It then follows from 1.8.9 that, for an initial vector 77, the mean 
first passage times are the components of the vector 77M. 
4.4.4 Theorem. The matrix M satisfies the equation 
M = P{M-MAg) + E. (1) 
proof. We calculate Mj[fy] by taking the mean of the conditional 
means, given the outcome of the first experiment. This gives 
Mj[fy] = 2 Pi*(M*[fj-] + 1) + ptj 
k^j 
— 2 PikMk\fj\ + 1 k^j 
= 2 - PtjNLiltj] + 1. 
That is, 
= 2 Pi^kj - PijinjJ + 1. 
k 
This proves the theorem.

--- Page 91 ---

Sec. 4 REGULAR MARKOV CHAINS 79 
4.4.5 Theorem. Let a = {a\, <12, . . . , ar] be the limiting 'probability 
vector for P. Then mu = 1 jat. 
proof. Multiplying equation (1) above by a we have 
aM = aP(M -Mdg) + aE 
— a(M — M&g) + aE. 
Therefore 
aM$g = ccE = rj. 
This states that aimu — 1 for every i, or mu — l/at. 
4.4.6 Theorem. Equation (1) of Theorem 4.4.4 has a unique 
solution. 
proof. Let M and M' be two solutions for (1). Then from the 
proof of Theorem 4.4.5 we have aMAg = aM'Ag = v. Hence Mdg = M'dg. 
This gives 
M-M’ = P(M-M'). 
But this means that each column of M — M' is a fixed column vector for 
P. Hence by Theorem 4.1.7 each column is a constant vector. Since 
M — M' has 0’s on the diagonal, these vectors must all be 0 vectors. 
Hence M — M'. 
4.4.7 Theorem. The mean first passage matrix M is given by 
M = (.I-Z + EZde)D (2) 
where D is the diagonal matrix with diagonal elements du— l/a<. 
proof. By Theorems 4.4.4 and 4.4.6 we need only show that M 
as defined by (2) satisfies equation (1) above. 
Let 
M = (I — Z + EZdg)D. 
Then 
M — D = (— Z + EZAg)D 
and 
P(M-D) - (— PZ + EZAg)D 
= M + (-I + Z-PZ)D. 
By Theorem 4.3.3(d) this is 
P(M-D) = M-AD 
= M-E. 
By (2), D = Mdg. Hence M = P(M — Mdg) + E. 
4.4.8 Theorem. Let P be the transition matrix for an independent 
trials process. Then M = (1 /pu}.

--- Page 92 ---

80 FINITE MARKOV CHAINS Chap. IV 
proof. From Theorem 4.4.7 and the fact that Z = 1 for an inde¬ 
pendent trials process, we have M — ED. For an independent trials 
process the limit matrix A = P. Hence pi} = a} and 1 la,• = 1 lpt1. Thus 
M = ED = {l/ptj}. 
We now illustrate the calculation of the mean first passage matrix 
for the Land of Oz example. We have found a to be (2/5j %/5). 
Hence the matrix D is 
D = 
The Z matrix was found in § 4.3. From this, using Theorem 4.4.7 
we obtain M by M = (I - Z + EZdg)D. Carrying out this calculation 
we obtain 
R N S 
Thus, for example, if it is raining in the Land of Oz today the mean 
number of days before a nice day is 4. The mean number of days 
before another rainy day is 5/2; before a snowy day 10/3. 
We shall next prove a theorem which connects the diagonal elements 
of Z with the mean time to reach sf for the initial probability vector 
n = a. We have seen previously that the mean number of times in 
state Sj is particularly simple in this case. This choice of initial vector 
is of special importance for the following reason. Assume that a 
regular Markov chain has gone through a large number of steps before 
it is observed. Then Theorem 4.1.6 suggests that a natural choice for 
the new initial vector v is a. The probabilities for any later time are 
then also given by a. In this case we say that the process is observed 
in equilibrium. 
4.4.9 Theorem. For a regular Markov chain 
aM = {Ma[fy]} = rjZdgD - {zjj/aj}. 
proof. Multiplying (2) by a we have 
aM = a(I-Z + EZag)D 
= (cc-a + r)Zdg)D 
aM = rjZdgD.

--- Page 93 ---

teEC- 4 REGULAR MARKOV CHAINS_ 81 
4.4.10 Theorem. Letc=^zu. ThenMa.T = c. 
i 
PROOF. 
MaT = (.I-Z + EZag)DaT 
= {I — Z + EZ&g)£ 
= £(vzdg£) = ci- 
In § 4.3 we compared the mean number of times y(n)j in a state s- 
under the assumption of two different starting distributions. We can 
make the same comparison for the function 
4.4.11 Theorem. For any two initial probability vectors n and tt' 
{Mw[fy] — Mw'[f;]} = (ir-ir')(I-Z)D. 
PROOF. 
{Mw[f,-] — M^'[f;-]} = ttM — tt'M 
= (n-TT')(I-Z + EZag)D. 
= (77-7r')(/-Z)Z>. 
In the Land of Oz example we see that 
IVLvLfs’] - M/jffs] = — io/3 = -2/3. 
Thus the mean time to the first snowy day is shorter starting with a 
nice day than it is starting with a rainy day. 
We will conclude this section by showing that the Markov chain is 
completely determined by the numbers mih for i^j. We shall use 
these numbers as the non-zero entries of the matrix M = M- D. This 
matrix has n(n-l) non-zero entries, which suffices to determine the 
chain. When we give the chain in terms of P, we specify n2 entries, 
but these satisfy n relations, since P must have row-sums 1. But 
there is no natural way of specifying just n{n- 1) entries of P, while 
is a natural way of giving this minimum information. 
4.4.12 Theorem. For any regular Markov chain 
(a) The matrix M has an inverse 
(b) a = (c-1) (M~l£)T 
(c) P = I + (D-E)M~ 1. 
proof. From equation (1) in § 4.4.4 we have 
M+D = PM + E-, 
hence 
(P-I)M = D-E. (3)

--- Page 94 ---

82 FINITE MARKOV CHAINS Chap. IV 
If M has no inverse, then there is a non-zero column vector y such that 
My = 0. Hence from (3) 
(D — E)y = (P — I)My = 0 
Dy — Ey 
y = D~^Ey = P-i^rjy = (rjy)(xT, 
where l = rjy is a number. And since y ^ 0, l=/= 0. 
_«T = mr 
MaT = (i/Z)My = 0. 
But, clearly, MaT > 0, and we have a contradiction. Therefore, M has 
an inverse. Formula (c) is then an immediate consequence of (3). 
To prove (b) we make use of § 4.4.9 and the fact that DaT = g. 
(.M+D)aT = c£ 
MaT = (c- 1)^ 
aT = (c-l)AH£ 
a - {C-\){M~H)T. 
We can now find a from formula (b), and the condition that a£= 1. 
This determines D, and then formula (c) will yield P. Thus the chain 
is determined by M. 
§ 4.5 Variance of the first passage time. In the previous section we 
found that the Z matrix enabled us to find the mean first passage time 
from si to Sj. In this section we shall show that the Z matrix also 
provides us with the variance of the first passage time. 
We recall that f)■ is the function whose value gives the number of 
steps required to reach sj for the first time after the initial step. We 
have found Hence to find Varj[fj] it is only necessary to find 
Mi[f2;-] and use the fact that Var^f^M^f^J-Mitf,-]2. We denote by 
W the matrix W = {M^f2,-]}. 
4.5.1 Theorem. The matrix W satisfies the equation 
W = P[W-WiB] - 2P[Z-EZag\D + E. (I) 
proof. Taking conditional means we have 
= 2 P^kVSi+ l)2] + Pij 
= 2. 2,] + 2 2?h*M*[f/] + 1. k^j k=fij 
Or 
W = P[W-Wdg] + 2 P[M-Mde] + E. (2)

--- Page 95 ---

Sec. 5 REGULAR MARKOV CHAINS 83 
From Theorem 4.4.7 we have M — MAg = (— Z + EZag)D. Putting 
this in (2) we have (1). 
4.5.2 Theorem. The values for Mj[f2j] are given by 
JUdg = D(2Z&gD — I). (3) 
PROOF. Multiplying equation (1) through by a and using the fact 
that aP = a, we have 
ccW — a[W — JTdg] — 2a[Z — EZag\D + y ; 
or, since aZ = a, and al) = aE — y, 
alUdg = —r] + 2yZagD. 
emeu = - 1 + 2 zu/ai 
1 2 zu 
(4) 
This gives 
or 
Wu = ai a2i 
Written in matrix form, this is (3). 
4.5.3 Theorem. The unique solution to (1) is 
W = M{2Z(isD-I) + 2{ZM-E{ZM)Ag). 
proof. The uniqueness proof is the same as that given for the 
matrix M in Theorem 4.4.6. It is then only a matter of verifying that 
the expression given for W satisfies (1). We omit the details of this. 
From the matrix W it is an easy matter to find the {Var*[f/]}. We 
denote by Jf2 = {Vari[f/]}. Then M% = W — Afsq. 
Let us find these variances for the Land of Oz example. We have 
previously found M, D, and Z for this example, so that to find W, the 
only new matrix we need is ZM. This is 
(1761/3 303 2592/3N 
203 363 203 
2592/3 303 1761/3/ 
From the formula W — M(2Z^gD — I) + 2(ZM — E(ZM)ag) we find 
71/e 26 54 
O 
CO 37 40 
54/3 28 71 
hi

--- Page 96 ---

84 FINITE MARKOV CHAINS Chap. IV 
and subtracting Msq from this we obtain 
/67/l2 12 62/9 
M2 = ( 56/9 12 56/9 
\62/9 12 67/12 
We observe that the variance Var^f;] in this example depends very 
little on the choice of the starting state s*. The first passage times for a 
regular chain are quite similar to absorption times for an absorbing 
Markov chain. They both have variances which are in general large 
compared to their means. 
The formulas for W and for M2 are very much simplified for the case 
of an independent trials process. 
4.5.4 Theorem. For an independent trials process 
W = ED(2D-I) = {(l/pij){'2/pij — 1)} 
and 
M2 = E(D2 — D) = {(l/pu)2- 1/pij}. 
proof. We recall that for an independent trials process Z is the 
identity matrix and M = ED. Thus, using Theorem 4.5.3, 
W = ED(2D-I) + 2{ED-ED) 
= ED(2D-I). 
From this we obtain 
M2 = W-Msa 
= 2ED2-ED-(ED)sq 
= E(D2 — D). 
The alternative expressions for W and M2 given in the statement of 
the theorem follow from the fact that pa = p^ for all i. 
§ 4.6 Limiting covariance. Let f and g be two functions defined on 
the states of a regular chain. Let f(s<)=/i and g(Si) = gi. Let f(«) and 
g<w> be the values of these functions on the n-th step. We are interested 
in finding 
It can be shown that this limit exists and is independent of n 
4.6.1 Theorem. 
lim — Cov„ n 
V 
2 flt’. k= l 
n 
2 s<« 
r 
i, j = 1 n—► oo

--- Page 97 ---

Sec. 6 REGULAR MARKOV CHAINS 85 
where 
Cij — diZij A ttjZji didij dfdj. 
proof. We shall assume the result that the limit is independent of 
77 and prove the theorem for the case v = a. 
and 
Hence, 
1 r » 
- Cova n 
Ma i «*> *=i 
= n J difi 
i = i 
2 tm k = 1 
= n j 3 = 1 
2 f«», 2 k=1 1=1 
= -Ma n 2 m~n 2 aiA( 2 g(0~w 2a^' U- = 1 i = 1 
i n n 
= -Ma n 
1=1 3=1 
r 
2 2 fwg(/) - n 2 f(*> 2 
* = 1 l = 1 k= 1 j=l 
-n 2 g<0 2a<A + w2 2 
J = 1 i= 1 i, j=l 
“"2 2 (Pr«[uW* = 1A u(i)j = 1]M - pra[u<*)< = l]fidjgj 
71 kj^l ij^l 
— Pra[u<^ = l]gjdifi + didtftg j) 
= ^ 2 2 (Pr«[uwi = lAu'!)i = l]/^ - didjfigj). 
Now 
M=1 tj=l 
'dipW-Wij if A: < l 
Pr0[uWj=l Au(,)i = l] = ^ dj2){lc~l)a if k > l 
(1) 
(2) 
didij if k = l. 
Hence we have from (1) and (2) 
1 n 
- Cova n 2 {m. 2 k=l 1=1 
1 r 
= - 2 2 (p{l~k)vai) 
Ui,j= 1 k<l 
1 r 
+- 2 a*/to 2 (p(k~l)^i) 71 £,7=1 k>l 
+ 2 {tt'idij 
i,j= 1

--- Page 98 ---

86 FINITE MARKOV CHAINS Chap. IV 
Collecting terms with the same d=\l — Tc\, we have 
1 " 
- Cova to 2 {m■ 2 s'" *=i z=1 
= 2 2 -2— (p{d)i]-a]) d= 1 TO 
+ 2 2 iv(d)n-<M) i,j=l d= 1 n 
(3) 
r _ 
+ 2 
i,j=1 
CO 
Since Z = 2 (P-A)d converges, it is Cesaro-summable (see § 1.10), 
d= 0 
that is 
Hence 
»-l rt _ 
Z = lim 2 ——(P — A)d. 
n-+ co d=0 
n — 1 
Z-7 = lim 2 ^(pd~A), 
<z=i TO 
, V7 to — d 
Zij-dij = lim 2 —— {p(d)ij-dj). 
Then from (3) 
n^-co rf=1 TO 
lim Cova 2 «<*>. 2 * (Z) 
k=l 
r 
Z=1 
— 2 \-aifi9){zi} ~ dij) + djf igj(zn — dij) + (didij — didj)ftgj] 
i,j= 1 
r 
2 + djZji a-idij didj)gj. 
i,j=l 
This completes the proof. 
If f and g are the same function, then the above theorem gives us 
4.6.2 Corollary. 
lim 
n = co 
- Var TO 77 
n 
2 fw *=i 
We shall need a slight extension of this last result. Suppose that f 
is not simply a function of the state, but f=l with probability ft on 
state Sf and 0 with probability 1 We may think of f as deter¬ 
mined as follows: We carry out the Markov chain, and if the process is 
m Si on a given step we flip a biased coin (probability ft for heads) to

--- Page 99 ---

Sec. 6 REGULAR MARKOV CHAINS 87 
determine whether f is 1 or 0. Again, f<w> is the value of the function 
on the n-th step. Then the above argument for the limiting variance 
applies except that in (1) a slight change must be made when k — l. 
Here, the term f2* should be simply /*, and we have a correction term 
2 ««/«(!-/<)• 
i= 1 
Thus we have proved 
4.6.3 Theorem. If f is a function that takes on the value 1 with 
'probability fi in Si, and is 0 otherwise, then 
lim 
71—>00 
— Var„ n k= 1 
2 fic*ifi+ 2 
i, j= 1 i= 1 
We can also extend the result to two such functions. If these take 
on their values independently of each other, then the proof of 4.6.1 
applies exactly. This proves 
4.6.4 Theorem. If f and g are functions that take on the value 1 
in si with probabilities fi and gt, respectively, independently of each 
other, and if the functions are 0 otherwise, then 
r 
— 2 
i, 3=1 
One application of the covariance is to obtain correlation coefficients. 
Let f and g be as in Theorem 4.6.1. Then 
4.6.5 Definition. 
Corr„ 2 **>, I g (J> 
k= 1 1=1 
Cov„ 
r - - i 
'biD 
i_i 
y- i f«> 
A-=! 
Varw 2g» y=i 
Dividing numerator and denominator of the right side by n, and using 
4.6.1 and 4.6.2, we have 
4.6.6 Theorem. 
lim Corr„ 
71 —► CO 
2 f“'. 2 §<n 
A-=l 1=1 
2 ,3 = 1  
2 ficafi ' 2 Qici)9i 
i, 3=1 i, 3=1

--- Page 100 ---

88 FINITE MARKOV CHAINS Chap. IV 
Another important application of Theorem 4.6.1 is the following: 
Let A and B be any two sets of states. Let yand yMb be respec¬ 
tively the number of times in set A in the first n steps and the number 
of times in set B in the first n steps. For these functions we have the 
following theorem. 
4.6.7 Theorem. 
lim icovw[y(«)^, y(»)B] = T c{j. 
n 00 n S; in A 
Sy in B 
proof. Let f be a function which is 1 on the states of A and 0 on 
all other states. Let g be a function which is 1 on states of B and 0 
otherwise. 
n n 
y(w)A = 2 k) and y(n)B — 2 §<Z)- 
4=1 i=l 
Hence the theorem follows from 4.6.1. 
From this theorem we see that 
4.6.8 Corollary. 
lim Corr„[y(«)^, y<»>B] 
n—► oo 
Taking A and B in 4.6.7 to be sets with a single element, we see 
that Cij represents the limiting covariance for the number of times in 
states i and j in the first n steps. The values of c« give the limiting 
variances for the number of times in state Si. We are often interested 
only in these variances, we denote them by the vector /3. The limiting 
correlation for the number of times in st and Sj is ——- If % = j, the 
V Cii-Cjj 
correlation is 1. 
For an independent trials process ci} = a4dy - , and hence all the 
formulas found above simplify. For example, if i^j, the limiting 
correlation is 
2 'l] 
s,- in A 
2 Cii' 2 s,- in A st in B 
s,. in A Sy in B 
_~~ ai(l3_ _ _ j didj 
V ~ di)dj(l - dj) V (1 -di)(\-dj) 
The diagonal entries of C, i.e. the limiting variances, have the following 
important use. Let /3 = {bj} = {cjj}. Then /3 is a vector which gives the 
limiting vdridnces for the number of times in each state. These

--- Page 101 ---

Sec. 6 REGULAR MARKOV CHAINS 89 
variances appear in the following important theorem (called the Central 
Limits Theorem for Markov Chains). 
4.6.9 Theorem. For an ergodic chain, let ybe the number of 
times in state S; in the first n steps and let a = {aj} and /3 = {b]} respec¬ 
tively be the fixed vector and the vector of limiting variances. Then 
if bj ^ 0, for any numbers r<s, 
i rs ——— er3?! 2 dx 
V 277 Jr 
as n—>oo, for any choice of starting state k. 
The proof of this theorem is beyond the scope of this book and appears 
only in the more advanced books on probability theory. However, for 
a discussion of this theorem in the case of independent trials processes 
see FMS Chapter 3. It is not possible to evaluate the integral in this 
theorem exactly, but for illustrative purposes we mention that the 
value for r — — 1 and s = 1 is approximately .681, for r = — 2 and s = 2 it 
is approximately .954, and for r = — 3 and s = 3 it is .997. 
Example. Let us consider the Land of Oz example. For this 
example we have found, 
« = (2/5, Vs, 2/s) 
and 
R N S 
( 86 3 - 14 
6 63 6 
- 14 3 86 
This is all the information we need to compute the matrix C = {cif). 
Carrying out this computation we find 
Pr; y <”)f — wa* r < -— : < s 
V nbj 
c = v 375 
R N S 
134 - 18 — 116\ R 
- 18 36 
-H 
1 N. 
-116 - 18 134/ S 
' give us the limiting variance 
36/375, 134/375} for being in each of the states. Thus the Central Limit 
Theorem would say, for example, that 
y(n>jv — w/5 
V(36/375)w

--- Page 102 ---

90 FINITE MARKOV CHAINS Chap. IV 
would for large n have approximately a normal distribution. From 
this we may estimate that the number of days in 375 days which would 
be nice would be unlikely (probability about .046) to deviate from 75 
by more than 12. 
Assume that we are interested only in bad weather or only in good 
weather. Then we would want to consider the number of times the 
process is in the set Ai = {R, S} and the number of times it is in the set 
A2 = {N}. Let Cij be the limiting covariance for the number of times 
in set Aj and Aj. Then from 4.6.7 we know that we can obtain 
G = {ctj} by simply adding elements of C. For example, 
C12 = CR]y + CsN — — 18/375 — 18/375 = — 36 / 375 = — 12/125- 
Ai A2 
£ = A d 12/125 -12/l25\ 
A2\-l2/l25 12/125/' 
It is easily verified that the row-sums of C must be 0. Since C is 
symmetric, the column-sums must also be 0. For a 2 x 2 matrix this 
tells us that the entries must all have the same absolute value. Thus 
we would expect C to have the special form that we found. 
§ 4.7 Comparison of two examples. In this section we shall compare 
the basic quantities for two regular Markov chains with the same 
limiting vector. One will be an independent trials example and the 
other will be a dependent example. The two examples are the random 
walk Example 3 of Chapter II with p = i/2 (denoted by Example 3a) 
and Example 7. The transition matrix for Example 3a is 
s2 
0 
0 
V 2 
0 
0 
S3 
1 
Va 
0 
V 2 
1 
The transition matrix for Example 7 is 
Si s2 S3 
Si 
z1 
.2 .4 
S2 i (1 
.2 .4 
= S3 
1 
.2 .4 
S4 ’ 
u 
.2 .4 
S5 Yi .2 .4 
s4 s5 
0 
0 
Va 
0 
0 
S4 s5

--- Page 103 ---

Sec. 7 REGULAR MARKOV CHAINS 91 
The limiting vector for each of these chains is a —(.1, .2, .4, .2, .1). 
Thus by the Law of Large Numbers we can expect in each case about 
.1 of the steps to be to si, about .2 to S2, etc. 
For a regular Markov chain the fundamental matrix is given by 
Z — (I — P + A)~x. If this is computed for Example 3a we obtain 
Sl 
s2 
Z = s3 
54 
55 
Si s2 S3 
00 
00 
— .04 .32 
.33 .86 .12 
-.02 .16 .72 
-.17 -.14 .12 
-.12 -.04 .32 
For an independent trials process, Z is the identity matrix. Hence 
for Example 7, Z — I. 
The first information we obtain from Z relates to the number of 
times in a state in the first n steps. Let y(n)3- be the number of times 
in state sj in the first n steps (counting the initial state). Then by 
Theorem 4.3.4 
— ncij -> Zij — ctj. 
For the independent case this limit is replaced by equality. In the 
dependent case, zy gives us a comparison of M*[y<w^] for fixed Sj and 
different starting states s*. For example, in Example 3a, zn>Z2i> 
Z31 > Z51 > z4i. Thus for large n 
Mi[y<«>i] > M2[y(«>i] > M3[y<">i] > M5[y<«>i] > M4[y<">i]. 
The fact that the process may be expected to be more often in si 
starting from S5 than from S4 may be seen also from the fact that to 
reach si from either of these states it is necessary to go through S3. 
From s5 the first step is to s3 while from s4 it is either to s3 or to s5. 
We can also find from Z the limiting variance for y^/Vn. This 
is given by 
j8 = {oiiZzjj- l-aj)}. 
In Example 3a this gives 
j3 = (.066, .104, .016, .104, .066). 
For the independent trials process = {% (1 — %)}. Thus for Example 7, 
j8 = (.09, .16, .24, .16, .09). 
The variance in the independent case is in each case larger than the 
corresponding variance in the dependent case. The variance for S3 
is much larger. This means that we can make more accurate predic¬ 
tions about y(w)3 in Example 3a than in Example 7. For example, the

--- Page 104 ---

92 FINITE MARKOV CHAINS Chap. IV 
Central Limit Theorem tells us that in 1000 steps the number of occur¬ 
rences of S3 in Example 3a will, with probability % .95, not deviate 
from 400 by more than 2^1000-.016 = 8. In Example 7 we could, 
with the same probability, only say that the number of occurrences of 
S3 would deviate from 400 by less than 2y'T000- .24^:31. 
Let us next compute the covariance matrix and some correlations. 
For Example 3a we have 
G = 
.066 .042 -.016 -.058 -.034 
.042 .104 .008 -.096 -.058 
-.016 .008 .016 .008 -.016 
-.058 -.096 .008 .104 .042 
-.034 -.058 
CO 
0 
1 .042 .066 
For the limiting correlation between si and each of the five states we 
have (rounded): (1.00, .51, -.49, -.70, -.52). 
For Example 7 the covariance matrix is 
C = 
.09 -.02 -.04 -.02 -.01 
-.02 .16 -.08 -.04 -.02 
-.04 -.08 .24 -.08 -.04 
-.02 -.04 -.08 .16 -.02 
-.01 -.02 -.04 -.02 .09 
The limiting correlations with sx are: (1.00, - .17, -.28, -.17, - .11). 
It is to be expected that often the limiting correlations between two 
different states will be negative, since—generally—the more often the 
process enters one state, the less often it will be in the other state. For 
the independent process all the correlations between pairs of different 
states are negative, though quite small. But for Example 3a the 
correlations are fairly large, and the correlation between sx and s2 is 
positive. 
We next consider the function which gives the number of steps 
taken to reach s,- for the first time. The values of M<[fj] are given by 
the matrix M = (/ — Z + EZdg)D. For Example 3a this is 
Si 
s2 
M = S3 
s4 
S5 
Sl s2 S3 s4 S5 
10 4.5 1 4.5 10 
5.5 5 1.5 5 10.5 
9 3.5 2.5 3.5 9 
10.5 5 1.5 5 5.5 
10 4.5 1 4.5 10

--- Page 105 ---

Sec. 7 REGULAR MARKOV CHAINS 93 
For an independent trials process the formula for M reduces to 
M = ED. Thus for Example 7 
Si s2 s3 s4 S5 
10 5 2.5 5 10 
10 5 2.5 5 10 
10 5 2.5 5 10 
10 5 2.5 5 10 
,10 5 2.5 5 10 
In the independent case the mean time to reach sj is independent of 
the starting state. This is not true for the dependent case. In fact, 
the mean time required to reach Si from S2 is only about half that 
required for any other starting state. We observe that the mean 
time to return to a state, Mi[f<], is the same in the two examples. 
This is because these means depend only on a. 
The variances Vart[fj] are given by 
M2 - M(2ZAgD-I) + 2(ZM-E{ZM)<lg)-Ms(l. 
For Example 3a this is 
Si S2 S3 S4 S5 
66 123/4 0 123/4 66 
531/4 13 V 4 13 661/4 
66 123/4 X/4 123/4 66 
661/4 13 V4 13 531/4 
66 123/4 0 123/4 66 
For the independent trials case the formula for M2 reduces to 
M2 = E(D2- D). Thus for Example 7 we have 
Mi = S3 
Sl S2 S3 S4 S5 
90 20 15 u 20 90' 
90 20 15/4 20 90 
90 20 15/4 20 90 
90 20 15/4 20 90 
90 20 15/4 20 90 
As in the case of the means, in the independent case, Example 7, the 
variances do not depend on the starting state. Unlike the case of the 
means this is almost true for the variances in the dependent case

--- Page 106 ---

94 FINITE MARKOV CHAINS Chap. IV 
Example 3a. We note finally that, as in the case of the variances for 
the variances for f) are in each case greater for the independent 
case than for the dependent. 
§ 4.8 The general two-state case. In this section we give for future 
reference the basic quantities for Example 11 of Chapter II. We 
recall that this was the general Markov chain with two states. The 
transition matrix was written in the form 
P-r n \ d 1 —d) 
We assume that Occ^l and Ocd^l but c and d are not both 1. 
This will give us the general regular two-state Markov chain. 
The limiting vector a is 
( d c \ 
“ ~ d’ 7+d) 
The fundamental matrix Z = (I-P + A)~1 is 
Z = 
The mean first passage matrix M is 
M 
* c d 
~d~ 
1 
d 
1 
c 
c -j- d 
and the variance matrix for the first passage time is 
'c(2 — c — d) l-c \ 
Mo = 
d2 
1 -d 
d2 
d(2-c-d) 
/ 
The limiting variance for the number of times in state s;- is given by 
0 = j cd(2 — c — d) cd(2 — c — d)\ 
{c + dy (c+dy

--- Page 107 ---

Sec. 8 REGULAR MARKOV CHAINS 95 
Compare this variance for the dependent case with the independent 
case having the same limiting vector. This independent process would 
have transition matrix 
P = 
/ d c 
c + d c d 
d c 
\c + d c + d 
and the limiting variance for the number of times in S; would be 
cd cd \ 
{c + d)‘ (c + dy 
Thus the limiting variance for the number of times in si will be greater 
in the dependent case if and only if 
2 — c — d > c + d. 
That is, if the sum of the diagonal elements is greater than the sum of 
the off-diagonal elements. Or in other words, if the probabilities for 
remaining in a state have a sum greater than the probabilities for a 
change of state. 
The covariance matrix is 
cd(2-c-d) /I —1\ 
G = (c + d)* \-l 1 / 
Thus dj > 0 if i=j, but c# < 0 if i+j. The limiting correlations are 
+ 1 and — 1 in the two cases, respectively. 
Exercises for Chapter IV 
For § 4.1 
1. Find the limiting matrix A for Example 13. (See Exercise 23, 
Chapter II.) 
2. Find the limiting matrix A for Example 14. (See Exercise 24, 
Chapter II.) 
3. Show that the four-state chain in Example 12 is regular. Find the 
fixed vector a. What is the relation of this vector to the fixed vector for the 
two-state chain which determined the four-state chain? 
4. Show that if a is the fixed probability vector for a chain with transition 
matrix P, then it is also a fixed vector for the chain with transition matrix 
pn 
5. Prove that if a transition matrix has column sums 1, then the fixed 
vector has equal components.

--- Page 108 ---

96 FINITE MARKOV CHAINS Chap. IV 
6. Given a probability vector a with positive components, determine a 
regular transition matrix which will have this as its fixed vector. 
For § 4.2 
7. For Example 14 find the mean and variance for the number of times in 
state si in the first n steps. 
8. Consider the Markov chain with transition matrix 
Si s2 
Start the process in s2, and compute the mean of for n = 1, 2, 3, 4, 5, 6. 
Compare these results with a\. 
For § 4.3 
9. Find the fundamental matrix for Example 11 when c = 1/2 and d = 1]4. 
10. Find the limit of the difference between the mean number of nice 
days in the Land of Oz in the first n days, starting with a rainy day and 
starting with a nice day. 
11. Find the fundamental matrix for the chain in Exercise 8 above. 
Interpret zn — z2i. 
12. Find the fundamental matrix for Example 14. (Use the result of 
Exercise 2 above.) 
13. Find the fundamental matrix for Example 13. (Use the result of 
Exercise 1 above.) 
For § 4.4 
14. Find the mean first passage matrix for Example 14. (Use the result 
of Exercise 12 above.) 
15. Find the mean first passage matrix for Example 13. (Use the result 
of Exercise 13 above). 
16. Verify Theorems 4.4.9 and 4.4.10 for Example 13. 
17. Prove that for an independent trials process, M has all rows the same. 
18. Given that the mean first passage matrix of a chain has the form 
lx 2 6\ 
M = 14 x 41 
\6 2 x) 
determine the transition matrix. 
19. Give two different transition matrices which have the same funda¬ 
mental matrix, and hence show that the fundamental matrix does not deter¬ 
mine the transition matrix. 
20. Prove that P has constant column sums if and only if M has constant 
row-sums.

--- Page 109 ---

Sec. 8 REGULAR MARKOV CHAINS 97 
For § 4.5 
21. Find M2 for Example 14. 
22. Using the result of Exercise 15 above, find M2 for Example 13. 
23. Find Mz for Example 11 with c = 1/2 and d = 1/4. 
24. A die is rolled a number of times. Find the mean and variance for 
the number of rolls between occurrences of two 6’s. 
25. Find the mean and variance of the first passage times in Exercise 8 
above. 
For § 4.6 
26. Find the limiting covariance matrix for Example 11 with c = 1/2 and 
d = 1/4- 
27. Find the limiting covariance matrix for Example 13. Interpret the 
diagonal entries. 
28. On a nice day a man in the Land of Oz takes his umbrella with proba¬ 
bility 1/2, onarainy day with probability 1 and on a snowy day with probability 
3/4. Find the limiting variance for the number of days that he will take his 
umbrella. 
29. For an absorbing chain let n5- be the number of times in state sj 
before absorption. Using the method of proof for Theorem 4.6.1, show that 
-1? A-1 n* ■ U / ] = n/cjUji + njciriij d ij 71 11 
where N — is the fundamental matrix. 
For § 4.8 
30. Find the limiting variance of the number of times in a state when c = d. 
How does this vary with c ? Interpret your formula as c->0. 
31. Find the limiting vector and the mean first passage matrix for the 
case where c = 2d. How do these vary with c? Interpret your results as 
c-^0. 
For the entire chapter 
32. Consider the following transition matrix for a Markov chain. 
/V 2 lh 1/6\Sl 
P= 3/4 0 1/4 Isa 
\ 0 1 0 /s3 
(a) Is the chain regular? 
(b) Find a, A, and Z. 
(c) Find M and M2. 
(d) Find the covariance matrix. 
(e) Use absorbing-chain methods to find the mean time to go from s3 to 
Si. Check your answer against part (c). 
33. Let P\ and P2 be two different transition matrices for a three-state

--- Page 110 ---

98 FINITE MARKOV CHAINS Chap. IV 
Markov chain. By a random device we select one of these matrices and carry 
out the resulting chain. (Say Pi is selected with probability p.) 
(a) Is this process a Markov chain ? 
(b) Show that the probability of being in a given state tends to a limit, 
and show how these probabilities may be obtained from the fixed 
vectors of the two matrices. 
34. Suppose that in Exercise 33 we use the random device before each 
step, to decide which matrix to apply on that step. 
(a) Is this process a Markov chain ? 
(b) Show that the limiting probabilities for being in the various states are 
normally not the same as those obtained in Exercise 33(b).

--- Page 111 ---

CHAPTER V 
ERGODIC MARKOV CHAINS 
§ 5-1 Fundamental matrix. We will now generalize the results 
obtained in the last chapter. There they were proved for regular 
chains, and now we will extend them to an arbitrary chain consisting 
of a single ergodic set, i.e. to an ergodic chain. We know that such a 
chain must be either regular or cyclic. A cyclic chain consists of d 
cyclic classes, and a regular chain may be thought of as the special 
case where d= 1. The results to be obtained will be generalizations of 
the previous results in the sense that if we set d = 1 in them, we obtain 
a result from the previous chapter. As a matter of fact, in most of 
the results d will not appear explicitly, so that the result of the previous 
chapter will be shown to hold for all ergodic chains. 
An ergodic chain is characterized by the fact that it consists of a 
single ergodic class, that is, it is possible to go from every state to 
every other state. However, if d> 1, then such transition is possible 
only for special ^-values. Thus no power of P is positive, and different 
powers will have zeros in different positions, these zeros changing 
cyclically for the powers. Hence Pn cannot converge. This is the 
most important difference between cyclic and regular chains. 
But while the powers fail to converge, we have the following weaker 
result. 
5.1.1 Theorem. For any ergodic chain the sequence of powers Pn 
is Euler-summable to a limiting matrix A, and this limiting matrix is 
of the form A = £a, with a a positive probability vector. 
proof. Consider the matrix (JcI + (1 - Jc)P), for some Ic, 0<Jc<l. 
This matrix is again a transition matrix. Since it has positive entries 
in all places where P is positive, the new matrix also represents an 
ergodic chain. And since the diagonal entries are positive, it is 
possible to return to a state in one step, and hence d= 1. Thus the 
new chain is regular. 
99

--- Page 112 ---

100 FINITE MARKOV CHAINS Chap. V 
From § 4.1.4 we know that {kI+{l-k)P)» tends to a matrix A = £a, 
with a probability vector a > 0. Thus 
A = lim (kI + (l — k)P)n 
n —► oo 
A = A” J (1)^-^- (i) 
But this states precisely that the sequence Pn is Euler-summable to A 
(see § 1.10). Indeed, it is Euler-summable for every value of k. 
5.1.2 Theorem. If P is an ergodic transition matrix, and A and 
a are as in Theorem 5.1.1, then 
(a) For any probability vector tt, the sequence ttP™ is Euler-summable 
to a. 
(b) The vector a is the unique fixed probability vector of P 
(c) PA=AP = A, J ' 
proof. If we multiply (1) by tt we obtain that the Euler sum of the 
sequence nPn is nA = n^a = a, which proves (a). 
Since a was obtained from the limiting matrix of {kl + (1 — k)P) it 
is the unique fixed probability vector of this regular transition matrix. 
But this matrix must have the same fixed vectors as P, since 
Tr(kl + (1 ~k)P) = tt 
implies that 
tt( 1 — k)P = tt( 1 — k) 
and since k # 1, 
TtP — TT. 
This proves (b). Part (c) follows from the fact that P£ = £ for anv 
transition matrix, and that aP = a. J 
We thus see that a and A have nearly the same properties in the 
ergodic case as they did for regular chains ; only, (a) had to be weakened 
to summability in place of convergence. We will now show that ergodic 
chains have a fundamental matrix which behaves just like the funda¬ 
mental matrix of regular chains. 
513. Theorem P is an er9°dic transition matrix, then the inverse 
matrix Z — (I—(P — A)) 1 exists, and 
(a) PZ = ZP 
(b) Z£ = £ 
(c) aZ — a 
(d) (I-P)Z = I-A.

--- Page 113 ---

^EC- 1 ERGQDIC MARKOV CHAINS_10i 
proof. Since the sequence P» is Euler-summable to A bv 8 5 1 1 
and since (P-A)* = Pn-A by § 5.1.2(c), the sequence (P-aCis 
uler-summable to 0. Hence the inverse Z exists (see § 1 11) 
Furthermore, the series 1 ’ 
00 
/+ 2 (pi~A) (2) 1 
is Euler-summable to Z Then (b) and (c) foUow from the fact that 
It; £, al — a, and multiplying Pi - A by either £ on the right or by a 
on the left yields 0. Result (d) is obtained by multiplying (2) by 
While for the theorems so far, Euler-summability of Pn sufficed, we 
will need the following stronger results. 
5.1.4 Theorem. If P is an ergodic transition matrix, 
(a) The sequence Pn is Cesaro-summable to A. 
00 
(b) The series /+ ^ (pi~A) ts Cesar o-summable to Z. 
i = 1 
PROOF. If n = led, then in n steps after starting at st we must be in a 
state in the cyclic class of s*. And if lc is sufficiently large, we may be 
in any state in the class. Hence Pd may be thought of as the tran¬ 
sition matrix of a Markov chain with d separate ergodic sets, each of 
which is non-cyclic. Therefore, Pkd tends to a limiting matrix A0, 
whose ij-entry is 0 if st- and s; are not in the same cyclic class, and 
otherwise the ij entry is gotten by taking the components of a belonging 
to the cyclic class and renormalizing them. 
If 0^l<d, then P**+i tends to Ph40 as k tends to infinity. Hence 
the sequence Pn has these d convergent subsequences, and hence 
(see §1.10) Pn is Cesaro-summable to the average of the limits. But 
two different summation methods cannot give different answers, hence 
A must be this average; that is, 
A = (1 /d)df PIA0, (3) 
1 = 0 
and P is Cesaro-summable to A. It is then an immediate consequence 
that since P{ — A is Cesaro-summable to 0, (b) must hold. 
Let us restate the summability result (b) as a limit. 
n n_- 
5.1.5 Corollary. /+ lim -(Pl-A) = Z. 
ft —» oo 1= i ^ 
Since we have now succeeded in generalizing many basic properties 
of Z to ergodic chains, and since d did not appear explicitly, we may

--- Page 114 ---

102 FINITE MARKOV CHAINS Chap. V 
now assert that many of the results of Chapter IV hold for all ergodic 
chains. In particular this applies to all results concerning the mean 
first passage time matrix M and the variance of first passage time 
matrix M%, that is to all results in §§ 4.4 and 4.5. We also have all 
the results of § 4.6 concerning limiting variances and covariances, 
since in the proof of § 4.6.1 we needed only the summability (5.1.5) of 
the infinite series for Z, not its convergence. And thus all the basic 
formulas of §§ 4.4, 4.5, and 4.6 may be applied to any ergodic chain. 
It is_worth making a special comment on § 4.4.11. We now know 
that M determines the transition matrix of any ergodic chain by 
means of the formula P = I + (D — E)M~1. Thus, in particular, M 
determines whether (or not) the chain is cyclic. This is quite surprising, 
and it would be^highly desirable to find necessary and sufficient con¬ 
ditions (i) that M be the mean first passage matrix of an ergodic chain, 
and (ii) that it represent a regular rather than a cyclic chain. One 
would like these conditions to be simpler than computing P from M 
and then checking P. 
What results on regular chains have we not generalized so far? 
The most important such results are: The geometric estimate § 4.1.5, 
the Law of Large Numbers §4.2.1, and the results in §§ 4.3.4-4.3.6 
on To be able to discuss these we shall have to find some sort 
of an upper bound on p(nhj-a}. 
It is clear that the geometric bound of § 4.1.5 cannot apply to this 
difference in the cyclic chain, since pWy will frequently be 0, and 
hence the difference in absolute value—is oq infinitely often. How¬ 
ever it can be shown, using the ideas of the proof of § 5.1.4, that if we 
add up d consecutive terms, that is, form 
2 {P(n+l)ij ~ CLj) 
then this sum is bounded geometrically. This suffices to prove the 
Law of Large Numbers, if in § 4.2.1 we take sums d terms at a time. 
This method also allows us to prove analogues of §§ 4.3.4-4.3.6, but 
we will not take these up. 
§5.2 Examples of cyclic chains. The simplest possible cyclic chain 
is obtained from the two-state Example 11 by choosing c = d= 1. We 
will call this Example 11a. The transition matrix is 
From the proof of § 5.1.1 we know that A may be obtained as the

--- Page 115 ---

Sec. 2 ERGODIC MARKOV CHAINS 103 
limiting matrix of (i/2)/+ (i/2)P= (i/2)jE-. But this is its own limiting 
matrix. Hence 
A = (i/2)E, a = (i/2> i/2), d = 2, 
z = ( 3/2 -1/2V . /*/* H 
\-l/s »/>/ VI* 3Uj 
M = 
^2 1) 
,1 21 
Mo = (o o\ 
l0 0 
It is very easy to find M directly, and to see that M2 must have all 
components 0. Similarly, the limiting variances are 0. 
As a less trivial example we take up the random walk Example 2 
for p = 1/2 (denoted by Example 2a). Its transition matrix is 
Si / 0 1 0 0 0 \ 
s2 / Va 0 i/2 0 0 \ 
P = s3 j 0 i/a 0 1/2 0 I. 
s4 y 0 0 i/2 0 1/2 / 
s5 \ 0 0 0 1 0 / 
Starting from an even-numbered state, the process can be in even- 
numbered states only in an even number of steps, and in an odd- 
numbered state in an odd number of steps; hence the even and odd 
states form two cyclic classes. Computing the other quantities we 
find: 
Z = 
a 
16 
- (Vs, 1U> V4; ' V4, Vs) 
f 23 18 • -2 -14 -9 
9 22 2 -10 -7 
-1 2 14 2 -1 
-7 -10 2 22 9 
. -9 -14 - -2 18 23 
Si s2 S3 s4 s5 
8 
7 
12 
1 
4 
5 
15 8 
16 9

--- Page 116 ---

104 FINITE MARKOV CHAINS Chap. V 
112 
160 48 8 
It is interesting to examine some of the entries of M and of M2. 
From either end state we can go to any state only by passing through 
the neighboring state. Hence the first row of M is, with one exception, 
1 greater than the second row, and similarly for the fifth and fourth 
rows. The one exception is stepping into the neighboring state itself. 
The third row is the average of the second and fourth, plus 1, except 
for stepping into a neighboring state. 
In M2 it is worth noting the equal entries. Some of the equalities 
are due to the symmetry of the process. But this does not account, 
for example, for the third column being constant. The second and 
fourth entries are the same in this column by symmetry. The other 
three are also 8, because from one of the states in the first cyclic set 
we must enter the second cyclic set, and then the variance is 8. The 
two 0 entries are due to the fact that from an end state we always go 
to its neighbor in one step. 
It is also interesting to think of the middle column in M and M2 
as arising from making s3 absorbing, and asking for the mean and 
variance of the number of steps needed for absorption. The resulting 
process behaves in all essential features like § 3.4.1 (with p = i/2); and 
hence the numbers 3, 4, and 8 are the same as the entries of r and t2 
there obtained. 
We shall conclude by computing the covariance matrix. 
From this we obtain the limiting variances 
P = (7/32, 318, V8, 3/8, 7/32). 
The fact that c23 = c43 = 0 means that the limiting correlations 
between s2 and s3, and s4 and s^are 0. On the other hand the correla¬ 
tion between Sl and s2 is 8/^84*.87. The reason for this is fairly 
obvious from the transition matrix.

--- Page 117 ---

Sec. 3 ERGODIC MARKOV CHAINS 105 
§5.3 Reverse Markov chains. We saw in §2.1 that a Markov 
process observed in reverse order would be a Markov process with 
transition probabilities given by 
Pij(n) = ~ - *i 1 fw-l = Sj] 
Pr„[fw = 
where fn is the n-th outcome function. It was observed also that if 
the forward process is a Markov chain, the reverse process will be a 
Markov chain only if Pr„[fw = g;] does not depend on n. This will be 
the case if the process is started in equilibrium. In this case 
rra\tn _ Sj] = at for all n, and Pu(n) becomes 
Vii = Pi fin) = at 
5.3.1 Definition. Let P be the transition matrix for an ergodic 
Markov chain. Let a be the fixed probability vector for P. Then the 
reverse Markov chain for P is a Markov chain with transition matrix 
given by 
P = {ft,} = = DpT2)-i, 
To justify the above definition we must show that P is a transition 
matrix. By Theorem 5.1.2 the afs are all positive, so that pi} is 
denned and non-negative. 
P£ = DPTD-H = DPTaT = D{aP)T=DaT = (. Hence P is a tran- 
sition matrix. 
5.3.2 Definition. A Markov chain is reversible if P = P. 
5.3.3 Theorem. A Markov chain is reversible if and only if 
D 1P is a symmetric matrix. 
proof. P = DPTD-1. Hence P — P if and only if 
D-ip = PTD~ i = (D-ip)P. 
That is, if and only if D~XP is a symmetric matrix. 
A reversible Markov chain in equilibrium will appear the same 
looked at backwards as forwards. An alternative way to describe 
reversibility is the following. A process is reversible if, in equilibrium, 
for any s« and s} the probability of s« followed by sj is the same as the 
probability of Sj followed by s(. That is, if for every n, su s} 
= si Af»+i = Sy] = Pra[fn = Sj Af»+i = Sj]. 
This last equation will be true if tqpiy = ajp}i or if Pij = ajpjilai. That 
is, if pij — pij for every i, j.

--- Page 118 ---

106 FINITE MARKOV CHAINS Chap. V 
It is obvious that any periodic chain with period greater than 2 
cannot be reversible. In fact for such a chain, any state which can 
be reached on the next step could not have been the result of the last 
step. Thus only chains of period 1 and 2 can be reversible. It is 
clear that if such a chain is reversible it will have the same period. 
As an example of a reversible chain of period 1, we can consider the 
Land of Oz example. In this case we find: 
D~iP 
/2/a 0 0\ /i/» 'A VA 
0 1/5 0 I I 1/2 0 1/2 
\o 0 2/5/ \V4 V4 V2/ 
R N S 
R /1/5 1/l0 1/l0 
= N | 1/io 0 1/io 
S \1/io 1/io V5 
which is a symmetric matrix. Hence by Theorem 5.3.3 the chain is 
reversible. An example of a reversible chain with period 2 is given 
by Example 2a. In this case the matrix D~lP is 
D~iP 
(0 Vs 0 0 0 \ 
Vs 0 i/8 0 0 \ 
0 Vs 0 i/8 0 I 
0 0 Vs 0 Vs / 
0 0 0 Vs 0 / 
This is again a symmetric matrix. 
Given an ergodic chain, we now ask for the relation between this 
chain and the associated reverse chain. We shall find the relation 
between the fixed vectors and the fundamental matrices and hence, 
from these, any quantities which depend on them. We shall denote 
A, Z, M, etc. for the reverse chain by A, Z, M, etc. 
5.3.4 Theorem, 
proof. Let 
Then 
The fixed probability vector for P and P is the same. 
aP — a. 
aP = aDPTD-l 
= rjPTD-l 
= (P$)T1>~1 
= 77ZL1 
a.

--- Page 119 ---

Sec. 3 ERGODIC MARKOV CHAINS 
5.3.5 Theorem. Z = DZTJD-i, 
107 
PROOF. z = (I—P + A)~K 
From the form of A it is clear that A = DATD~1. From Theorem 
Z = (I-DPt])-i + j)atd-i)-i 
= D{I-Pt + at)~xD~x 
- DZTD~X. 
5.3.6 Theorem. Any quantity whose value depends only on Zdg 
and A is the same for the reverse process as for the forward process. 
proof. By Theorem 5.3.5, £dg=Zdg, and by § 5.3.4, A = A. 
An example of the application of the above theorem is the mean 
and variance of the first passage time to state st, if we start in s* or if 
we have as initial vector a. Similarly, the limiting variance for the 
number of times m a state depends only on Zdg and A. Hence these 
quantities are the same for the forward and reverse processes. Addi¬ 
tional examples are provided by the following theorem. 
5.3.7 Theorem. C = C. 
proof. 6ij = aAu + apn - aidij - apj 
— afajZji/ai) + afa^/af) — aid^ — a^aj 
MjZji ”f" Q'iZij CLidij — CLiCLj 
= Cij. 
Thus all results that depend only on the covariance matrix are the 
same for the reverse process. 
5.3.8 Theorem. M-M = {ZD) - (ZD)t. 
proof. M-M = (I-Z + EZdg)D -{I-Z + EZ<xg)D 
= (Z-Z)D. 
The theorem then follows from § 5.3.5. 
5.3.9 Theorem. W-W = (ZD - (ZD)T)(2ZdgD-3I) 
+ 2(Z2D-(Z2D)T). 
PROOF. 
W-W = (M-M)(2ZAgD-I) + 2(ZM-ZM)-2E(ZM-ZM)lig. (1) 
M-M = ZD-{ZD)t. (2)

--- Page 120 ---

108 FINITE MARKOV CHAINS Chap. V 
Since ZM = {Z- Z* + EZ&g)D, and ZM = (Z - Z^ + EZdg)D, we have 
ZM-ZM = {Z-Z)D + {Z*-Z*)D 
= (ZD)T-(ZD) + {Z*D)-{Z*D)T. (3) 
Since this is the difference of a matrix and its transpose, it has 0 
diagonal entries, hence 
(ZM-ZM) dg = 0. (4) 
We obtain our theorem by combining (1), (2), (3), and (4). 
We shall now illustrate the application of the above theorems for a 
process which is not reversible. Such a process is the random walk 
Example 3a. Here 
Si S2 S3 S4 S5 
51 / 0 0 1 0 0 \ 
52 f V2 0 i/a 0 0 \ 
P = S3 I 0 1/2 0 1/2 0 I 
s4 V 0 0 1/2 0 i/2 J s5 \ 0 0 1 0 0 / 
and a = (.1, .2, .4, .2, .1). From this we find 
Most of the entries in this matrix are obvious. For example, if the 
process is ever in state si it must have come from s2, hence pi2=l. 
The fixed vector for p is again a = (.l, .2, .4, .2, .1). 
In Chapter IV we found Z for this example to be 
Si s2 S3 s4 s5 
Si / .88 — .04 .32 -.04 -.12 
s2 ; I .33 .86 .12 -.14 -.17 
Z = S3 -.02 .16 .72 .16 -.02 
s4 l -» 
-.14 .12 .86 .33 
S5 \ -.12 
1 
b 
4^ .32 -.04 .88

--- Page 121 ---

Sec- 3 ERGODIC MARKOV CHAINS 
From this we find 2i = BZTD~1 to be 
Si S2 s3 s4 s5 
Si / .88 .66 -.08 — .34 — .12 
821 / -.02 .86 .32 — .14 — .02 
= S3 .08 .06 .72 .06 .08 
s4 ' l -.02 -.14 .32 .86 -.02 
S5 \ -.12 -.34 -.08 .66 .88, 
We found M to be : 
Sl S2 S3 s4 S5 
Sl 
/10 
4.5 1 4.5 10 
S2 i f 5.5 5 1.5 5 10.5 
M = sa I 9 3.5 2.5 3.5 9 
s4 ' { 10.5 5 1.5 5 5.5 
S5 \io 4.5 1 4.5 10 , 
obtain M ■■ = M+ (ZD — {ZD)T): 
Sl s2 s3 s4 S5 
We found W to be 
Sl s2 S3 s4 S5 
Sl /166 33 1 33 166 
S2 / 83.5 38 2.5 38 176.5 
CO 
02 
II 147 25 6.5 25 147 
s4 ' l 176.5 28 2.5 28 83.5 
S5 \ 166 33 1 33 166 
From this we obtain W= W + {2D- (ZD)t)(2zArD- 31) + 2(Z*L 
(Z*D)T) : 
W - 
166 1 4 49 
147 38 1 38 
130 29 6.5 29 
147 38 1 38 
166 49 4 1 
109

--- Page 122 ---

no FINITE MARKOV CHAINS Chap. V 
Hence 
m2 
66 0 0 13 66 
66 13 0 13 66 
66 13 V 4 13 66 
66 13 0 13 66 
66 13 0 0 66 
The zeros and the equal variances are easily deduced from P. 
Exercises for Chapter V 
For § 5.1 
1. Compute the limiting matrix A and the fundamental matrix for the 
ergodic chain with transition matrix 
2. Compute M and M2 for the chain in Exercise 1 above. 
3. Find the covariance matrix C for the chain in Exercise 1 above. 
4. In Example 2 let p = 2/3. Find the fixed probability vector and the 
fundamental matrix. 
5. For the example of Exercise 4 above find the mean first passage matrix. 
Check your results by obtaining P from M. 
6. Given that for an ergodic chain 
/2 3 3\ 
M = I 1 4 4 , 
\l 4 4/ 
show that the chain is cyclic. 
7. Prove that the matrix 
/ 3 4 io/3 \ 
M = 5 6 7 8 */3 3 8/3 
\10/3 4 3 / 
is not the first passage matrix of an ergodic chain. 
8. Let P be the transition matrix for an ergodic chain. Let P be the matrix 
P with diagonal entries replaced by 0’s and the rows renormalized to have 
sum 1. Show that the resulting chain is again ergodic; and if « = {cij} is the

--- Page 123 ---

Sec. 3 ERGODIC MARKOV CHAINS 111 
fhXpedfile't0r f°r t?e °riginal chain> then a = {a1{l~pjj)} is proportional to 
the fixed vector for the new chain. What is the interpretation of the 
components of the new fixed vector in terms of the original chain ? 
9. Carry out the procedure indicated in the previous exercise for the Land 
oi Uz example. 
For § 5.3 
10. Find the reverse transition matrix for the chain in Exercise 1 above. 
Compute the fundamental matrix for this reverse chain from the fundamental 
matrix tor the original chain. 
11. For which values ofp is the chain in Exercise 1 reversible? 
12. Find the reverse transition matrix for Example 2 with n = 2/3. Com- 
pute the fundamental matrix for the reverse chain and compare your result 
with the result of Exercise 4 above. 
13. Compute M for the example of the last exercise directly from the 
iundamental matrix there found. Compute M from M (see Exercise 5) 
using Theorem 5.3.8, and compare your answers. 
14. Prove that every independent process is reversible. 
15. Prove that every two-state ergodic chain is reversible. 
16. Prove that if an ergodic chain has a symmetric transition matrix 
(i.e., pij = pji), then the chain is reversible. 
17. Show for an ergodic chain that 
(?) chainis reversible, then pi}pjkpki = Pjipkjpilc. 
(b) It the transition matrix has all positive entries, then the above condi¬ 
tion assures reversibility. [Hint: Show that for fixed i the row 
vector A = {pijjpji} is a fixed vector of P. Hence this vector must be 
proportional to a.] 
For the entire chapter 
18. The general (finite) random walk is defined as follows. The states are 
numbered s0, Si, - . ., sn. If the process is in s{, then it moves to s*_i with 
Probability qi, it stays in s* with probability r*-, and moves to Sj+i with 
probability pf. (Where pt+qt+u = 1, q0 = 0, pn = 0.) 
(a) Under what conditions is a random walk ergodic ? 
(b) From the equation aP = a prove, by mathematical induction, that 
ai+iqi+l = (M'Pi- 
(c) Prove that an ergodic random walk is reversible. 
(d) Find a formula for the fixed vector a.

--- Page 124 ---

CHAPTER VI 
FURTHER RESULTS 
§ 6.1 Application of absorbing chain theory to ergodic chains. We 
have seen that the 7j matrix enables us to find, the mean and variance 
of the first passage time to state sj. Assume now that we are interested 
in more detailed behavior of the process in going to sj. For example, 
we might ask for the mean number of times that it will be in each of 
the other states before reaching sj for the first time. The answer to 
this and other similar questions is furnished by applying the absorbing 
Markov chain theory. To do this we change our process by making 
Sj into an absorbing state. The resulting process will be an absorbing 
process with a single absorbing state. The behavior of this process 
before absorption is exactly the same as the behavior of the original 
process before hitting sj for the first time. Hence we can translate 
all of the information we have about an absorbing Markov chain 
into information about our original chain. In particular it provides 
us with an alternative way to find the mean and variance of the first 
passage time from s$ to sj, these being the mean and variance of the 
time before absorption in the new process. Since any proper subset 
of an ergodic set is an open set, we can apply the results of § 3.5 to 
obtain the behavior of our process before it hits a subset for the first 
time. 
Let us illustrate the above ideas with the Land of Oz example. 
Assume that we are interested in the behavior of the process before 
the first rainy day. We make state R absorbing and have the new 
absorbing Markov chain with transition matrix : 
R N S 
R 
P = N V2 0 i/2 
S W4 1/4 V 2, 
112

--- Page 125 ---

Sec. 1 FURTHER RESULTS 113 
The basic results for this absorbing chain are obtained from the 
fundamental matrix N = (I-Q)~i where Q is the matrix obtained by 
considering only non-absorbing states. For example, let rtj be the 
number of times the process is in state s} before being absorbed. Then 
the values of Mt[n*] are given by the matrix N, in this case 
N S 
* = Nf/s 4/3l S \2/3 8/3/ 
For example, calculated from a nice day, the mean number of nice 
days before the next rainy day is 4/3. We can find Var,[n/] from the 
matrix 
No = N(2Ndg-I)-NS(l 
N S 
_ N /4/9 4 \ 
_ s \2/s 40/9/ 
Let t be the function which gives the total number of steps before 
absorption. Then, from Theorem 3.3.5, we have that the column 
vector T = {M*[t]} is given by r = N£. In the example we are con¬ 
sidering, this is 
T . f/S ^ 
'2/3 Vs/ll/ \10/3 
The function t represents in the original process the time to reach 
state R for the first time. Thus the mean first passage time to R, 
starting in state N, is 8/3, and, starting in state S it is 10/3. These values 
agree with those found in the matrix M calculated from the Z matrix 
in § 4.4. Similarly, from Theorem 3.3.5, we have that the Var<[t] is 
given by the column vector T2 = (21V — I)t — rSq. Calculating this, we 
have 
r2 = 
The vector r2 gives us the variance of the time before absorption. In 
terms of the original process this is the variance of the first passage 
time to state R. Again the above values check with those found from 
the matrix Jf2 obtained in § 4.5. 
By successively making each state absorbing we could find all the 
non-diagonal elements of M and Jf2 for an ergodic chain. However,

--- Page 126 ---

114 FINITE MARKOV CHAINS Chap. VI 
the use of the Z matrix is much more natural and convenient. We 
would normally use the absorbing methods only to obtain the more 
detailed information not available by the Z matrix methods. 
As an example of a cyclic chain we consider Example 2a. We make 
states Si and s2 absorbing. We then have 
^2 
S3 s4 S5 S3 s4 
S3 (° Va #\ S3 
(2 
2 
Q = s4 Va 0 Va | N = s4 I 2 4 
s5 \ o 1 0/ s5 u 4 
S3 s4 s5 
10 4 
12 6 
12 6 
S3 
T2 = S4 
S5 
The entries of N and N2 give the mean and variance of the number of 
times that the process is in each state before reaching s2. (The state 
Si can only be reached through s2.) The vectors r and t2 give the 
mean and variance of the steps needed to reach s2, hence of the first 
passage times. We can verify that the components of r and t2 agree 
with the corresponding entries (in the second column) of M and M2 in 
§5.2. 
As a second application of absorbing theor}^ to ergodic chains, 
consider the following problem. Assume that we have an ergodic 
Markov chain with r states and that the process is observed only when 
it is in a subset S of the states having s elements. A new Markov 
chain is obtained: A single step in the new process corresponds in the 
old process to the transition (not necessarily in one step) from a state 
in S to another state in S. Let sy and s* be two states of S. The new 
transition probability will be found by finding the probability that the 
original process starting in Sj hits S for the first time at state s}. This 
is the probability that it goes to sj in one step, plus the probability 
that it goes to a state in § and from this state enters S for the first 
time at state sp Using the results of Chapter III we can easily find 
these transition probabilities. To do this we relabel the states so 
that those in S come first. We then write the transition matrix P 
in the form 
S S 
S IT U\ 
§ U QJ

--- Page 127 ---

Sec. 1 FURTHER RESULTS 115 
The new process will be an s-state Markov chain with transition 
matrix which we denote by P. We shall now find this matrix. 
Assume a starting state in S. Then the probability of going to each 
of the states in S on the first step is given by the matrix T. To take 
more than one step, it must enter a state of S, with probabilities given 
by U. Then from a given state of S it enters S for the first time at 
state Sj with probabilities given by (I-Q)-iR (see Theorem 3.3.7). 
Putting all of this information together, we have that 
P = T+U(I-Q)~iR. 
It is easily seen that P again represents an ergodic chain. 
6.1.1. Theorem. Let a = (a4, a2, , as, as+1, . . . , ar) be the fixed 
'probability vector for P. Then ai = (ai, a2, • • . , as), normalized to 
have sum 1, is the fixed probability vector for P. 
proof. Since an ergodic chain has a unique probability vector fixed 
point, it is sufficient to prove that ai is a fixed vector for P. Let 
a2 — fis+i, • • • ; &r). Then we can write a = (on, a2). Since a is a fixed 
vector for P we have 
«i = a\T + a2R 
and 
«2 = &\U + a2Q. 
From this last equation we have a2(I -Q) = aiU or 
a2 = ai U(I-Q)-1. 
Putting this result in the first equation we have 
cci = a\T + aiTJ (I — Q)~^R 
which states that a\ is a fixed vector for P. 
As an example of the above procedure let us consider Example 6. 
The transition matrix for this random walk example is 
Si 
s2 
P = S3 
54 
55 
Si s2 S3 S4 S5 
0 V 4 V4 V 4 V4 
V 3 V 3 Vs 0 0 
0 Vs Vs Vs 0 
0 0 Vs Vs Vs 
V 4 V 4 V 4 V4 0 
The fixed vector for this Markov chain is a = (4/38, 9/38, 12/38j v/38fi/38). 
Assume now that the process is observed only when it is in si and s2.

--- Page 128 ---

116 FINITE MARKOV CHAINS Chap. VI 
Then we find the new transition matrix as follows. From the dis¬ 
cussion of this example in 3.5.5 we have 
'21/9 12/9 4/9' 
{I-QY1 = 15/9 24/9 8/£ 
9/" 9/9 12/c 7 9 79 
Thus the new transition probabilities are 
P = T+U(I-Q)~iB 
(0 
l4/3 4/3 
v« 
V4\ /1/4 V4 V 
+ 
o 
12/9 
24/9 
9/9 
5/6 
^1 °/ 2 7 17/27> 
The fixed vector for P is a = (4/i3, 9/i3) which is simply the first two 
components of a normalized to have sum 1. 
For a cyclic example, let us consider the random walk Example 2a. 
We observe the process in S = {si, s2, s3}. 
P = 
0 1 0 0 0 
V 2 0 V 2 0 0 
0 X/2 0 Va 0 
0 0 V 2 0 X/2 
0 0 0 1 0 
« - (4/8> 1/i, V4, X/4> 1ls)- 
0 O' 
1 -1/2\-x/0 0 l/2^ 
-1 [0 0 0 
« - (Vs, 2/s, 2/s). 
P differs only slightly from the first three states of P. The process 
can leave S only through s3, and must return there. Hence only p33 
is changed. We note that, in accordance with § 6.1.1, cc consists of 
the first three components of a normalized.

--- Page 129 ---

Sec. 2 FURTHER RESULTS 117 
§ 6.2 Application of ergodic chain theory to absorbing Markov 
chains. In the preceding section we saw that absorbing Markov chain 
theory could furnish us with new information about ergodic chains. 
We shall now show that certain results of absorbing chain theory can 
be obtained by using the theory of ergodic chains. 
We will need the following generalization of § 5.1.2(b). 
6.2.1 Theorem. Every Markov chain with a single ergodic set has a 
unique probability vector fixed point. This vector has positive com¬ 
ponents for the ergodic states, and zero for the transient states. 
proof. Let us write the transition matrix in canonical form. 
The matrix S is the transition matrix of the ergodic set; hence it has 
a limiting vector ai>0. Let a=(ai, ct2), where ct2 is a vector with s 
components all 0. Then we see that a is a probability vector fixed 
point of P. Conversely, let us suppose that j8= (£1, @2) is a probability 
vector fixed point. Then faQ = fa. Hence £2$” = £2, and fa = 
lim faQn — 0. Thus fa is a probability vector fixed point of S; hence 
n—► 00 
by § 5.1.2(b) we have i = ai, and /3 = a. 
Assume now that we have an absorbing Markov chain with r states, 
r — s of which are absorbing, and s non-absorbing. As usual we shall 
label the absorbing states so that they come first. The transition 
matrix then has the form : 
We now change this process into a new process as follows. Let 
7T = {pi, p2, . . . , pr) be the initial probability vector for the given 
process. Whenever this process reaches an absorbing state it is 
started over again with the same initial vector 77. The resulting 
process is a new Markov chain with transition matrix given by 
P' = 
Pl,P2, ■ ■ , Pr-s Pr-s+1, • • • ,Pr 
Pi, P2, • • , Pr-s Pr—s+l, • • • , Pr 
Pl,P2, . • > Pr-s Pr—s+1, • • • ,Pr 
R Q

--- Page 130 ---

118 FINITE MARKOV CHAINS Chap. VI 
The matrix P' is obtained by making all rows corresponding to absorb¬ 
ing states the same vector 77. Let 771 = (pi, . . . , pr_s) and 772 = 
(Pr-s+i, • • • , pr)- Then P' may be written in the form : 
P' = 
6.2.2 Theorem. The matrix P' represents a Markov chain with a 
single ergodic set. 
proof. Let I be the set of states for which 77 has positive components. 
Let J be the set of all states to which the process can go starting in I. 
It is clear that from sa, a= 1, 2, . . . , r — s, we can go only to states 
in J. However, from any state we can go to some sa, since the original 
chain was absorbing. Hence all states in J are transient. 
Since from any state we can go to an sa, and from this to all states 
in I, and hence in J, we see that J is an ergodic set. Hence the new 
process has the single ergodic set J, and each sa e J. 
Let a be the fixed probability vector for P'. Write cc in the form 
“ 7(oCl’ aa) where 0:1 = (fll’ °2> • • • > ar-s) and a2 = (ar-s+1, ar_s+2, . . . , ar). 
I hen, since ccP' = a, we have the two equations 
ai€r-sni + a2i? = «i (1) 
«l|rr-s772 + cc2Q = a2. (2) 
By §6.2.1 we know that oci>0, hence ai^r_s>0. Let a— 1 q. 
cc 2 ^ ^ 
The result d = {di, a2} will still be a fixed vector; remembering that 
«i£r-s= 1, our equations become 
771 + = di 
772 + a 2Q = a2 
From equation (2') we have 
(1') 
(2') 
«2 = 772(/ — Q)~l. 
This inverse exists because the original chain was absorbing. From 
Theorem 3.3.5 we see that «2 gives us the mean number of times in 
each of the states before absorption, for the given initial probability 
vector 77. J 
Using the result just obtained in equation (F) we have 
«1 = 77l + 772(/ — Q)~iR.

--- Page 131 ---

Sec. 2 FURTHER RESULTS 119 
The vector 77-1 gives the probability in the original process of being 
absorbed at each absorbing state on the initial step, and 7r2(/ — Q)~rR 
gives this probability for being absorbed in each absorbing state if the 
initial step is to a non-absorbing state. Hence di gives the proba¬ 
bilities for absorption in each of the given states, for the initial 
probability vector 7r. 
We thus see that the single vector d furnishes us with both absorption 
probabilities and the mean number of times in a transient state before 
absorption. This method is more economical than the method of 
Chapter III, if we are interested in a given initial probability vector. 
It must be remembered, however, that Chapter III furnishes the 
solution for any initial vector. 
Let us carry out this procedure for the random walk Example 1. 
The transition matrix is 
Sl 
s5 
P = s2 
S3 
s4 
Si s5 s2 S3 s4 
I q 0 
l 0 0 
\o p 
0 0 0\ 
0 0 0 \ 
0 p 0 J. 
q 0 p J 0 q 0/ 
Let 77= (0, 0, 0, 1, 0). 
the above procedure is 
Then the new Markov chain obtained by 
Sl S5 s2 S3 s4 
Si 
S5 
P' = s2 
53 
54 
(000 
0 0 0 
q 00 
0 0 q 
0 p 0 
This is the same as Example 3 of § 2.2, with the states reordered. 
The fixed vector is 
Thus 
a 
1 
2+p2 + q2 0q2, v2> q> L v)- 
a 
1 
p2 + q2 
{q2,p2, q. i,p). 
From this we see that, in the original process, the absorption proba¬ 
bilities are q2/{p2' + q2) for state si and p2/(p2 + q2) for state S5. The

--- Page 132 ---

120 FINITE MARKOV CHAINS Chap. VI 
mean number of times in each of the states s2, S3, S4 are q/(p2 + q2), 
l/(p2 + q2), and p/{p2 + q2) respectively. This is in agreement with the 
results found in § 3.4.1. 
If we let 7t — (0, 0, 0, 0,1), then the resulting chain is cj^clic with 
d = 2. The same is true if n— (0, 0, c, 0, d), d—l—c. We will work 
out this example. 
si /O 0 c 0 d 
S5 / 0 0 c 0 d 
P' = s2 ( q 0 0 p 0 
53 V 0 0 q Op 
54 \0 p 0 q 0 
In calculating a it is simplest not to find a first. In solving the 
equation &P' = a, the condition «i + a2= 1 is very helpful. 
« = ^2|g2 (q2 + P2qc-pq2d,p*+pqM-p\c, 
q +p2c—pqd, l-qc-pd, p + qU-pqc). 
If we let p = q = 1/2, we obtain 
« = (1/2 + 1/4(c-d), 1/2-1 U(c-d), l + i/2{c-d), 1, 1 — 1/2(c — d)). 
The first two components furnish the probabilities of absorption in 
si, s5 for the chain starting with 77. As is to be expected, the larger c 
is, the more likely it is that the process is absorbed in sx. The last 
three components furnish the mean number of times in a state before 
absorption. It is interesting to note that for s3 this is 1, no matter 
what c is. 
An interesting application of the last result can be made to ergodic 
chain theory. Let P be the transition matrix for an ergodic chain 
with fixed probability vector a. Let us make one of the states, say si, 
into an absorbing state. Then every time this process reaches si we 
will start it again with the probabilities tt = {py}. Then by the above 
result the fixed vector for this new process will give us the mean 
number of times in each state before absorption. But the new process 
is just the original process, and time before absorption means time 
between occurrences of state Si. Hence by re-normalizing a to have 
first component 1 we will obtain the mean number of times in each 
state between occurrences of state si for the original ergodic chain. 
Since Si was arbitrary, this gives us the following theorem : 
6.2.3 Theorem. Let a be the fixed probability vector for an ergodic

--- Page 133 ---

Sec. 2 FURTHER RESULTS 121 
chain. Then the mean number of times in state sj between occurrences 
of state si is ajlat. 
Note that if the transition matrix for the chain has column sums 1, 
then the fixed vector has all components equal. This means, by this 
theorem, that the mean number of times in each of the other states, 
between occurrences of a given state, is the same. 
6.2.4 Corollary. Let a be the vector obtained from a by deleting 
component l; let p be the l-th row of P with component l deleted; let Q 
be the matrix obtained from P by deleting row l and column l; and let 
N = (I — Q)~x, t = N£. Then 
— a = pN, 
ai (a) 
— = 1+pr 
ai (b) 
proof. In (a) the left side is the mean number of times in each 
of the other states between occurrences of si. The right side is the 
same quantity computed from absorbing chain theory. In (b) we 
have mu computed from regular and absorbing chain theory, respect¬ 
ively. 
6.2.5 Theorem. If Z is the fundamental matrix of an ergodic chain, 
and A is its limiting matrix, and N is the fundamental matrix of the 
absorbing chain obtained by making s/ absorbing, and we construct N* 
from N by inserting an l-th row and l-th column of all zeros, then 
Z = A + {I-A)N*{I-A). (3) 
proof. Without loss of generality we may choose i= 1. Then, 
using the notation of § 6.2.4, P and A are of the form 
P = 
and hence, 
I-P = 
/ P n p 
\ S-Qi Q 
1 -p li 
Qi-t I-Q 
A 
«i a 
aii 
I-A = 
I 1-ai 
-a ) 
\ i-id / 
N* = 0 
0 
\ 0 N 
and

--- Page 134 ---

122 FINITE MARKOV CHAINS Chap. VI 
Then 
(I-P)N* 
(/-P)N* {I-A) 
Making use of § 6.2.4, 
(aipT — pN + prd\ 
{ - dig I - £a j 
Hence 
(I-P)N* {I-A) = I-A 
[I-P + AJA + (I-A)N*(I-A)\ = A + {I-P)N*(I-A) 
= A + (I-A) 
= I. 
A + {I-A)N*{I-A) = (I-P + A)-1 = Z. 
It is interesting to note that in (3) we may use any N* obtained by 
making any one state absorbing. 
6.2.6 Corollary. If in §6.2.5 we let N = {nWij} and N£ = {tWi}, 
and nW(j = nWjt = = 0 if i = l, then 
Zij — aj + n^ij ^ — ajt^i + aj ^ (a) 
k ^ l k 7^ l 
niij = (1 laMnWjj-nWii + diri + tWi-tWp (b) 
These quantities are obtained directly from § 6.2.5, making use of 
§ 4.4.7 for (b). These formulas may be used to derive many interesting 
results. A few of these are given below. The number nWtJ is the 
number of times the process is in sj, starting in s$, before reaching s* 
for the first time. 
6.2.7 Corollary. 
^(Z).. 
(a) mij + mji = mit + —- (1 -A<*>w), for i,j ^ l. aj 
(b) niji + mij nW n (c) n(l)n aj 
% ' ' nVfi at 
proof. From § 6.2.6(b), if i, j ^ l, 
viij + mji = rriij + t(lf — (1 laj){n^fj — n^ij + dij) + mu 
l —~h(i),. — i n(,lhj — djj — W'(lfj — n(l\j-\-dij 
i3 nWj} n^jj 
Hence (a) follows. 
rriji + mij = $(,>/ + (l/a*)w<*>w-f<*>J.

--- Page 135 ---

Sec. 3 FURTHER RESULTS 123 
Hence (b) follows. 
j __ rriji + mij _ 
mij + mji n^u/ai 
Hence (c) follows. 
If in the Land of Oz example we make R 
(see § 6.1), 
V 3 4/3 
2/3 8/3 
A + (I-A)N*(I-A) 
2/5 Vs 2/s\ 
Vs Vs 2/s I 
2/s Vs Vs/ 
/ Vs -Vs 
+ - 2/s 4/s 
\-2/s -Vs 
- Vs\ /o 0 
-2/s 0 4/3 
Vs/ \0 Vs 
(86 3 
6 63 
-14 3 
absorbing, we obtain 
-Vs -2M 
- 2/s */» - 2/» 
- 2/5 - Vs 8/s/ 
as we saw in § 4.3. Using results from § 4.4 and from § 6.1, we can 
illustrate Corollaries 6.2.6 and 6.2.7. 
mNs = (1 /as){nss — nNs) + tN — ts 
= (5/2)(8/3-4/3) + 8/a -i°/3 = 8/3 
msR + niRs =- or 10/3 + 10/3 = (8/3)/( 2/s)- ' ' ' 
§ 6.3 Combining states. Assume that we are given an r-state 
Markov chain with transition matrix P and initial vector n. Let 
A = {Ai, A2, . . . , At} be a partition of the set of states. We form a 
new process as follows. The outcome of the J-th experiment in the 
new process is the set Ak that contains the outcome of the j-th step 
in the original chain. We define branch probabilities as follows: At 
the zero level we assign 
Pr„[f0 G A j]. 
At the first level we assign 
Pr„[fi g A*|f0 g Af]. 
(1)

--- Page 136 ---

124 FINITE MARKOV CHAINS Chap. VI 
In general, at the n-th level we assign branch probabilities, 
Pr„[fn g At\in~i e As A • • • Afi e Aj Afo e A*]. (2) 
The above procedure could be used to reduce a process with a very 
large number of states to a process with a smaller number of states. 
We call this process a lumped process. It is also often the case in 
applications that we are only interested in questions which relate to 
this coarser analysis of the possibilities. Thus it is important to be 
able to determine whether the new process can be treated by Markov 
chain methods. 
6.3.1 Definition. We shall say that a Markov chain is lumpable 
with respect to a partition A = {Ai, A2, . . . , Ar} if for every starting 
vector 77 the lumped process defined by (1) and (2) is a Markov chain 
and the transition probabilities do not depend on the choice of 77. 
We shall see in the next section that, at least for regular chains, 
the condition that the transition probabilities do not depend on 77 
follows from the requirement that every starting vector give a Markov 
chain. 
Let piA. = ^ Pile■ Then piA . represents the probability of moving 
k € A,- 
from state s* into set Aj in one step for the original Markov chain. 
6.3.2 Theorem. A necessary and sufficient condition for a Markov 
chain to be lumpable with respect to a partition A = {A:, A2, . . . , As} 
is that for every pair of sets A* and Ah pkA. have the same value for 
every sk in At. These common values {fuf form the transition matrix 
for the lumped chain. 
proof. For the chain to be lumpable it is clearly necessary that 
Prw[fi g Aj | f0 g At] 
be the^ same for every 77 for which it is defined. Call this common 
value pij. In particular this must be the same for 77 having a 1 in its 
*'th component, for state s* in A*. Hence pkA= Pr^D G Aj] = ptj for 
every s* in A*. Thus the condition given is necessary. To prove it is 
sufficient, we must show that if the condition is satisfied the probability 
(2) depends only on As and At. The probability (2) may be written 
in the form 
Prw'[fi G A*] 
where 77' is a vector with non-zero components only on the states of As. 
It depends on 77 and on the first n outcomes. However, if PrA[fi g Aj] = 
Pst for all sk in As, then it is clear also that PrMfi g Ad = <6., Thus the 
probability in (2) depends only on As and At.

--- Page 137 ---

Sec. 3 FURTHER RESULTS 125 
6.3.3 Example. Let us consider the Land of Oz example. Recall 
that P is given by 
R N s 
R /7a 74 74' 
P = N 72 0 7 2 
S V/4 74 72, 
Assume now that we are interested only in “good and bad 
weather. This suggests lumping R and S. We note that the proba¬ 
bility of moving from either of these states to N is the same. Hence 
if we choose for our partition A = ({N}, {R,S}) = (G, B), the condition 
for lumpability is satisfied. The new transition matrix is 
G B 
B \V4 3/4/ 
Note that the condition for lumpability is not satisfied for the 
partition A = ({R}, {N,S}) since Pna^Pnr^1^ and Psa1=Psr=1I4- 
Assume now that we have a Markov chain which is lumpable with 
respect to a partition A = {Ai, . . . , As}. We assume that the original 
chain had r states and the lumped chain has s states. Let U be the 
s x r matrix whose i-th row is the probability vector having equal 
components for states in At and 0 for the remaining states. Let V be 
the r x s matrix with the j-th column a vector with 1 s in the com¬ 
ponents corresponding to states in A j and 0 s otherwise. Then the 
lumped transition matrix is given by 
P = UPV. 
In the Land of Oz example this is 
U 
P = 
0 1 
A/2 0 
u 
0 1 
72 o 7

--- Page 138 ---

126 FINITE MARKOV CHAINS Chap. VI 
Note that the rows of PV corresponding to the elements in the 
same set of the partition are the same. This will be true in general 
for a chain which satisfies the condition for lumpability. The matrix 
U then simply removes this duplication of rows. The choice of V is 
by no means unique. In fact, all that is needed is that the i-th row 
should be a probability vector with non-zero components only for 
states in A*. We have chosen, for convenience, the vector with equal 
components for these states. Also it is convenient for proofs to 
assume that the states are numbered so that those in Ai come first, 
those in A2 come next, etc. In all proofs we shall understand that this 
had been done. 
The following result will be useful in deriving formulas for lumped 
chains. 
6.3.4 Theorem. If P is the transition matrix of a chain lumpable 
with respect to the partition A, and if the matrices U and V are defined 
—as above—with respect to this partition, then 
VUPV = PV. (3) 
proof. The matrix VU has the form 
where W1, W2, and W3 are probability matrices. Condition (3) states 
.that the columns of PV are fixed vectors of VU. But since the chain 
s lumpable, the probability of moving from a state of A* to the set A} 
is the same for all states in At, hence the components of a column of 
PV corresponding to A,- are all the same. Therefore they form a 
fixed vector for Wj. This proves (3). 
6.3.5 Theorem. If P, A, U, and V are as in Theorem. 6.3.4, then 
condition (3) is equivalent to lumpability. 
proof. We have already seen that (3) is implied by lumpability. 
Conversely, let us suppose that (3) holds. Then the columns of PV 
are fixed vectors for VU. But each Wj is the transition matrix of an 
ergodic chain, hence its only fixed column vectors are of the form cf. 
Hence all the components of a column of PV corresponding to one set 
Aj must be the same. That is, the chain is lumpable by § 6.3.2.

--- Page 139 ---

Sec. 3 FURTHER RESULTS 127 
Note that from (3) 
and in general 
P2 = UPVUPV 
= UP2V 
pn = UP*V. 
This last fact could also be verified directly from the process. 
Assume now that P is an absorbing chain. We shall restrict our 
discussion to the case where we lump only states of the same kind. 
That is, any subset of our partition will contain only absorbing states 
or only non-absorbing states. We recall that the standard form for 
an absorbing chain is 
We shall write U in the form 
where entries of U \ refer to absorbing states and entries of U 2 to non- 
absorbing states. Similarly we write V in the form 
Then, if we consider the condition for lumpability, VUPV = PV, 
we obtain in terms of the above matrices the equivalent set of con¬ 
ditions : 
V1U1V1 = V\ (4a) 
V2U2RV1 = RV1 (4b) 
V2U2QV 2 
<N1 
o> 
II (4c) 
Since U\V\ = I, the first condition is automatically satisfied. 
The standard form for the transition matrix P is obtained from

--- Page 140 ---

128 FINITE MARKOV CHAINS Chap. VI 
Multiplying this out we obtain 
/ 
P = 
U2RV1 
o 
u2qv2 
Hence we have 
R = U2RV i 
Q = u2qv2. 
From condition (4c) we obtain 
Q2 = u2qv2u2qv2 
= u2q*v2. 
More generally we have 
Qn = U 2Qn V 2. 
From the infinite series representation for the fundamental matrix 
N we have 
= I + Q + Q2+ • ■ • 
= U2IV2+U2QV2+ 
= U2{I + Q + Q2+ . . . )V2 
ft = u2nv2. 
From this we obtain 
r = U2NV2g 
T = U2N£ 
t = U2 r 
and 
£ = $R = £/2AW2L2ffFi 
£ = U2NRVi 
& = C/2JSFi. 
Hence all three of the quantities N, t, and B are easily obtained for the 
lumped chain from the corresponding quantities for the original chain. 
An important consequence of our result t-C72t is the following. 
Let A< be any non-absorbing set, and sk be a state in A*. We can 
choose the Fth row of U2 to be a probability vector with 1 in the s* 
component. But this means that U = tk for all s* in A*. Hence when 
a chain is lumpable, the mean time to absorption must be the same 
for all starting states s& in the same set Aj 
As an example of the above, let us consider the random walk example

--- Page 141 ---

Sec. 3 FURTHER RESULTS 129 
with transition matrix 
P - 
Si S5 s2 S3 s4 
1 0 0 0 0 
0 1 0 0 0 
Vs 0 0 Vs 0 
0 0 Vs 0 Vs 
0 Vs 0 Vs 0 
We take the partition A = ({si, S5}, {S2, S4}, {S3}). For this partition 
the condition for lumpability is satisfied. Notice that this would not 
have been the case if we have unequal probabilities for moving to the 
right or left. 
From the original chain we found 
S2 s3 s4 
S2 /3/2 1 Vs 
N = S3 I 1 2 1 
S4 VI2 1 3/2 
Si 
'3/4 
V® 
S2 
B = s3 
s4 
The corresponding quantities for the lumped process are 
0 
1 
0 
0 
V2 
'V2 V2 000 
P = 0 
0 
Vs 
0 
V4 3/4/ 
0 
0 
0 
V* 
0 
0 
0 
Vs 
0 
Vs 
Hi H2 H3 
1 0 0 
0 
] 
Vs 
0

--- Page 142 ---

130 FINITE MARKOV CHAINS Chap. VI 
ft = 
X/2 0 
o 1 
3/2 
Va 
1 
2 
1 
-4 2 A3 
A 
T = 
o 
l 
ft = 
ll 2 0 
0 1 
4.2 
4 3 
Assume now that we have an ergodic chain which satisfies the con¬ 
dition for lumpability for a partition A. The resulting chain will be 
ergodic. Let A be the limiting matrix for the lumped chain. Then 
we know that 
A = 
A = 
A = 
lim 
n -—> co 
lim 
n—± oo 
P+P2 + . . . +pn 
n 
tjpv +up*v + • • • +upnv 
n 
UA V. 
In particular, this states that the components of a are obtained from 
a by simply adding components in a given set. Similarly from the 
infinite series representation for the fundamental matrix £ we have 
A = UZV. 
There is in general no simple relation between M and M. However 
the mean time to go from a state in A* to the set A}, in the original 
process is the same for all states in A*. To see this we need only make 
the states of A, absorbing. We know that the mean time to absorption 
is the same for all starting states chosen from a given set. If, in

--- Page 143 ---

Sec. 3 FURTHER RESULTS 131 
addition, Aj happens to consist of a single state, then m{j may be 
found from M. J 
We can also compute the covariance matrix of the lumped process 
As a matter of fact we know (see §4.6.7) that the covariances are 
easily obtainable from C even if the original process is not lumpable 
with respect to the partition, that is, if the lumped process is not a 
Markov chain. In any case 
<?= 2 ««• k in 
l in A, 
Let^ us carry out these computations for the Land of Oz example 
For A we have 
A = 
£ = 
0 1 
iVa 0 i/ 
86/75 3/75 - 14/75\ 
6/75 63/75 6/75 
14/ 7 5 3/75 86/75/ 
/63/75 12/ 75\ 
3/ 75 72/75^ 
c = ' 12l 125 -12/l25\ 
i — 12/l25 12/] 
R 
M = N 
S 
From the fundamental matrix Z we find, 
N B 
M = 
N 15 1 
B \4 5/4)

--- Page 144 ---

132 FINITE MARKOV CHAINS Chap. VI 
Note that the mean time to reach N from either R or S is 4. Here 
N in the lumped process is a single element set. This common value 
is the mean time in the lumped chain to go from B to N. Similarly, 
the value 5 is obtainable from M. We observe that the mean time 
to go from N to B is considerably less than the mean time to go from 
N to either of the states in B in the original process. 
§ 6.4 Weak lumpability. In practice if one wanted to apply Markov 
chain ideas to a process for which the states have been combined, 
with respect to a partition A = {Ai, A2, . . . , An], it is most natural to 
require that the resulting process be a Markov chain no matter what 
choice is made for the starting vector. However, there are some 
interesting theoretical considerations when we require only that at 
least one starting vector lead to a Markov chain. When this is the 
case we shall say that the process is weakly lumpable with respect to the 
partition A. We shall investigate the consequences of this weaker 
assumption in this section. We restrict the discussion to regular 
chains. The results of this section are based in part on results of 
C. K. Burke and M. Rosenblatt, f 
For a given starting vector 77, to determine whether or not the 
process is a Markov chain we must examine probabilities of the form 
Frw[fm+1 e A*|fm e As A • ■ • Afi e A} /\fQ e A*]. (i) 
For a given 77 the process will be a Markov chain if these probabilities 
do not depend upon the outcomes before the w-th. 
We must find conditions under which the knowledge of the outcomes 
before the last one does not affect the probability (1). Let us see how 
such knowledge could affect it. Given the information in (1), we know 
that after n steps the underlying chain is in a state in A,, but we do 
not know in which state it is. We can, however, assign probabilities 
for its being in each state of As. We do this as follows: For any 
probability vector ft we denote by ft the probability vector formed by 
making all components corresponding to states not in A* equal to 0 
and the remaining components proportional to those of ft We shall 
say that ft is ft restricted to Aj. (If (3 has all components 0 in Aj we do 
not define ft.) Consider now the information given in (1). The fact 
that f0 g At may be interpreted as changing our initial vector to 77*. 
Learning then that fi e A} may be interpreted as changing this vector 
to (77iP)K We continue this process until we have taken into account 
all of the information given in (1). We are led to a certain assignment 
of probabilities for the states in As. From these probabilities we can 
1" V Burke and M. Rosenblatt, Markovian 
of Mathematical Statistics, 29: 1112-1122, 1958. 
function of a Markov chain,” Annals

--- Page 145 ---

Sec. 4 FURTHER RESULTS 133 
easily compute the probability of a transition to A* on the next step. 
Hut note that this probability may be quite different for different kinds 
of information. For example, our information may place high proba- 
bihty for being m a state from which it is certain that we move to A,. 
A different history of the process may place low probability on this 
state, ihese considerations give us a clue as to when we could expect 
that the past could be ignored. Two different cases are suggested. 
irst would be the case where the information gained from the past 
would not do us any good. For example, assume that the probability 
for moving to the set At from a state in A, is the same for all states 
m «. Then clearly the probabilities for being in each state of As 
would not affect our predictions for the next outcome in the lumped 
process. This is the condition we found for lumpability in § 6 3 2 
A second condition is suggested by the following: Assume that no 
matter what the past information is, we always end up with the same 
assignment of probabilities for being in each of the states in As. Then 
again the past can have no influence on our predictions. We shall 
see that this case can also arise. 
We have indicated above that the information given in (1) can be 
represented by a probability vector restricted to As. This vector is 
obtained from the initial vector -n by a sequence of transformations, 
each time taking into account one more bit of information. That is 
we form the sequence 
TTl = 77* ' 
7T2 — (7Tl py 
773 = (77 2-P)^ (2) 
77 m = (77^-1 P)5 
We denote by Ys the totality of vectors obtained by considering all 
finite sequences A*, Ay, . . . , As, ending in As. 
6.4.1 Theorem. The lumped chain is a Markov chain for the initial 
vector 77 if and only if for every s and t the 'probability Pr^fj e At] is 
the same for every ft in Ys. This common value is the transition 
probability for moving from set As to set At in the lumped process. 
proof. The probability (1) can be represented in the form 
Pr^[fi e A<] for a suitable /3 in Ys. To do this we use the first n outcomes 
for the construction (2). By hypothesis this probability depends only 
on s and t as required. Hence the lumped process is a Markov chain. 
Conversely, assume that the lumped chain is a Markov chain for initial 
vector 77. Let /3 be any vector in Ys. Then p is obtained from a 
possible sequence, say of length n, A,, Ay, ... , As. Let these be the

--- Page 146 ---

134 FINITE MARKOV CHAINS Chap. VI 
given outcomes used to compute a probability of the form (1). This 
probability is Pr^ffi e A*] and by the Markov property must not 
depend upon the outcomes before At. Hence it has the same value 
for every /3 in Ys. 
6.4.2 Example. Consider a Markov chain with transition matrix 
Ai 
P = 
Ax a2 
V4 V* V 2 
0 Ve 5/e 
7/s 
Let A = ({si}, {s2, s3}). Consider any vector of the form (1 — 3a, a, 
2a). Any such vector multiplied by P will again be of this form. 
Also any such vector restricted to Ai or A2 will be such a vector. 
Hence for any such starting vector the set Yi will contain the single 
element (1, 0, 0) and Y2 the single element (0, 1/3, 2/3). Thus the 
condition of § 6.4.1 is satisfied trivially for any such starting vector. 
On the other hand assume that our starting vector is 77 = (0, 0, 1). 
Let 7ti = (77-P)2 = (0, 1, 0) and tt2 = (t7iP)2 = (0, Ve, 5/6). Then 771 and 
tt2 are in Y2 and PrWl[L e AJ = 0 while Pr„a[fi e Ai] = 35/48. Hence this 
choice of starting vector does not lead to a Markov chain. 
We see that it is possible for certain starting vectors to lead to 
Markov chains while others do not. We shall now prove that if there 
is any starting vector which gives a Markov chain, then the fixed 
vector a does. 
6.4.3 Theorem. Assume that a regular Markov chain is weakly 
lumpable with respect to A = {Ai, A2, . . . , A,}. Then the starting 
vector a will give a Markov chain for the lumped process. The tran¬ 
sition probabilities will be 
Vi) = Pr«‘[fi e A*]. 
Any other starting vector which yields a Markov chain for the lumped 
process will give the same transition probabilities. 
proof. Since the chain is weakly lumpable there must be some 
starting vector 77 which leads to a Markov chain. Let its transition 
matrix be {pa}. For this vector 77 
Pr7r[f» G Aj|fn—1 G Af f\fn— 2 G Afc] = ptj 
for all sets for which this probability is defined. But this may be 
written as 
P*P-‘[f2 G Aj|fi e Ai /\fo g A*].

--- Page 147 ---

Sec. 4 FURTHER RESULTS 135 
Letting n tend to infinity we have 
P*"a[f2 G Af|fl e A* /\f0 E A*] = fi^. 
We have proved that the probability of the form (1), with a as 
starting vector, does not depend upon the past beyond the last out¬ 
come for the case n= 1. The general case is similar. Therefore, for a 
as a starting vector, the lumped process is a Markov chain. In the 
course of the proof we showed that fin for a starting vector v is the 
same as for a, hence it will be the same for any starting vector which 
yields a Markov chain. 
By the previous theorem, if we are testing for weak lumpability we 
may assume that the process is started with the initial vector a. In 
this case the transition matrix P can be written in the form 
P = UPV 
where V is as before but U is a matrix with i-th row a*. When we 
have lumpability there is a great deal of freedom in the choice of U 
and in that case we chose a more convenient U. We do not have this 
freedom for weak lumpability. 
We consider now conditions for which we can expect to have weak 
lumpability. If the chain is to be a Markov chain when lumped then 
we can compute P2 in two ways. Computing it directly from the 
underlying chain we have P2=UP2V. By squaring P we have 
UPVUPV. Hence it must be true that 
UPVUPV = UPPV. 
One sufficient condition for this is 
VUPV = PV. (3) 
This is the condition for lumpability expressed in terms of our new U. 
It is necessary and sufficient for lumpability, and hence sufficient for 
weak lumpability. A second condition which would be sufficient for 
the above is 
UPVU = UP. 
This condition states the rows of UP are fixed vectors for VU. The 
matrix VU is now of the form 
VU = 0 W2 0 
\ o 0 w3

--- Page 148 ---

136 FINITE MARKOV CHAINS Chap. VI 
where Wj is a transition matrix having all rows equal to ah To say 
that the i-t.h row of UP is a fixed vector for VU means that this 
vector, restricted to Aj, is a fixed vector for Wj. But this means that 
the components of this vector must be proportional to ah Hence we 
have 
{a*P)i = ah (5) 
This means that if we start with a, the set Y*, obtained by construction 
(2), consists, for each i, of a single element, namely ah Conversely, 
if each such set has only a single element, then (5) is satisfied and 
hence also (4). To say that Yi has only one element for each i is to 
say that when the last outcome was A* the knowledge of previous 
outcomes does not influence the assignment of the probabilities for 
being in each of the states of A«. Hence we have found that (4) is 
necessary and sufficient for the past beyond the last outcome to 
provide no new information, and is sufficient for weak lumpability. 
Example 6.4.2 is a case where (4) is satisfied. Recall that we found 
that each Yi had only one element. 
We can summarize our findings as follows: We stated in the intro¬ 
duction that there are two obvious ways to make the information 
contained in the outcomes before the last one useless. One way is to 
require that even if we know the exact state of the original process 
our predictions would be unchanged. This is condition (3). The 
other is to require that we get no information at all from the past 
except the last step. This is condition (4). Each leads to weak 
lumpability. We have thus proved : 
6.4.4. Theorem. Either condition (3) or condition (4) is sufficient 
for weak lumpability. 
There is an interesting connection between (3) and (4) in terms of 
the process and its associated reverse process (see § 5.3). 
6.4.5 Theorem. A regular chain satisfies (3) if and only if the 
reverse chain satisfies (4). 
proof. Assume that a process satisfies (3). Then 
VUPV = PV. 
Let P0 be the transition matrix for the reverse process then P = 
DPT0D~1. Hence 
VUDPTqD- iv = DPT0D~1V 
or, transposing, 
VTD-'P0DUTVT = VTD^P0D,

--- Page 149 ---

Sec. 4 FURTHER RESULTS 137 
and 
VT D~1PqDUtVt D~x = VTB-1P0. 
We observe that V^D~^D~W. Furthermore, VUD is a sym¬ 
metric matrix so that VUD = DUTVT or DUTVTD~^=VU. Using 
these two facts, our last equation becomes 
D-WPoVU = D~WP0. 
Multiplying on the left by D gives condition (4) for P0. The proof 
of the converse is similar. 
6.4.6 Theorem. If a given process is weakly lumpable with respect 
to a partition A, then so is the reverse process. 
PROOF. We must prove that all probabilities of the form 
Pra[fx g A*|f2 g Ay e Ah /\ ■ ■ ■ /\f« e A*] 
depend only on A* and A;. We can write this probability in the form 
Prg[f! g Af Af*2 e Ay Af3 e An A ■ • • Af« e A*] 
Pra[f2 G A; A fa g Ah A • • • Af» e At] 
= g Af A • • ■ A f3 e AA|f2 G A y Afi g Af] Pr^H e A{ /\f2 e Ay] 
Pra[fw g At A • • • Afa e A*|f2 g Ay] Pra[f2 g Ay] 
By hypothesis the forward process is a Markov chain, so that the 
first term in the numerator does not depend on A*. Hence this whole 
expression is simply 
Pra[U e At ff2 e Ay] 
Prg[f2 G Ay] 
which depends only on A* and Ay. 
6.4.7 Theorem. A reversible regular Markov chain is reversible 
when lumped. 
proof. By reversibility, 
P = DPTD~1 
and 
P = UPV. 
Hence 
P = U DPT V.

--- Page 150 ---

FINITE MARKOV CHAINS 138 Chap. VI 
We have seen that VTD~1 = t)~W. Hence UD = t)VT. Also 
D~lV = UTD~1. Thus we have 
and hence 
P = LvtptutT)-i, 
P = f)PTp-1. 
This means that the lumped process is reversible. 
6.4.8 Theorem. For a reversible regular Markov chain, weak 
lumpability implies lumpability. 
proof. Let P be the transition matrix for a regular reversible chain. 
Then, if the chain is weakly lumpable, 
UPPV= UPVUPV 
or 
UP(I-VU)PV = 0. 
Since U = P)VTD~1, we have 
DVTD~lP(I- VU)PV = 0, 
or, multiplying through by LM and using the fact that for a reversible 
chain D~XP = PTD~x, we have 
VTPTD~i{I-VU)PV = 0. 
Let W = D~i-D-iVU. Then W = D~1 — UTD~1U. We shall show 
that W is semi-definite. That is, for any vector f}TWfi is non¬ 
negative. It is sufficient to prove that 
2 akbh > di l ^ akbkY 
k in A< y k in A( J 
where d% is the i-th diagonal entry of 1), or equivalently 
2 Ukdib2ic ^ 2 (akdibk)2. 
k in A,- k in A( 
But since the coefficients akdt are non-negative and have sum 1, this 
is a standard inequality of probability theory. It can be proved by 
considering a function f which takes on the value bk with probability 
akdi. Then the inequality expresses that M[f2] ^ (M[f])2; and, by 
§ 1.8.5, this simply asserts that the variance of f is non-negative. 
Since W{ is semi-definite, W = XTX for some matrix X. Thus 
VtptxtxPV = 0 
or 
{XPV)T{XPV) = 0.

--- Page 151 ---

Sec. 4 
This can be true only if 
FURTHER RESULTS 
Hence 
or 
or 
Hence 
XPV = 0. 
XTXPV = 0, 
D~i(I-VU)PV = 0, 
(I-VU)PV = 0. 
PV = VUPV. 
139 
Note that while we have given necessary and sufficient conditions 
for lumpability with respect to a partition A, we have not given 
necessary and sufficient conditions for weak lumpability. We have 
given two different sufficient conditions (3) and (4). It might be 
hoped that for weak lumpability one of the two conditions would have 
to be satisfied. It is, however, easy to get an example where neither 
is satisfied as follows: If we take a Markov chain and find a method 
of combining states to give a Markov chain, we can then ask whether 
the new chain can be combined. If so, the result can be considered a 
combining of states in the original chain. To get our counterexample, 
we take a chain for which we can combine states by condition (3) and 
then combine states in the new chain by condition (4); the result 
considered as a lumping of the original chain will obviously be a 
Markov chain, but it will satisfy neither (4) nor (3). Consider a 
Markov chain with transition matrix 
V 4 1/l6 3/l6 V 2 
0 1/l2 1/l2 5/e 
0 1/l2 V12 5/e 
00 
V 32 3/32 0 
For the partition A — ({si}, {s2, S3}, {S4}) the strong condition (3) is 
satisfied. Hence we obtain a lumped chain with transition matrix 
Ai Ao a3 
A: 
r/4 V 4 XM 
*0 
II 
DO f 0 V« 5/e 
a3 \vs Vs 0/ 
But this is Example 6.4.2, which satisfies (4). Hence we can lump it 
by ({Ai}, (A2, A3}). The result is a lumping of the original chain by

--- Page 152 ---

140 FINITE MARKOV CHAINS Chap. VI 
A = ({si}, {s2, s3, S4}). It is easily checked that neither (3) nor (4) is 
satisfied in the original process for this partition. 
We conclude with some remarks about a lumped process when the 
condition for weak lumpability is not satisfied. We assume that P is 
regular. Then if the process is started in equilibrium 
Va = Pra[fM+1 g Aj\fn e A*] 
is the same for every n. Hence the matrix P = {pij} may still be 
interpreted as a one-step transition matrix. Also 
di = Pra[fw g At] 
is the same for all n. The vector a = {di} will be the unique fixed 
vector for P. Its components may be obtained from a by simply 
adding the components corresponding to each set. Similarly we may 
define two-step transition probabilities by 
p(2)(J. = Pra[fw+2 e Ay|fn g A<]. 
The two-step transition matrix will then be Pw = {pWi}). It will no 
longer be true that P2 = P<2). 
We can also define the mean first passage matrix M for the lumped 
process. ^ It cannot be obtained by our Markov chain formulas. To 
obtain M it is necessary first to find the mean time to go from 
state i to set A} in the original process. We can do this by making 
all of the elements of A* absorbing and find the mean time to absorp¬ 
tion. (A slight modification is necessary if i is in Ay.) From these 
we obtain the mean time to go from A< to Ay, by 
mu = 2 a*icmktA. 
k in A; 
where a*k is the &-th component of ah 
§ 6.5 Expanding a Markov chain. In the last two sections we 
showed that under certain conditions a Markov chain would, by 
lumping states together, be reduced to a smaller chain which gave 
interesting information about the original chain. By this process we 
obtained a> more manageable chain at the sacrifice of obtaining less 
precise information. In this section we shall show that it is possible 
to go in the other direction. That is, to obtain from a Markov chain 
a larger chain which gives more detailed information about the process 
being considered. We shall base the presentation on results obtained 
by S. Hudson in his senior thesis at Dartmouth College. 
Consider now a Markov chain with states Si, s2, . . . , sr. We form 
a new Markov chain, called the expanded process, as follows. A state

--- Page 153 ---

Sec. 5 FURTHER RESULTS 141 
is a pair of states (s<, sj) in the original chain, for which pi}> 0. We 
denote these states by sm. Assume now that in the original chain the 
transition from si to sj and from sj to sk occurs on two successive steps. 
We shall interpret this as a single step in the expanded process from 
the state s to the state s y*). With this convention, transition 
from state &W) to state sm in the expanded process is possible only if 
j = k- Transition probabilities are given by 
Or 
Pmm = Pju 
= o for j ^ k. 
P(i])(kl) = Pjl-djic. 
6.5.1 Example. Consider the Land of Oz example. The states 
for the expanded process are RR, RN, RS, NR, NS, SR, SN, SS. Note 
that NN is not a state, since pNN = 0 in the original process. The 
transition matrix for the expanded process is 
RR RN RS NR NS SR SN SS 
RR I112 X/4 1/4 o 0 0 0 o\ 
RN / 0 0 0 1/a i/2 0 0 0 \ 
RS / 0 0 0 0 01/4 1/4 1/2 \ 
NR / 1/2 1j\ x/4 0 0 0 0 0 I 
NS I 0 0 0 0 01/4 1/4 i/2 I 
SR 1 i/2 1/4 1/4 0 0 0 0 0 / 
SN \ 0 0 0 i/2 i/2 0 0 0 
SS \ 0 0 0 0 01/4 1U V2/ 
Let us first see how the classification of states for the expanded 
process compares with the original chain. We note that p(»)W)(^) = 
p^-VjkP/ci > 0 if and only if > 0. Hence if the original chain is 
ergodic, so will the expanded process be, and if the original chain is of 
period d, then the expanded chain will also be of period d. A state 
S(«) in the expanded process is absorbing only if i=j and only if state 
sj is absorbing in the original chain. 
Assume that the original chain is an absorbing chain. Let be 
a non-absorbing state in the expanded process. Since the original 
chain was absorbing, there must be an absorbing state sk such that it 
is possible to go from s} to s*. Thus it is possible to go from sW) to 
s(*A) in the expanded process. Thus the expanded process is also 
absorbing. 
It is interesting to observe that from the expanded process we can

--- Page 154 ---

142 FINITE MARKOV CHAINS Chap. VI 
go back to the original process by lumping states. For this we form 
the partition A = {Ai, A2, . . . , Ar} of the states in the extended chain, 
with Ai the set of all states of the form s^i)- Then the condition for 
lumping is that P(ki)Aj should not depend on k. But this is true by 
the Markov property for the original chain. The lumped process is 
then the same as the original chain. In our example, the partition is 
A = {(RR, SR, NR), (SN, RN), (RS, NS, SS». 
We next compare the basic quantities for our expanded process 
with the corresponding quantities for the original chain. We shall 
treat only the regular case. The other cases may be treated similarly. 
6.5.2 Theorem. Let a = {«<} be the fixed vector for a regular chain 
with transition matrix P. Let a = (cqy)} be the fixed vector for the 
expanded chain. Then 
— UiPij- 
proof. It is obvious that aipt] is positive. Also, 
y aW) = y aipn = y a} = 1. 
tii) i, i j 
Hence we need only prove that d = {aipi}} is a fixed vector for the 
transition matrix for the expanded process. That is, 
2 awPapm = am- 
(ij) 
But 
2 amPW)(ki) = y aipijpjidjic 
(W u 
= y ajpjidjk 
j 
= Pkl 
= U(kl)- 
In our example, this gives for the fixed vector 
a = (.2, .1, .1, .1, .1, .1, .1, .2). 
Note that the result we have proved is intuitively obvious, since 
represents the probability that after a large number of steps the process 
will be in state Si and then move to state s}. The probability that this 
will occur is clearly a<p^. 
6.5.3 Theorem. The fundamental matrix for the expanded chain is 
% = {zW)m)} = {d«j)(ki) + {zjk-ak)pkl}.

--- Page 155 ---

Sec. 5 FURTHER RESULTS 143 
PROOF. 
00 
zW)(ki) = dW)m+ 2 (2>(B)(«)(*!)-«(«)) 
n = 1 
oo 
= d<i))Vcl)+ 2 (P{n~1]J!cPlcl — dkPkl) 
n = 1 
oo 
= dW)(kl)+Pkl 2 (P(n)jk-dk) 
n = 0 
For our example 
RR 
Z = 
RN RS NR NS SR SN SS 
.187 .187 -.080 —.080 —.147 —.147 — .293\ 
.920 -.080 .320 .320 -.080 -.080 -.160 
-.293 -.147 .853 -.080 -.080 .187 .187 .373 
.373 .187 .187 .920 -.080 -.147 -.147 -.293 
— .293 —.147 —.147 —.080 .920 .187 .187 .373 
373 .187 .187 -.080 -.080 .853 -.147 -.293 
-.160 -.080 -.080 .320 .320 -.080 .920 -.160 
-.293 -.147 -.147 -.080 -.080 .187 .187 1.373/ 
We next consider the mean first passage times for the expanded 
process. 
6.5.4 Theorem. 
1 (Zjk — Zi/c) 
m(ij)(kl) = 
UkPkl ak 
proof. From the matrix expression for M in terms of the funda¬ 
mental matrix we have 
mW)(kl) = {d(i])(ki)—Z(i])(ki)+Z(kl)(kl)) 
= [dan (ki) —Pki(zjk - dk) 
— d(ij) ^1) +Pki{zik - dk) + 1] 
1 
d(kl) 
1 
dkPkl 
= [l~Pkl{Zjk-Zik)] 
dkPkl 
1 (Zjk Zik) 
dkPkl dk 
Again, as was to be expected, mW)m does not depend on i.

--- Page 156 ---

144 FINITE MARKOV CHAINS Chap. VI 
For our example, we obtain 
M 
RR RN RS NR NS SR SN SS 
RR 
/3 
7Vs 62/3 10 10 10 102/3 81/3 
RN / 72/3 10 9Vs 6 6 91/3 10 72/3 
RS 81/3 IO2/3 10 10 10 62/3 71/3 5 
NR 5 P/s 62/3 10 10 10 IO2/3 8V3 
NS Si/a IO2/3 10 10 10 62/3 P/s 5 
SR 5 P/s 62/3 10 10 10 IO2/3 81/3 
SN y‘/> 10 91/3 6 6 9Vs 10 72/3 
ss V/s IO2/3 10 10 10 62/3 P/z 5 
In comparison, the mean first passage matrix for the original chain is 
R N S 
R /2.5 4 3.3 
M = N j 2.7 5 2.7 
S \3.3 4 2.5 
Consider next the reverse transition matrix for the expanded pro¬ 
cess. The transition probabilities are 
Hence 
and 
P(i))(kl) = a(ki)PVci)(t}) 
a(ij) 
P(ij)(kl) = 0 if i 7^ l 
PWHki) — 
a(ki)P(ki)(ij) 
aW) 
_ U-kpkiPij 
CliPij 
_ dkPki 
a* 
— Pik• 
Hence the reverse process for the expanded process is simply the 
reverse process for the original chain expanded. 
One application of the expanded process is the following: It often 
happens that the transition matrix P for a chain is not known and 
must be estimated from data. If a large number n of outcomes for

--- Page 157 ---

Sec. 5 FURTHER RESULTS 145 
the process are known, then an obvious estimate is 
PH = 
y {n)i] 
y {n)i 
where y(n)a is the number of transitions from state s* to state sj, and 
y(»)j is the number of times the process is in state s$. To study the 
properties of this estimate it is necessary to study the properties of 
y(«)y and for a Markov chain. In particular, the limiting vari¬ 
ances and covariances for these quantities are important. We know 
that we can obtain the limiting covariances for y<re>i, yfrom the 
fundamental matrix for the basic chain. But how do we obtain the 
limiting covariances for yy^n\ki) ? We simply observe that these 
are the limiting covariances for the number of times in a pair of states 
for the expanded chain. Hence we can express these covariances in 
terms of Z and a for the expanded process. We can then use Theorems 
6.5.2, 6.5.3 to express the limiting covariances in terms of quantities 
relating to the original chain. Carrying out this computation gives : 
6.5.5 Theorem. The limiting covariances for the expanded chains 
are given by 
cW)(ki) = ctiPijPkiZjk + akPkiPijZu + akpud(ij)(ki) ~ 3aiPi}akPki- 
For our example these covariances are 
I .309 .001 
/ .001 .074 
-.012 -.033 
.001 .041 
- .065 .007 
-.012 .001 
-.065 -.026 
-.157 -.065 
-.012 .001 
-.033 -.041 
.061 .001 
.001 .074 
-.033 -.026 
.027 -.033 
.001 .007 
-.012 -.065 
-.065 -.012 
.007 .001 
-.033 .027 
-.026 -.033 
.074 .001 
.001 .061 
.041 -.033 
.001 -.012 
-.065 -.157 
-.026 -.065 
.001 -.012 
.007 -.065 
.041 .001 
-.033 -.012 
.074 .001 
.001 .309 
Exercises for Chapter VI 
For § 6.1 
1 Consider Example 2 with p = x\2. Assume that the process is observed 
only when it is in the set {s2, s3, s4}. Find the resulting transition matrix. 
Find M for the new process. What do the entries of M mean in terms ol the 
original chain?

--- Page 158 ---

146 FINITE MARKOV CHAINS Chap. VI 
2. The following table gives the probability of a team ending up in a 
certain position next year, given what its position is this year. For each 
position in the second division, calculate the mean number of years to reach 
the first division. 
First division 
Second division 
1 2 3 4 5 6 7 8 
r 1st .3 .3 .3 .1 0 0 0 0 
2nd .1 .2 .2 .2 .2 .1 0 0 
1 3rd .1 .1 .2 .2 .1 .1 .1 .1 
4th 0 .1 .1 .2 .2 .2 .1 .1 
r 5th 0 .05 .05 .2 .3 .2 .1 .1 
6th 0 0 .1 .1 .2 .3 .2 .1 
1 7th 0 0 0 0 .1 .3 .3 .3 
8th 0 0 0 0 .1 .2 .4 .3 
3. Consider a chain with a single ergodic set, which has transient states. 
Let si be a transient state and sj an ergodic state. Let f} be the time required 
to reach sj, g the first ergodic state reached and t the time required to reach 
the ergodic set. Then 
Mi[fj] = ^ Prfe = + = *]]. 
s* ergodic 
Use this result to find, for Example 4 with p = 2/3, the mean time to reach 
state si for the first time starting in state S3. 
4. It is raining in the Land of Oz. Find the mean number of days until 
each kind of weather has occurred at least once. 
5. Prove that when a process is observed only when in a subset of an 
ergodic chain, the resulting process is also an ergodic chain. 
For § 6.2 
6. Find the mean number of rainy days between nice days in the Land of 
Oz. 
7. For the Markov chain in Exercise 2 of Chapter II, assume that when the 
duel ends a new duel is started. Find the fixed vector for the resulting chain. 
Use this to determine the absorption probabilities and the mean number of 
times in each state for the original chain. 
8. Consider the chain with transition matrix 
si s2 s3 
si /0 1 0\ 
P = s2 (1/4 0 3/4 J- 
S3 \o 1 0/ 
Find the fundamental matrix Z by making state si into an absorbing state, 
calculating N, and using the result of Theorem 6.2.5. Find mi2 + m2i by 
using Corollary 6.2.7. 
9. From 6.2.5 deduce the identity 
I-A = (I-P)N*(I-A).

--- Page 159 ---

Sec. 5 FURTHER RESULTS 147 
For § 6.3 
10. For Example 2 with p = 1/2, which of the following partitions produces 
a Markov chain when lumped ? 
(a) A = ({si, s3) s5}, {s2) s4}). 
(b) B = ({si, s5}, {s2, s4}, {s3}). 
Which produce Markov chains if p ^ x/2 ? 
11. Show that Example 3 is lumpable with respect to the partition 
H = ({si, S5}, {s2, s4}, {s3}). Find the fundamental matrix for the lumped 
chain from the fundamental matrix for the original chain (see § 4.7). 
12. Find a three-cell partition which makes Example 6 lumpable. 
13. Let P be the transition matrix of an independent trials chain and a be 
any number with 0<a<l. Show that P' =a,P + (1 —a)I is lumpable with 
respect to any partition. 
14. Show that Example 12 is lumpable with respect to the partition 
A = ({sisi, s2si}, {s]S2, s2s2}). How is the resulting transition matrix related 
to the two-state chain which determined the four-state chain ? 
15. Prove that for a lumpable ergodic chain, a = aV. 
16. Give an example of a Markov chain which is not itself an independent 
trials chain, but which can be lumped to an independent trials chain. Check 
your answer by computing Z and UZV. 
For § 6.4 
17. Show that the Markov chain with transition matrix 
si /V2 1U VA 
52 V 2 V 2 0 
53 \0 1/4 3lJ 
is weakly lumpable, but not lumpable, with respect to A = ({si}, (s2, s3}). 
Find the transition matrix for the lumped process. Show that tbe reverse 
process is lumpable. 
18. Show that the Markov chain with transition matrix 
314 XU 0 Ov 
0 1/4 0 0 \ 
Vs 5/s Vs Vs 1 
V16 V8 V4 V2 J 0 Vs 3/s 3lJ 
is weakly lumpable with respect to ({si, s2, s3}, {s4, s5}). 
19 For the Land of Oz example, let Ai = {R, N} and A2 = {S}. Compute 
Pra[f2 e Ai|fi e Ai] and Prjf2 e Ax|fi e A: a f0 e Ai]. Use the result to show 
that the chain is not weakly lumpable with respect to A = (Ai, A2).

--- Page 160 ---

148 FINITE MARKOV CHAINS Chap. VI 
20. Prove that if P is lumpable with respect to a given partition, and if P 
has column sums 1, then PT is weakly lumpable with respect to the same 
partition. 
For § 6.5 
21. A coin is tossed a sequence of times. Represent this as a two-state 
Markov chain. Form the expanded process. Find M and M% for the 
expanded chain. Interpret the diagonal entries in terms of the original 
chain. 
22. Let P be the transition matrix of an independent trials chain. Find 
formulas for Z and M of the expanded chain. Check the latter against 
Exercise 21. 
23. Let P be the transition matrix of an independent trials chain. Find 
a formula for the limiting covariances of the expanded chain. [Hint : 
Write the formula in the form akai( •••)•] Use this formula to compute 
the limiting covariances in Exercise 21.

--- Page 161 ---

CHAPTER YII 
APPLICATIONS OF MARKOV CHAINS 
§ 7.1 Random walks. We will consider four simple, related random 
walks. The first three are walks on a line, with states 0, 1, . . . , n: 
| || q | j 
012 t-1 i t+1 n-1 n 
Figure 7-1 
In each of the first three types of random walks we have probability p 
of moving to the right (from i to i + 1) and probability q of moving to 
the left (from i to i - 1), for states i = 1, 2, . . . , n- 1. The three types 
differ in their behavior at the “boundaries,” 0 and n. 
AARW: A random walk having both 0 and n as absorbing states. 
APRW : A random walk having 0 as an 
absorbing state, while n is partially re¬ 
flecting.” That is, at n it moves back to 
n — l with probability q and stays at n 
with probability p. 
PPRW: A random walk partially re¬ 
flecting at both boundaries. That is it is 
like APRW at n, and at 0 it moves to 1 
with probability p and stays at 0 with 
probability q. 
The fourth random walk will move 
on a circle, with states numbered 1, 2, 
. . . , n, as in Figure 7-2. 
CRW: The process moves on the circle, 
with probabilitv p, and counterclockwise with probability q. 
149 
taking one step clockwise

--- Page 162 ---

150 FINITE MARKOV CHAINS Chap. VII 
We will illustrate each of these random walks for n — 5. Since the 
behavior is often quite different for p = V2 than for any other p-value, we 
will carry out the illustration for both p=1/2 and p — 2/3. Then we will 
solve each random walk. It will be convenient to let r stand for pjq. 
Let us first consider AARW. This is an absorbing chain with two 
absorbing states 0 and n. 
AARW for n = 5, p = l/2 
P - 
0 5 1 2 3 4 
1 0 0 0 0 0 
0 1 0 0 0 0 
V* 0 0 V2 0 0 
0 0 V 2 0 V2 0 
0 0 0 V2 0 V2 
0 V2 0 0 V* 0 
N = 
1.6 1.2 .8 
1.2 2.4 1.6 
.8 1.6 2.4 
.4 .8 1.2 
T = 
B = 
AARW for n — 5, p = 2/3 
1 
2 
3 
4 
/ 
V 
0 
.8 
.6 
4 
2 
0 
P = 
1 0 0 0 0 
0 \ 
0 1 0 0 0 
"\ 
V3 0 0 2/3 0 
0 1 0 0 V 3 0 2/s 0 / 0 0 0 Vs 0 2/3 / 
0 2/3 0 0 V-3 0 /

--- Page 163 ---

Sec. 1 APPLICATIONS OF MARKOV CHAINS 151 
N = 1/ 31 
42 36 24 
63 54 36 
27 63 42 
9 21 45, 
l/31 
The matrix I — Q has the form 
1 
147\ 
0 t 
/IB 16 
174 1 7 24 
141 / 
B = 1/31 
3 28 
78/ \ 1 30 
1 
2 
3 
4 
I-Q = 
-? 
0 
0 
-p 
1 
-q 
o 
o 
-p 
i 
°\ 0 
-p 
;/ 
when n~ 5. In general it has entries % (i, j — l, 2, . . . , n— 1) which 
are 0 except that %= 1, = — p, Sj+ij— —q. We note from the 
numerical examples that the entries of N decrease on both sides of the 
diagonal. N will have the form 
Uij = 
(rl — l)(rn_< — 1) if/ < i 
[ri—\)(rn~i — r^~i) if/ ^ i (p-q)(rn- 1) 
except where p — 1/2, here the solution simplifies to 
f j(n-i) if / < i 
[i(n-j) if j i. 
Let us verify the solution for p^1/2, by computing N(I — Q). 
i,j-th entry is 
n — 1 
Uij = -■ 
n 
(1) 
(2) 
Its 
1 
{p-q){m- 1) 2 (rk-l)(rn-{-l)skj+ 2 (H-l)(rn-i-rk-i)Skj. 
k — x + 1 
We recall that 0 only for lc=j —l,j, j +1. Suppose that / < i. 
Then all terms in the second sum are 0. The first sum simplifies to 
j'Tl—i — J 
(p-q)(rn-\) W-1 - 1)(■-P) + (rt - 1)(1) + (r*+i - 1)(■- q)\ 
f-n—i_1 
(p-q){rn- 1) 
ri 
K+iH + {p-\+q) = 0.

--- Page 164 ---

152 FINITE MARKOV CHAINS Chap. VII 
The answer for j>i is also 0. Hence all off-diagonal entries are 0. 
Let us compute the three non-zero terms for the i,i-th entry. 
\p-q)(rn- i) “ !)(-P) + “ lWn~i - 1)(1) 
+ (r*— \){rn~i — r)( — q)\ 
1 
(.p-q){rn- 
rn^l+\-q + rn~i(p~ 1+q) 
+ r*(^- 1 -f-rg') + (— p + 1 —rq) 
rn(p-q) + (q-p) = j 
(p-q)(rn- 1) 
Thus N = (/ — $)_1 as required. 
The solution for p = xf2 may be verified similarly. We can also 
obtain it by a limit process from the general solution: We write p — q 
as q(r— 1), and we let q->xj2, r—>1. 
Let us next compute r. 
n — 1 
h = .2 = (p-g)(r«-l) 
l 
^ — l)(rn_i — 1) + ^ — r^_<) 
w — 1 
j=l j = i+1 
1 
(Z>-tf)(rB- 1) (r«-i_ 1)^ 2 + (H- 1) 
(n — i)rn — + i 
(p ~q)(rn — 1) 
Or more simply, 
n —1 
(71 — i — l)rw-i — ^ 
j = i + i 
1 / i \ 
(3) 
Similarly, 
U = i(n — i) if p — xj2. (4) 
An interesting question to consider is finding the maximum value 
of fj. For p = x/2 we see that U increases till the middle, and then 
decreases symmetrically. If % is even, the mid-point i = n/2 yields 
ti — (nj2)2. For the general case we can write down the ratio of two 
terms, and find the value of i for which this ratio is one. This will not 
yield an integer in general, but the i nearest will give a maximum.

--- Page 165 ---

Sec. 1 APPLICATIONS OF MARKOV CHAINS 153 
We find the approximate solution 
*max — l°gr((r — \)n). 
UP>q\that is r then the resulting tmax is of the order of magnitude 
o (n im&x)I{p — q). Thus we see that if the absorption time is 
of a lower order of magnitude for large n than for p = i/2. For example, 
if p — 2/3, and hence r = 2, imax — \og2n, and £max is about 3(w — log2w) 
lor large n. For the very small n = 5 of our example imax = 2.3, and 
i = 2 is the maximum point, but the value tmax obtained is too large 
for so small an n. 
Finally, we shall compute B. It is sufficient to find bin, since 
= 1 bin■ We note that is 0 except for rn-i>n =p. 
bin 
n- 1 2 nikrkn = nitn-i-p = 
k= 1 
p(r4 — 1 )(rn~i — rn~ x~i) 
{p-q){rn-1) 
Hence 
y'Ti — rn—i 
hin = ~n_ 1 * Va- (5) 
Similarly, 
b^ = i/n Up = i/2. (6) 
For p = i/2 the solution is very intuitive. The probability of ending 
up at the right-hand boundary is proportional to the original distance 
from the left-hand boundary. But the solution for p^1/^ has some 
surprising features. Let us study the case p>1/2, that is r>l. We 
find that for a given starting position i the probability of ending up at 
n is not negligible for any n. Indeed, if we keep i fixed and let n 
tend to infinity, bin approaches the limit 1-r-C If i is fairly large 
this probability will be close to 1 no matter how large n is! Even 
for i =1 we have a probability (r - 1 )/r of ending up at n. The absorp¬ 
tion time in this case is about njp, surprisingly small. For p — 2j3 
this probability is 1/2- This means that if p = 2j3 we may put the right- 
hand boundary n as far out as we wish, start the process at i= 1, 
and still have a better than even chance of ending up at n rather 
than at 0. 
This particular random walk is often referred to as “gambler’s 
ruin: We may think of two men playing a certain game repeatedly 
in which player A has probability p of winning. Let i dollars be his 
original capital, n-i the fortune of his opponent, and assume that 1 
dollar is bet each time. Then H’s fortune carries out the random 
walk AARW with the given p. Absorption at n means that A ends 
up with all the money, while absorption at 0 means that he is ruined. 
We see that for a fair game (p = 1/2) the probability of ruin is equal to

--- Page 166 ---

154 FINITE MARKOV CHAINS Chap. VII 
the fraction of the two fortunes held by the opponent. But the nature 
of the solution changes drastically if player A has an advantage in 
the game. In this case he has a good chance of winning out even if his 
opponent has a much greater capital. For example, if he has p = 2/3, 
that is r — 2 (meaning the odds are 2:1 in his favor), then he has a 
better than even chance of ruining a rich opponent even if he has only 
1 dollar to start with! 
We will briefly mention two applications of this result. First of 
all, gambling houses can exist due to it. They fix the odds so that 
r > 1. Then by making sure that their original capital i is large 
enough (measured in terms of the size of one wager), they will have 
probability 1— r~f, very near to 1, of staying in business no matter 
how much is bet at their gambling tables. We also see that the 
absorption time is enormous, which is the reason that gambling houses 
have not yet acquired all the money in the world. For r near to 1, 
(3) is roughly equal to (4). We may estimate, very conservatively, 
that the gambling house can cover 10,000 bets, while the gamblers 
can provide 1,000,000 bets. Then i(n — i) is about 1010, which would 
put the absorption time into thousands of years. This leaves ample 
opportunity for the raising of new gamblers. 
A second application is to a simple model for the principle of natural 
selection in the theory of evolution. Suppose that on an isolated 
island the population of some species is fixed at n by the supply of 
food. Let us suppose that a mutant is born with a slightly better 
chance of survival than the regular member of the species. A simple 
model of the struggle for survival is given by assuming that in each 
generation the mutants gain one place with probability p>1/2 or lose 
one place with probability q. We then know that the mutants have 
probability of more than (r- 1 )/r of taking over the island. If p = .51, 
this probability is .04; if p=.G, the probability is 1/3. Hence we see 
that relatively minor advantages can result in the survival of the 
mutants. 
While this simple model serves to illustrate how mutants may take 
over a large species, the estimate for the absorption time is unrealistic. 
Even for p=.6 and n as small as 100 we obtain n/p or about 167 
generations before the mutants take over. This brings out the 
unrealistic nature of the assumption that only one place is changed in 
each generation. For a realistic time for absorption we need a more 
sophisticated model. 
We will now show that the solutions for the other three random 
walks may be obtained from AARW by various tricks. Let us first 
illustrate APRW for n = 5.

--- Page 167 ---

Sec. 1 APPLICATIONS OF MARKOV CHAINS 155 
APRW for n — 5, p = x/2 
N 
APRW for n = 5, p = 2/3 
0 1 2 3 4 5 
1 0 0 0 0 0 
Va 0 Va 0 0 0 
0 Va 0 V2 0 0 
0 0 Va 0 Va 0 
0 0 0 Va 0 Va 
0 0 0 0 Va V* 
2 
4 
6 
6 
6 
N 
2 
4 
6 
8 
8 
0 1 2 3 4 5 
1 0 0 0 0 0 
Vs 0 2/3 0 0 0 
0 V 3 0 2/3 0 0 
0 0 Va 0 V 3 0 
0 0 0 Vs 0 V 3 
12 
18 
21 
21 
21 
The N matrix for APRW may be obtained from AARW through 
the following observations. Consider i >j. If the process 
n 
Figure 7-3

--- Page 168 ---

156 FINITE MARKOV CHAINS Chap. VII 
starts in i, it must eventually enter j. Hence hij = nij/njj= 1, and 
Uij — njj. (This is conspicuous in N above.) If k<j, then hkj may be 
gotten as the probability of ending up at j in AARW with n —j. Hence 
nkj = hkf/ijj = bkjUjj, with bjcj from AARW. Thus it suffices to find Ujj. 
Let us first compute the case p — 1! 2. We note that Q (except for the 
last row and column) is a submatrix of Q for larger n. And in the 
larger Q these rows and columns are filled out with 0’s. Hence is 
independent of n. But as we let n-^co in AARW, the difference 
between it and APRW disappears. Hence rijj for APRW is the limit 
as n-^co of njj for AARW. Thus 
nii = 2j if i ^ j 
riij = - • 2j = 2i if i < j 
> for p 
The same argument is applicable if r < 1. Thus 
V 2- (V 
Hi] — 
Uij 
ri~ 1 
p-q 
yj — fj—i fj — l 
ri —l p — q 
fl - fj—i 
p-q 
if i > j 
if i < j 
for p ^ V2. (8) 
We can also obtain wy for i^j from N for AARW by letting n-^oo. 
For r > 1 the above argument breaks down, since no matter how large 
n is in AARW, the probability of ending up at n does not become 
negligible. But here we make use of the fact that if in AARW we 
renumber state i as n — i, we have the original process with p and q 
interchanged. We then find that the above formulas also hold for 
r > 1. 
For t we obtain 
ti — (2n — % 1 )^ 
J ^71 + 1  i~\~l 
ti -----i 
p—q L r—1 
if P = V 2; 
if P ^ V2. 
(9) 
(10) 
Since there is only one absorbing state, all absorption probabilities 
are 1. 
In both AARW and APRW there is a simple relation between n^ 
and 7iji. In fact it may be verified from our formulas for these quan¬ 
tities that 
riij = ri-iriji. 
We shall see that there is a simple probabilistic proof of this fact. 
Assume that j<i. Let d = i—j. Then any path which allows the 
process to go from i to j in n steps must take d more steps to the left

--- Page 169 ---

Sec. 1 APPLICATIONS OF MARKOV CHAINS 157 
than to the right. Hence the probability that the process starting at 
i will follow such a path is 
rpkqk+d 
where 2k + d = n. On the other hand each such path, looked at back¬ 
wards, may be considered a path from j to i. For the process to follow 
this path it must make d more steps to the right than to the left. 
Hence the probability that, starting at j, the process will follow this 
path is 
jpk+dqk * 
There are the same number of paths from i to j in n steps as there 
are from^' to i, but the ratio of the probability for each path is 
rpknk^rd nd 
£__i-- — _ — fj-i' rpk~\ dqk jpd 
Hence, p(n)ij = ri~ip(~n’>ji. But then 
ntf 
00 
2 p(»)y = ri~* 
n = 0 
CO 
2 P(n)ji = ri-%{. 
« = 0 
Note that this also shows that when p—1/2, then r= 1, n^ — n^. 
We now turn to the regular chains, starting with PPRW. 
PPR W for n = 5, p = 1l2 
P = 
a = (1/e 
M = 
0 1 2 3 4 
nl2 V2 0 0 0 
V* 0 lh 0 0 
0 X/2 0 V2 0 
0 0 V 2 0 V2 
0 0 0 V2 0 
0 0 0 0 V2 
v«. Ve, 1 /•. b> Ve) 
0 1 2 3 4 
1 6 2 6 12 20 
10 6 4 10 18 
18 8 6 6 14 
24 14 6 6 8 
28 18 10 4 6 
,30 20 12 6 2

--- Page 170 ---

158 FINITE MARKOV CHAINS Chap. VII 
V3 
0 
V 3 
0 
0 
0 
2 
0 
2/3 
0 
Vs 
0 
0 
3 
0 
0 
2/3 
0 
Va 
0 
4 
0 
0 
0 
2/3 
0 
X/3 
if = 
2/63, 4/63, V 63; 16/63) : 32/63) 
0 1 2 3 4 5 
1 63 3/2 15/4 51/s 147/l6 387/32' 
93 63/2 9/4 39/8 123/l6 339/32 
138 45 63/4 21/s 87/l6 267/32 
159 66 21 63/8 45/l6 483/32 
168 75 30 9 63/l6 93/32 
171 78 33 12 3 63/32/ 
We may obtain APRW from PPRW by making 0 absorbing. Thus, 
if we know a for PPRW, we may obtain M from N of APRW by 
Corollary 6.2.6, 
wy = — (rijj — rut) + k — tj for i j. Ct'i 
l 
n + 1 V- If p —1/2, we have column sums 1, and hence a 
obtain 
n+ 1 i =■ j' 
mi} = - (2»-»+l)i-(2»-i; + l)j i > jl 
j(j+!)-»(» +1) »<ij 
For the above example suggests that o<+i = ra<. Indeed, 
this yields a fixed vector, as can be seen from writing our equations as 
V - Vs 
Thus we 
(11) 
qao + qai = ao or a\ — mo 
V pai-\ + qa,i+\ — at or -cii+i + qrai-i = a{ 
pan-x+pan = an or an-1 = - an. 
r

--- Page 171 ---

Sec. 1 APPLICATIONS OF MARKOV CHAINS 159 
Thus 
Then 
di = rn + l . rx. 
mi) = 
rn+l_l 1 
ri r — 1 
jrfl—j+l  fpTl— 4*+l 
p-q L 
i 
r— 1 -(i-j) 
p-q L 
{j-i - 
fj — ft 
i = j 
i > j 
i < j 
r p * V2- (i2) 
(r — 1 )rx+J 
Let us compare m0n with mn0. If p = 1/2, m0n = mno = n(n+1), 
which is of the order n2 for large n. But if p ^ */2, we observe a com¬ 
pletely different behavior. Say r > 1, then 
mon — 
1 
n rn— 1 
p — q L (r— \)rn 
is roughly 
p-q ■ n for large n. Hence it takes a surprisingly short 
time to go “all the way” in the favored direction. 
'din 0 — 
p-q 
rra+l — < 
r — 1 ■n 
which is roughly 
(P- 
■ rn for large n. Since r > 1, this increases 
q){r- 1) 
exponentially in n. For very large n it will take a tremendously 
long time to go “all the way” in the wrong direction. This type of 
behavior is typical of random walks. We will see another example of 
this in the Ehrenfest model. 
Finally we consider CRW. 
CRW for n = 5, p = 1/z 
1 
2 
P = 3 
4 
5 
1 
2 
M = 3 
4 
5 
2 
Va 
0 
V* 
0 
0 
(546 
4 5 4 
6 4 5 
6 6 4 
4 6 6 
3 4 
0 0 
V2 0 
0 V2 
V2 0 
0 1/2 
6 4 
6 6 
4 6 
5 4 
4 5

--- Page 172 ---

160 FINITE MARKOV CHAINS Chap. VII 
CRW for n = 5, p = 2/3 
2 
V3 
0 
Vs 
0 
0 
3 
0 
2/3 
0 
Vs 
0 
4 
0 
0 
2/s 
0 
V3 
1 
/ 5 
78/31 141/si 174/31 147/31 
21 f 147/si 5 78/31 141/si 174/31 
3 
174/si 147/s1 5 78/sx 141/si 
4 ' l 141/si 174/31 147/31 5 78/31 
5 \ 78/si 141/si 174/31 147/31 5 , 
This process always has a = -ri, since P always has column sums 1. n 
This is also obvious from the fact that no position in the circle is 
distinguished. For the same reason we expect that my should depend 
only on how far i and j are apart (and on the direction if ^^i/2). 
This is certainly so in the examples above. 
If state n is made absorbing in CRW, we obtain AARW—with 0 
and n identified. Hence M for CRW is obtainable from N for AARW. 
But there is an even simpler method. If we want my, we renumber 
the states so thatj becomes n, and then my is just an absorption time 
for AARW. Specifically, if the distance from j to i (clockwise) is d, 
then my = t&. Thus 
my = n 
1 C fn _ rn—d ^ 
mij -7 (13) 
my = d(n-d) p = i/2, i ^ j 
where d is the clockwise distance from j to i. 
The remarks made for r of AARW are applicable here. Thus, for 
example, the distance (clockwise) that it takes longest to travel is 
approximately logr((r- 1 )n). We also note that my is generally of a 
lower order of magnitude for any p^1/2 than for p = lj2. 
Let us next find the transition matrices for the reverse processes 
of PPRW and CRW.

--- Page 173 ---

Sec. 2 APPLICATIONS OF MARKOV CHAINS 161 
For PPRW, remembering that ai+i = ra<, 
„ at+i 
Vi,i+1 = —— Pi+i,i = rq = p = Pi,i+i CLi 
« Ui 1 Pi+i,i = -—Pi,i+i = -p = q = Pi+i,i- 0-1+1 r 
Hence the process is reversible, contrary to one’s intuition. But for 
CRW ai+i = ai, hence 
«<+l „ 
Pi,i+1 = -Pi+i,i = 2 a< 
„ ai 
Pi+i,i = -— Pi,i+i = P- 
®i+1 
Hence the reverse pi'ocess is a CRW with p replaced by q, as one would 
expect. It is reversible only when p = q — rl2- 
Let us close by giving a practical application for PPRW. We 
consider a gambling house that wishes to keep a close check on its 
roulette wheel. Suppose that it is a wheel having in addition to the 
numbers 1 to 36, half of which are red and half black, the numbers 0 
and 00 which are not colored. We will devise a simple automatic 
check to see that the house is taking its share of the bets on red. We 
set up an electric counter which starts at 0 and adds 1 every time red 
comes up, subtracts 1 every time red fails to come up. The counter 
does not go below 0. If the counter reaches a specified number n, 
then it rings a bell, and the house changes the wheel. 
If the wheel is properly balanced, then we have PPRW with 
p= 18/38 = 9/19. Hence it will take a very long time to reach n. The 
house adjusts n so that m0n corresponds to its normal periodic servicing 
of the wheel. However, if the wheel fails to function properly—for 
example, if p rises to 1/2, in which case the house no longer makes a 
profit—then the bell will ring much sooner. A similar check on black 
will assure that the house continues to make its profit on all bets. 
Let us consider a concrete example of this. Suppose that n — 40 is 
selected. Then m0n for proper functioning is about 18,000. If the 
wheel is turned 400 times in a day, the bell will ring on the average 
once in 45 days, allowing for normal servicing. But if p rises to x/2, 
then mon = 1640, and the bell will ring after four days. If p rises above 
the break-even figure (that is, the house is losing money), then the bell 
will ring very quickly. 
§ 7.2 Applications to sports. Let us apply some of our results to the 
game of tennis. We will first consider the problem of a single game of 
tennis played between two players. We will assume that player A

--- Page 174 ---

162 FINITE MARKOV CHAINS Chap. VII 
has probability p of winning any given point, and player B has 
probability q^p. 
If we keep score in the ordinary manner, there are 20 possible scores 
during a game. These are: 0-0, 0-15, 15-0, 0-30, 15-15, 30-0, 0-40, 
15-30, 30-15, 40-0, 15-40, 30-30, 40-15, 30-40, 40-30, Advantage B, 
Deuce, Advantage A, Game B, Game A. However, it is easily seen 
that we may lump the following pairs: {30-30, Deuce}, {30-40, 
Advantage B), {40-30, Advantage A). The resulting random walk is 
then represented by Figure 7-4. 
The game of tennis may conveniently be broken down into two 
stages. At the beginning the process goes through some of the lower 
twelve states, always moving up, and in four or five steps it arrives at 
one of the five states in the top row. This we will refer to as the

--- Page 175 ---

Sec. 2 APPLICATIONS OF MARKOV CHAINS 163 
preliminary process. The preliminary process is of an extremely simple 
nature. It is followed by a random walk of type AARW, with n = 4. 
The state Game B is the absorbing state 0 of AARW, and Game A is 
the absorbing state n. 
We will describe the entire process as an AARW random walk, with 
initial probabilities furnished by the preliminary process. These initial 
probabilities tt = (co, c\, c2, c3, C4) are given by an elementary proba¬ 
bility calculation. We find that 
c0 = q4(l + 4p), Ci = 4p2q2, c2 = 6p2q2, 
c3 = 4p3q2, c 4 = p4(l + 4q). 
If p = i/2} c0 = 3/16, Ci = v8, c2 — 3/8) c3 = 1J8, c4 = 3/i6- Using the basic 
quantities of AARW with n — 4, 
iV = 
T = 
v-q 
;r4-l) 
1; 
^*4_|*3 
1 
r4- 1 
r4 —r2 c 
r4- 1 
1 
1 3 
(r — l)(r3— 1) 
(r- l)(r2 — 1) 
(r-1)2 
{^4} 
(r—l)(r3 —r) (r— l)(r3 — r2) 
(r— l)2 (r2—l)(r2 —r) 
(r— l)(r2 — 1) (r— l)((r3— 1) 
/3/2 1 V*\ /3 
iV = ( 1 2 1 £ = I 4 
\1/2 1 W \3, 
/Vl\ {M = p = lh 
we can find all interesting quantities. The most interesting one is, 
of course, the probability that A will win. For p = 1/2 we obtain 
(3/ie) - 0 + (i/s) • (V4) + (3/s) - (V2) + (Vs) - (3/4) + (3/ie) -1 = V*. 
This was to be expected, by symmetry. If p > V2, we obtain 
_i— [g4( 1 4- 4p). 0 + 4p2q3(r4 - r3) + 6p2g2(r4 - r2) + 4p3q2(r4 - r) 
+ p4(l + 4g)(r4 — 1)] 
which simplifies to 
Pa = 
p4(l- I6g4) 
p4-q4

--- Page 176 ---

164 FINITE MARKOV CHAINS Chap. VII 
For example, A p = .5l, then pA = .525, and if p = .6, then pA = .136. 
For the absorption time and the number of times in a state we will 
carry out the computation only when p = 1/ 2. The interesting cases 
are ones where p is near 1/2, and absorption times do not depend very 
drastically on p. 
When p = !/2 we find that the mean number of times in the three 
interesting transient states is: 1 for Deuce, and 5/s for Advantage B 
and for Advantage A. The absorption time is 9/4. To find the actual 
length of the game, we must also take into account the preliminary 
stage. If we do, we find that the mean length of a tennis game 
between equally matched opponents is 27/4= 63/4. Since the minimum 
length of the game is 4, this shows that for equally matched players 
the average length is not much above the minimum. 
Should it prove from records that games actually are much longer 
than this, and that the average number of times in Deuce is well above 
1, as seems to be the case, then it would indicate that the present model 
for tennis is too simple. Perhaps a player “plays harder” when he is 
behind. This would lead to a somewhat more complicated random 
walk. 
Let us return to the probability that the better player A wins. This 
is always greater for a game than for an individual point. Thus, the 
game serves to magnify the difference between the two players. This 
is further magnified since several games are played in a set, and several 
sets in a match. The probabilities for a set, in which a player must 
win at least six games, but by a margin of at least two, may be com¬ 
puted just as above. We are led to the same AARW, but with a 
longer preliminary stage. A match is won by the first player winning 
three sets. This is a straightforward computation. The following 
figures will illustrate the magnification achieved in sets and 
matches: 
p = .51 P = •' 
Probability of winning point .510 .600 
Probability of winning game .525 .736 
Probability of winning set .573 .966 
Probability of winning match .635 .9996 
Thus there is always a good chance that the better player will win 
the match. And if there is a fairly significant difference between 
players, then it is practically certain that the better player wins. 
Let us compare these results for tennis with the World Series in 
baseball. Here the team that first wins four games is declared winner. 
If we assume that team A has probability p of winning anv one game.

--- Page 177 ---

Sec. 2 APPLICATIONS OF MARKOV CHAINS 165 
then we find that 
Pa = p4(l + 4g + 10g2 + 20g3), 
where the various terms correspond to series of 4, 5, 6, and 7 games 
respectively. If p — .51, Pa = -522, while if p—.6, then ^ = .710. 
The World Series also magnifies differences between teams, but not 
nearly as well as a match of tennis—or even a single game of tennis. 
If we compute the mean length of a series, we find this to be 
t — 4:(pi + q4) + 20(pAq + pq'i) + 60(p/lq2 + p2q'l)+ 140(p4g3+ p3g4). This is 
largest (5.81) when p = 1/2, and decreases monotonically to 4 as p is 
increased to 1. Hence we should be able to estimate p from the 
observed length of World Series. In the 50 World Series played under 
the stated rules from 1905 to 1957, 10 ended in four games, 13 in five 
games, 12 in six games, and 15 in seven games. This yields a mean 
length of 5.64, which would agree very well with p = 5/s- This would 
suggest that the teams playing in the World Series have, on the average, 
not been matched too closely. 
Let us now consider the efficiency of various procedures in magnifying 
the differences between players or teams. We have found that tennis 
(one game) gives slightly more magnification than the World Series, 
but it also requires more steps on the average. To be able to compare 
the efficiency of two rules, we will have to take them so that the mean 
length of a series is the same. 
Tennis may be compared to the World Series as follows: The latter 
requires four wins, while the former requires that the winner have 
four wins and be ahead by two. This is really a hybrid between two 
types of procedures. The pure procedure would be to require that the 
Winner end up ahead by four points. It can easily be seen that this 
rule gives much more magnification than the other, but also requires 
a great deal more time. Let us therefore consider two classes of 
rules. 
Rule Wn : The first person to win n points is declared winner. 
Rule An : The first person to get ahead of his opponent by n points is 
declared winner. 
We will compare these two rules, selecting n in each case so that the 
mean length of a game be a given large number, which we will denote 
by N2, and seeing how much they magnify differences. Since we will 
be interested particularly in large n, we will allow asymptotic approxi¬ 
mations. Since we are particularly interested in magnification of very 
small differences, we will let the players differ by e, that is let p = 
(l + e)/2 and g = (l-e)/2, and compute the final difference just to the 
first order term in e.

--- Page 178 ---

166 FINITE MARKOV CHAINS Chap. VII 
The following identities will be useful : 
(n + k\ /2n+l 
k ) \ n 
n 
k = n 2 77 + 1 
2 & = o 
A /n + k 
k = 0 l 
2n+ 1 
n— 1 
2n + 1 I2n 
\ «■ / \-/ 2« \ n 
If we use Wn, the probability that the better player wins is 
l + e\* 'A1 /n-l+k\/l-e 
”A (n-l+k\ 
pa=pI\ * y 
n — 1 / 
.?.( 
■f ( jfc = 0 \ 
2 
1 + ne 
2« 
1 +ne 
2n 
k M 2 
71— 1 + //l\fc 
2»~i — W2«-1 2n — 1 /2t7 — 2 
2«-! \ 71 — 1 
Using Stirhng’s formulaf and simplifying we obtain 
1 n 
Pa*2 + J-,e 
The expected length of the game (for which we may use p = 1/2) is 
V (n-\+k\ 
k ) 
2n—\t2n—2^ 
«-i /? 
t = 2.(*/,). y 
k = 0 \ 
(» + *)(Va)* 
(Va)”-1 
2 
2n 
V 
n2n~1 + n2n-'1 
■ V 71. 
2"-l \ 77- 1 
77 
We will simply use tx 2n. Thus if we want txN2, we must choose 
N2 A 
n = -- This yields pAx^2 + —€> pA-pBxNand thus a 
magnification factor is N - or about .8N. 
V 77 
t See W. Feller, Introduction to Probability Theory and Its Applications, John Wiley 
& Son, Inc., New York, 1957, Chapter 2.

--- Page 179 ---

Sec. 3 APPLICATIONS OF MARKOV CHAINS 167 
The method An may be represented as AARW from 0 to 2n, starting 
at n. Hence 
Pa 
n_if-n 
r2n — 1 
Here we find that the terms in e cancel and that e2 terms must be 
carried. We find 
rn x l + 2ne + 2(n + n2)e2 
r2n X 1+ 4we + 4(?i + 2w2)e2 
2ne + 2(n + 3n2)e2 l!<z + x/2( 1 + 3n)e 
PA ~ 4ne + 4(7i + 2w2)e2 = l + (l + 2w)e' 
~ V2+2 e- 
For the mean length we find (using px1/2) t = n(2n — n)^n2. Thus 
N 
we choose n = N, and obtain Pa~1/2 + ~2 e, Pa~Pb~Ne; yielding a 
magnification factor of N. 
We thus find that W^12 and AN are comparable, in the sense that 
each yields an approximate mean time of N2. The former magnifies 
minute differences by about .8N, while the latter multiplies them by N. 
Thus the An rule is more efficient. Any mixture of the two, as in 
tennis, will lie in between, and hence will also be less efficient than An. 
§ 7.3 Ehrenfest model for diffusion. There is a simple model for a 
system of statistical mechanics which is due to T. Ehrenfest. In this 
model we consider a gas which is contained in a volume that is divided 
into two regions A and B by a permeable membrane. We assume that 
the gas has s molecules. At each instant of time a molecule is chosen 
at random from the set of 5 molecules and moved from the region that 
it is in to the other region. We are interested in the way in which the 
composition of the two regions changes with time. For example, if 
we start with all the molecules in one region, how long on the average 
will it be before each regions has half the molecules ? Such questions 
can be answered by using the methods of Markov chains. 
We form a Markov chain as follows: We assume first that the mole¬ 
cules are identifiable. We take as states a vector y = (x\, X2, , xs) 
where Xj is 1 if the j-th molecule is in region A, and 0 otherwise. Know¬ 
ing the state tells us the exact composition of A and hence also of B. 
There are 2s states. If the process is in state y, then choosing a molecule 
at random and moving it to the other region means that we change 
the state y to a state 8 by simply changing one coordinate of y. It is 
clear that from y there are s states to which the process can mo’\ e and

--- Page 180 ---

168 FINITE MARKOV CHAINS Chap. VII 
these transitions occur each with probability 1/s. It is possible to go 
from any state y to any other state 8 in a sequence of steps, but it is 
possible to go from y to y only in an even number of steps; and it is 
possible in two steps. Hence we have an ergodic chain with period 2. 
It is clear also that we can go from y to 8 in one step if and only if we 
go from 8 to y in one step. If possible, the probability in each case is 
1/s. Hence the transition matrix is symmetric. This tells us two 
things. First, the process is a reversible process. Secondly, the fixed 
probability vector is a constant vector with components 1/2*. 
The transition matrix for the case s — 3 is 
The fixed vector is a-(Vs, Vs, Vs, Vs, Vs, x/s, Vs, Vs)- From this 
we see, for example, that the mean number of steps required to return 
to any one state is 8. We also 
have, by Theorem 6.2.3, that 
the mean number of times in 
each of the states between 
occurrences of a particular 
state is 1. In general, for a 
chain with s states the mean 
time to return to a state will 
be 2s and the mean number of 
times in state 8 between occur¬ 
rences of a state y is 1. 
There is a simple random 
walk interpretation for the 
process we are considering. 
The vectors y are the corner 
points of an s-dimensional cube. The possible states to which the 
process can move from y in one step are the corner points connected 
to y by an edge. There are s such points, and the probability of

--- Page 181 ---

Sec. 3 APPLICATIONS OF MARKOV CHAINS 169 
moving to any one is 1/s. For the case s — 3, we have the cube in 
Figure 7-5. 
We define the distance between two states y and 8, denoted by 
d(y, 8), to be the minimum number of steps required to go from y to 8. 
In terms of the coordinates of y and 8 this is 
S 
d(r>8) = 2 \xi-yA- i = 1 
It is clear from the random walk interpretation that the mean time 
to go from y to S depends only on the distance between y and S. Let 
msd be this mean time for two points a distance d apart. For fixed s, 
we compute md — msd as follows. Let y and 8 be two points a distance 
d apart. This means that they have exactly d coordinates different. 
On one step there are d choices which will make the process one unit 
closer to 8, and s - d choices which will make it one unit farther from 8. 
Hence, considering the possible first steps, we have 
d s — d 
md = 1 + - md-i H-md+i, 0 < d < s, 
where we let ?»o = wis+i = 0. These equations have a unique solution 
which may be written as follows. Let 
then 
d 
md - 2 Qss-i> 0 < d < s. 
i = 1 
The values of Qst for values of s up to 6 are given in Figure 7-6. 
VALUES FOR Qst 
s 
i 1 2 3 4 5 6 
0 1 1 1 1 1 1 
1 3 2 12/3 n/2 l2/5 
2 7 3 2/3 22/3 21/ 5 
3 15 6x/2 H/s 
4 31 112/5 
5 63 
Figure 7-6

--- Page 182 ---

170 FINITE MARKOV CHAINS Chap. VII 
For values of s up to 6 the values of msd are given in Figure 7-7. 
VALUES FOR msd 
s 
1 2 3 4 5 6 
1 1 3 7 15 31 63 
2 4 9 182/g 37l/2 742/5 
3 10 201/3 40l/6 783/5 
5 2F/3 412/g 804/5 
5 422/3 821/5 
6 831/s 
Figure 7-7 
We note that the means for a given s increase as we increase the 
distance. This is to be expected. However, they increase very slowly. 
We shall call the above chain the microscopic chain. In physical 
applications this chain is often not as interesting as one obtained 
from it by lumping. The macroscopic chain is a Markov chain obtained 
from the microscopic chain by lumping all states having the same 
number of molecules in region A. Let us verify that the condition 
for lumpability is satisfied. Let V» be the set of all states in the 
microscopic process with i molecules in region A. The set V» has 
j elements. From any element of V* the process moves to one of 
elements of V<+i or to one of elements in V4_i. The 
probability of moving to V*+i is (s-i)/s and the probability of moving 
to Vj_i is i/s. These probabilities are the same for all elements of Vi. 
Hence the condition for lumpability is satisfied, and we obtain a new 
Markov chain with states Vo, Vi, V2, • • • , V«. The transition proba¬ 
bilities are 
Pi,i+i = 1 — i/s 
Pi,i-i = i/s 
Pij — 0 otherwise. 
We shall refer to this new process as the macroscopic process. 
By the results of § 6.3 we know that the lumped process will again 
be ergodic and reversible. It is again of period 2. The fixed vector a

--- Page 183 ---

Sec. 3 APPLICATIONS OF MARKOV CHAINS 171 
for the lumped process is easily obtained from a. 
is the sum of the components of a for states in 
/s\ 
states in V* we have 
The component dt 
V*. Since there are 
Hence 
By Theorem 6.2.3 we see that the mean number of times in any state 
\i, between occurrences of state Yj, is 
g /s) 
In particular the mean number of times in each state between 
occurrences of 0 is 
For our example s = 3 the partition for the lumping is V=(|000) 
{100, 010, 001), (110, 101, 011}, {111}). The transition matrix is 
V2 V3 
0 0 
2/ 3 o 
For the macroscopic chain we are also primarily interested in the 
mean first passage times. We know that in general it is not easy to 
obtain the mean first passage times for the lumped chain from the 
original chain. However, in the case we are considering we are 
helped by two special features of the process. First, for the lumped 
chain we can obtain all of the values of m0- from the knowledge of 
only mi0. In fact, since it is possible to go from Vi+1 to V0 only bv 
going through V, we have 
~ Wd+1,0 — Wfoo 0 ^ i < S 
and, by symmetry, 
- nig-i'S-i-i = ms-it0-ms-i-1>0 0 < i <s.

--- Page 184 ---

172 FINITE MARKOV CHAINS Chap. VII 
Thus we need only find the mean times to go from any state to Vo. 
In lumping, the set which became state Vo was a set with only one 
element, namely the vector y — (0, 0, 0, . . . , 0). Hence the mean time 
to go from state V< in the lumped chain to Vo is the same as the mean 
time to go from any element in the set Vi to y = (0, 0, 0, , 0). But 
any such element is a distance i from this vector. Hence this mean 
time is msi. Using our results for the microscopic process we have the 
values rriij for the macroscopic process. These are best expressed as 
follows: 
s—i s — i —1 
1Ui,i +1 = ms—i,0 — »*s-!-l,0 = ^ QSs-k~ 2 = 
ifc=l k =1 
0 < i < s — 1. 
To go from Vi to \j we must go through every intermediate point. 
Hence 
i-i 
mi = 2 for i < j 
and 
it=i 
s-j-l 
rriij = ms-i,s-j = 2 for > 3- 
lc = a-i 
From the vector a, 
mu = 
2s 
C) From these values we obtain 
m,i+i + mi+i'i = 
2 Q 2 u i = 0 \K/ i=i + l \K. 2s 
(A (1) 
Let w<i+1>ii be the mean number of times in state Vi before reaching 
state Vi+i, the process is started in state Vi. Then by § 6.2.7(b) we 
have 
W«+Dii = m,i+i + mi+1}i 
mu 
M 2‘

--- Page 185 ---

Sec. 3 APPLICATIONS OF MARKOV CHAINS 173 
Assume now that s is even. From (1) we obtain 
m0,s/2 + ^s/2,0 = 
Also, since ms/2,o = ms/2,s, we have 
m0,s — TOois/2 + TOs/2> (2) 
From the point of view of physical interpretations the most interest¬ 
ing case is where there is a large number of molecules. For this we 
would like to find estimates for m0<s/2 and ms/2)0. The first represents 
the time taken to go from no molecules in the given region to an equal 
number m each. The second is the time taken to go from an equal 
number in each to 0 in region A. If the model is to have any similarity 
to actual physical situations we would expect that the time to equalize 
the molecules would be much shorter than the time to reach an extreme 
situation from equalization. 
We consider first ra0,s/2. We estimate the time required to go from 
V< to Vi+i. We know that the mean number of times in V* before 
reaching Vi+1 is s/(s-i) = 1 + i/(s - i). Each time it is in Vt- (except for 
the last time) at least two steps are taken in going to V*-! and back to 
V*. Hence the mean time to go from V* to Vf+1 is at least 
1 + 2 
s — i 
Hence we may find a lower bound for 
-s + i 
s — i 
mo,s/2 by finding 
s/2 — 1 
I i = 0 
<s + i 
s — i 
This sum is greater than: 
-2+ // JZTxdx = 5(2 log 2 — x/2) — 2. 
Hence s(2 log 2 - 1 /2) - 2 is a lower bound for m0,s/2. A better lower 
bound is obtained as follows. We assumed above that each time the 
process went from Vt-i to V*, it did so in one step. If instead we use 
our lower bound for the mean time to go from to Vt- we have a 
new lower bound for the time to go from V< to Vt+1. This is 
i + A-./i + Lt+i) = 1 +_rf!_ \ s-i+\) {s-i){s-i+\)

--- Page 186 ---

174 FINITE MARKOV CHAINS Chap. VII 
The sum of these values between Vo and Vs/2-i is greater than 
-4 + 
r ( 1+ 
Lsx 
dx (s — x)(s — x+ 1) 
= —4 + 5 jVa-21og 2 + 5(5+!) log 
Using the first two terms in the Taylor series for log (s + 2)/(s+ 1) we 
obtain the estimate 
— 5 + 5(5/2 —2 log 2) 
for our improved lower bound. 
We next obtain an upper bound for mo,s/2- We have 
«/2-l , l 
m0. 
i, (;) ■ (;) 
= 2 +r 2 (t) 1 = 0 I |k=0 ' ' 
\ l I 
1(1 + 1) 
s-l+l^ (s-l+l)(s-l + 2)+ ‘ 
(;)[|.(.+i 
l\ 
(s-l+l) . . .s\ 
= 0) 
5 — l 
s-2l 
Hence, 
r(s-i)/2 s 
-7rdx+1Us Jo s-2x 1 
S(1U + 1/2 log 5). 
This approximation can be improved by using a better estimate for 
the terms of the sum where l is large. Thus we have found that 
5 + 5(5/2 —2 log 2) < mo,s/2 < «[1/4 -h x/2 log 5]. 
It would appear that these values are asymptotically of a greater 
order of magnitude than a constant times 5, but less than a constant 
times 5 log 5. We see from these estimates that the process takes a 
very short time to go from 0 to s/2. For 5= 100 the minimum time it 
could take is 50 and the actual mean time required is about 140.

--- Page 187 ---

Sec- 3 APPLICATIONS OF MARKOV CHAINS 
Consider next mo,«. We know from (2) that 
»/2 —1 
But 
Hence 
Thus 
2*1 
,s ■■ = 2. 2 
i = 0 
si 2-1 
I 1 
Is — 1\ 
<f». II o 
( < ) 
1) ^ mo,s ■■ s-1 
< 1 1 s/2-1 
8 — 1 Is — 1 
(V) 
m0,s ~ 2* 
K) where 1 < A ^ 2. 
175 
Our estimates for m0,s/2 show that this quantity is of a lower order of 
magnitude than mo,s. Hence the above estimates serve also for 
ms/2,s = 'Ms/2,o and we have 
ms/2,0 ~ where 1 ^ A < 2. 
Thus, as predicted, the mean time to go from equalization to the 
extreme of 0 molecules is very much larger than to go from 0 to 
equalization. For s= 100 the first time is approximately 2i°o or about 
1000 billion billion billion while the second is only 140. 
Other interesting quantities are m0,0 and ms/2>s,2. For these we 
have 
m0>0 = 2s 
For s — 100, m5o,5o is approximately 12.5. 
We conclude this section with some remarks about the macroscopic 
chain and reversibility. It is sometimes argued that a process of the 
type we have considered has a “direction” because of the very great 
tendency to move toward equalization. It is true that if the process 
is started out of equilibrium—say, in state 0—then it will certainly 
move towards the center. However, if the process is observed after 
it has been going for a long time, then we may consider it as started in 
equilibrium and in this case the process will appear the same looked

--- Page 188 ---

176 FINITE MARKOV CHAINS Chap. VII 
at in the reverse direction as in the forward. For example, if the 
process is then observed in \x the probability that it moves to state 
V*-i is the same as the probability that it came from \x-i- Another 
way to put this is the following. If a sequence of outcomes for a large 
number of steps is recorded and then handed to a physicist, he would 
be unable to tell whether he was given them in the order of increasing 
time or in the order of decreasing time. 
§ 7.4 Applications to genetics. A problem that is frequently 
discussed in genetics is the following: Two animals are mated. From 
their offspring, two are selected by some method, and these are mated. 
Then the procedure is repeated. This type of problem may be treated 
as a Markov chain. As states we take the possible combinations of 
parents, and the transition probabilities are determined by the laws of 
genetics, from the assumptions concerning the way parents are selected. 
The simplest such problem is obtained if we classifv parents only 
according to the pair of genes they carry in one position in the chromo¬ 
somes. We will discuss this case. Here we may further simplify the 
problem by assuming that the gene is either a or b, and hence that any 
individual animal must be of type aa or ab or bb. For example, if a 
dominates b, then aa is a pure dominant, ab is a hybrid, and bb is a pure 
recessive animal. Then a pair of parents must be of one of the following 
six types: (aa, aa), (bb, bb), (aa, ab), (bb, ab), (aa, bb), (ab, ab). This 
problem, for the simple case that the new parents are selected at 
random from the offspring, is treated in Feller and in FM. 
We will discuss a class of problems of which the above problem is a 
special case. We will assume that one offspring is selected at random, 
and that this offspring selects a mate. In its selection it is k times as 
likely to pick a given animal unlike itself than a given animal like 
itself. Thus k measures how strongly “opposites attract each other.” 
In this we take into account that in a simple dominance situation, 
aa and ab type animals are alike as far as appearances are concerned. 
The resulting transition matrix is 
(aa, aa) 
(bb, bb) 
(aa, ab) 
P = (bb, ab) 
(aa, bb) 
' 0 0 0 0 0 
0 1 0 0 0 0 
V4 0 1/2 0 0 V4 
0 1 0 k 
0 1 
2(k+l) (k+ 1) 2(k +1) 
0 0 0 0 0 1 
i 1 1 2*(fc+l) 1 4(& + 3) 4(3&+ 1) k + 3 (* + 3)(3A+l)(fc + 3)(3A: + l) Jc -f- 3

--- Page 189 ---

Sec. 4 APPLICATIONS OF MARKOV CHAINS 177 
The first two states are absorbing. They correspond to having 
developed a pure strain: pure dominant in the first state, and pure 
recessive in the second. We will compute the fundamental matrix N. 
the vector t, and the matrix B. 
N = 1 
(2*+l)(*+3) 
/4(k2+5k + 2) 2k(k+l)2 *(*+1) (3*+l)(*+3)\ 
I 2(3*+1) (4*2 + 9* + 3)(* + 1) *(*+1) (3*+l)(*+3) \ 
I 4(3*+1) 4k(k +1)2 (4*2+ 9*+ 3) 2(3*+l)(* + 3) I 
\ 4(3*+1) 4k(k+ l)2 2*(* + 1) 2(3*+ 1)(*+ 3) J - 
/2*3+ 12*2 + 33*+ 11 \ (aa, ab) 
T __ 1 4*3+17*2 + 29*+ 8 j (bb,ab) 
(2*+ 1)(* + 3) I 4*3+18*2 + 45*+13 I (aa, bb) 
\4*3+ 16*2 + 38*+ 10/ (ab, ab) 
(aa, ab) 
(bb, ab) 
(aa, bb) 
(ab, ab) 
B = 
4(2*+ !)(*+ 3) 
V 
(aa, aa) (bb, bb) 
4*2+ 23*+ 9 4*2+ 5* + 3N 
9*+ 3 8*2 + 19* + 9 
18* +6 8*2+10*+ 6 
18*+ 6 8*2 + 10* + 6 ’ 
(aa, ab) 
(bb, ab) 
(aa, bb) 
(ab, ab) 
Since we know that we will eventually end up with a pure strain, the 
two most interesting questions concern the number of generations 
needed to reach a pure strain and the probability of getting pure 
dominants or pure recessives. In particular, we will be interested in 
the effect that * has on these quantities. 
A large * has the effect of producing more mixed matings. Hence we 
would expect a large * to slow down the process. Indeed, every entry 
in r is monotone increasing in *. Some typical values of this vector are 
/32/3\ /45/e\ /5.64\ 
/ 22/3 1 45/6 1 6.64 
41/3 62/3 , 8.28 
W/s/ \5*/8/ \7.28/ 
o 
II *= 1 k — 2 
We see that increasing * will slow down the process considerably, 
especially if we start in one of the last three states. The fact that 
the time to absorption from (aa, bb) is always one more than from

--- Page 190 ---

178 FINITE MARKOV CHAINS Chap. VII 
(ab, ab) is due to the fact that from the former state we always go to the 
latter in one step (a pure dominant and a pure recessive parent must 
have hybrid offspring). 
The effect of k on the probability of absorption is not so clear. 
One would guess that large k favors the recessive strain, since then both 
dominants and hybrids would tend to select recessive mates. Indeed, 
for k > 1 the probability of absorption in (bb, bb) increases with k. 
But for k< 1 a surprising situation develops. As k is decreased from 
1, the probability of absorption in (aa, aa) increases, until it reaches a 
maximum at k=xl3, and then decreases back to the same value as at 
^= Thus k = 0 and &=1 yield the same absorption probabilities. 
This means that if in mating like always selects like, the probability 
of absorption is the same as for random mating (though of course the 
time to absorption is much less than for random mating). Some typical 
values for the probability of absorption in (aa, aa) are 
r\ r\ / 07 \ .25 
.77 
.27 
.54 .50 
V.54/ \.5o/ 
k = i/3 k= 1 k = 7 
The fact that the last two entries are always the same is due to the 
already observed fact that the process always goes directly from 
(aa, bb) to (ab, ab). The probability from (bb, ab) is half this much; 
since when the process leaves (bb, ab), it is equally likely to go to 
(ab, ab) or to be absorbed in (bb, bb). Similarly, the value* at (aa, ab) 
is half the (ab, ab) value, plus 1/2. It is interesting to note that for 
random mating the probability of absorption in (aa, aa) is proportional 
to the number of a genes present at the start. 
Let us collect the quantities for the special case of random mating, 
k = 1. 
P 
0 0 0 0 0 
1 0 0 0 0 
0 
V 2 0 0 V 
V4 0 V2 0 V 
0 0 0 0 1 
1/l6 V 4 V 4 Vs V 
(aa, aa) 
(bby bb) 
(aa, ab) 
(bb, ab) 
(aa, bb) 
(ab, ab)

--- Page 191 ---

Sec. 4 APPLICATIONS OF MARKOV CHAINS 179 
The standard deviation in r is around 4.7 for every entry. This is of 
the same order of magnitude as the entries of r, hence we may expect 
very large fluctuations. 
We observe from P that the process satisfies the condition for lumpa- 
bility if we combine the first two states and combine the next two states. 
This partition has a simple interpretation. The state {bb, bb) results 
from {aa, aa) by interchanging a and b, and {bb, ab) results from 
{aa, ab) similarly. On the other hand {aa, bb) and {ab, ab) are un¬ 
changed. Hence the partition represents the process if we do not care 
which gene is the dominant gene. The first state of the new process, 
which we may denote by {aa, aa), represents any pair of like pure 
parents. The second state, to be denoted by {aa, ab), represents one 
pure and one hybrid parent. The remaining two states represent the 
combination of unlike pure parents, and of two hybrid parents, re¬ 
spectively. The lumped transition matiix is 
P = 
{aa, aa) 
{aa, ab) 
{aa, bb) 
{ab, ab) 
To obtain N for the lumped process, we add the first two columns

--- Page 192 ---

180 FINITE MARKOV CHAINS Chap. VII 
which correspond to lumped transient states: 
/10/3 Ve 4/3\ 
/10/3 Ve 4/3j 
I 8/3 4/3 8/3 )' 
\ 8/3 V3 8/3/ 
We then observe that the first two rows are identical, as must be the 
case for lumpability. Hence we have 
/10/3 Ve V 3\ (■aa, ab) 
CO 
CO 
00 
II 
<55 
8/3 (aa, bb) 
\ 8/3 X/3 8/3/ (ab, ab) 
and 
/45/ 
* = $$= 62/ 
\5V 
The vector f is also obtainable directly from r. The latter has identical 
entries for the two states to be lumped, as must be the case, and con¬ 
traction yields f. For many purposes the lumped process yields 
sufficient information. (Of course, it does not yield any interesting 
information as far as the absorption probabilities are concerned, since 
there is only one absorbing state after lumping.) For example, t is 
completely determined by f. 
/7/io Vs Va\ /2113/36\ 
8 = 8/l0 V4 1 I T2 = ( 222/3 ). 
\8/io V4 Vs/ \222/3 / 
The last two columns of A, corresponding to single-state cells, are 
obtainable directly from H. But the first column is new. f 2 is directly 
obtainable from r2. 
A generalization of this lumped process is discussed in Kempthorne.-f- 
We still restrict ourselves to a single position in the chromosomes, but 
we no longer assume that only two types of genes can occur in’ this 
position. There may be any number of different kinds of genes. We 
consider the process in the lumped form: We care about the number of 
different genes present and about their combination, but we do not 
distinguish states that differ only in having the genes permuted. Thus 
f O. Kempthorne, An Introduction to Genetic Statistics, New York, John Wilev & Son 
-Inc., 1950. J *

--- Page 193 ---

Sec- 4 APPLICATIONS OF MARKOV CHAINS 
we have the following seven states: 
181 
The transition matrix is: 
Si: (<aa, aa) 
s2: {aa, ab) 
s3: {aa, bb) 
s4: {ab, ab) 
s5: {aa, be) 
s6: {ab, ac) 
St: {ab, cd) 
P = 
r 0 0 0 0 0 0 
V 4 Vs 0 V 4 0 0 0 
0 0 0 I 0 0 0 
Vs Va Vs V 4 0 0 0 
0 0 0 Vs 0 Vs 0 
V16 V 4 0 3/l6 Vs 3/s 0 
o 0 0 V4 0 Vs V4 
si 
s2 
S3 
s4 
Ve have indicated the equivalence classes in the transition matrix, 
the equivalence classes are determined by the number of different genes 
present m the parents. Clearly, this number either stays the same or 
it decreases, hence the process can move from an equivalence class with 
a given number of genes only to one with fewer genes, i.e. from the 
bottom up in P. The single state with only one type of gene present is 
absorbing. The four-state lumped process we considered above 
corresponds to the two top equivalence classes in the present chain 
bmce numbers concerning these classes are not affected by equivalence 
classes lower down, all quantities we are about to compute will agree in 
their upper left corner with those previously found. 
I10/ 3 V 6 4/3 0 0 
0 \ 
V 3 4/3 8/s 0 0 
° \ 
CO 
oo 
V3 8/3 0 0 
° 
8/s 5/is 20/9 10/9 8/9 
° V 3 2/s 16/9 2/9 16/9 » / 
CO 
00 
V 27 56/27 4/27 32/27 V 3 ' 
N =

--- Page 194 ---

182 FINITE MARKOV CHAINS Chap. VII 
T = 
S2 S3 S4 S5 
(7/l0 V8 1/2 0 
8/l0 X/4 1 0 
8/l0 X/4 5/8 0 
8/l0 5/ 24 5/6 VlO 
8/io Ve 2/3 Vs 
8/l0 7/36 7/9 2/lo 
S6 S7 
0 
0 
0 
V 2 
7/l6 
2/3 
We note that S2, s4, and S6 are the “likely states,” in the sense that if 
the process starts low enough for it to be possible to reach the state 
there is always a fairly good chance of reaching the state. The other 
states are quite unlikely no matter how the process is started. It is 
quite surprising that starting in s7—that is, starting with four different 
genes we expect to reach a pure strain in 72/3 generations. But it 
must be remembered that the standard deviation of this quantity is 
4.97, and hence much longer processes are not too unlikely. 
§ 7.5 Learning theory. This section will be devoted to the study of 
a mathematical model for certain kinds of learning, due to W. K. Estes. 
We will discuss only some relatively simple special cases, but the 
techniques here used are applicable to more general situations. 
In a typical experiment the subject is placed in front of a pair of 
lights, and he is asked to guess whether the light on his left or the light 
on his right will be turned on next. Thus, he has two possible responses. 
We denote by A0 the guess “left,” and by Ai the guess “right.” Then 
the experimenter turns on one of the lights. Let E0 mean turning 
on the left light, and E4 the right light. The procedure is repeated a 
large number of times, and a record is kept of the sequence of both 
Ai and E,. The purpose of the theory is to predict, for given behavior 
of the experimenter, how the subject’s guesses will change in the long 
run. A variety of experiments has shown that the model is in good 
agreement with the facts.

--- Page 195 ---

Sec. 5 APPLICATIONS OF MARKOV CHAINS 183 
In a large class of interesting experiments the experimenter will 
choose his actions with fixed probabilities, depending only on the 
action of the subject. These probabilities may be given in Figure 
E0 E, 
Ao/1— v v \ 
Ai\ w 1 -w) 
Figure 7-8 
That is a guess “left” (A0) is reinforced by turning on the left light 
(E°) with probability 1-w; otherwise the right light is turned on. 
And a guess “right” is reinforced with probability 1 - w Here v and 
are numbers between 0 and 1, which are kept fixed for the duration 
ol the experiment. 
While v and w are usually positive numbers, the cases in which they 
are not lead to interesting experiments. For example, if w = o and 
then A0 is always reinforced, but Ax is only occasionally rein¬ 
forced. The cases v > 0 and w = 0 is similar. If v = w = Q, then every 
action of the subject is reinforced. 
Another class of interesting special cases is where v + w= 1. Here 
l-v = w and v = 1 - w, and hence the probability of E0 or of Ei is 
independent of the action of the subject. 
The model assumes that the subject has a certain unknown number s 
of stimulus elements. Each stimulus element is at each stage of the 
experiment connected to one of the two possible responses Ai} the 
original connections not being known. It is then assumed that the 
tollowing takes place at each stage of the experiment: 
(1) The subject samples ’ a subset of the stimulus elements, by 
means of an independent trials process, in which any one stimulus 
element is sampled with probability t or not sampled with probability 
(2) If in the sampled set there are k stimulus elements connected to 
A0 and l to Ax, then the subject performs Ax with probability l/(k + l). 
Some convention is necessary if no stimulus element is sampled; we 
will assume that the probability of Ax is the same in this case as if all 
stimulus elements had been sampled. 
(3) If the experimenter performs E0, then any stimulus element that 
was previously connected to Ax and which was just sampled by the 
subject is reconnected to A0. Similarly, after Ex, all the sampled 
stimulus elements are connected to Ax.

--- Page 196 ---

184 FINITE MARKOV CHAINS Chap. VII 
We can represent the model as an (s+ 1) state Markov chain, in which 
state Si occurs when exactly i stimulus elements are connected to Ai, 
and i = 0, 1, . . . , s. All the interesting quantities depend only on 
how many stimulus elements are connected each way, and hence these 
quantities are functions on the chain. For example, the probability of 
action Ai from state si is obtained as follows: 
Pr,[A.] =21 
k=0 1=0 
k, l not both 0 
S — l 
k f\l 
l i 
m s 
where k is the number of stimulus elements sampled from those con¬ 
nected to Ao, l from those connected to Ai, m — k + l, and the last term 
arises from the assumption concerning the case where no stimulus 
element is sampled. We rewrite this as a sum on m, and then use a 
binomial identity: 
2 2 771 — 1 = 771 
s- 1 
k 
X 
1) m 
= 2 <ra(i-f)‘ 
m= 1 
- + (1-0S- 
S S 
tm(l — t)s~m 
i 
s 
It will be convenient to let the column vector y represent these 
probabilities. 
Let us next construct the transition matrix P. 
take into account all four possibilities in Figure 7-8. 
of Ai and E0) with a transition from s* down to sj has probabilities wX, 
where 
For this we must 
The combination 
Xij — 2 • = n 
ti-j+jc( 1 _ t y-i+j-k . i-j 
i—j + k 
0 
if j < i 
if j > i 
The downward transition s* to Sj, by means of the combination A0 and 
E0, has probabilities (1 — v)(Y — X), where

--- Page 197 ---

Sec. 5 APPLICATIONS OF MARKOV CHAINS 185 
If we let x*ij - xs-it s_j, and y*tj = ys_u s_j> then vX* and (1 - w)t Y* _ X*) 
represent the probabilities of upwards transition. Thus 
P = ™X + vX* + (l-v)(Y-X) + {\-w){Y*-X*) + {v + w~l)(l-tyi, 
or 
P = v(X + X*-7) + w(X + Z*-7*) + (7+ T*)-(X + Z*) 
+ {v + w-\)(\-tyi. (i) 
The final term represents the cases where no stimulus element is sampled 
which were not included in the two previous terms. 
We wish to compute Py. First we find Yy. 
S 
2 k = 0 
i 
2 s 
"U iy^ 
(1-0. 
Hence 
Quite similarly, 
Yy = (l-t)y. 
Y*y = t£+(l-t)y. 
And by a somewhat longer argument we find that 
(2) 
(3) 
(X + X*)y = t£+(l-2t-(l-ty)y. (4) 
If we write Py by means of (1), and make use of (2), (3), and (4), we 
obtain 
Py = t#£+(l-2f-(l-f)*)y-(l_*)y] 
+ w[t£+{\-2t-(\-ty)y-t£-{l-t)y] 
+ t£+2(l-t)y-t£-(l-2t-(l-ty)y 
+ (v + w — 1)(1 — t )sy. 
This simplifies to 
Py = vt£ + [l-{v + w)t~\y. (5) 
Let us introduce the vector 8 = y — 
—) v + w/ £ for the cases where v

--- Page 198 ---

186 FINITE MARKOV CHAINS Chap. VII 
and w are not both 0. It will be shown later that 8 has an important 
interpretation in the model. 
ps = = [i-(»+»X]r-(^)[\-(v + w)t)( 
or 
P8 — [1 — (v + w)t]8 (6) 
and 
Pn8 = [1 — (?; + w)t~\n8. (7) 
The values of v and w determine the nature of the Markov chain. If 
0 < v ^ 1 and 0 < w ^ 1, then P > 0 and hence the chain is regular. 
If v = 0, then i = 0 is an absorbing state; and if w = 0, then i = s is an 
absorbing state. Hence if either v or w is 0, we have an absorbing chain. 
If v — w — 0, then we have two absorbing states. 
Let us find the probability of an Ai response after n steps. This 
will be given by the vector Pny. the probability depending on the 
starting state. If v = w = 0, then Py = y from (5), hence Pny = y. The 
probability of an Ai response is unchanged. In the other cases, 
(7) provides the answer 
Pny — [v[(v + w)]g + [1 — {v + w)t~]n8. 
The first term (as will be seen below) is the limiting probability for 
an Ai response, and the second is the deviation due to the initial 
position. 
Let us first discuss the regular case. Here v>0 and w>0. Since 
0<£< 1, we have |1 - (v + w)t\ < 1. From (7), 
AS = 0 
and thus 
v ay -- 
v + w (8) 
This proves that the limiting probability of an Ai response is v/(v + w). 
Applying this to Figure 7-8 we find that the limiting probabilitv of an 
Ei action by the experimenter is 
—--v-\-(1 — w) —- v+w v+w ' v+w 
Thus in equilibrium the probabilities for the experimenter and the 
subject are in agreement.

--- Page 199 ---

Sec. 5 APPLICATIONS OF MARKOV CHAINS 187 
It is interesting to note that the subject does not maximize the number 
ot correct guesses. Instead, he brings about an equilibrium in which he 
upgUeSSmg nght” With the Same frecluency with which “right” comes 
Since v/(v + w) is the mean number of Ai responses per trial in equili¬ 
brium, and since ijs is the mean number when in state s<, the vector 
I 1 / 11 \ ^ 
8= - v + w)} &lves the deviation between the mean number of 
Ai responses in a given state and in equilibrium. 
From (6), (7), 
{I — P + A)8 = (v + w)tS 
1 Z8 = 
(Z — A)8 = 
(v + w)tc 
(v + w)t^ 
(Z-A)y 1 
(v + w)t 8. (9) 
Thus the total deviation from equilibrium (for the number of Ai 
responses) is proportional to the deviation vector 8. Hence the total 
deviation may be large because i/s for the starting state may have been 
far from the equilibrium v/(v + w), or because t is small. 
To obtain the limiting variance for the number of Ai responses, we 
must use § 4.6, since we have a function on the chain which takes on the 
value 1 in s* with probability ft = ijs. While we have no general 
formula for this limiting variance, in any concrete example it is easy to 
compute it from § 4.6. 
Let us next consider the case of an absorbing chain with one absorbing 
state i = 0, that is, v = 0 and w>0. (The case v>0, w = 0 is similar.) 
In this case 8 = y. Let y be gotten from y by deleting its first component 
(which is 0). From (6), 
Py — P8 — (1 —wt)8 — (1 —wt)y 
Qy = (1 —wt)y 
CI — Q)y= (wt)y 
Nr=^y- (10) 
This case is one in which the subject is being conditioned to give Ao 
responses. If he gives an A0 response, it is always reinforced. But an 
Ai response is also reinforced occasionally, with probability l-w. Ny

--- Page 200 ---

188 FINITE MARKOV CHAINS Chap. VII 
gives the mean of the total number of Ai (or “wrong”) responses. For 
starting state Sj this is ljwt-i/s. Thus there may be a large number of 
errors for one of three reasons: The fraction i/s of stimulus elements that 
need reconditioning was high at the start; or the learning parameter t is 
low; or w is low, that is, an Ai response is frequently reinforced. 
It is sometimes reasonable to assume that the stimulus elements 
were originally connected at random. This gives us an initial proba- 
Then the mean of the total number of 
“wrong” responses is 
71 wt^ Lt)^ 2 wt 
We could expect to average this number of Ai responses in a large 
homogeneous population. This is one simple way of estimating t. 
Finally we discuss the case v — w — 0, where every action of the subject 
is reinforced. Then both i — 0 and i — s are absorbing states, and the 
most interesting question is whether the subject ends up conditioned 
to Ao or to Ai responses. 
From (5) we see that in this case Py = y. But y has a 0 component 
for so, and 1 for ss, hence (see Theorem 3.3.9) y gives the probabilities 
for absorption in ss. Thus the probability of being conditioned entirely 
to Ai responses is equal to the fraction of stimulus elements originally 
connected to Ai. 
To obtain more detailed information about the model we will have 
to make a simplifying assumption. Since in most applications the 
learning parameter t is quite small, we shall assume that terms of higher 
order in t are negligible. This may also be interpreted psychologically. 
If we drop terms in powers of t higher than the first, we assume that the 
sampling of more than one stimulus element at a time is quite unlikely. 
Under this assumption P is considerably simplified: 
Pi,i+i = (s-i)tv 
Pi,i-i = itw 
pu = 1 -1 (iw + (s - i)v) 
Pij — 0 otherwise. 
Let us consider first the regular case under this simplifying assump¬ 
tion. The most important quantity lacking in our previous treatment 
was the vector of limiting probabilities a. We shall show that under 
our present assumption 
viws~i/(v + w)s. 
bility vector tt = 
af =

--- Page 201 ---

Sec- 5 APPLICATIONS OF MARKOV CHAINS 
Let us compute aP. 
189 
S 
2 ak'Pki = j + ajPjj + aJ+1pj+1J] 
= (j-^)vi~lwS~j+1(s-j+l)tv+ -t(jw + (s-j)v)] 
+ \jS+^vi+XwS~}~lU + l)tw /(v + w)s 
- - dj 
tvhvs~i 
(v+wy (/ l)<a -3 + 1)H> - (''j (jw + (s -j),-) 
+G+i)u+1)* We then find that the two v-terms cancel each other and the two 
w- erms also cancel, and hence the expression in brackets is 0 Thus 
the right side reduces to ah and hence a = {at} is the fixed vector of P. 
ius we know that if t is small, the limiting probabilities are very 
nearly given by this a. Indeed a is the limit of the limiting probabilities 
t—>0. 
It is interesting to note that for w = v=l/st the process we obtain is 
the same as that obtained for the macroscopic process in the Ehrenfest 
model. 
Next let us consider an absorbing case, say w>0 and z/ = 0, that is, 
where the subject is being conditioned to response A0. Then 
Pi,i~ i = itw 
Pa = 1 — itw 
and all other entries are 0. The matrix I-Q = {ci}} where cu = itw, 
and cM_! = - itw, and all other entries are 0. We wifi show that 
na = l/jtw if j ^ i 
= 0 otherwise. 
Let us compute N(I -Q). 
if i < j: 
if i = j: 
if i > j: 
2 nOcCkj — Uij(jtw) (j+ \)tw~\ 
Jc= 1 
= 0 + 0 = 0 
= (l/jtw)jtw + 0 = 1 
= (l/jtw)jtw + (I/O- + 1 )tw)( -0+1 )tw) 1-1 
= 0,

--- Page 202 ---

190 FINITE MARKOV CHAINS Chap. VII 
which verifies that N is the desired inverse. It is interesting to note 
that N depends on i, j, t, but not on s. We then obtain 
u = (i/M 2 (Vi)- 3 = 1 
Thus the time of conditioning is inversely proportional to t and to w, 
and depends on the number of stimulus elements that need to be 
conditioned from Ai to Ao- The time will be large if t is small. It 
will be large if w is small—that is, Ai is frequently reinforced. But 
since the series l/j diverges, the time may also be large because many 
stimuli elements were originally conditioned to Ai. 
Finally we shall consider the case of two absorbing states, w = v — 0. 
Here the first-order terms in t drop out, and hence we will have to carry 
out our computation in terms of t2. 
Pi,i-1 = Pi,i+1 = (ll2)i(s-i)t2 
Pa = 1 —i(s — i)t2. 
A computation quite similar to the one above will verify that 
nij = (2/t2s)(i/j) if * < j. 
= (2/*2«)(-s-i)/(s~j) if i > j. 
and hence 
k = (2/* 2) 
sfjJr-iJ si=r+io\ 
Again the sum may be large because t is small or because there are 
many terms (s is large). But in this case tt is inversely proportional to 
t2, and hence we expect a much longer time for conditioning. 
There is one special case in which more precise information is avail¬ 
able without the above simplifying assumption. This is the case 
where v + w= 1, or w= 1 — v. Here the action of the experimenter is 
independent of what the subject does. We found an exact solution 
for a in this case, in terms of simple recursion equations.! 
It was shown that the limiting probabilities may also be obtained 
from the following auxiliary process: We start with s stimuli elements 
completely unconditioned. We select a subset of these, picking each 
stimulus element with probability t. Then by a random device we 
assign these to Ao with probability w or to Ai with probability 1 — w. 
We then apply the same procedure to the remaining stimuli elements, 
till all are assigned. Then the limiting probability a* for the original 
process is simply the probability of assigning i stimuli elements to Ai 
t Cf. J. G. Kemeny and J. L. Snell, “Markov Processes in Learning Theory,” Psycho- 
metrika, 22 (No. 3): 221-230, 1957.

--- Page 203 ---

Sec. 6 APPLICATIONS OF MARKOV CHAINS 191 
These probabilities are easily obtained for any s of reasonable size. 
J™ +fPplicf0ns to mobilitJ ^eory. In this section we shall 
nsider the application of Markov chain ideas to a problem in sociology, 
e problem of mtergenerational occupational mobility. The results 
of this section were prepared jointly with J. Berger. The problem may 
e stated as follows. A partition A = {Ax, A2, . . . , Ar} is made of the 
set of all occupations. The cells are called the occupational classes. 
These are usually ordered with respect to some socially relevant 
criterion, for example, prestige of occupation. The question is then 
asked, to what extent does the occupational class of the father, grand- 
iather, etc., affect the occupational class of the son ? In any such study 
a matrix is constructed which represents for each class the fraction of 
the sons that would be expected to go into each of the occupations. 
u VtT ta^ aS °Ur basi° examPle a matrix constructed from data 
collected by Glass and Hall from England and Wales for 1949.t 
olio wing Praise we classify the occupations as upper, middle, and 
lower. The estimated matrix is 
UPPER MIDDLE LOWER 
UPPER /.448 .484 ,068\ 
P = MIDDLE I .054 .699 .247 . 
lower \.011 .503 .486/ 
We see, for example, that of the upper class, 44.8 percent of the sons 
went to the upper class, 48.4 percent to the middle, and 6.8 percent to 
the lower. 
There are two ways to make use of P. One is to consider at each 
state the total population, and predict the fraction of the population 
which will be in each of the occupational classes. We shall call this 
the “collective process”. 
. ^ se°ond way is to study a single family history. From this point of 
view we consider this history as the outcomes of a Markov chain with 
transition matrix P. We shall call this the “individual process.” 
We assume every family has exactly one son. 
We proceed now to discuss the relation of the basic concepts of 
Markov chain theory to these two processes. 
We begin with the assumptions for a Markov chain. The basic 
f D. V. Glass and J. R. Hall, “Social Mobility in Great Britain: A Study of Inter- 
generation Changes in Status,” in D. V. Glass (Ed.), Social Mobility in Great Britain 
London, Routledge & Kegan Paul, 1954. 
* S;/' Pw,iS’ “Measuring Social Mobility,” Journal of the Royal Statistical Society, 
118 :56-66, 1955.

--- Page 204 ---

192 FINITE MARKOV CHAINS Chap. VII 
assumption is that the knowledge of the past beyond the last outcome 
does not influence our predictions. In the individual process this would 
mean, for example, that the knowledge of the occupation of the grand¬ 
father would not affect our predictions for the son. 
We also assume that the same P serves for every generation. This 
is clearly not completely realistic. However, there is still a great deal 
of interest in studying what would happen if the present P were to 
continue to be appropriate. 
It is also assumed that changes in distributions in occupational 
classes from one generation to the next are to be accounted for only by 
the process described by P; that is to say, for purposes of this analysis 
we ignore the effect of differential reproduction and migration rates, as 
these may be related to occupations of the system. 
The classification of states has obvious interpretations in mobility. 
An ergodic set is a set of occupations from which it is impossible to leave. 
In most industrialized societies we would expect only one ergodic set. 
However, if we take as state the pair, occupation and race, then dis¬ 
crimination against a certain race may cause the resulting chain to 
have more than one ergodic set. When studying occupations, a son 
can have the same occupation as his father. Thus we would not 
expect cyclic chains. An absorbing state would mean that for a given 
occupation the son must follow his father’s footsteps. Again, in 
industrialized societies occupations do not usually have this property. 
We shall therefore assume that our basic chain is regular. 
Let us next see the interpretation of the powers of P. For the 
individual process the ij-th entry of Pn will give the probability that, 
after n generations the family will be in the j-th occupation class if it 
started in the i-th. For the collective process, p{nUj represents the 
fraction of the descendants of people in the i-th occupational class that 
will be in the j-th occupational class after n generations. If we start 
with fractions tt = (pi, p2, ^3) in each of the classes, then after n genera¬ 
tions there will be fractions given by 7rPn. In our example, assume 
that there are at present 20 percent in the upper class, 70 percent in the 
middle class, and 10 percent in the lower class. Then after one 
generation the percentages are 12.9, 63.6, and 23.5. We obtain these 
by 
(.448 .484 .068\ 
.054 .699 .247 ) = (.129 .636 .235). 
.011 .503 .486/ 
The fixed probability vector a has the following interpretations. In 
the individual process it represents the long-range predictions for the

--- Page 205 ---

Sec. 6 APPLICATIONS OF MARKOV CHAINS 193 
occupation of an individual. Our basic theorem for regular chains tells 
us that these predictions are independent of the present occupational 
class. For the collective process, the fixed vector gives the equilibrium 
fractions. When these fractions are realized, the fractions in successive 
generations remain the same. No matter what the initial fractions are, 
they will, after a number of generations, be close to those given by the 
fixed vector. 
In our example, the fixed vector is a= (.067, .624, .309). The actual 
fraction in each of the classes from the data which determined the 
matrix P was a =(.076, .634, .290). Thus we can see that the system 
may be considered to be nearly in equilibrium. 
The mean first passage times have the usual interpretation for the 
individual process but do not seem to have a natural interpretation for 
the collective process. For our example, the mean first passage time 
matrix is 
U M L 
M = M 25.1 1.6 4.3 . 
L \26.5 1.9 3.2/ 
The standard deviations for the first passage times are 
U M L 
Since the standard deviations of the first passage times are of the 
same order of magnitude as the means, the means are not to be taken as 
typical values. However, the relative size is of interest. For example, 
the mean time to go from lower to upper is about five times as big as 
the mean time to go from upper to lower. 
Assume now that the individual process is in equilibrium. Then the 
reverse transition matrix gives the probabilities for the father’s occupa¬ 
tion when that of the son is known. If P is reversible, then, given that 
a man is in class t, the probability that his son will be in a given occu¬ 
pational class j is the same as the probability that his father was in this 
class/. The condition for reversibility has an interesting interpretation 
for the collective process. Recall that the condition for reversibility 
may be expressed by saying that D~^P should be a symmetric matrix. 
In other words, that ciipa = djpa. In the collective process dipij repre¬ 
sents, in equilibrium, the fraction of the people in the i-th occupational

--- Page 206 ---

194 FINITE MARKOV CHAINS Chap. VII 
class which move in one generation from the i-th to the ^-th class. 
Also cijPji represents the fraction which move from the ^‘-th class to 
the i-th class. Hence the condition for reversibility means that there 
should be an “equal exchange” between classes. If there is an equal 
exchange then clearly the total numbers in each class will remain fixed, 
i.e. the process will be in equilibrium. However, equal exchange is a 
much stronger condition. The above discussion suggests that for 
the collective process the matrix D~lP is an interesting matrix. We 
call this matrix the exchange matrix. For our basic example, the ex¬ 
change matrix is 
U M L 
u /.030 .032 .005 
D-ip = M 1 .034 .436 .154 
L \.003 .155 .150 
Note that there is approximately equal exchange between the 
classes.f 
Finally we consider the question of lumpability for mobility pro¬ 
cesses. This is particularly important for the following reason. If we 
decide that the Markov assumption is reasonable for a certain method of 
classification, then we cannot arbitrarily treat a coarser classification 
as a Markov chain. This is because the coarser classification is obtained 
from the finer by lumping states. Me know that only under very 
special conditions will this again result in a Markov chain. Of course 
the method of classification itself has a great deal to do with whether 
or not the Markov assumption is realistic. Hence the coarser analysis 
may be taken as a Markov chain even when the condition for lumpa¬ 
bility is not satisfied. However, we must then admit that the finer 
analysis is not a Markov chain. We cannot have both, unless the con¬ 
dition for lumpability is satisfied. 
We shall illustrate the above ideas in terms of some actual mobility 
studies. The example that we have been considering was actually 
obtained from a finer analysis of the data obtained by Glass and Hall 
for England and Wales in 1949. These authors used seven classes. 
They are: 
1. Professional and high administrative. 
2. Managerial and executive. 
3. Inspectional, supervisory, and other non-manual (higher grade). 
t 11 °r a m°re detailed discussion of the exchange properties of a system see J. Berger 
and J. L. Snell, “ On the Concept of Equal Exchange,” Behavioral Science, 2, (No. 21 •

--- Page 207 ---

Sec. 6 APPLICATIONS OF MARKOV CHAINS 195 
4. Same (lower grade). 
5. Skilled manual and routine grades of non-manual. 
6. Semi-skilled manual. 
7. Unskilled manual. 
From their data we obtain the transition matrix 
P = 
/ 0.388 0.147 0.202 0.062 0.140 0.047 0.016' 
0.107 0.267 0.227 0.120 0.207 0.053 0.020 
0.035 0.101 0.188 0.191 0.357 0.067 0.061 
0.021 0.039 0.112 0.212 0.431 0.124 0.062 
0.009 0.024 0.075 0.123 0.473 0.171 0.125 
0.000 0.013 0.041 0.088 0.391 0.312 0.155 
0.000 0.008 0.036 0.083 0.364 0.235 0.274) 
Our previous example was obtained from this study by calling {1,2} 
the upper class, {3,4,5} the middle class, and {6,7} the lower class. 
The fixed vector is 
a = (.023 .041 .088 .127 .410 .182 .129). 
The above transition matrix was estimated from a sample of 3497. 
The distribution of the occupations in this sample was 
a = (.030 .046 .094 .131 .409 .170 .121). 
We see that these numbers are fairly close to the equilibrium vector. 
1 
2 
3 
M = 4 
5 
6 
7 
1 2 3 4 5 6 7 
/43.9 26.2 9.9 9.2 4.0 8.4 11.5 
63.1 24.2 10.1 8.5 3.5 8.1 11.1 
70.3 30.5 11.4 8.0 2.9 7.6 10.3 
72.3 33.0 12.7 7.9 2.6 7.0 10.0 
73.7 33.9 13.5 8.7 2.4 6.5 9.3 
74.9 34.6 14.1 9.1 2.6 5.5 8.8 
75.0 34.8 14.3 9.2 2.7 5.9 7.7/ 
The diagonal entries of M are the reciprocals of the fixed vector. 
Since the fixed vector is close to the actual fraction of people in each

--- Page 208 ---

196 FINITE MARKOV CHAINS Chap. VII 
class, when this fraction is small the mean time to return is corres¬ 
pondingly large. For a given occupational class it is interesting to 
compare the mean time to reach this class, starting in each of the other 
classes. We observe that in general the mean time to go from any 
state i to a given state j decreases as i gets closer to state j. Positions 
of these occupational classes correspond to their relative social prestige, 
and hence “closer” means closer in terms of prestige. 
We have discussed only regular chain concepts in this section. We 
know that absorbing chain ideas can be fruitfully used to study regular 
chains. For example, we can study the behavior of the middle classes 
3,4,5 by making the upper classes 1 and 2 and lower classes 6 and 7 
into absorbing states. Doing this, we obtain an absorbing chain with 
matrices Q and R given by 
3 4 5 
3 /.188 .191 .357 
II 
O1 
! .112 .212 .431 
5 \.075 .123 .473 
1 2 6 7 
3 / .035 .101 .067 ,061\ 
R = 4 1 1 .021 .039 .124 .062 
5 ' \.009 .024 .171 .125/ 
The basic quantities for this chain are ; 
3 4 
3 /l.44 
N = 4 j .36 
5 \ .29 
3 
B = 4 
5 
.58 
1.60 
.45 
1 
6 
1.45 
1.55 
2.47 
2 6 
.20 .42 
.14 .49 
.11 .49 
3 
r = 4 
5 
From r we obtain the mean time to leave the set {3,4,5} for the first 
time for each starting state in the set. We see that this is between 
3 and 4 for each starting state. From B we find the probabilities of 
leaving by moving to each of the states 1,2,6,7. Combining states

--- Page 209 ---

bEC- 6 APPLICATIONS OF MARKOV CHAINS_197 
1 and 2’ and 6 and 7 we can find the probability of moving out of each 
ot the middle classes by moving to the upper or to the lower class, 
lhese probabilities are: 
U L 
3 / .28 .72 
'I .20 .81 
5' \ * 15 .85 
In each case the probability of leaving by way of the lower class is 
much higher than leaving by way of the upper class. 
It is interesting to observe that the probability of leaving the middle 
class by way of the upper class decreases the lower the level of the 
occupational class. 
As mentioned earlier, our basic example in this section was obtained 
by combining states in this seven-state chain. The partition used was 
A = ({1>2}> {3)4,5}, {6,7}). The first set being the upper class, the 
second the middle class, and the third the lower class. It is interesting 
then to check the condition for lumpability with respect to this parti¬ 
tion. To do this we must find the matrix PV (see §6.3). We obtain, 
A2 A3 
.404 .062 
.553 .073 
736 .128 
754 .128 
671 .296 
520 .467 
483 .509 
To satisfy the condition for lumpability it is necessary that the 
components of a column of this vector be constant within the sets 
Ai, A2. A3. This is certainly not the case. For example the probability 
of moving to Ai is quite different for the states of A2. It is .033 from 
state 5, .060 from state 4, and .136 from state 3. Thus we would not be 
justified in treating both of our processes as Markov chains. 
If we choose to believe that the seven-state chain is a Markov chain, 
then the three-state process is not a Markov chain, but in equilibrium

--- Page 210 ---

198 FINITE MARKOV CHAINS Chap. VII 
the matrix P, the vector a, and the matrix M are all well defined. If 
we compute these matrices by the method given in § 6.4, we obtain, 
U M L 
u /.43 .50 .07\ 
II .05 .70 .25 
L (.0! .50 .48/ 
a = (.06 .63 .31) 
U M L 
u /15.7 2.0 5.9' 
II 
s 26.3 1.6 6.3 
L ' V 28.0 2.0 3.0, 
We see that these quantities are all quite close to those obtained 
by treating the three-state chain as a Markov chain. 
The next example we consider is obtained from data collected by 
N. Rogofff in a study made from marriage-license applications for 
Marion County, Indiana. The interest in this example lies in the fact 
that data were obtained for two different time periods, 1905 to 1912 
and 1938 through the first half of 1941. Hence it is possible to compare 
the transition matrices for these two different time periods. Within 
the first sample there were 10,253 and within the second, 9,892. In 
the Rogoff study a very fine analysis of the occupations is made. 
However, for illustrative purposes, we have made a coarse analysis. 
This classification may be considered as non-manual, manual, and 
farming. We treat first the 1910 case. The transition matrix is 
NON-MANUAL MANUAL FARM 
NON-MANUAL / .594 .396 .009 
MANUAL I .211 .782 .007 
FARM ^ .252 .641 
GO 
O r—1 
The fixed vector is amo=(.343 .648 .009). The actual fractions 
in each of the classes are given by 
«i9io = (-310 .658 .034) 
Note that the equilibrium vector predicts significantly fewer farmers 
than there actually are. This would suggest that the 1940 data should 
show a decrease in the fraction in farming. 
f N. Rogoff, Recent Trends in Occupational Mobility, Glencoe, Ill., The Free Press 
1953.

--- Page 211 ---

:c- 6 APPLICATIONS OF MARKOV CHAINS 
For the 1940 case we find 
199 
NON-MANUAL MANUAL FARM 
NON-MANUAL ( .622 .375 .003 
P = MANUAL I .274 .721 .005 
FARM ' V .265 .694 .042 
The fixed vector is a194o=(.420 .576 .004). The actual 
in each class in 1940 is given by 
fraction in 
«i94o = (.373 .616 .011). 
As predicted, the fraction of farmers has significantly decreased. It is 
interesting to observe that the equilibrium vectors for 1910 and 1940 
predict larger fractions in the upper class than there actually are. In 
the case of England the equilibrium vector predicted smaller numbers 
in the upper classes. 
The final example that we discuss illustrates equal exchange. The 
data was obtained from a study made by Blumen, Kogan, and McCarthy 
on labor mobility.-)- This was a very large study based on social security 
records. A 1-percent sample of all workers who are or have been in 
covered employment since the inception of the social security system 
m 1937 has been kept. The study was based on a 10-percent sample 
from this record. It presents the following transition matrix for the 
group of males in the age bracket 20 to 24. We omit a discussion of 
the classification used in the study. 
1 2 3 4 5 
-0.832 0.033 0.013 0.028 0.095 
0.046 0.788 0.016 0.038 0.112 
0.038 0.034 0.785 0.036 0.107 
0.054 0.045 0.017 0.728 0.156 
0.082 0.065 0.023 0.071 0.759, 
The fixed vector is 
a = (.270 .184 .076 .148 .322). 
The actual fractions in the classes considered were 
a = (.282 .170 .068 .137 .343). 
f L Blumen, M. Kogan, P. J. McCarthy, The Industrial Mobility of Labor as a Prob¬ 
ability Process, Cornell Studies in Industrial and Labor Relations, Vol. VII, 1955.

--- Page 212 ---

200 FINITE MARKOV CHAINS Chap. VII 
The most interesting feature of this example is the exchange matrix. 
This is 
D-iP 
.225 .009 .004 .008 .026 
.008 .145 .003 .007 .021 
.003 .002 .060 .003 .008 
.008 .007 .002 .108 .023 
.026 .021 .007 .023 .244 
The almost perfect symmetry of this matrix indicates that this system 
may be considered to be in equal exchange in equilibrium, or that the 
process is reversible. 
§ 7.7 The open Leontief model. In the Leontief input-output 
model, we consider an economy in which there are r industries and we 
make the simplifying assumption that each industry produces exactly 
one kind of goods. We regard the natural factors of production such 
as land, timber, minerals, etc. as free, and do not consider them as 
entering into the cost of finished goods. In general, the industries are 
interconnected in the sense that each must buy a certain amount 
(positive or zero) of the other’s products in order to run its industry. 
We shall define technological coefficients as follows: qa is the amount of 
the output of industry j that must be purchased by industry i in order 
that industry i may produce $1 worth of its own goods. Let Q be 
the rxr matrix with entries qt). By their definition, the technological 
coefficients are non-negative. 
It is easy to see that the sum of the qa, for i fixed, gives the total value 
of the inputs needed by the i-th industry in order to produce $1 worth 
of its goods. If the i-th industry is to be profitable, or at least to break 
even, this sum must be less than or equal to the value of its output, i.e. 
<7<i + (Z«2+ • • • + qir ^ 1 • For obvious reasons we shall call the i-th 
industry profitable if the strict inequality holds and profitless if the 
equality holds. We make the assumption that every industry is either 
profitable or profitless and thus rule out the possibility of unprofitable 
industries. 
We can restate the above conditions as 
Q > 0 (1) 
QZ ^ f (2) 
Having discussed the inputs of the industries we next discuss their 
outputs. Let Xi denote the monetary value of the output of the i-th 
industry and let it=(xi, x%, . . . , xr) be the row vector of outputs. 
Since the i-th industry needs an amount Xiqi} of the output of the j-th

--- Page 213 ---

Sec. 7 APPLICATIONS OF MARKOV CHAINS 201 
industry, the vector of inputs needed by the industries is simply 4 
Then the j/-th component of 7tQ gives the total value of the output that 
must be produced by the j-th industry in order to meet the inter¬ 
industry demand for its product. 
Let us assume that the economy supplies for consumption an amount 
C* °f the output of the t-th industry. Let y = (Cl, c2, . . . , cr) be the 
consumption vector; we shall require that 
y > 0. (3) 
The requirement that the production vector of the economy be 
adjusted so that the inter-industry needs as well as the consumption 
needs may be fulfilled is now easy to write in vector form ; it is 
7T = nQ + y. (4) 
Rewriting (4) as 
7T{I-Q) = y, (5) 
we see that it is a set of r simultaneous equations in r unknowns. 
To be economically meaningful, we must find non-negative solutions 
to (5). Since the demand vector y may be arbitrary, we see that 
equations (5) are in general non-homogeneous and will have a solution 
if and only if the matrix I-Q has an inverse. Moreover, the solutions 
to (5) will be non-negative for every y if and only if (/ - Q)-i has all non- 
negative components. We must therefore search for necessary and 
sufficient conditions that the inverse of I — Q be non-negative. 
We will solve this problem by imbedding our model in a Markov 
chain. (This solution was worked out by the authors jointly with 
G., L. Thompson.) 
By the Markov chain associated with an input-output model we shall 
mean a Markov chain with the following properties: 
(1) The states are the r processes of the model plus one additional 
absorbing state So, called the banking state. 
(ii) The transition matrix P is defined as follows: 
Poo — 1 
Poj = 0 j > 0 
Pit = (P) i,j > 0 
r 
Pio = 1 - ^ qtj i > 0. 
5=x 
The intuitive interpretation of this is the following: If industry i 
receives a dollar for its use, then it spends it by buying ptj from industry 
j. The remainder of the dollar, if any—that is, the amount pi0—is

--- Page 214 ---

202 FINITE MARKOV CHAINS Chap. VII 
the profit, and we may think of it as being deposited in a bank. The 
fact that the banking state is an absorbing state, means that the bank 
gets money but does not spend it. 
We immediately see that if Q is a matrix satisfying (1) and (2), then 
a non-negative solution to equations (5) exists for every r^O, if and 
only if the associated Markov chain is absorbing, with the banking state 
So as its only absorbing state. If so is the only absorbing state for an 
absorbing chain, then (I — Q)~1 = N exists and is non-negative; hence 
tT — yN is the desired solution. Otherwise (I — Q)_1, which gives the 
mean number of times in various states before reaching so, would have 
to have infinite entries, i.e. cannot exist. 
There is a simple economic interpretation of this result. We see that 
from every state it must be possible to “reach” the banking state. 
Only a profitable industry “reaches” the bank directly. A profitless 
industry must reach the bank through a profitable one. Hence our 
condition states that every industry must be either profitable or must 
depend on a profitable industry. For example, if we assume that every 
industry depends on labor, and that labor is a profitable industry 
(which presumably means that labor is paid more than subsistence 
wages), then our condition will be met, and all demands can be ful¬ 
filled. 
If the above condition is violated, then the economy cannot fulfill all 
possible demands. Let us ask what kinds of demands it can fulfill. 
First of all we consider the case that there is no profitable industry. 
This means that each industry needs all it produces to pay for raw 
materials, and it would seem that it could meet no outside demand. 
That this is indeed the case is easily proved. 
If there is no profitable industry, then each row of Q has row sum 1, 
hence Q£ = £. If we multiply equation (4) by £ on the right, we find 
that 
= TrQ€ + y£ = ug + yg, 
hence y£ = 0. This says that the sum of all the demands is 0. Hence 
no (positive) demand can be fulfilled. 
Let us now consider the general case where our condition is violated. 
The associated Markov chain is not an absorbing chain with the single 
absorbing state. Then there must be an ergodic set other than {s0}, 
i.e. a closed group of industries none of which is profitable, and which 
depend on no industry outside the group. Let us take the set of all 
such industries, that is the union of all ergodic sets other than {s0}. 
The submatrix Q of these industries has the property Qtj = £, as above, 
and hence can fulfill no outside demand. Thus the entire economy 
can fulfill no demands of goods produced by these industries. And

--- Page 215 ---

Sec. 7 APPLICATIONS OF MARKOV CHAINS 203 
any goods whose production required raw materials from these indus- 
nes also cannot be supplied, since these would act as outside demands 
on the closed group of industries. 
However, if we remove this closed group of profitless industries and 
Vf muneS dependinS on them> remaining industries (if any) 
will fulfill our requirement, and hence can satisfy arbitrary demands. 
These results can be summarized: If there are industries which depend 
on no profitable industry, then these cannot fulfill an outside demand, 
and neither can any industry depending on them. The remaining 
industries can fulfill any outside demand. In terms of states this 
means that any ergodic state (other than s0), or any transient state 
from which such a state is reachable, can fulfill no demand. 
o find what industries can support a demand, we use the following 
simple algorithm for the classification of states : 
(a) Make a check opposite each row of Q whose row-sum is less than 
... r’th,at f’ check each row corresponding to a profitable industry. 
(b) Check the columns having the same indices as the rows already 
marked and then check, in these columns, rows which have 
positive entries. 
(c) Iterate (b) until it produces no new rows. Then one of two 
possibilities may occur: 
(1) All rows are checked. 
(2) Not all rows are checked. 
If case (cl) occurs then the associated Markov chain is absorbing 
wit the single absorbing state s0. Hence any non-negative demand 
can e met. If case (c2) occurs, then the rows which are not checked 
correspond to the maximal profitless closed group. We can find all 
states depending on these by marking these rows (removing previous 
check marks) and applying (b) repeatedly. Any state so marked will 
not be able to fulfill an outside demand. 
We thus see that the entire question of what outside demands can 
be met by the economy is settled by a very simple algorithm. The 
computation” requires finding the row-sums, and then simple itera¬ 
tions in which only the positivity of components is checked. This 
algoiithm is practical even for very large matrices. 
The industries which can meet no outside demand form a totally 
useless segment of the economy. From here on we will assume that 
they have been deleted; then (I-Q)-i exists. 
Next we want to raise the question: If an order for one dollar’s 
worth of good is given by a customer to industry i, how much of it 
ends up in the hands of the various industries ?

--- Page 216 ---

204 FINITE MARKOV CHAINS Chap. VII 
First of all we must ask what the total demand is on the various 
industries. We have a y vector whose i-th component is 1, and which 
has 0’s as other components. Hence v — yN is simply the i-th row of 
N. This gives a direct interpretation to the entries of N; nij is the 
amount industry j must produce to fill a dollar order for industry i. 
Since industry j makes pjo profit on a unit production, the answer to 
our question is that if industry i is given a dollar, the profit of industry 
j will be nupjo. 
The sum of all the profits is 2/iijPjo = bto — 1 (since so is the only 
i 
absorbing state). This shows that a dollar spent by the consumer ends 
up as profit in the hands of the profit-making industries. 
A related question is the following: If a dollar order is given to 
industry i, how much activity does this result in ? It will result in 
units of production in industry j. The sum of these is U, the i-th 
component of r. This is normally much greater than 1. For an order 
y, the total production is yN£ = yr. 
Let us consider an example. Suppose that the technological co¬ 
efficients for six industries are given by 
then 
(1/2 0 1/4 0 0 
V4 V4 V4 0 0 
V2 0 1/2 0 0 
0001/4 3/4 
00010 
0 1/4 0 V 4 0 
000 
0 1/4 0 
V4 V4 0 
0 1/2 0 
0 0 i/4 
0 0 1 
V 4 0 1/4 
From the first column we see that si, s2, and s6 are the profitable indus¬ 
tries. The classification of states is given by Figure 7-9. 
Here {s4, s5} is the ergodic set of industries which are profitless and

--- Page 217 ---

Sec. 7 APPLICATIONS OF MARKOV CHAINS 205 
do not depend on profitable industries. Industry s6 is profitable, but it 
depends on the former. Hence s4, s5, s6 are useless, and may be 
Figure 7-9 
deleted. Industry s3 is profitless but not useless. The deleted transi¬ 
tion matrix is 
Hence 
o 0 0 \ So 
xh 0 XU | si 
V4 V4 V4 I S2‘ 
Va 0 ijJ s3 
N = T = 
Thus, for example, a unit order to s2 will stimulate a total of 6 dollars’ 
production, 8/3 from s4, 4/3 from s2, and 2 from s3. On this dollar 
order s4 makes a profit of 8/3-1/4=2/3) S2 makes 4|3. i/4_ i/3> an(j 
s3 makes 2.0 = 0 (s3 is profitless). 
If we place an outside demand of y— (1,3,2) on the economy, then 
yN = (20,4,16) units will have to be produced by the various industries. 
The total production is worth yr = 40 dollars. 
We note that P is lumpable into the partition [{s0}, {s1} s2}, {s3}]. The 
lumped process yields 
' - (i :) \ 0 Va Vb/ 
For 
ySr = (24,16) 
& = 
(4 2\ 
[4 4 
T = 
y = (4,2) yr = 40.

--- Page 218 ---

206 FINITE MARKOV CHAINS Chap. VII 
The “lumped” industries si and S2 are here considered as acting as one 
“industrial group.” This process will yield the total demands on the 
industrial group, but not the breakdown into industrial demands. 
For practical problems the computations may be prohibitive. 
Hence we are often happy to solve a lumped version of the economy. 
The condition for lumpability becomes the following: Any industry 
in one industrial group makes the same total per unit demands on the 
members of (its own or) another industrial group. (Then all industries 
in the group make the same per unit profit.) While these conditions 
are unlikely to be met exactly, they may be put to a good approxima¬ 
tion. This allows us to take industrial groups as basic entities, and 
yields a smaller and more manageable model. 
We thus see that even in a non-probabilistic model a great deal of 
information can be obtained from Markov chain theory.

--- Page 219 ---

APPENDICES 
Page 
I—Summary of Basic Notation 
25 Mj[f], Var*[f], Pr*[p] denote the mean value of the function f, 
variance of f, and probability of the statement p when the chain 
is started in state s«. 
19 R — matrix with entries 
19 p = {rfi row vector with component rj 
19 y = |cf} column vector with component Cf 
20 column vector with all entries 1 
20 17 row vector with all entries 1 
20 E matrix with all entries 1 
20 / identity matrix 
20 O matrix with all entries 0 
21 AT is the transpose of A 
21 Asq results from A by squaring each entry 
21 Adg results from A by setting off-diagonal entries equal to 0 
II—Basic Definitions 
25 A finite Markov chain is a stochastic process which moves through 
a finite number of states, and for which the probability of entering 
a certain state depends only on the last state occupied. 
35 An ergodic set of states is a set in which every state can be reached 
from every other state, and which cannot be left once it is entered. 
35 A transient set of states is a set in which every state can be reached 
from every other state, and which can be left. 
An ergodic state is an element of an ergodic set. 
207 
35

--- Page 220 ---

208 APPENDICES 
Page 
35 A transient state is an element of a transient set. 
35 An absorbing state is a state which once entered is never left. 
37 An absorbing chain is one all of whose ergodic states are absorbing; 
or equivalently—which has at least one absorbing state, and 
such that an absorbing state can be reached from every state. 
37 An ergodic chain is one whose states form a single ergodic set; or— 
equivalently—a chain in which it is possible to go from every 
state to every other state. 
37 A cyclic chain is an ergodic chain in which each state can only be 
entered at certain periodic intervals. 
37 A regular chain is an ergodic chain that is not cyclic. 
HI—Basic Quantities for Absorbing Chains 
STANDARD FORM FOR TRANSITION MATRIX 
44 
46 nj number of times in state sj before absorption 
46 ukj function that is 1 if process is in safter k steps, 0 otherwise 
50 t number of steps taken before absorption 
52 bij probability starting in state Si that the process is absorbed 
in state sj 
61 r* number of times in transient state s* before leaving the state 
61 hij probability starting in si that the process is ever in sj 
61 m total number of transient states entered before absorption 
46 A = {Mj[n3-]} 
49 A2 = {Vari[n5-]} 
49 t = {Mj[t]} . 
49 T2=={Varj[t]} 
49 B = { by} 
62 p = {Mj[m]} 
63 P transition matrix for process of changes of state

--- Page 221 ---

APPENDICES 209 
IV—Basic Formulas for Absorbing Chains Page 
46 N=(I — Q)~1 (Fundamental matrix) 
49 N2 = N(2Nas -I)- Nsq 
49 B = NR 
61 H = [N-I]Ndg~i 
49 r = Ng 
49 T2 = (2N-I)t-tS(1 
61 Mj[r<] = -—-— 
1 — Pii 
61 Var {[rt] = —Pt' 
(! -Pu)2 
62 n = (NNAg-*)£ 
V—Basic Quantities for Ergodic Chains 
70 a = {ai] fixed probability vector for P 
88 Cij = lim - Cov*[y*(»),yy(»)] 
n —> oo ™ 
88 /3 = {6y} vector of limiting variances for the number of times in 
each of the states 
70 A matrix with each row a 
79 D diagonal matrix with j-th entry 1 jctj 
105 P transition matrix for the reverse process 
75 Z = (I — P + A)~l (Fundamental matrix) 
81 C BZ'ii 
i 
78 M = {mij} matrix of mean number of steps required to reach sy 
for the first time, starting in s* 
82 W — {wij} matrix of variances for the number of steps required 
to reach sy, starting in state st 
76 y,<»> number of times in state sy in the first n steps

--- Page 222 ---

210 
Page 
105 
78 
81 
83 
85 
79 
83 
81 
81 
81 
81 
APPENDICES 
VI—Basic Formulas for Ergodic Chains 
M = {mij} = (I — Z + EZdg)D 
M = M-M& g 
W = {wtj} = M{2ZdgD -I) + 2(ZM - E(ZM)ds) 
C — {pij} = {piZij + Ct'jZji — Ctidij — CliCCjj 
1 
mu = — 
at 
2zu 1 
Wii= 5" 
af at 
aM = rjZdgD 
MaT = c£ 
a = {c-\)[(M)-^Y 
P = I+(D-E)(M)~ 1

--- Page 225 ---

■

--- Page 230 ---

Date Due 
JAN 5 63 
FEB 11 >|3; 
SEl 9'63 
a a c7 
MR 1 7® £ 
JLt^4 OC 
1KTT6T 
JY2 5’65 
2 5 W 
FE 15 70 
ilAUi 
@ | printed IN U. s. A.

--- Page 231 ---

MARYGROUE COLLEGE LIBRARY 
Finite Markov chains 
519 K31 
3 1T57 □□□343AS E 
519 
K31 Kemeny, J. G. 
Finite Markov chains, 
by Kemeny and Snell 
519 
K31
